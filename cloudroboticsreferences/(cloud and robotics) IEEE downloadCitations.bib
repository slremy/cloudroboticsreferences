@INPROCEEDINGS{6137767,
author={F. Ren},
booktitle={2011 7th International Conference on Natural Language
Processing and Knowledge Engineering},
title={Robotics cloud and robotics school},
year={2011},
pages={1-8},
abstract={In the near future, robots with advanced intelligence will be
used for the service of society, families, and individuals. In this
paper, we propose the concept of an advanced intelligence machine, which
is a device that uses both natural and artificial intelligence and is
capable of affective recognition and generation of affective speech and
behavior. We describe the concept of a “robotics cloud” for use by
advanced intelligence machines in human-robot interactions and provide a
theoretical framework and technology blueprint of the construction of a
robotics cloud. In addition, we propose a “robotics school” for building
robotics clouds and describe a framework for the school, its
certification program, and a method for authentication of the school.
The influence of robots on the development of society, scientific and
technological progress, and human-robot relationships are also discussed.},
keywords={artificial intelligence;human-robot interaction;advanced
intelligence machine;affective recognition;artificial
intelligence;human-robot interaction;natural intelligence;robotics cloud
concept;robotics school;Heart;Physiology;Robot sensing
systems;Servers;Advanced Intelligence;Human-Robot
Interaction;Machine;Robotics Certification;Robotics Cloud;Robotics School},
doi={10.1109/NLPKE.2011.6137767},
month={Nov},}
@INPROCEEDINGS{7913002,
author={M. R. Mishi and R. Bibi and T. Ahsan},
booktitle={2017 International Conference on Electrical, Computer and
Communication Engineering (ECCE)},
title={Multiple motion control system of robotic car based on IoT to
produce cloud service},
year={2017},
pages={748-751},
abstract={The world of control is an exciting field that has exploded
with new technologies where the Internet of Things (IoT) vision becomes
reality. This paper proposes a multiple motion controlling mechanism of
a robotic car using Raspberry Pi which works as master and Arduino UNO
which works as slave. Each device is uniquely identifiable by the
controlling software which is the core concept of IoT. Client manages
the activities of the car from remote or distant places over the
internet by voice commands and Universal Windows Application and also
able to get data and feedback. The main contribution of this paper is
that it leverages the efficiency of robot's motion controlling system
because robotic car can receive direct commands at a time from multiple
sources which make the maneuvering system more efficient. Both device
and client do not need to be online at the same time. Commands and data
are stored in cloud service which delivers them when the device is ready
to receive. A GPS system is incorporated thus clients can trace the car.
The system has ultrasonic distance sensor for avoiding obstacles coming
in between its path. We present the architecture and design of the
Raspberry Pi and Arduino communication software and illustrate how to
control the car by means of commands and application.},
keywords={Automobiles;Cloud computing;Control systems;Global Positioning
System;Robot sensing systems;Wireless
communication;Arduino;GPS;IoT;Raspberry Pi;Ultrasonic distance
sensor;cloud service;motion control},
doi={10.1109/ECACE.2017.7913002},
month={Feb},}
@INPROCEEDINGS{7889324,
author={R. Mead},
booktitle={2017 IEEE International Conference on Consumer Electronics
(ICCE)},
title={Semio: Developing a cloud-based platform for multimodal
conversational AI in social robotics},
year={2017},
pages={291-292},
abstract={Semio is developing a cloud-based platform to allow humans to
use robots through natural communication-speech and body language. The
platform allows developers to create and deploy speech/gesture-based
applications to be executed by robots, and allows non-expert users to
access and use those robot applications through natural communication.},
keywords={artificial intelligence;cloud computing;gesture
recognition;human-robot interaction;speech-based user
interfaces;Semio;body language;cloud-based platform;gesture-based
applications;human-robot interaction;multimodal conversational
AI;natural communication;nonexpert users;robot applications;social
robotics;speech-based applications;Human-robot interaction;Robot
kinematics;Service robots;Software;Speech;Speech recognition},
doi={10.1109/ICCE.2017.7889324},
month={Jan},}
@INPROCEEDINGS{7880361,
author={G. Magyar and P. Sinčák and J. Magyar and K. Yoshida and A.
Manzi and F. Cavallo},
booktitle={2017 IEEE 15th International Symposium on Applied Machine
Intelligence and Informatics (SAMI)},
title={CoWoOZ #x2014; A cloud-based teleoperation platform for social
robotics},
year={2017},
pages={000049-000054},
abstract={This paper presents our cloud-based teleoperation platform,
called CoWoOZ - ClOud-based Wizard of OZ. It describes similar systems,
such as Robot Management System (RMS), OpenWoZ, CoWoOZ's predecessor
Telescope and the first version of the discussed platform. A special
emphasis is given to the description of new functionalities added to the
system, e.g. scenario management, custom script builder, client
application, etc. The paper also contains a comparison of these systems,
which can help researchers to choose the right one for their work.},
keywords={cloud computing;human-robot
interaction;telerobotics;CoWoOZ;OpenWoZ;RMS;Wizard of Oz;cloud-based
teleoperation platform;robot management system;social robotics;Decision
support systems;Handheld computers;Human-robot
interaction;Informatics;Machine intelligence;Robots;Telescopes;Wizard of
Oz;cloud computing;human-robot interaction;teleoperation},
doi={10.1109/SAMI.2017.7880361},
month={Jan},}
@INPROCEEDINGS{7877182,
author={N. Jangid and B. Sharma},
booktitle={2016 7th International Conference on Intelligent Systems,
Modelling and Simulation (ISMS)},
title={Cloud Computing and Robotics for Disaster Management},
year={2016},
pages={20-24},
abstract={Nowadays, IT community is experiencing great shift in
computing and information storage infrastructures by using powerful,
flexible and reliable alternative of cloud computing. The power of cloud
computing may also be realized for mankind if some dedicated disaster
management clouds will be developed at various countries cooperating
each other on some common standards. The experimentation and deployment
of cloud computing by governments of various countries for mankind may
be the justified use of IT at social level. It is possible to realize a
real-time disaster management cloud where applications in cloud will
respond within a specified time frame. If a Real-Time Cloud (RTC) is
available then for intelligent machines like robots the complex
processing may be done on RTC via request and response model. The
complex processing is more desirable as level of intelligence increases
in robots towards humans even more. Therefore, it may be possible to
manage disaster sites more efficiently with more intelligent cloud
robots without great lose of human lives waiting for various assistance
at disaster site. Real-time garbage collector, real-time specification
for Java, multicore CPU architecture with network-on-chip, parallel
algorithms, distributed algorithms, high performance database systems,
high performance web servers and gigabit networking can be used to
develop real-time applications in cloud.},
keywords={Java;cloud computing;emergency management;humanoid
robots;multiprocessing systems;network-on-chip;rescue robots;IT
usage;Java;RTC;cloud computing;disaster management clouds;distributed
algorithms;gigabit networking;high-performance Web
servers;high-performance database systems;information storage
infrastructures;intelligent cloud robots;intelligent machines;multicore
CPU architecture;network-on-chip;parallel algorithms;real-time disaster
management cloud;real-time garbage collector;Cloud
computing;Hardware;Java;Multicore processing;Operating systems;Real-time
systems;Robots;Cloud Robotics;Network on Chip(NoC);Real-Time Cloud
(RTC);Real-Time Garbage Collector;Real-Time Specification for Java (RTSJ)},
doi={10.1109/ISMS.2016.45},
ISSN={2166-0670},
month={Jan},}
@INPROCEEDINGS{7841487,
author={A. Rahman and J. Jin and A. Cricenti and A. Rahman and D. Yuan},
booktitle={2016 IEEE Global Communications Conference (GLOBECOM)},
title={A Cloud Robotics Framework of Optimal Task Offloading for Smart
City Applications},
year={2016},
pages={1-7},
abstract={Cloud robotics is an emerging paradigm that enables autonomous
robotic agents to communicate and collaborate with cloud computing
infrastructures. It further complements Internet of Things (IoT) to
improve the performance of smart city applications. By offloading heavy
data- intensive computation to the ubiquitous cloud, quality of service
(QoS) guarantee can be ensured. Unlike their mobile counterpart, the
robots have unique characteristics of mobility, skill- learning, data
collection and decision-making capabilities, which makes offloading
decisions significantly complex. This paper proposes a generic cloud
robotics framework to realize smart city vision while taking into
consideration its various complexities. Specifically, task offloading is
formulated as a constrained optimization problem capable of handling
Direct Acyclic Graph (DAG) known as task flow. Given the constraints, a
genetic algorithm (GA) based scheme is further developed to solve the
problem. The performance of the algorithm is verified by evaluating the
results via three benchmarks. To the best of our knowledge, this is one
of the first attempts of task offloading approach for smart city
applications of cloud robotics.},
keywords={cloud computing;control engineering computing;directed
graphs;optimisation;quality of service;robots;DAG;Internet of
Things;IoT;QoS;autonomous robotic agents;cloud computing;cloud robotics
framework;constrained optimization problem;decision-making
capabilities;direct acyclic graph;mobile counterpart;optimal task
offloading;quality of service;smart city applications;ubiquitous
cloud;Cloud computing;Genetic algorithms;Optimization;Robot sensing
systems;Smart cities;Wireless sensor networks},
doi={10.1109/GLOCOM.2016.7841487},
month={Dec},}
@INPROCEEDINGS{7832130,
author={C. Huang and L. Zhang and T. Liu and H. Y. Zhang},
booktitle={2016 IEEE International Conference on Information and
Automation (ICIA)},
title={A control middleware for cloud robotics},
year={2016},
pages={1907-1912},
abstract={Future robot applications will require cloud computing. Using
an architecture of cloud robotics would increase performance over
hardware constraints (e.g., processors, memory, and power). In this
paper, we propose a control middleware for cloud robotics, which not
only can receive the robots' requests and allocate the cloud resources
based on the robots demand, but also can ensure the connection between
the robots and the cloud more stability. And then, we use an autonomous,
humanoid robot to test the first stage of the control middleware.
Effectiveness of this control middleware is demonstrated by experiments.},
keywords={cloud computing;humanoid robots;middleware;mobile
robots;resource allocation;autonomous robot;cloud computing;cloud
resource allocation;cloud resource requests;cloud robotics
architecture;control middleware;hardware constraints;humanoid
robot;robot applications;Cloud computing;Computer
architecture;Robots;Scheduling;Scheduling algorithms;Allocating and
scheduling;Cloud rbotics;Control middleware},
doi={10.1109/ICInfA.2016.7832130},
month={Aug},}
@INPROCEEDINGS{7816824,
author={Y. Li and H. Wang and B. Ding and P. Shi and X. Liu},
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence
Computing, Advanced and Trusted Computing, Scalable Computing and
Communications, Cloud and Big Data Computing, Internet of People, and
Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)},
title={Toward QoS-Aware Cloud Robotic Applications: A Hybrid
Architecture and Its Implementation},
year={2016},
pages={33-40},
abstract={Robots have been widely adopted to perform critical tasks in
various fields in the past several years. Due to the limitations of its
local resources, the capability of a single robot is usually severely
restricted. Cloud robotics has been recently proposed to extend the
capability of robots by leveraging the rich services provided in cloud.
However, robotic applications generally give considerable attention to
quality of service (QoS) because these applications interact directly
with the physical world. Fulfilling strict QoS requirements while
utilizing cloud services, the performance of which is highly
unpredictable, is a great challenge. In this paper, we propose a novel
hybrid architecture for cloud robotics, named RoboCloud, to address this
challenge. Instead of simply integrating robots with public cloud
services, RoboCloud introduces a task-specified mission cloud with
controllable resources, predictable behavior. The mission cloud can
extend the capability of the robot on its specified, predictable tasks
without sacrificing QoS. For general tasks beyond the capability of the
mission cloud, we opt for public clouds, i.e., various cloud services
available on the Internet. We successfully apply this architecture to
realize robotic object recognition, which is a fundamental capability of
autonomous robots who interact with an open environment. We can minimize
the recognition latency of objects that are predicted to appear in a
specific task without sacrificing the capability to recognize
unfamiliar, unexpected objects by seamlessly integrating a specialized
mission cloud, a public object recognition service on the Internet.},
keywords={cloud computing;control engineering computing;object
recognition;quality of service;robot programming;robot vision;task
analysis;Internet;QoS-aware cloud robotic applications;autonomous
robots;critical tasks;hybrid architecture;public object recognition
service;quality of service;specialized mission cloud;Cloud
computing;Computer architecture;Object recognition;Quality of
service;Robot kinematics;cloud robotics;mission cloud;mobile cloud
computing;object recognition;public cloud},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0028},
month={July},}
@ARTICLE{7403967,
author={L. Wang and M. Liu and M. Q. H. Meng},
journal={IEEE Transactions on Cybernetics},
title={A Hierarchical Auction-Based Mechanism for Real-Time Resource
Allocation in Cloud Robotic Systems},
year={2017},
volume={47},
number={2},
pages={473-484},
abstract={Cloud computing enables users to share computing resources
on-demand. The cloud computing framework cannot be directly mapped to
cloud robotic systems with ad hoc networks since cloud robotic systems
have additional constraints such as limited bandwidth and dynamic
structure. However, most multirobotic applications with cooperative
control adopt this decentralized approach to avoid a single point of
failure. Robots need to continuously update intensive data to execute
tasks in a coordinated manner, which implies real-time requirements.
Thus, a resource allocation strategy is required, especially in such
resource-constrained environments. This paper proposes a hierarchical
auction-based mechanism, namely link quality matrix (LQM) auction, which
is suitable for ad hoc networks by introducing a link quality indicator.
The proposed algorithm produces a fast and robust method that is
accurate and scalable. It reduces both global communication and
unnecessary repeated computation. The proposed method is designed for
firm real-time resource retrieval for physical multirobot systems. A
joint surveillance scenario empirically validates the proposed mechanism
by assessing several practical metrics. The results show that the
proposed LQM auction outperforms state-of-the-art algorithms for
resource allocation.},
keywords={cloud computing;mobile robots;multi-robot systems;resource
allocation;LQM auction;ad hoc networks;cloud computing framework;cloud
robotic systems;cooperative control;dynamic structure;hierarchical
auction-based mechanism;joint surveillance scenario;link quality
indicator;link quality matrix;multirobotic applications;physical
multirobot systems;real-time resource allocation;real-time resource
retrieval;resource-constrained environments;robots;Ad hoc networks;Cloud
computing;Real-time systems;Resource management;Robot kinematics;Robot
sensing systems;Cloud robotics;hierarchical auction;real-time resource
allocation},
doi={10.1109/TCYB.2016.2519525},
ISSN={2168-2267},
month={Feb},}
@INPROCEEDINGS{7815054,
author={D. d. S. Pereira and B. A. Santana and R. S. Maia and A. Souza},
booktitle={2016 IEEE 13th International Conference on Mobile Ad Hoc and
Sensor Systems (MASS)},
title={Cloud Robotic Clone-Based in Mobile Ad-Hoc Network of CellBot Team},
year={2016},
pages={375-376},
abstract={Multi-Robot Systems (MRS) is a field of Robotic that develops
robots to act as a team, with coordination, performing task
collaboratively. These systems are used to deal with complex tasks, in
which time and space are restricted. To achieve the correct operation of
the MRS the communication skills is a fundamental point to be
investigated. This paper presents an Architecture of communication based
on cloud computing technology for a MRS composed by CellBots.},
keywords={cloud computing;control engineering computing;mobile ad hoc
networks;mobile robots;multi-robot systems;CellBot team;MRS;cloud
computing technology;cloud robotic clone;communication skills;complex
tasks;mobile ad-hoc network;multirobot systems;Ad hoc networks;Cloud
computing;Computer architecture;Mobile communication;Robot
kinematics;Robot sensing systems;CellBot;Cloud Computing;Cloud
Robotic;MRS},
doi={10.1109/MASS.2016.060},
month={Oct},}
@INPROCEEDINGS{7808098,
author={R. Doriy and S. K. Prasad and P. Sablani},
booktitle={2016 IEEE International Conference on Recent Trends in
Electronics, Information Communication Technology (RTEICT)},
title={Use of cloud computing platforms towards robotics applications},
year={2016},
pages={1577-1583},
abstract={With the recent advancements in the field of robotics and
cloud computing, one technology backing the other can have a huge
potential under the domain of Cloud Robotics. Majority of systems are
still operating independently using on-board computation, and confined
memory and storage. Cloud Computing is a promising technology which is
intended to harness the power of networked computers and communication
system in a more cost effective way. There is a wide gap in Cloud
Computing technology and robotics technology. Generally, the robots
cannot use the Cloud Computing instance directly. Moreover, there are
many Cloud Computing platforms are present around, deciding which one to
use when is still an issue. In this paper we take a look at some
prominent open source cloud solutions and evaluate and compare the
stability, performance and features of these clouds keeping the key
requirements of cloud robotics under consideration.},
keywords={cloud computing;control engineering computing;public domain
software;robots;storage management;cloud computing;cloud
robotics;memory;on-board computation;open source;storage;Cloud
computing;Computer architecture;Conferences;Databases;Market
research;Robots;Cloud Computing;Cloud
Robotics;CloudStack;OpenNebula;OpenStack},
doi={10.1109/RTEICT.2016.7808098},
month={May},}
@INPROCEEDINGS{7793536,
author={M. Pirani and A. Bonci and S. Longhi},
booktitle={IECON 2016 - 42nd Annual Conference of the IEEE Industrial
Electronics Society},
title={A scalable production efficiency tool for the robotic cloud in
the fractal factory},
year={2016},
pages={6847-6852},
abstract={The present paper proposes an effective metrics for production
efficiency and a bottleneck detection algorithm of recursive nature for
its application on lightweight embedded systems on board of the robotics
and automation components of the factory of the future. The proposed
methodology is particularly suited if the fractal paradigm is applied to
the factory seen as a complex system of systems but with relevant
self-similarities across the several layers of components and structures
from the shop-floor up to the enterprise level. A performance test has
been conducted to demonstrate the viability of the technology for tiny
embedded devices with the use of declarative embedded database language.
Due to the high scalability of the algorithm and its simplicity, it
seems suitable also for the robotic cloud paradigm, where constituent
mechatronics, sensors and actuators components are provided as a
service. The results provided suggests that, with the use of similar
recursive and distributed form of computing, production bottlenecks or
fault detection can be scaled to address the complex and pervasive
cyber-physical systems problems that characterize the 4th industrial
revolution strategies.},
keywords={actuators;control engineering computing;cyber-physical
systems;industrial robots;mechatronics;actuators components;automation
components;bottleneck detection algorithm;complex cyber-physical
system;declarative embedded database language;embedded devices;fault
detection;fractal factory;industrial revolution strategies;lightweight
embedded systems;mechatronics;performance test;pervasive cyber-physical
system;production bottlenecks;robotic cloud;robotics;scalable production
efficiency tool;sensors;Fractals;Measurement;Production facilities;Robot
sensing systems;Service robots;Manufacturing automation;embedded
systems;industrial cyberphysical systems;robotics},
doi={10.1109/IECON.2016.7793536},
month={Oct},}
@INPROCEEDINGS{7784040,
author={S. Wen and B. Ding and H. Wang and B. Hu and H. Liu and P. Shi},
booktitle={2016 IEEE International Conference on Real-time Computing and
Robotics (RCAR)},
title={Towards migrating resource-consuming robotic software packages to
cloud},
year={2016},
pages={283-288},
abstract={Cloud robotics augments robots by various cloud computing
resources. Although it is regarded as a revolutionary paradigm in
robotics, building a cloud robotic application is still a complex task
today. Great efforts have to be made to develop the robotic-oriented
cloud services, deploy them onto the cloud and integrating robotic
software with them together. In this paper, we propose a cloud platform,
micROS-cloud, which supports the direct deployment of Robot Operating
System (ROS) software packages. The packages can be transformed into
Internet-accessible services automatically. With the mechanisms named
on-demand instantiation and container-based isolation, multiple robots
can access such a cloud service simultaneously, even if the
corresponding ROS package is originally designed just to serve a single
robot. With micROS-cloud, general ROS applications can be easily
migrated into the cloud robotic environment, and a lot of existing ROS
packages in the robotic community can be transformed into o cloud
services quickly. A prototype of this platform and the experiments on it
involving multiple robots have validated the effectiveness of our work.},
keywords={cloud computing;multi-robot systems;operating systems
(computers);resource allocation;robot programming;software
packages;Internet-accessible services;ROS packages;ROS software
packages;cloud computing resources;cloud platform;cloud robotic
environment;cloud robotics;container-based
isolation;micROS-cloud;on-demand instantiation;resource-consuming
robotic software packages;robot operating system software
packages;robotic community;robotic-oriented cloud services;Cloud
computing;Computational modeling;Computers;Containers;Simultaneous
localization and mapping},
doi={10.1109/RCAR.2016.7784040},
month={June},}
@INPROCEEDINGS{7776604,
author={A. Manzi and L. Fiorini and R. Limosani and P. Sincak and P.
Dario and F. Cavallo},
booktitle={2016 5th IEEE International Conference on Cloud Networking
(Cloudnet)},
title={Use Case Evaluation of a Cloud Robotics Teleoperation System
(Short Paper)},
year={2016},
pages={208-211},
abstract={The paper describes a generic Cloud Robotics teleoperation
system which allows to control in real-time a robot (connected with a 4G
network) having its video stream as feedback. The proposed system relies
on the Azure Cloud Platform and on recent web technologies.
Particularly, we present an use case experiment in which an operator in
Slovakia controls a robot situated in Italy in order to evaluate its
realtime feasibility. We test the system to assess its performances
providing the throughput value of the communication and the average
delay between consecutive received packets on both robot and
teleoperation side. Additionally, regarding the video streaming, we test
several packet sizes to establish a suitable image quality. The results
show how the chosen technology allows to have real-time performances in
terms of video and velocity commands streaming.},
keywords={cloud computing;control engineering
computing;telerobotics;video streaming;Azure cloud
platform;Italy;Slovakia;Web technologies;communication throughput
value;consecutive received packets;generic cloud robotics teleoperation
system;image quality;use case evaluation;velocity commands
streaming;video streaming;Cloud
computing;Delays;Protocols;Robots;Servers;Streaming media;Throughput},
doi={10.1109/CloudNet.2016.49},
month={Oct},}
@INPROCEEDINGS{7774942,
author={Y. Lei and Z. Fengyu and W. Yugang and Y. Xianfeng and Z. Yang
and C. Zhumin},
booktitle={2016 Sixth International Conference on Instrumentation
Measurement, Computer, Communication and Control (IMCCC)},
title={Design of a Cloud Robotics Visual Platform},
year={2016},
pages={1039-1043},
abstract={In recent years, the vision of robot has got great
development. Because of the low sampling rate and the heavy calculation
burden, the vision is obstructed to apply for robot. In order to solve
the above problem, in this paper, we present the Cloud Robotics Visual
Platform(CRVP) for the development of robot vision. The main
contributions of the paper are as follows. (1) The parallel
computational model of Map/Reduce is used to decrease the image time
cycle of image training and recognition. (2) The image recognition
engine is built with the Service-Oriented Architecture. (3) The H264
encoding algorithm and the Real time protocol are adopted to realize the
video transmitting. Finally, the experiment is done using ECS of Aliyun.
The results show that the platform could effectively improve the robot
image recognition cycle and offload the computing load.},
keywords={cloud computing;control engineering computing;image
recognition;robot vision;sampling methods;video
coding;Aliyun;CRVP;ECS;H264 encoding algorithm;Map-Reduce;calculation
burden;cloud robotics visual platform;image recognition engine;image
time cycle;image training;robot image recognition cycle;sampling
rate;service-oriented architecture;video transmitting;Cloud
computing;Image recognition;Protocols;Robot kinematics;Servers;Streaming
media;cloud robot; map/reduce; real time protocol; visual platform},
doi={10.1109/IMCCC.2016.252},
month={July},}
@INPROCEEDINGS{7731693,
author={A. Aggoun and A. Rhiat and F. Fages},
booktitle={2016 3rd International Conference on Logistics Operations
Management (GOL)},
title={Panorama of real-life applications in logistics embedding bin
packing optimization algorithms, robotics and cloud computing
technologies},
year={2016},
pages={1-4},
abstract={In this paper, we are interested in one hand to review a set
of problems encountered in logistics, and in a second hand to highlight
the contribution of Constraint Programming, Metaheuristics and numerical
solvers using non-linear inequalities in solving them. One of the
objectives is to address a panorama of bin packing applications in
logistics and their embedding in cloud computing.},
keywords={bin packing;cloud computing;constraint handling;industrial
control;industrial robots;logistics;optimisation;production engineering
computing;bin packing optimization algorithms;cloud computing;constraint
programming;logistics;metaheuristics;nonlinear inequalities;numerical
solvers;robotics;Cloud
computing;Containers;Optimization;Programming;Robots;Shape;Vehicles;Bin
Packing;Cloud Computing;Constraint
Programming;IOT;Metaheuristics;Operational
Logistics;Optimization;RFID;Robotics},
doi={10.1109/GOL.2016.7731693},
month={May},}
@INPROCEEDINGS{7583017,
author={S. A. Miratabzadeh and N. Gallardo and N. Gamez and K. Haradi
and A. R. Puthussery and P. Rad and M. Jamshidi},
booktitle={2016 World Automation Congress (WAC)},
title={Cloud robotics: A software architecture: For heterogeneous
large-scale autonomous robots},
year={2016},
pages={1-6},
abstract={The paper proposes a software architecture for cloud robotics
which intends three subsystems in the cloud environment: Middleware
Subsystem, Background Tasks Subsystem, and Control Subsystem. The
architecture invokes cloud technologies such as cloud computing, cloud
storage, and other networking platforms arranged on the assistances of
congregated infrastructure and shared services for robotics, for
instance Robot Operating System (ROS). Since the architecture is looking
for reliable, scalable, and distributed system for the heterogeneous
large-scale autonomous robots, Infrastructure as a Service (IaaS) is
chosen among the cloud services. Three major tasks can be handled by the
proposed software architecture Computing, Storage, and Networking.
Hadoop-MapReduce provides the appropriate framework in the cloud
environment to process and handle these tasks.},
keywords={cloud computing;control engineering
computing;middleware;mobile robots;software
architecture;Hadoop-MapReduce;IaaS;background tasks subsystem;cloud
computing;cloud robotics;cloud service;cloud storage;cloud
technology;control subsystem;heterogeneous large-scale autonomous
robots;infrastructure as a service;middleware subsystem;robot operating
system;software architecture;Cloud computing;Computer architecture;Robot
sensing systems;Service robots;Hadoop-MapReduce;Robot Operating System
(ROS);cloud robotics;heterogeneous system;large-scale autonomous},
doi={10.1109/WAC.2016.7583017},
month={July},}
@INPROCEEDINGS{7560952,
author={B. V. S. Krishna and J. Oviya and S. Gowri and M. Varshini},
booktitle={2016 Second International Conference on Science Technology
Engineering and Management (ICONSTEM)},
title={Cloud robotics in industry using Raspberry Pi},
year={2016},
pages={543-547},
abstract={Cloud robotics is a field of robotics that attempts to invoke
cloud technologies such as cloud computing, cloud storage, and other
Internet technologies centered on the benefits of converged
infrastructure and shared services for robotics. Our project is a cloud
robot which is used in industrial and manufacturing environment. It
works on a ROS platform. Here we use Raspberry Pi controller to control
the various devices attached to it. For testing this implementation,
Android phone, camera, DC motor, sensors and a Raspberry pi controller
have been used. The movement of the robot is provided by DC motors and
the direction is controlled from an android environment using Robot
Operating System (ROS). The controller and the receiver end is connected
by Wi-Fi. The data input from the gas, temperature and Infra Red sensors
is given to the Raspberry Pi controller. A camera is used to provide
visual input of the surrounding environment to the robot. The data
obtained by the sensors and camera are processed by the controller and
stored in cloud.},
keywords={DC motors;cameras;cloud computing;control engineering
computing;industrial robots;infrared detectors;microcomputers;operating
systems (computers);production engineering computing;smart
phones;temperature sensors;wireless LAN;Android environment;Android
phone;DC motor;ROS platform;Raspberry Pi controller;Robot operating
system;Wi-Fi;camera;cloud robotics;cloud technologies;converged
infrastructure;gas;industrial environment;infrared sensors;manufacturing
environment;shared services;temperature sensors;Cameras;Cloud
computing;Robot sensing systems;Service robots;Temperature sensors;Cloud
robotics;ROS;Raspberry Pi},
doi={10.1109/ICONSTEM.2016.7560952},
month={March},}
@INPROCEEDINGS{7558510,
author={P. Sincak and P. Smolar and J. Ondo},
booktitle={2015 13th International Conference on Emerging eLearning
Technologies and Applications (ICETA)},
title={The major challenges in intelligent robotics (Towards cloud
robotics and its impact to crowdsourced image processing for intelligent
agents serving for robots)},
year={2015},
pages={1-6},
abstract={The goal of this paper is to point out the trends in
Intelligent Robotics, which is towards Cloud Based Robotics and
particularly the problems and proposing solutions in the domain of image
processing which role is to provide information for decision processes
for robots to act and to interact with people. This is a very
non-trivial problem which is based on building intelligent Agents on the
Cloud and Robot is in fact peripheral unit for an Intelligent Robot on
the Cloud environment. The paper contains also a selected image
processing approach based in clustering image processing and some
modification shows some more generalization ability of studied MF-ARTMAP
neural networks.},
keywords={ART neural nets;cloud computing;human-robot interaction;image
processing;intelligent robots;neurocontrollers;pattern clustering;robot
vision;MF-ARTMAP neural networks;cloud environment;cloud
robotics;clustering;crowdsourced image processing;human machine
interaction;intelligent agents;intelligent robotics;peripheral
unit;robot decision processes;Artificial intelligence;Cloud
computing;Computers;Neural networks;Robot sensing systems;Spirals},
doi={10.1109/ICETA.2015.7558510},
month={Nov},}
@INPROCEEDINGS{7558620,
author={F. Nagata and K. Takeshita and K. Watanabe and M. K. Habib},
booktitle={2016 IEEE International Conference on Mechatronics and
Automation},
title={Generation of triangulated patches smoothed from original point
cloud data with noise and its application to robotic machining},
year={2016},
pages={535-540},
abstract={The authors presented a machining system based on an
industrial robot for foamed polystyrene materials. The developed robotic
CAM system provided a simple interface without using any robot languages
between operators and the machining robot. In this paper, a robotic
preprocessor is reviewed for the machining robot to convert STL data
into cutter location source data called CLS data. The STL originally
means Stereolithography which is a file format proposed by 3D Systems
and recently is supported by many CAD/CAM softwares. It is also known as
Standard Triangulated Language in Japan. The STL is widely spread to
rapid prototyping using a 3D printer which is a typical additive
manufacturing system. The STL deals with a triangular representation of
a 3D surface geometry. Then, triangulated patches generated from
smoothed point cloud data (PCD) is described. The process allows to
reconstruct 3D digital data of a real object written with STL format
from original PCD with noise for reverse engineering. The effectiveness
and usefulness of the proposed system are demonstrated through actual
machining experiments.},
keywords={CAD/CAM;computational geometry;control engineering
computing;industrial robots;machining;reverse engineering;3D surface
geometry;CAD-CAM software;CLS data;STL data;additive manufacturing
system;cutter location source data;foamed polystyrene
material;industrial robot;reverse engineering;robotic CAM system;robotic
machining;robotic preprocessor;smoothed point cloud data;standard
triangulated language;stereolithography;triangulated
patches;Machining;Robot kinematics;Service robots;Smoothing
methods;Solid modeling;Three-dimensional displays},
doi={10.1109/ICMA.2016.7558620},
month={Aug},}
@INPROCEEDINGS{7557399,
author={S. Noreen and E. Zahoor},
booktitle={2016 IEEE World Congress on Services (SERVICES)},
title={CRC2: A Mediator Based Approach for Cloud Robotics},
year={2016},
pages={87-94},
abstract={Cloud robotics is a field of robotics which enables robots to
benefit from the powerful computational, storage, and communication
resources provided by the Cloud. This allows for smart and low-cost
robots having brain in the Cloud. However, network latency and too much
reliance on the Cloud could lead to a brainless robot. The need for a
mediator is thus evident to coordinate the communication. In this paper,
we propose a multi-process mediator framework for Cloud robotics, named
CRC2. The proposed framework makes use of the smartphone resources and
offers a service-based approach to establish and manage a collaborative
communication channel between robot and the Cloud. The proposed
framework is lightweight and is focused to minimize communication
latency. In addition, it automates robotic tasks execution and provides
smartphone resources integration.},
keywords={cloud computing;control engineering computing;resource
allocation;robots;smart phones;CRC2;brainless robot;cloud reliance;cloud
robotics;collaborative communication channel;communication latency
minimization;communication resources;computational resources;low-cost
robots;mediator based approach;multiprocess mediator framework;network
latency;robotic task execution;service-based approach;smart
robots;smartphone resource integration;storage resources;Cloud
computing;Clouds;Computer architecture;Robot kinematics;Robot sensing
systems;Servers;Cloud Robotics;Mediator},
doi={10.1109/SERVICES.2016.18},
month={June},}
@INPROCEEDINGS{7556193,
author={J. Y. Huang and W. P. Lee},
booktitle={2016 Asia-Pacific Conference on Intelligent Robot Systems
(ACIRS)},
title={Enabling vision-based services with a cloud robotic system},
year={2016},
pages={84-88},
abstract={Today cloud computing technologies have been rapidly advancing
and researchers often provide various types of resources on the internet
to share with each other. In this work, we present a cloud-based robotic
service framework for robots to work on such a distributed platform. Our
work includes two important services, face recognition and behavior
recognition, to supports vision-based robot tasks. Experiments are
conducted to validate the proposed methodology and to evaluate its
corresponding performance. The results show that successful recognition
can be achieved in the static and dynamic experimental environments.},
keywords={cloud computing;control engineering computing;face
recognition;robot vision;Internet;behavior recognition;cloud computing
technologies;cloud robotic system;face recognition;vision-based robot
tasks;vision-based services;Cloud computing;Face;Face detection;Face
recognition;Feature extraction;Streaming media;ROS;cloud computing;cloud
robot;human recognition;machine learning;service robot},
doi={10.1109/ACIRS.2016.7556193},
month={July},}
@INPROCEEDINGS{7521103,
author={F. Rosique and P. Sánchez and D. Alonso and J. A. López},
booktitle={2015 10th International Joint Conference on Software
Technologies (ICSOFT)},
title={Towards a unified platform for agent-based cloud robotics},
year={2015},
volume={1},
pages={1-6},
abstract={This paper describes a platform that aims to design, build,
and validate a new generation of cloud robotic platforms that enable
agent-based intelligent control of robots deployed in unknown and
dynamic environments. The platform will consider: (1) novel techniques
for programming reactive plans and robotic behaviours through missions
and novel mechanisms for building new behaviours from existing ones,
both for experienced and non-expert users; (2) novel multi-layered cloud
platform as the infrastructure to maintain a continuous link between the
robots acting on a physical environment and their agent counterparts, to
provide sensor data from robots to agents, and to provide high-level
autonomous decisions from agents to robots.},
keywords={Cloud computing;Computer architecture;Proposals;Robot sensing
systems;Solid modeling;Three-dimensional
displays;Agents;Cloud;Drone;Robotics;Teleo-reactive},
month={July},}
@INPROCEEDINGS{7508099,
author={I. S. Bereznyak and E. V. Chepin and A. A. Dyumin},
booktitle={2016 6th International Conference - Cloud System and Big Data
Engineering (Confluence)},
title={The actions language as a programming framework for cloud
robotics applications},
year={2016},
pages={119-124},
abstract={In past few years, the cloud computing paradigm is gaining
influence on robotics field very rapidly. As the mobile robotics can
obtain huge benefits from cloud computing paradigm, many robotic
laboratories around the world conduct researches in this area. As a
result, new frameworks and tools are emerging. In our early works, we
had developed the prototypes for some cloud robotics services. As well,
we have our own scripting language for programming (and modeling)
robotics behavior - Actions Language. The concepts used in this language
are close enough to the cloud computing paradigm. In this paper, we give
brief descriptions and state of researches for cloud robotics and
special robotic languages. After that, we describe and investigate our
approach for programming robotics behavior using cloud services.},
keywords={authoring languages;cloud computing;mobile robots;robot
programming;Actions Language;cloud computing paradigm;cloud robotics
applications;cloud services;mobile robotics;programming
framework;robotic languages;robotics behavior modeling;robotics behavior
programming;scripting language;Cloud computing;Mobile
communication;Programming profession;Robot sensing systems;cloud
computing;control software;distributed systems;domain specific
languages;robotics},
doi={10.1109/CONFLUENCE.2016.7508099},
month={Jan},}
@ARTICLE{7482658,
author={J. Wan and S. Tang and H. Yan and D. Li and S. Wang and A. V.
Vasilakos},
journal={IEEE Access},
title={Cloud robotics: Current status and open issues},
year={2016},
volume={4},
pages={2797-2807},
abstract={With the development of cloud computing, big data, and other
emerging technologies, the integration of cloud technology and
multi-robot systems makes it possible to design multi-robot systems with
improved energy efficiency, high real-time performance, and low cost. In
order to address the potential of clouds in enhancing robotics for
industrial systems, this paper describes the basic concepts and
development process of cloud robotics and the overall architecture of
these systems. Then, the major driving forces behind the development of
cloud robotics are carefully analyzed from the point of view of cloud
computing, big data, open source resources, robot cooperative learning,
and network connectivity. Subsequently, the key issues and challenges in
the current cloud robotic systems are proposed, and some possible
solutions are also given. Finally, the potential value of cloud robotic
systems in different practical applications is discussed.},
keywords={Big data;Cloud computing;Energy efficiency;Internet of
things;Multi-robot systems;Open source code;Real-time systems;Service
robots;Cloud computing;big data;cloud computing;cloud robotics;internet
of things;open source},
doi={10.1109/ACCESS.2016.2574979},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7497838,
author={F. Vázquez-Abad and S. Bernabel},
booktitle={2016 13th International Workshop on Discrete Event Systems
(WODES)},
title={Stochastic path optimization for robotic bees using cloud
computing},
year={2016},
pages={135-140},
abstract={We study the problem of dynamic routing of robotic bees
towards the hive, with the intended purpose of minimizing the time it
takes for all the bees to arrive at the destination. Due to uncertainty
in position measurements, the stochastic problem cannot ensure
collision-free paths. We study the effects that the algorithm parameters
have in reducing the computational complexity and expected number of
collisions. The dynamic path allocation assumes signals are received
every ε units of time. We provide a weak convergence proof that the
stochastic dynamic allocation converges to the optimal deterministic
path when ε → 0. Next we explore via experimentation how the various
algorithm parameters affect the overall performance. A k-nearest
neighbors strategy is implemented to lessen the need for small step size
ε and safety parameter. Δ. In this manner we achieve a faster completion
time, reduce collisions and computational complexity.},
keywords={cloud computing;collision avoidance;computational
complexity;control engineering computing;mobile robots;stochastic
programming;cloud computing;collision-free paths;computational
complexity;dynamic path allocation;dynamic routing;k-nearest neighbors
strategy;optimal deterministic path;position measurements;robotic
bees;stochastic dynamic allocation;stochastic path
optimization;stochastic problem;time minimization;weak convergence
proof;Clustering algorithms;Collision avoidance;Optimization;Resource
management;Robots;Safety;Stochastic processes},
doi={10.1109/WODES.2016.7497838},
month={May},}
@INPROCEEDINGS{7486690,
author={Y. Ma and Y. Hao and Y. Qian and M. Chen},
booktitle={2016 2nd International Conference on Control, Automation and
Robotics (ICCAR)},
title={Cloud-assisted humanoid robotics for affective interaction},
year={2016},
pages={15-19},
abstract={In recent years, the humanoid robot is received great
attention, and gradually develop to households and personal service
field. The prominent progress of cloud computing, big data, and machine
learning fields provides a strong support for the research of the robot.
With affective interaction ability of robot has a broad market space and
research value. In this paper, we propose a cloud-assisted humanoid
robotics for affective interaction system architecture, and introduce
the essential composition, design and implementation of related
components. Finally, through an actual robot emotional interaction test
platform, validating the feasibility and extendibility of proposed
architecture.},
keywords={Big Data;cloud computing;humanoid robots;learning (artificial
intelligence);affective interaction system architecture;big data;cloud
computing;cloud-assisted humanoid robotics;machine learning;robot
emotional interaction test platform;Cloud computing;Hardware;Humanoid
robots;Robot sensing systems;Service robots;affective interaction;cloud
computing;humanoid robotics},
doi={10.1109/ICCAR.2016.7486690},
month={April},}
@INPROCEEDINGS{7451838,
author={K. Sugiura and K. Zettsu},
booktitle={2016 11th ACM/IEEE International Conference on Human-Robot
Interaction (HRI)},
title={Analysis of long-term and large-scale experiments on robot
dialogues using a cloud robotics platform},
year={2016},
pages={525-526},
abstract={To build conversational robots, roboticists are required to
have deep knowledge of both robotics and spoken dialogue systems. Unlike
using stand-alone speech recognition/ synthesis toolkits, a cloud
robotics platform for human-robot communication enables high-quality
speech recognition and synthesis that is optimized to human-robot
interactions. This is challenging because we need to build a wide
variety of functionalities ranging from a stable cloud platform to
high-quality multilingual speech recognition and synthesis engines. From
this background, we constructed rospeex [1], which is a cloud robotics
platform for multilingual spoken dialogues with robots. Over 20,000
unique users have used rospeex in the two years since it was launched.
In this paper, we propose a method to reduce the response time in
rospeex; and analyze its effectiveness. We also analyze the server logs
of rospeex that we have collected.},
keywords={cloud computing;human-robot interaction;interactive
systems;natural language processing;speech recognition;speech
synthesis;speech-based user interfaces;cloud robotics
platform;conversational robots;high-quality multilingual speech
recognition engine;high-quality multilingual speech synthesis
engine;human-robot communication;human-robot interactions;large-scale
experiment analysis;long-term experiment analysis;robot dialogues;spoken
dialogue systems;Engines;Robots;Servers;Speech;Speech recognition;Speech
synthesis;Time factors},
doi={10.1109/HRI.2016.7451838},
month={March},}
@ARTICLE{7110581,
author={M. Liu},
journal={IEEE Transactions on Cybernetics},
title={Robotic Online Path Planning on Point Cloud},
year={2016},
volume={46},
number={5},
pages={1217-1228},
abstract={This paper deals with the path-planning problem for mobile
wheeled- or tracked-robot which drive in 2.5-D environments, where the
traversable surface is usually considered as a 2-D-manifold embedded in
a 3-D ambient space. Specially, we aim at solving the 2.5-D navigation
problem using raw point cloud as input. The proposed method is
independent of traditional surface parametrization or reconstruction
methods, such as a meshing process, which generally has
high-computational complexity. Instead, we utilize the output of 3-D
tensor voting framework on the raw point clouds. The computation of
tensor voting is accelerated by optimized implementation on graphics
computation unit. Based on the tensor voting results, a novel local
Riemannian metric is defined using the saliency components, which helps
the modeling of the latent traversable surface. Using the proposed
metric, we prove that the geodesic in the 3-D tensor space leads to
rational path-planning results by experiments. Compared to traditional
methods, the results reveal the advantages of the proposed method in
terms of smoothing the robot maneuver while considering the minimum
travel distance.},
keywords={computational complexity;differential geometry;indoor
navigation;mobile robots;path planning;tensors;2.5-D navigation
problem;3-D ambient space;3-D tensor voting framework;Riemannian
metric;graphics computation unit;high-computational complexity;minimum
travel distance;mobile tracked-robot;mobile wheeled-robot;point
cloud;robotic online path planning;Graphics processing
units;Measurement;Navigation;Path planning;Robots;Tensile
stress;Three-dimensional displays;Mobile robots;navigation;path planning},
doi={10.1109/TCYB.2015.2430526},
ISSN={2168-2267},
month={May},}
@INPROCEEDINGS{7419019,
author={G. Wang and J. Cheng and R. Li and K. Chen},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={A new point cloud slicing based path planning algorithm for
robotic spray painting},
year={2015},
pages={1717-1722},
abstract={In order to make traditional point cloud slicing algorithm
suitable for robotic spray path planning of complex surface, four
improvements are put forward in this paper. Firstly, adaptively choose
the dominant eigenvector direction of oriented bounding box of spray
target as the normal vector direction of the slicing plane. Secondly,
propose intersection-projection joint segmentation method to obtain
points for constructing spraying path. Thirdly, use polynomial fitting
and uniform interpolation method to construct smooth spray path.
Finally, propose an iteration formula of interval between adjacent
slicing planes to obtain equal distance spray path for variable
curvature surfaces. Numerical examples show that the proposed algorithm
can generate robotic spraying path directly from point cloud model, and
suitable for complex, irregular shape spray target.},
keywords={computer graphics;control engineering computing;curve
fitting;eigenvalues and eigenfunctions;industrial
robots;interpolation;painting;path planning;polynomials;production
engineering computing;spraying;sprays;complex surface;eigenvector
direction;intersection-projection joint segmentation method;iteration
formula;normal vector direction;oriented bounding box;path planning
algorithm;point cloud slicing;polynomial fitting;robotic spray
painting;robotic spray path planning;slicing planes;uniform
interpolation;variable curvature surfaces;Fitting;Path
planning;Robots;Shape;Spraying;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7419019},
month={Dec},}
@INPROCEEDINGS{7404662,
author={C. Pillajo and R. Hincapié and E. Pilatasig},
booktitle={2015 CHILEAN Conference on Electrical, Electronics
Engineering, Information and Communication Technologies (CHILECON)},
title={Implementation of a network control system for a Robotic
Manipulator as cloud service},
year={2015},
pages={791-795},
abstract={Today there are many designs and applications involving
communication, control and computing. These applications generate
network control systems, which are systems composed of a lot of
integrated entities, sensor nodes coupled through a common communication
network for manipulation and physical control processes, as well as
robotic devices, network control systems are generating applications
that are becoming ubiquitous and useful in several areas such as health
care, disaster management, flexible manufacturing, education, etc., but
network control systems implementation still faces several challenges
such as cost and efficiency of resources. This work focuses on the
implementation of a cloud service for remote control of a robotic
manipulator, that develops an infrastructure (IaaS) for the robotic arm,
a service is programmed in C # (PaaS) that gives access to remote users
(SaaS) to create a MRaaS unit, finally an optimal control model
restriction and control event-triggered communication are studied to be
applied to a Robotic Manipulator as a Service.},
keywords={C# language;cloud computing;control engineering
computing;manipulators;networked control systems;optimal
control;telerobotics;C # language;IaaS;MRaaS unit;PaaS;SaaS;cloud
service;control event-triggered communication;network control
system;optimal control model restriction;robotic arm;robotic manipulator
as a service;robotic manipulator remote control;Cloud
computing;Manipulators;Robot kinematics;Robot sensing
systems;Workstations;Cyberphisical Systems;IaaS;MRAAS Systems;Network
Control Systems;PaaS;SaaS},
doi={10.1109/Chilecon.2015.7404662},
month={Oct},}
@INPROCEEDINGS{7402157,
author={L. M. F. Christino and F. d. S. Osório},
booktitle={2015 12th Latin American Robotics Symposium and 2015 3rd
Brazilian Symposium on Robotics (LARS-SBR)},
title={GPU-Services: Real-Time Processing of 3D Point Clouds for Robotic
Systems Using GPUs},
year={2015},
pages={151-156},
abstract={The GPU-Services project fits into the context of research and
development of methods for data processing of three-dimensional sensors
data applied to mobile robotics. Such methods are called services on
this project, which include 3D point clouds pre-processing algorithms,
segmentation of the data, separation and identification of planar zones
(ground, roads), and detection of elements of interest (edges,
obstacles). Due to the large amount of data to be processed in a short
time, these services will use parallel processing elements, using the
GPU to perform partial or complete processing of these data. The project
aims to provide services for an autonomous car, forcing the services to
approach a system for real-time processing, which should complete the
whole data processing before the next frame came from the sensors (~10
to 20Hz). The sensor data is structured in the form of a cloud of
points, allowing for great parallel processing. However, its major
difficulty is the high rate of data received from the sensor (around
700,000 points/sec), and this gives the motivation of this project: to
use the full potential of sensor and to efficiently use the parallelism
of GPU programming. The GPU services are divided into steps, but always
seeking the processing speed given by their intrinsic parallelism: The
first step is to organize an environment for parallel processing
development in conjunction with the system already being used in our
autonomous car, The second step is an intelligent extraction and
reorganization of the data provided by the sensor (Velodyne multi-layer
laser sensor), The third stage is a pre-segmentation of non-planar data,
The fourth stage is performing the segmentation of data received from
the previous steps in order to find objects, curbs and ground plane, The
fifth stage is to develop a methodology that unite the results of
previous steps in order to explore the topology of the environment, i.e.
Will aim to structure the results into a topolog- cal form (identifying
pathways and links between pathways, such as curves and intersections)
to assist other projects that focus on vehicle control and autonomous
navigation systems.},
keywords={control engineering computing;graphics processing units;mobile
robots;parallel programming;solid modelling;3D point cloud
processing;GPU programming;GPU-Services project;Velodyne multilayer
laser sensor;data processing;graphics processing unit;intrinsic
parallelism;mobile robotics;parallel processing development;robotic
systems;three-dimensional sensors data;Graphics processing
units;Parallel processing;Robot sensing systems;Three-dimensional
displays;Vehicles;3D point cloud;GPU;parallel processing;sensor data
procesing;services},
doi={10.1109/LARS-SBR.2015.50},
month={Oct},}
@INPROCEEDINGS{7382326,
author={Wei Sun and Yunfeng Lv and Mengyun Hu},
booktitle={2015 12th International Conference on Fuzzy Systems and
Knowledge Discovery (FSKD)},
title={Robust controller design for the cloud-based robotic visual servo
system with time-delay uncertainty},
year={2015},
pages={2384-2389},
abstract={The control performance of the cloud-based robotic visual
servo system is always limited by the varying time delay for the data
transmission and image processing. The varying image processing delay
produced by the varying number of the extracted features for pose
estimation and the data transmission delay are modeled as a stochastic
process. The cloud-based robotic visual servo system is modeled as a
discrete-time linear uncertain system, and the uncertain system matrix
reflects the variation nature of the time varying delay. In the present
paper, our primary work is presenting the robust control method to deal
with the stabilization problem for the considered system. Then the
sufficient asymptotic stabilization conditions and the feedback
controller are derived. An experiment is conducted to validate the
proposed approach.},
keywords={asymptotic stability;cloud computing;delays;discrete time
systems;feature extraction;feedback;linear matrix inequalities;linear
systems;pose estimation;robot vision;robust control;stochastic
processes;time-varying systems;uncertain systems;visual
servoing;cloud-based robotic visual servo system;control
performance;data transmission delay;discrete-time linear uncertain
system;feature extraction;feedback controller;pose estimation;robust
control method;robust controller design;stabilization problem;stochastic
process;sufficient asymptotic stabilization conditions;time-delay
uncertainty;uncertain system matrix;varying image processing delay;Cloud
computing;Delay effects;Delays;Robot sensing
systems;Servomotors;Visualization;asymptotic stabilization;cloud-based
robotic;time varying delay;uncertain;visual-based control},
doi={10.1109/FSKD.2015.7382326},
month={Aug},}
@INPROCEEDINGS{7371633,
author={W. Sun and J. Xu and Q. Liang and Y. Lv},
booktitle={2015 International Conference on Computer Science and
Mechanical Automation (CSMA)},
title={A Framework for Energy-Optimal Cloud Robotic System under MIMO
Wireless Channel},
year={2015},
pages={113-118},
abstract={The advances of cloud computing and robot enable the cloud
robotics become a newly emerging research hotspot of intelligent
automobile robot. Lots of technical challenges of cloud robotics remain
to be worked out. For example, the computation challenges, the
communication challenges, and the security challenges. In this paper we
intend to design an optimal execution framework for the cloud robotics
under MIMO (multiple-input multiple-output) wireless channel model,
which aims to minimize the energy consumption of the robot and extend
the battery lifetime. The model of the energy consumption for cloud
execution is derived, and the problem of optimization is analyzed during
the process. Additionally, a model and optimization of the standalone
execution by robot are proposed. Based on the models, we provide a
theoretical framework, which decide whether to offloading task. Finally,
the numerical results illustrate that the framework is significant for
minimizing the energy consumption of robot.},
keywords={MIMO communication;cloud computing;intelligent robots;mobile
robots;optimisation;power aware computing;wireless channels;MIMO
wireless channel;battery lifetime extension;cloud computing;energy
consumption minimization;energy-optimal cloud robotic system;intelligent
automobile robot;multiple input multiple output wireless channel
model;Cloud computing;Computational modeling;Energy
consumption;MIMO;Optimization;Receivers;Robots;Cloud
Robotics;MIMO;Offloading Task;Optimal Energy Consumption},
doi={10.1109/CSMA.2015.29},
month={Oct},}
@INPROCEEDINGS{7373467,
author={L. S. Terrissa and S. Ayad},
booktitle={2015 10th International Symposium on Mechatronics and its
Applications (ISMA)},
title={Towards a new cloud robotics approach},
year={2015},
pages={1-5},
abstract={For many years, robots appear in most fields (industrial,
medical, education and military). Recently, the success in computer
technology has advanced the the knowledge in creating intelligent
robots. The cloud computing has potential to provide significant
benefits to robots and automation systems. It enables a remotely
processing with an unlimited access to dynamic global datasets and huge
amount of computing and networking resources. In this paper, we will
propose a new cloud robotics architecture where robots can be provided
as a service easily, efficiently and cheaply. we will define a new kind
of layer which is the Virtual robot Layer compared with classical cloud
computing architecture. This layer consists of two entities : the Robot
Management System and the Virtual Robot System.},
keywords={cloud computing;intelligent robots;robot programming;software
architecture;virtualisation;automation systems;cloud computing
architecture;cloud robotics architecture;computer technology;intelligent
robots;robot management system;virtual robot layer;virtual robot
system;Cloud computing;Computer
architecture;Mechatronics;Monitoring;Service robots;Virtual machining},
doi={10.1109/ISMA.2015.7373467},
month={Dec},}
@INPROCEEDINGS{7354254,
author={K. Sugiura and K. Zettsu},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Rospeex: A cloud robotics platform for human-robot spoken
dialogues},
year={2015},
pages={6155-6160},
abstract={To build conversational robots, roboticists are required to
have deep knowledge of both robotics and spoken dialogue systems.
Although they can use existing cloud services that were built for other
services, e.g., voice search, it will be difficult to share
robotics-specific speech corpora obtained as server logs, because they
will get buried in non-robotics-related logs. Building a cloud platform
especially for the robotics community will benefit not only individual
robot developers but also the robotics community since we can share the
log corpus collected by it. This is challenging because we need to build
a wide variety of functionalities ranging from a stable cloud platform
to high-quality multilingual speech recognition and synthesis engines.
In this paper, we propose “rospeex,” which is a cloud robotics platform
for multilingual spoken dialogues with robots. We analyze the logs we
have collected by operating rospeex for more than a year. Our key
contribution lies in building a cloud robotics platform and allowing the
robotics community to use it without payment or authentication.},
keywords={cloud computing;control engineering computing;human-robot
interaction;interactive systems;cloud robotics platform;human-robot
spoken dialogues;multilingual spoken dialogues;rospeex;Cloud
computing;Microphones;Robots;Servers;Speech;Speech recognition;Speech
synthesis},
doi={10.1109/IROS.2015.7354254},
month={Sept},}
@INPROCEEDINGS{7354021,
author={R. Toris and J. Kammerl and D. V. Lu and J. Lee and O. C.
Jenkins and S. Osentoski and M. Wills and S. Chernova},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Robot Web Tools: Efficient messaging for cloud robotics},
year={2015},
pages={4530-4537},
abstract={Since its official introduction in 2012, the Robot Web Tools
project has grown tremendously as an open-source community, enabling new
levels of interoperability and portability across heterogeneous robot
systems, devices, and front-end user interfaces. At the heart of Robot
Web Tools is the rosbridge protocol as a general means for messaging ROS
topics in a client-server paradigm suitable for wide area networks, and
human-robot interaction at a global scale through modern web browsers.
Building from rosbridge, this paper describes our efforts with Robot Web
Tools to advance: 1) human-robot interaction through usable client and
visualization libraries for more efficient development of front-end
human-robot interfaces, and 2) cloud robotics through more efficient
methods of transporting high-bandwidth topics (e.g., kinematic
transforms, image streams, and point clouds). We further discuss the
significant impact of Robot Web Tools through a diverse set of use cases
that showcase the importance of a generic messaging protocol and
front-end development systems for human-robot interaction.},
keywords={client-server systems;human-robot interaction;online
front-ends;open systems;user interfaces;ROS topics;Web
browser;client-server paradigm;cloud robotics;front-end development
systems;front-end human-robot interface;front-end user interface;generic
messaging protocol;heterogeneous robot system;human-robot
interaction;interoperability;open-source community;portability;robot Web
tool;robot operating system;rosbridge protocol;visualization
library;Browsers;Cloud computing;Computer architecture;Human-robot
interaction;Libraries;Protocols;Robots},
doi={10.1109/IROS.2015.7354021},
month={Sept},}
@INPROCEEDINGS{7354018,
author={W. J. Beksi and J. Spruth and N. Papanikolopoulos},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={CORE: A Cloud-based Object Recognition Engine for robotics},
year={2015},
pages={4512-4517},
abstract={An object recognition engine needs to extract discriminative
features from data representing an object and accurately classify the
object to be of practical use in robotics. Furthermore, the
classification of the object must be rapidly performed in the presence
of a voluminous stream of data. These conditions call for a distributed
and scalable architecture that can utilize a cloud computing
infrastructure for performing object recognition. This paper introduces
a Cloud-based Object Recognition Engine (CORE) to address these needs.
CORE is able to train on large-scale datasets, perform classification of
3D point cloud data, and efficiently transfer data in a robotic network.},
keywords={cloud computing;control engineering computing;image
classification;object recognition;robot vision;3D point cloud
data;CORE;cloud computing infrastructure;cloud-based object recognition
engine;discriminative features;distributed architecture;robotic
network;scalable architecture;voluminous data stream;Cloud
computing;Engines;Entropy;Object recognition;Robot sensing
systems;Three-dimensional displays},
doi={10.1109/IROS.2015.7354018},
month={Sept},}
@INPROCEEDINGS{7352578,
author={J. Będkowski and M. Pełka and K. Majek and T. Fitri and J.
Naruniec},
booktitle={2015 International Conference on Electrical Engineering and
Informatics (ICEEI)},
title={Open source robotic 3D mapping framework with ROS #x2014; Robot
Operating System, PCL #x2014; Point Cloud Library and Cloud Compare},
year={2015},
pages={644-649},
abstract={We propose an open source robotic 3D mapping framework based
on Robot Operating System, Point Cloud Library and Cloud Compare
software extended by functionality of importing and exporting datasets.
The added value is an integrated solution for robotic 3D mapping and new
publicly available datasets (accurate 3D maps with geodetic precision)
for evaluation purpose Datasets were gathered by mobile robot in stop
scan fashion. Presented results are a variety of tools for working with
such datasets, for task such as: preprocessing (filtering, down
sampling), data registration (ICP, NDT), graph optimization (ELCH, LUM),
tools for validation (comparison of 3D maps and trajectories),
performance evaluation (plots of various outputs of algorithms). The
tools form a complete pipeline for 3D data processing. We use this
framework as a reference methodology in recent work on SLAM algorithms.},
keywords={SLAM (robots);mobile robots;operating systems
(computers);public domain software;3D data processing;PCL;ROS;SLAM
algorithm;cloud compare software;mobile robot;open source robotic 3D
mapping framework;performance evaluation;point cloud library;reference
methodology;robot operating system;stop scan fashion;Cameras;Mobile
communication;Robot kinematics;Robot sensing systems;Three-dimensional
displays;XML;3D mapping;data registration;mobile mapping},
doi={10.1109/ICEEI.2015.7352578},
month={Aug},}
@INPROCEEDINGS{7354019,
author={E. Cardarelli and L. Sabattini and C. Secchi and C. Fantuzzi},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Cloud robotics paradigm for enhanced navigation of autonomous
vehicles in real world industrial applications},
year={2015},
pages={4518-4523},
abstract={Autonomous vehicles require advances sensing technologies, in
order to be able to safely share the environment with human operators.
Those sensing technologies are in fact necessary for identifying the
presence of unforeseen objects, and measuring their position and
velocity. Furthermore, classification is necessary for effectively
predicting their behavior. In this paper we consider the presence of
sensing systems both on-board each vehicle, and installed on
infrastructural elements. While the simultaneous presence of multiple
sources of information heavily improves the amount (and quality) of
available data, it generates the need for effective data fusion and
storage systems. Hence, we introduce a centralized cloud service, that
is in charge of receiving and merging data acquired by different sensing
systems. Those data are then distributed to the autonomous vehicles,
that exploit them for implementing advanced navigation strategies. The
proposed methodology is validated in a real industrial environment to
safely perform obstacle avoidance with an autonomously driven forklift.},
keywords={cloud computing;collision avoidance;control engineering
computing;mobile robots;position measurement;sensor fusion;velocity
measurement;autonomous vehicle navigation;autonomous
vehicles;autonomously driven forklift;behavior prediction;centralized
cloud service;cloud robotics paradigm;data fusion;infrastructural
elements;obstacle avoidance;position measurement;sensing
technologies;storage systems;velocity measurement;Data
integration;Mobile robots;Navigation;Robot sensing systems;Sensor
systems;Vehicles},
doi={10.1109/IROS.2015.7354019},
month={Sept},}
@INPROCEEDINGS{7354058,
author={J. Even and F. Ferreri and A. Watanabe and Y. Morales and C.
Ishi and N. Hagita},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Audio augmented point clouds for applications in robotics},
year={2015},
pages={4846-4851},
abstract={This paper presents a method for representing acoustic
information with point clouds by tying it to geometrical features. The
motivation is to create a representation of this information that is
well suited for mobile robotic applications. In particular, the proposed
approach is designed to take advantage of the use of multiple coordinate
frames. As an illustrative example, we present a way to create an audio
augmented point cloud by adding estimated audio power to the point cloud
created by a RGB-D camera. A few applications of this method are
presented.},
keywords={acoustic applications;audio acoustics;geometry;mobile
robots;RGB-D camera;acoustic information;audio augmented point
clouds;estimated audio power;geometrical features;mobile robotic
applications;multiple coordinate
frames;Acoustics;Arrays;Microphones;Robot kinematics;Three-dimensional
displays;Transforms},
doi={10.1109/IROS.2015.7354058},
month={Sept},}
@INPROCEEDINGS{7325398,
author={S. Chtourou and M. Kharrat and N. Ben Amor and M. Jallouli and
M. Abid},
booktitle={2015 IEEE 13th International Symposium on Intelligent Systems
and Informatics (SISY)},
title={Proof of concept of a cloud compilation service for robotics
wireless programming},
year={2015},
pages={295-300},
abstract={Ubiquitous computing, known also as internet of things, is
often considered as the post internet revolution, where devices are
embedded in everyday objects that can communicate and take decision
automatically without human intervention. However, in spite of their
quick popularity and the maturity of both embedded architectures and
computer science programming, designing flexible ubiquitous system
remains a complex task especially for beginners. The main objective is
to create a flexible platform designed to facilitate the design of
embedded systems for ubiquitous applications. This platform is suitable
for users with low programming skills. As a proof of concept, the
authors choose as ubiquitous system a mobile robot that its
functionality can be modified easily by the proposed platform. The
compilation of the code corresponding to the new robot functionality is
obtained using a distant CLOUD server, the program is compiled there,
then loaded to the robot using a wireless programmer. In this paper the
authors present the general structure of the designed system, then they
go through the different steps to implement the robot compiler in the
cloud and test it.},
keywords={Internet of Things;cloud computing;control engineering
computing;embedded systems;mobile robots;robot programming;Internet of
Things;cloud compilation service;computer science programming;distant
CLOUD server;embedded architectures;embedded systems;mobile robot;robot
compiler;robotics wireless programming;ubiquitous computing;Cloud
computing;Computers;Programming profession;Robots;Servers;Wireless
communication;Amazon Web Services;Cloud
Computing;Compiler;Microcontroller;Robot;Wireless programming},
doi={10.1109/SISY.2015.7325398},
ISSN={1949-047X},
month={Sept},}
@INPROCEEDINGS{7301653,
author={S. Rosa and L. O. Russo and G. Toscana and S. Primatesta and M.
Kaouk Ng and B. Bona},
booktitle={2015 IEEE 20th Conference on Emerging Technologies Factory
Automation (ETFA)},
title={Leveraging the cloud for connected service robotics applications},
year={2015},
pages={1-6},
abstract={Cloud robotics is a new approach to robotics that exploits the
internet as a resource for parallel computing and data sharing. Robots
are no more considered as isolated devices but now they can add new
functionalities, communicate with the environment and share knowledge
base. Exploiting this new technology a robot can also take advantages by
off-loading heavy computations to the cloud, thus reducing hardware
costs, power consumption. The Joint Open Lab on Connected Robotic
Applications laB (JOL CRAB) is a research laboratory created by Telecom
Italia in collaboration with Politecnico di Torino. The aim is to
investigate technologies and develop concepts where the focus is mainly
placed on the relationship between robots and the cloud computing,
addressing issues that arise from the use of robotic services in
public/enterprise environments not only of technological, legal,
economical, sociological or psychological kind, but also related to
ergonomics, cognitive perception, and relational experience. The
collaboration between university and the industry led to different field
trials where issues and feasibility of new services were evaluated and a
community of stakeholders was created in the territory. We present these
field trials, the different issues that arise and how that can be solved.},
keywords={cloud computing;control engineering computing;parallel
processing;power aware computing;robots;Internet;JOL CRAB;Politecnico di
Torino;Telecom Italia;cloud computing;cloud robotics;cognitive
perception;connected service robotics applications;data
sharing;ergonomics;hardware cost reduction;joint open lab on connected
robotic application lab;parallel computing;power
consumption;public-enterprise environments;relational experience;robot
technology;stakeholder community;Cloud computing;Graphical user
interfaces;Robot kinematics;Robot sensing systems;Service
robots;Three-dimensional displays},
doi={10.1109/ETFA.2015.7301653},
ISSN={1946-0740},
month={Sept},}
@INPROCEEDINGS{7279332,
author={H. Zhang and L. Zhang and Z. Fang and H. Trannois and M. Huchard
and R. Zapata},
booktitle={2015 IEEE International Conference on Information and
Automation},
title={CRALA: Towards a domain specific language of architecture-centric
Cloud robotics},
year={2015},
pages={456-461},
abstract={Cloud robotic system is a mono- or multi- robot system that
profits one or more services of Cloud Computing. In a few short years,
Cloud robotics as a newly emerged field has already received much
research and industrial attention. The use of the Cloud for robotics and
automation brings some potential benefits largely ameliorating the
performance of robotic systems. However, there are also some challenges.
First of all, from the viewpoint of architecture, how to model and
describe the architectures of Cloud robotic systems? How to deploy these
architectures in Clouds? Merely a language could explicitly describe or
model the architecture of Cloud robotic systems. In this paper, we
present an architecture approach to support design and implementation of
Cloud robotic system.},
keywords={cloud computing;mobile robots;multi-robot systems;software
architecture;CRALA;architecture-centric cloud robotics;cloud
computing;cloud robotic system design;cloud robotic system
implementation;domain-specific language;explicit analysis;monorobot
system;multirobot system;Cloud computing;Computer architecture;Robot
sensing systems;Service robots},
doi={10.1109/ICInfA.2015.7279332},
month={Aug},}
@INPROCEEDINGS{7279524,
author={S. Tian and S. G. Lee},
booktitle={2015 IEEE International Conference on Information and
Automation},
title={An implementation of cloud robotic platform for real time face
recognition},
year={2015},
pages={1509-1514},
abstract={In this paper, we present our work on designing and
implementing a cloud-based robot system, the RC-Cloud Robot System,
which connects cloud computing infrastructure for accessing distributed
computing resources and big data and executing multitask like face
detection, face recognition and etc. We have deployed one application,
“real-time face recognition application” in the RC-Cloud robot system.
The experimental results show that the RC-Cloud robot system is capable
of executing computation-intensive tasks for robot client efficiently
and sharing information among all the clients in the system.},
keywords={Big Data;cloud computing;control engineering
computing;distributed processing;face recognition;robot vision;RC-Cloud
robot system;big data;cloud computing infrastructure;distributed
computing resources;face detection;real time face recognition;Cloud
computing;Computers;Face recognition;Robot sensing
systems;Servers;XML;Cloud robot;Cloud robotics;Face recognition
robot;RC-Cloud robot system},
doi={10.1109/ICInfA.2015.7279524},
month={Aug},}
@INPROCEEDINGS{7251449,
author={J. P. de A. Barbosa and F. do P. de C. Lima and L. dos S.
Coutinho and J. P. R. Rodrigues Leite and J. Barbosa Machado and C.
Henrique Valerio and G. Sousa Bastos},
booktitle={2015 International Conference on Advanced Robotics (ICAR)},
title={ROS, Android and cloud robotics: How to make a powerful low cost
robot},
year={2015},
pages={158-163},
abstract={With the constant development in the robotics field, many
solutions require a high hardware complexity to be executed resulting in
an expensive robotic system. Furthermore, these solutions are usually
closed box implementations that are hard to be reproduced. Hence, this
project proposes a system structure to reduce these difficulties in
robotics development. This system consists of a low cost robot with an
embedded Android phone in a ROS environment. For testing this
implementation, two Android phones, a laptop and a robot with an Arduino
microcontroller have been used. The results of the study demonstrated
several advantages of using this system, such as incorporating
processing and sensing from the smartphone on the robot, having external
heavy processing in a Server outside the robot and being able to connect
the robot to the cloud. Therefore, using a popular Android device
running a Rosjava application can be very useful in robotics
applications and development, mainly due to the possibilities provided
by this simple system.},
keywords={Java;cloud computing;microcontrollers;operating systems
(computers);robot programming;smart phones;Android device;Arduino
microcontroller;ROS environment;Rosjava application;closed box
implementation;cloud robotics;constant development;embedded Android
phone;hardware complexity;laptop;low cost robot;robotic system;robotics
development;system structure;Androids;Google;Humanoid robots;Robot
sensing systems;Smart phones;Android;ROS;Rosjava;cloud robotics;robotic
systems},
doi={10.1109/ICAR.2015.7251449},
month={July},}
@INPROCEEDINGS{7127391,
author={M. K. Ng and S. Primatesta and L. Giuliano and M. L. Lupetti and
L. O. Russo and G. A. Farulla and M. Indaco and S. Rosa and C. Germak
and B. Bona},
booktitle={2015 10th International Conference on Design Technology of
Integrated Systems in Nanoscale Era (DTIS)},
title={A cloud robotics system for telepresence enabling mobility
impaired people to enjoy the whole museum experience},
year={2015},
pages={1-6},
abstract={We present a novel robotic telepresence platform composed by a
semi-autonomous mobile robot based on a cloud robotics framework, which
has been developed with the aim of enabling mobility impaired people to
enjoy museums and archaeological sites that would be otherwise
inaccessible. Such places, in fact, very often are not equipped to
provide access for mobility impaired people, in particular because these
aids require dedicated infrastructures that may not fit within the
environment and large investments. For this reason, people affected by
mobility impairments are often unable to enjoy a part or even the entire
museum experience. Solutions allowing mobility impaired people to enjoy
museum experience are often based on recorded tours, thus they do not
allow active participation of the user. On the contrary, the presented
platform is intended to allow users to enjoy completely the museum
round. A robot equipped with a camera is placed within the museum and
users can control it in order to follow predefined tours or freely
explore the museum. Our solution ensures that users see exactly what the
robot is seing in real-time. The cloud robotics platform controls both
navigation capabilities and teleoperation. Navigation tasks are intended
to let the robot reliably follow pre-defined tours, while main concern
of teleoperation tasks is to ensure robot safety (e.g., by means of
dynamic obstacle detection and avoidance software). Proposed platform
has been optimized to maximize user experience.},
keywords={cameras;cloud computing;collision avoidance;graphical user
interfaces;handicapped aids;mobile robots;museums;robot
vision;telerobotics;virtual reality;archaeological sites;cameras;cloud
robotics;dynamic obstacle avoidance software;dynamic obstacle detection
software;mobility impaired people;museum experience;navigation
capabilities;robot safety;robotic telepresence platform;semiautonomous
mobile robot;teleoperation;Cameras;Computer architecture;Mobile
robots;Navigation;Robot sensing systems},
doi={10.1109/DTIS.2015.7127391},
month={April},}
@INPROCEEDINGS{7116844,
author={P. Benavidez and M. Muppidi and P. Rad and J. J. Prevost and M.
Jamshidi and L. Brown},
booktitle={2015 Annual IEEE Systems Conference (SysCon) Proceedings},
title={Cloud-based realtime robotic Visual SLAM},
year={2015},
pages={773-777},
abstract={Prior work has shown that Visual SLAM (VSLAM) algorithms can
successfully be used for realtime processing on local robots. As the
data processing requirements increase, due to image size or robot
velocity constraints, local processing may no longer be practical.
Offloading the VSLAM processing to systems running in a cloud deployment
of Robot Operating System (ROS) is proposed as a method for managing
increasing processing constraints. The traditional bottleneck with VSLAM
performing feature identification and matching across a large database.
In this paper, we present a system and algorithms to reduce
computational time and storage requirements for feature identification
and matching components of VSLAM by offloading the processing to a cloud
comprised of a cluster of compute nodes. We compare this new approach to
our prior approach where only the local resources of the robot were
used, and examine the increase in throughput made possible with this new
processing architecture.},
keywords={SLAM (robots);cloud computing;feature extraction;image
matching;mobile robots;operating systems (computers);ROS;VSLAM component
matching;VSLAM feature identification;VSLAM processing;cloud
deployment;cloud-based realtime robotic visual SLAM;computational
time;data processing requirements;feature identification;image
size;local robot resources;realtime processing;robot operating
system;robot velocity constraints;storage requirements;visual SLAM
algorithms;Buildings;Cameras;Databases;Feature extraction;Robot vision
systems;Simultaneous localization and mapping;VSLAM;cloud;cooperative
VSLAM;indoor robot},
doi={10.1109/SYSCON.2015.7116844},
month={April},}
@INPROCEEDINGS{7102233,
author={A. A. Dyumin and L. A. Puzikov and M. M. Rovnyagin and G. A.
Urvanov and I. V. Chugunkov},
booktitle={2015 IEEE NW Russia Young Researchers in Electrical and
Electronic Engineering Conference (EIConRusNW)},
title={Cloud computing architectures for mobile robotics},
year={2015},
pages={65-70},
abstract={In last decade, classic IT-infrastructures in modern
enterprises have been changed sufficiently by cloud computing. However,
cloud approach can be used effectively in other applications. In this
paper, we explore applicability of cloud paradigm in mobile robotics.
First approach is using some classic cloud architectures, such as
Infrastructure-as-a-Service, Platform-as-a-Service and
Software-as-a-Service, with heterogeneous distributed mobile robotic
system - for example, it can be used to offload a time consuming
computations from mobile platforms to remote nodes or services. Second
approach is using some special “robotic” types of clouds, such as
Robot-as-a-Service and Function-as-a-Service to leverage abilities of a
single robotics platform in mobile robotic system as services to robotic
system's users or other mobile robotic platforms. We explore both
approaches and give them some architectural examples.},
keywords={cloud computing;control engineering computing;mobile
robots;cloud architectures;cloud computing
architectures;function-as-a-service;heterogeneous distributed mobile
robotic
system;infrastructure-as-a-service;platform-as-a-service;robot-as-aservice;software-as-a-service;Computer
architecture;FAA;Quality of service;Robots;Semiconductor optical
amplifiers;Software as a service;cloud computing;cloud robotics;mobile
robotics;software architectures},
doi={10.1109/EIConRusNW.2015.7102233},
month={Feb},}
@INPROCEEDINGS{7090603,
author={M. L. Lam and K. Y. Lam},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Path planning as a service PPaaS: Cloud-based robotic path
planning},
year={2014},
pages={1839-1844},
abstract={Cloud computing framework can provide promising solutions for
service robots to overcome various challenges in unstructured
environments. In this paper, we present the design and implementation
details of a novel cloud-based system architecture for robotic path
planning. The proposed system, called Path Planning as a Service
(PPaaS), is based on a three-layered architectural design: cloud server
layer, cloud engine layer, and client robot layer. The cloud server
layer is implemented with Hadoop Distributed File System. It contains a
path plan database, which stores the solution paths that can be shared
among robots. The cloud engine layer, which is the core of the
architecture, utilizes Rapyuta cloud engine to create configurable
containers for different robotic platforms. It also provides an
on-demand path planning software in the cloud, which computes the
optimal paths for robots to reach the goal positions. A load balancer is
suggested to manage the container allocation in machines and coordinate
local/cloud computation load balancing. The client robot layer is formed
by a set of physical robots that employ PPaaS in the cloud. The
communication framework of the entire system is based on Robot Operating
System (ROS) platform, despite that robot clients do not necessarily run
ROS. We have extended the proposed system for a case study of people
tracking and following by multiple robots. We have also experimentally
verified the feasibility and effectiveness of solving the shortest path
problem via parallel processing in the cloud. The technical
implementation details as well as the challenges of the proposed cloud
robotics system are presented in the paper.},
keywords={cloud computing;control engineering computing;data
handling;mobile robots;multi-robot systems;operating systems
(computers);parallel processing;path planning;resource
allocation;service robots;software architecture;Hadoop distributed file
system;PPaaS;ROS platform;Rapyuta cloud engine;client robot layer;cloud
computing framework;cloud engine layer;cloud server layer;cloud-based
robotic path planning;cloud-based system architecture;communication
framework;configurable containers;container allocation
management;coordinate cloud computation load balancing;coordinate local
computation load balancing;load balancer;on-demand path planning
software;parallel processing;path plan database;path planning as a
service;people tracking;physical robots;robot operating system
platform;service robots;shortest path problem;three-layered
architectural design;Cloud computing;Containers;Databases;Engines;Path
planning;Robot kinematics},
doi={10.1109/ROBIO.2014.7090603},
month={Dec},}
@ARTICLE{6853392,
author={G. Mohanarajah and D. Hunziker and R. D'Andrea and M. Waibel},
journal={IEEE Transactions on Automation Science and Engineering},
title={Rapyuta: A Cloud Robotics Platform},
year={2015},
volume={12},
number={2},
pages={481-493},
abstract={In this paper, we present the design and implementation of
Rapyuta, an open-source cloud robotics platform. Rapyuta helps robots to
offload heavy computation by providing secured customizable computing
environments in the cloud. The computing environments also allow the
robots to easily access the RoboEarth knowledge repository. Furthermore,
these computing environments are tightly interconnected, paving the way
for deployment of robotic teams. We also describe three typical use
cases, some benchmarking and performance results, and two
proof-of-concept demonstrations. Note to Practitioners - Rapyuta allows
to outsource some or all of a robot's onboard computational processes to
a commercial data center. Its main difference to other, similar
frameworks like the Google App Engine is that it is specifically
tailored towards multiprocess high-bandwidth robotics
applications/middlewares and provides a well-documented open-source
implementation that can be modified to cover a large variety of robotic
scenarios. Rapyuta supports the outsourcing of almost all of the current
3000+ ROS packages out of the box and is easily extensible to other
robotic middleware. A pre-installed Amazon Machine Image (AMI) is
provided that allows to launch Rapyuta in any of Amazon's data center
within minutes. Once launched, robots can authenticate themselves to
Rapyuta, create one or more secured computational environments in the
cloud and launch the desired nodes/processes. The computing environments
can also be arbitrarily connected to build parallel computing
architectures on the fly. The WebSocket-based communication protocol,
which provides synchronous and asynchronous communication mechanisms,
allows not only ROS based robots, but also browsers and mobiles phones
to connect to the ecosystem. Rapyuta's computing environments are
private, secure, and optimized for data throughput. However, its
performance is in large part determined by the latency and quality of
the network connection - nd the performance of the data center.
Optimizing performance under these constraints is typically highly
application-specific. The paper illustrates an example of performance
optimization in a collaborative real-time 3-D mapping application. Other
target applications include collaborative 3-D mapping, task/grasp
planning, object recognition, localization, and teleoperation, among
others.},
keywords={cloud computing;middleware;parallel processing;programming
environments;public domain software;robot programming;software
packages;AMI;Amazon Machine Image;Amazon data center;Google App
Engine;ROS based robots;ROS packages outsourcing;Rapyuta;RoboEarth
knowledge repository;WebSocket-based communication protocol;asynchronous
communication mechanisms;benchmarking;browsers;collaborative real-time
3D mapping application;commercial data center;data
throughput;ecosystem;mobiles phones;multiprocess high-bandwidth
robotics;network connection latency;network connection quality;object
recognition;open-source cloud robotics platform;parallel computing
architectures;performance optimization;proof-of-concept
demonstrations;robot onboard computational processes;robotic
middleware;robotic teams deployment;secured customizable computing
environments;task/grasp planning;use cases;Cloud
computing;Containers;Data structures;Open source
software;Protocols;Robots;Cloud robotics;cloud-based mapping;networked
robots;platform-as-a-service (PaaS)},
doi={10.1109/TASE.2014.2329556},
ISSN={1545-5955},
month={April},}
@ARTICLE{7060735,
author={L. Wang and M. Liu and M. Q. H. Meng},
journal={IEEE Transactions on Automation Science and Engineering},
title={Real-Time Multisensor Data Retrieval for Cloud Robotic Systems},
year={2015},
volume={12},
number={2},
pages={507-518},
abstract={Cloud technology elevates the potential of robotics with which
robots possessing various capabilities and resources may share data and
combine new skills through cooperation. With multiple robots, a cloud
robotic system enables intensive and complicated tasks to be carried out
in an optimal and cooperative manner. Multisensor data retrieval (MSDR)
is one of the key fundamental tasks to share the resources. Having
attracted wide attention, MSDR is facing severe technical challenges.
For example, MSDR is particularly difficult when cloud cluster hosts
accommodate unpredictable data requests triggered by multiple robots
operating in parallel. In these cases, near real-time responses are
essential while addressing the problem of the synchronization of
multisensor data simultaneously. In this paper, we present a framework
targeting near real-time MSDR, which grants asynchronous access to the
cloud from the robots. We propose a market-based management strategy for
efficient data retrieval. It is validated by assessing several
quality-of-service (QoS) criteria, with emphasis on facilitating data
retrieval in near real-time. Experimental results indicate that the MSDR
framework is able to achieve excellent performance under the proposed
management strategy in typical cloud robotic scenarios.},
keywords={cloud computing;quality of service;sensor
fusion;sensors;service robots;synchronisation;QoS criteria;asynchronous
access;cloud cluster hosts;cloud robotic scenarios;cloud robotic
systems;cloud technology;market-based management strategy;multiple
robots;quality of service criteria;real-time MSDR framework;real-time
multisensor data retrieval;real-time
responses;synchronization;unpredictable data
requests;Optimization;Quality of service;Real-time systems;Resource
management;Robot sensing systems;Cloud robotic system;multisensor
fusion;real-time data retrieval},
doi={10.1109/TASE.2015.2408634},
ISSN={1545-5955},
month={April},}
@ARTICLE{7006734,
author={B. Kehoe and S. Patil and P. Abbeel and K. Goldberg},
journal={IEEE Transactions on Automation Science and Engineering},
title={A Survey of Research on Cloud Robotics and Automation},
year={2015},
volume={12},
number={2},
pages={398-409},
abstract={The Cloud infrastructure and its extensive set of
Internet-accessible resources has potential to provide significant
benefits to robots and automation systems. We consider robots and
automation systems that rely on data or code from a network to support
their operation, i.e., where not all sensing, computation, and memory is
integrated into a standalone system. This survey is organized around
four potential benefits of the Cloud: 1) Big Data: access to libraries
of images, maps, trajectories, and descriptive data; 2) Cloud Computing:
access to parallel grid computing on demand for statistical analysis,
learning, and motion planning; 3) Collective Robot Learning: robots
sharing trajectories, control policies, and outcomes; and 4) Human
Computation: use of crowdsourcing to tap human skills for analyzing
images and video, classification, learning, and error recovery. The
Cloud can also improve robots and automation systems by providing access
to: a) datasets, publications, models, benchmarks, and simulation tools;
b) open competitions for designs and systems; and c) open-source
software. This survey includes over 150 references on results and open
challenges. A website with new developments and updates is available at:
http://goldberg.berkeley.edu/cloud-robotics/.},
keywords={cloud computing;control engineering computing;grid
computing;groupware;learning (artificial
intelligence);outsourcing;parallel processing;robots;Big Data;automation
system;cloud computing;cloud robotics;collective robot
learning;crowdsourcing;human computation;parallel grid computing;robots
sharing trajectory;Automation;Cloud computing;Computational
modeling;Robot kinematics;Robot sensing systems;Big data;cloud
automation;cloud computing;cloud robotics;crowdsourcing;open source},
doi={10.1109/TASE.2014.2376492},
ISSN={1545-5955},
month={April},}
@INPROCEEDINGS{7031107,
author={R. Imai and R. Kubo},
booktitle={2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE)},
title={Cloud-based remote motion control over FTTH networks for home
robotics},
year={2014},
pages={565-566},
abstract={The Internet technologies make it possible to control home
robots from remote locations. This paper proposes a cloud-based remote
motion control system over a 20-km fiber-to-the-home (FTTH) network,
i.e., a passive optical network (PON). The physical location of the
cloud server, in which the controller is installed, greatly affects the
stability of the control system. Remote position control of a rotary
motor is performed as a fundamental experiment for robot control. The
experimental results show that the controller accepts a 20-ms round-trip
time delay of cloud networks in the proposed system.},
keywords={cloud computing;control engineering computing;delays;home
computing;motion control;passive optical networks;position
control;service robots;stability;telecontrol;FTTH networks;Internet
technologies;PON;cloud networks;cloud server;cloud-based remote motion
control;fiber-to-the-home network;home robotics;passive optical
network;robot control;rotary motor;round-trip time delay;stability;Cloud
computing;Delay effects;Motion control;Optical fiber subscriber
loops;Passive optical networks;Robot sensing systems},
doi={10.1109/GCCE.2014.7031107},
ISSN={2378-8143},
month={Oct},}
@INPROCEEDINGS{7018606,
author={S. Nakagawa and H. Osawa and K. Asakura and N. Obara and Y.
Tsuchiya and M. Narita},
booktitle={2014 10th France-Japan/ 8th Europe-Asia Congress on
Mecatronics (MECATRONICS2014- Tokyo)},
title={Web application technologies for integration of remote operation,
camera image and voice communication into a cloud-based robotics platform},
year={2014},
pages={256-261},
abstract={Web application technologies for network-based various type of
devices that include robots are explored. Web application development of
robot services are required to provide integration of robot control and
multimedia information that are characterized by dynamic, two-way
communication and real-time. In this paper, we design the web
application in consideration of these characteristics of robot services.
The target of our web application is a robotic telepresence service. We
adopt RSNP (Robot Service Network Protocol) as a robot software
platform, and adopt HTML5 technology in web application development. In
addition, we implement a prototype system, and verify the effectiveness
and usefulness of the web application.},
keywords={Internet;cloud computing;control engineering
computing;multimedia systems;robot programming;HTML5 technology;RSNP;Web
application technologies;cloud-based robotics platform;multimedia
information;remote operation integration;robot control;robot service
network protocol;robot software platform;robotic telepresence
service;voice communication;Browsers;Delays;Internet;Multimedia
communication;Robot control;Servers;HTML5;RSNP;Robot Service;Voice;Web
application},
doi={10.1109/MECATRONICS.2014.7018606},
month={Nov},}
@INPROCEEDINGS{7005212,
author={S. Rosa and L. O. Russo and B. Bona},
booktitle={Proceedings of the 2014 IEEE Emerging Technology and Factory
Automation (ETFA)},
title={Towards a ROS-based autonomous cloud robotics platform for data
center monitoring},
year={2014},
pages={1-8},
abstract={Data center monitoring has been a critical subject of research
in recent years. In this paper we present a robotic system, based on the
Robot Operating System (ROS), in which a mobile robot equipped with a
laser range sensor and an Inertial Motion Unit (IMU) is able to
autonomously navigate in a data center room for accurate monitoring of
critical measurements, such as servers' external temperature, humidity
and other physical quantities. The robot is able to autonomously create
a map of a previously unknown room, localize therein and execute a list
of measurements at different locations, which are provided by the user
via a web Graphical User Interface (GUI). The application is based on a
cloud robotics infrastructure which encloses the ROS nodes and exposes
REST APIs to the user. We discuss our implementation choices with
regards to the particular requirements of the scenario, both in terms of
robot navigation and software infrastructure, and present some
preliminary results in a real scenario.},
keywords={application program interfaces;cloud computing;computer
centres;graphical user interfaces;laser ranging;mobile robots;optical
sensors;path planning;GUI;IMU;REST API;ROS nodes;ROS-based autonomous
cloud robotics platform;Web graphical user interface;data center
monitoring;data center room;inertial motion unit;laser range
sensor;mobile robot;robot navigation;robot operating system;server
external temperature;software infrastructure;Graphical user
interfaces;Monitoring;Robot sensing systems;Temperature
measurement;Temperature sensors},
doi={10.1109/ETFA.2014.7005212},
ISSN={1946-0740},
month={Sept},}
@INPROCEEDINGS{6973810,
author={L. Gherardi and D. Hunziker and G. Mohanarajah},
booktitle={2014 IEEE 7th International Conference on Cloud Computing},
title={A Software Product Line Approach for Configuring Cloud Robotics
Applications},
year={2014},
pages={745-752},
abstract={The computational requirements of the increasingly
sophisticated algorithms used in today's robotics software applications
have outpaced the onboard processors of the average robot. Furthermore,
the development and configuration of these applications are difficult
tasks that require expertise in diverse domains, including software
engineering, control engineering, and computer vision. As a solution to
these problems, this paper extends and integrates our previous works,
which are based on two promising techniques: Cloud Robotics and Software
Product Lines. Cloud Robotics provides a powerful and scalable
environment to offload the computationally expensive algorithms
resulting in low-cost processors and light-weight robots. Software
Product Lines allow the end user to deploy and configure complex
robotics applications without dealing with low-level problems such as
configuring algorithms and designing architectures. This paper discusses
the proposed method in depth, and demonstrates its advantages with a
case study.},
keywords={cloud computing;control engineering;intelligent robots;robot
vision;software product lines;cloud computing;cloud robotics;computer
vision;control engineering;light-weight robots;onboard
processors;robotic software applications;software engineering;software
product line approach;sophisticated algorithms;Cloud
computing;Computational modeling;Computer
architecture;Containers;Robots;Software as a service;Cloud
Computing;Robotics;Software Product Lines},
doi={10.1109/CLOUD.2014.104},
ISSN={2159-6182},
month={June},}
@INPROCEEDINGS{6924484,
author={M. J. Meena and S. S. Prabha and S. Pandian},
booktitle={2014 Asia-Pacific Conference on Computer Aided System
Engineering (APCASE)},
title={A cloud-based mobile robotic system for environmental monitoring},
year={2014},
pages={122-126},
abstract={Cloud computing is one of the major emerging technologies of
the early 21st century, and has the potential to boost socio-economic
development significantly. While cloud computing has traditionally meant
software technology, of late there has been active research on
cyber-physical cloud systems involving vehicles and mobile robots. In
this paper, a set of mobile field robots linked by wireless
communication is connected to a cloud server for the purpose of
environmental monitoring. One of the robots is used as a base station,
while the others are equipped with air quality sensors and on-board
microcontrollers for navigation and control. The air quality data are
transmitted to the base station robot, which forwards them to the
cloud-based server for storage, retrieval, and visualization. The cloud
server can also be used to perform computation-intensive functions like
image processing using the Google Recognition Engine. A prototype system
consisting of three networked mobile robots with open source
microcontrollers and air quality sensors connected to a cloud server is
developed, and experimental results are presented to illustrate the
effectiveness of the proposed method.},
keywords={air quality;cloud computing;control engineering
computing;environmental monitoring (geophysics);microcontrollers;mobile
robots;radiocommunication;sensors;Google recognition engine;air quality
data;air quality sensors;base station robot;cloud computing;cloud
server;cloud-based mobile robotic system;cloud-based
server;computation-intensive functions;cyber-physical cloud
system;environmental monitoring;image processing;mobile field
robots;navigation;networked mobile robots;on-board microcontrollers;open
source microcontrollers;socio-economic development;software
technology;wireless communication;Cloud computing;Clouds;Environmental
monitoring;Mobile robots;Robot sensing systems;air quality;cloud
comuting;environmental monitoring;mobile robot;wireless communication},
doi={10.1109/APCASE.2014.6924484},
month={Feb},}
@INPROCEEDINGS{6917443,
author={S. L. Remy and J. Svacha and A. Walcott-Bryant},
booktitle={The 4th Annual IEEE International Conference on Cyber
Technology in Automation, Control and Intelligent},
title={Design implications for networked controllers using web standards
in cloud robotics},
year={2014},
pages={100-105},
abstract={Computation, networking and controls are in the midst of a
true transformation. Previously it was assumed that delays in network
communication would inhibit control systems from leveraging networked
resources such as remote compute power. Now, industrial and residential
applications of the Internet of Things force us to revisit principled
deployment and support of distributed sensing and actuation. Web
standards provide key resources in the development and support of
flexible distributed systems, however the conventional wisdom is that
their use adds to the network delay and uncertainty, and makes a
difficult control problem intractable. We quantify the effect of an
important set of web standards (both formal and informal) in the control
of physical systems. The results reported in this paper indicate that
although the use of web standards provides an effective path to flexible
implementation of distributed control, there are software design choices
that impact the observed performance in significant ways.},
keywords={Internet of Things;cloud computing;control engineering
computing;distributed control;robots;Internet of things;Web
standards;cloud robotics;control systems;design implications;distributed
actuation;distributed control;distributed sensing;industrial
applications;network communication;networked controllers;networked
resources;remote compute power;residential applications;software design
choices;Computers;Control systems;Delays;Jitter;Mathematical
model;Software;Standards},
doi={10.1109/CYBER.2014.6917443},
month={June},}
@INPROCEEDINGS{6907168,
author={K. Sugiura and Y. Shiga and H. Kawai and T. Misu and C. Hori},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Non-monologue HMM-based speech synthesis for service robots: A
cloud robotics approach},
year={2014},
pages={2237-2242},
abstract={Robot utterances generally sound monotonous, unnatural, and
unfriendly because their Text-to-Speech (TTS) systems are not optimized
for communication but for text-reading. Here we present a non-monologue
speech synthesis for robots. We collected a speech corpus in a
non-monologue style in which two professional voice talents read
scripted dialogues. Hidden Markov models (HMMs) were then trained with
the corpus and used for speech synthesis. We conducted experiments in
which the proposed method was evaluated by 24 subjects in three
scenarios: text-reading, dialogue, and domestic service robot (DSR)
scenarios. In the DSR scenario, we used a physical robot and compared
our proposed method with a baseline method using the standard Mean
Opinion Score (MOS) criterion. Our experimental results showed that our
proposed method's performance was (1) at the same level as the baseline
method in the text-reading scenario and (2) exceeded it in the DSR
scenario. We deployed our proposed system as a cloud-based speech
synthesis service so that it can be used without any cost.},
keywords={cloud computing;hidden Markov models;human-robot
interaction;service robots;speech synthesis;DSR scenario;MOS
criterion;TTS system;cloud robotics approach;dialogue scenario;domestic
service robot scenario;hidden Markov model;mean opinion
score;nonmonologue HMM-based speech synthesis;robot utterance;scripted
dialogues;speech corpus;text-reading scenario;text-to-speech
system;Hidden Markov models;Service robots;Speech;Speech
synthesis;Training},
doi={10.1109/ICRA.2014.6907168},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907157,
author={L. Wang and M. Liu and M. Q. H. Meng},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Hierarchical auction-based mechanism for real-time resource
retrieval in cloud mobile robotic system},
year={2014},
pages={2164-2169},
abstract={In order to share information in the cloud for multi-robot
systems, efficient data transmission is essential for real-time
operations such as coordinated robotic missions. As a limited resource,
bandwidth is ubiquitously required by applications among physical
multi-robot systems. In this paper, we proposed a hierarchical
auction-based mechanism, namely LQM (Link Quality Matrix)-auction. It
consists of multiple procedures, such as hierarchical auction, proxy
scheduling. Note that the proposed method is designed for real-time
resource retrieval for physical multi-robot systems, instead of
simulated virtual agents. We validate the proposed mechanism through
real-time experiments. The results show that LQM-auction is suitable for
scheduling a group of robots, leading to optimized performance for
resource retrieval.},
keywords={cloud computing;control engineering computing;data
communication;hierarchical systems;information retrieval;mobile
robots;multi-robot systems;resource
allocation;scheduling;LQM-auction;cloud mobile robotic
system;coordinated robotic missions;data transmission;hierarchical
auction-based mechanism;link quality matrix-auction;physical multirobot
systems;proxy scheduling;real-time resource retrieval;simulated virtual
agents;Bandwidth;Real-time systems;Relays;Robot kinematics;Robot sensing
systems;Spread spectrum communication},
doi={10.1109/ICRA.2014.6907157},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907684,
author={R. C. Luo and S. Y. Chen and K. C. Yeh},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Human body trajectory generation using point cloud data for
robotics massage applications},
year={2014},
pages={5612-5617},
abstract={Intelligent robot for improving the quality of human life is
desirable. To let robot perform the intelligent functions of massage
which can serve for humans, adaptive massage trajectory generator is
needed to suit for different body shape with different people. Base on
the state of the art human pose recognition techniques, we develop a
methodology to build the relationship between camera and human body and
generate the specified massage trajectories. Assume we can detect human
pose and label each part of human body, we use the RANSAC algorithm to
estimate the frontal and sagittal planes and refine it by calculating
the minimum moment of inertia of point cloud. The experimental results
demonstrate our work which provide a useful method for massage
trajectory generation. As an example of service robot application, this
method is useful for executing the massage application autonomously.},
keywords={intelligent robots;iterative methods;manipulators;pose
estimation;robot vision;service robots;trajectory control;RANSAC
algorithm;adaptive massage trajectory generator;autonomous massage
application execution;body shape;camera;dual arm robot;estimate;frontal
plane estimation;human body trajectory generation;human life quality
improvement;human pose detection;human pose recognition
technique;intelligent functions;intelligent robot;minimum moment of
inertia;point cloud data;robotic massage application;sagittal plane
estimation;service robot application;Cameras;Equations;Mathematical
model;Robot kinematics;Three-dimensional displays;Trajectory},
doi={10.1109/ICRA.2014.6907684},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6903144,
author={F. Nordlund and M. Higashida and Y. Teranishi and S. Shimojo and
M. Yokoyama and M. Shimomura},
booktitle={2014 IEEE 38th International Computer Software and
Applications Conference Workshops},
title={Designing of a Network-Aware Cloud Robotic Sensor Observation
Framework},
year={2014},
pages={288-294},
abstract={This paper proposes a network-aware cloud networked robotic
sensor observation framework based on the topic-based
publisher/subscriber messaging paradigm. Cloud network robotics systems
have some advantages in making use of rich computation or storage
resources of clouds, low maintenance cost, and easy development of
applications. But a problem occurs when systems that need fast feedback
from application modules are run over networks with intermittent
connection. In this framework, the placement of the robot application
modules is changed according to the QoS provided by the physical
network. We also propose a map cell-based robotic sensor observation
algorithm that suits to the framework. A prototype system of a data
center monitoring robot that is based on the proposed framework and
algorithm is implemented and a preliminary experiment was carried out to
demonstrate the benefit of the system. The experiment result shows that
the performance of the data center monitoring could be improved and hot
spots can be found faster by making the cloud networked robot
network-aware.},
keywords={cloud computing;computer centres;control engineering
computing;message passing;mobile robots;networked control
systems;quality of service;QoS;application development;application
modules;cloud network robotics system;cloud storage resources;data
center monitoring robot;maintenance cost;map cell-based robotic sensor
observation algorithm;network intermittent connection;network-aware
cloud networked robot;network-aware cloud robotic sensor observation
framework;robot application module placement;topic-based
publisher-subscriber messaging paradigm;Bandwidth;Monitoring;Quality of
service;Robot kinematics;Robot sensing systems;Temperature
measurement;Big Data Analysis;Cloud Networked Robotics;Network
Qos;Publish/Subscribe Messaging Paradigm;Sensor Networks},
doi={10.1109/COMPSACW.2014.51},
month={July},}
@INPROCEEDINGS{6822423,
author={G. Magyar and T. Câdrik and M. Virciková and P. Sincâk},
booktitle={2014 IEEE 12th International Symposium on Applied Machine
Intelligence and Informatics (SAMI)},
title={Towards adaptive cloud-based platform for robotic assistants in
education},
year={2014},
pages={285-289},
abstract={This paper introduces a cloud based web application for
educational purposes using the humanoid robot NAO. The application is
based on the Google App Engine, which is part of the Google Cloud
Platform. The proposed solution is tailored for NAO humanoid platform,
and basic research has been also accomplished in enhancing a human robot
interaction. Emotional component of robot synthetic emotions are being
investigated for interaction including fuzzy based Plutchik emotion
model. The Emotions should increase an effectiveness of the teaching
procedure and also simple logical evaluation as Learning Effectiveness
coefficient is designed to evaluate an overall NAO Teacher. The Cloud
based database contains various educational approaches developed for NAO
robot as interface. The overall goal is not to replace a teacher but
create an environment which will support the idea “Learning by Gaming”
in sense to improve the speed and quality of learning a procedure.},
keywords={cloud computing;computer aided instruction;control engineering
computing;educational robots;fuzzy set theory;human-robot
interaction;humanoid robots;Google App engine;Google cloud platform;NAO
robot;NAO teacher;adaptive cloud-based platform;cloud based Web
application;cloud based database;educational approaches;educational
purposes;emotional component;fuzzy based Plutchik emotion model;human
robot interaction;humanoid robot NAO;learning by gaming;learning
effectiveness coefficient;logical evaluation;robot synthetic
emotions;robotic assistants;teaching procedure;Educational
institutions;Engines;Google;Mobile robots;Service robots},
doi={10.1109/SAMI.2014.6822423},
month={Jan},}
@INPROCEEDINGS{6802882,
author={S. Srivastava and A. Sarkar and B. S. Manoj},
booktitle={2013 IEEE International Conference on Advanced Networks and
Telecommunications Systems (ANTS)},
title={Hazard control algorithms for heterogenous multi-agent
cloud-enabled robotic network},
year={2013},
pages={1-6},
abstract={Robots exploring unknown terrains autonomously always face the
threat of getting isolated or lost from the rest. Geo-coordination
technologies such as GPS cannot be used in extra terrestrial situations.
Therefore, an inbuilt mechanism is needed in the robot that helps to
track the robot's position and prevents it from getting off the
network's coverage. We define hazard-control as the task of executing a
series of decisions for recovering lost robots and bringing them back to
the robotic swarm network. For a swarm of robots, this task involves
communicating with other robots in the system to make sure that all the
robots are within the range of the multi-hop network formed by the
robots. If one robot goes away of the robotic network, hazard-control
algorithms help the robots to recover the lost robot back in the
network. We propose three robot-centric hazard control algorithms that
help to get the lost robot in a robotic swarm back in the network: (i)
Random Motion algorithm, (ii) Variable Spiral algorithm, and (iii)
Motion Reversal algorithm. We study the performance of the proposed
algorithms, compared their efficiency and identify the scenarios where
each algorithm is most suited.},
keywords={Global Positioning System;multi-agent systems;multi-robot
systems;path planning;rescue robots;geocoordination technologies;hazard
control algorithms;heterogeneous multiagent cloud-enabled robotic
network;motion reversal algorithm;multihop network;network
coverage;random motion algorithm;robot position;robot-centric hazard
control algorithms;robotic swarm network;terrestrial situations;variable
spiral algorithm;Algorithm design and analysis;Approximation
algorithms;Hazards;Robot kinematics;Robot sensing
systems;Spirals;GPS;algorithm;hazard control;path planning
algorithms;robotic network;swarm;wifi based geolocation},
doi={10.1109/ANTS.2013.6802882},
ISSN={2153-1676},
month={Dec},}
@INPROCEEDINGS{6803195,
author={C. Mouradian and F. Z. Errounda and F. Belqasmi and R. Glitho},
booktitle={2014 IEEE World Forum on Internet of Things (WF-IoT)},
title={An infrastructure for robotic applications as cloud computing
services},
year={2014},
pages={377-382},
abstract={Robotic applications are becoming ubiquitous. They are widely
used in several areas (e.g., healthcare, disaster management, and
manufacturing). However, their provisioning still faces several
challenges such as cost and resource usage efficiency. Cloud computing
is an emerging paradigm that may aid in tackling these challenges. It
has three main facets: Infrastructure as a Service (IaaS), Platform as a
Service (PaaS) and Software as a Service (SaaS). This paper focuses on
the IaaS aspects of robotic applications as cloud computing services. It
proposes an architecture that enables cost efficiency through
virtualization and dynamic task delegation to robots, including robots
that might belong to other clouds. Overlays and RESTful Web services are
used as cornerstones. A prototype is built using LEGO Mindstorms NXT as
the robotic platform, and JXTA as the overlay middleware. Related work
is reviewed, the functional entities and interfaces of the architecture
are described, and the prototype architecture is presented along with
the implemented scenario.},
keywords={Web services;cloud computing;control engineering
computing;middleware;robots;ubiquitous computing;IaaS;JXTA;LEGO
Mindstorms NXT;PaaS;RESTful Web services;SaaS;cloud computing
services;cost efficiency;disaster management;dynamic task
delegation;healthcare;infrastructure as a service;manufacturing;overlay
middleware;platform as a service;resource usage efficiency;robotic
platform;software as a service;ubiquitous robotic
applications;virtualization;Cloud computing;Computer
architecture;Engines;Logic gates;Robot sensing
systems;Virtualization;Infrastructure as a Service (IaaS);Robotic
applications;cloud computing;task allocation;task delegation},
doi={10.1109/WF-IoT.2014.6803195},
month={March},}
@INPROCEEDINGS{6724524,
author={F. Z. Errounda and F. Belqasmi and R. Glitho},
booktitle={2013 Fourth International Conference on the Network of the
Future (NoF)},
title={Towards cloud-based architectures for robotic applications
provisioning},
year={2013},
pages={1-3},
abstract={Robotic applications are widely used in various domains (e.g.
healthcare, agriculture). However, provisioning them in a cost-efficient
manner remains an uphill task. Cloud computing is a new paradigm with
three key facets: Infrastructure as a Service (IaaS), Platform as a
Service (PaaS) and Software as a Service (SaaS). Rapid application
development/deployment, pay-per-use and efficient use of resources are
among the expected benefits. Cloud computing is a promising technology
for application provisioning and can bring robotic application
provisioning to the next level. This paper identifies the shortcomings
of the state of the art in cloud-based architectures for robotic
applications provisioning. It sketches an overall business model to
tackle the identified shortcomings. It proposes an overlay-based
architecture to handle the cloud interactions aspects of the proposed
business model. The implementation aspects of the overlay-based
architecture are discussed. Research directions are also identified.},
keywords={cloud computing;control engineering
computing;robots;IaaS;PaaS;SaaS;cloud computing;cloud interaction
aspect;cloud-based architecture;infrastructure as a
service;overlay-based architecture;platform as a service;robotic
application provisioning;software as a service;Business;Cloud
computing;Clouds;Computer architecture;Fires;Ontologies;Robots;cloud
computing;machine-to-machine communication;overlay network;robotic
applications},
doi={10.1109/NOF.2013.6724524},
month={Oct},}
@INPROCEEDINGS{6697184,
author={M. Tenorth and K. Kamei and S. Satake and T. Miyashita and N.
Hagita},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Building knowledge-enabled cloud robotics applications using the
ubiquitous network robot platform},
year={2013},
pages={5716-5721},
abstract={In this paper, we discuss how networked robot architectures
can facilitate the development, deployment, management and adaptation of
distributed robotic applications. Our aim is to modularize applications
by factoring out environment, task-, domain-, and robot-specific
knowledge components and representing them explicitly in a formal
knowledge base that is shared between the robots and service
applications. Robot control decisions can then be formulated in terms of
inference tasks that are evaluated based on this knowledge during task
execution. The explicit and modular knowledge representation allows
human operators with different areas of expertise to adapt the
respective parts of the knowledge independently. We implemented this
concept by integrating knowledge representation methods of the RoboEarth
project with the distributed task execution capabilities of the
Ubiquitous Network Robot Platform.},
keywords={cloud computing;knowledge based systems;knowledge
representation;service robots;ubiquitous computing;RoboEarth
project;distributed robotic applications;domain-specific knowledge
components;formal knowledge base system;inference
tasks;knowledge-enabled cloud robotic applications;modular knowledge
representation;robot control decisions;robot-specific knowledge
components;task execution;task-specific knowledge components;ubiquitous
network robot platform;Hardware;Knowledge based systems;Ontologies;Robot
kinematics;Robot sensing systems;Semantics},
doi={10.1109/IROS.2013.6697184},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6650415,
author={Z. Dogmus and A. Papantoniou and M. Kilinc and S. A. Yildirim
and E. Erdem and V. Patoglu},
booktitle={2013 IEEE 13th International Conference on Rehabilitation
Robotics (ICORR)},
title={Rehabilitation robotics ontology on the cloud},
year={2013},
pages={1-6},
abstract={We introduce the first formal rehabilitation robotics
ontology, called RehabRobo-Onto, to represent information about
rehabilitation robots and their properties; and a software system
RehabRobo-Query to facilitate access to this ontology. RehabRobo-Query
is made available on the cloud, utilizing Amazon Web services, so that
1) rehabilitation robot designers around the world can add/modify
information about their robots in RehabRobo-Onto, and 2) rehabilitation
robot designers and physical medicine experts around the world can
access the knowledge in RehabRobo-Onto by means of questions about
robots, in natural language, with the guide of the intelligent
user-interface of RehabRobo-Query. The ontology system consisting of
RehabRobo-Onto and RehabRobo-Query is of great value to robot designers
as well as physical therapists and medical doctors. On the one hand,
robot designers can access various properties of the existing robots and
to the related publications to further improve the state-of-the-art. On
the other hand, physical therapists and medical doctors can utilize the
ontology to compare rehabilitation robots and to identify the ones that
serve best to cover their needs, or to evaluate the effects of various
devices for targeted joint exercises on patients with specific disorders.},
keywords={Web services;cloud computing;control engineering
computing;medical robotics;natural language interfaces;ontologies
(artificial intelligence);patient rehabilitation;Amazon Web
services;REHABROBO-ONTO;REHABROBO-QUERY;cloud computing;formal
rehabilitation robotics ontology;intelligent user-interface;medical
doctors;natural language;ontology system;physical medicine
experts;physical therapists;rehabilitation robot designers;software
system;targeted joint exercises;Joints;Ontologies;Read only
memory;Rehabilitation robotics;Robot kinematics;Shoulder;1},
doi={10.1109/ICORR.2013.6650415},
ISSN={1945-7898},
month={June},}
@INPROCEEDINGS{6617612,
author={S. Jordán and T. Haidegger and L. Kovács and I. Felde and I.
Rudas},
booktitle={2013 IEEE 9th International Conference on Computational
Cybernetics (ICCC)},
title={The rising prospects of cloud robotic applications},
year={2013},
pages={327-332},
abstract={Cloud Robotics is an emerging field within robotics, currently
covering various application domains and robot network paradigms. This
paper provides a structured, systematic overview of the numerous
definitions, concepts and technologies linked to Cloud Robotics and
cloud technologies in a broader sense. It also presents a roadmap for
the near future, describing development trends and emerging application
areas. Cloud Robotics may have a significant role in the future as an
explicitly human-centered technology, capable of addressing the dire
needs of our society.},
keywords={cloud computing;robots;cloud robotic;cloud
technologies;human-centered technology;robot network paradigms;Cloud
computing;Computational modeling;Google;Robot sensing
systems;Servers;Service robots;cloud computing;cloud
robotics;distributed systems;networked robotics},
doi={10.1109/ICCCyb.2013.6617612},
month={July},}
@INPROCEEDINGS{6567422,
author={L. Turnbull and B. Samanta},
booktitle={2013 Proceedings of IEEE Southeastcon},
title={Cloud robotics: Formation control of a multi robot system
utilizing cloud infrastructure},
year={2013},
pages={1-4},
abstract={The pathway for the concept of cloud robotics is continually
unfolding and revealing new opportunities in science. With this, the
focus of the research paper is aimed at identifying the progress
completed towards the development of a full scale cloud infrastructure
to implement formation control on a multi robot system. A small scale
cloud infrastructure was developed utilizing a single virtual machine
operating with the boundaries of a hypervisor's resource pool. A robot
with minimal hardware was constructed to work within the control of the
cloud. Once the proof of concept on a lower tier has been completed,
more advance robotics concepts, such as Null-Spaced-base behavior
control and advanced neural network control, will be tested by
offloading the computational load to the cloud infrastructure. The goal
is to demonstrate the ability to simplify the robot hardware and
implement control on a global scale utilizing the cloud infrastructure.},
keywords={cloud computing;control engineering computing;multi-robot
systems;position control;advanced neural network control;cloud
robotics;formation control;full scale cloud infrastructure;hypervisor
resource pool;multirobot system;null-spaced-base behavior control;small
scale cloud infrastructure;virtual machine;Cloud
computing;Hardware;Robot sensing systems;Virtual machine
monitors;Virtual machining;Virtualization;Cloud robotics;cloud
computing;formation control;hypervisor;image
processing;swarm;virtualization;vision acquisition},
doi={10.1109/SECON.2013.6567422},
ISSN={1091-0050},
month={April},}
@INPROCEEDINGS{6494498,
author={B. Dhiyanesh},
booktitle={2012 International Conference on Emerging Trends in
Electrical Engineering and Energy Management (ICETEEEM)},
title={Dynamic resource allocation for machine to cloud communications
robotics cloud},
year={2012},
pages={451-454},
abstract={The computation and information sharing capabilities of
networked robotics by proposing a clond robotic architecture. The cloud
robotics architecture leverages the combination of a virtual ad-hoc
clond formed by machine-to-machine (M2M) communications among
participating robots, and an infrastructure cloud enabled by
machine-to-cloud (M2C) communications. Clond robotics utilizes elastic
computing models, in which resources are dynamically allocated from a
shared resource pool in the clond, to support task offloading and
information sharing In robotic applications. The propose communication
protocols, and several elastic computing models to handle different
applications. The technical challenges in computation, communications
and security, and illustrate the potential benefits of cloud robotics in
several applications.},
keywords={cloud computing;multi-robot systems;protocols;resource
allocation;software architecture;M2C communications;M2M
communications;cloud communication robotic cloud;cloud robotic
architecture;communication protocols;computation capabilities;dynamic
resource allocation;dynamical resource allocation;elastic computing
model;information sharing capabilities;infrastructure
cloud;machine-to-cloud communications;machine-to-machine
communications;networked robotics;shared resource pool;task
offloading;virtual ad hoc cloud;cloud computing;elastic
computing;machine-to-machine communication;robotics},
doi={10.1109/ICETEEEM.2012.6494498},
month={Dec},}
@INPROCEEDINGS{6491284,
author={L. Wang and M. Liu and M. Q. H. Meng},
booktitle={2012 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Towards cloud robotic system: A case study of online
co-localization for fair resource competence},
year={2012},
pages={2132-2137},
abstract={The cloud transforms the potential of robotics, which enable
poor-equipped robots to fulfill complex tasks. Robots are relieved from
hardware limitation, while large amount of available resources and
parallel computing capability are available in the “cloud”. We
implemented a data management system using Twisted-based server-client
platform and Robotic Operating System (ROS), aiming at co-localization
of cloud robots. However, resource competition is pervasive for
practical applications of networked robotics. As a major bridge, the
limited bandwidth becomes a bottleneck needs to be considered for the
architecture design. We propose an infrastructure which considers
multi-robot autonomous negotiation (MRAN) module. The framework is
validated by enabling several poor-equipped robots to retrieve location
data from a dynamically updated map which is built by a well-equipped
robot. Experiment results demonstrate that the proposed framework is
feasible for current robotic applications. Furthermore, it achieves
better performance under resource competition, and optimizes Quality of
Service (QoS) using a shared network with limited bandwidth.},
keywords={client-server systems;cloud computing;control engineering
computing;multi-robot systems;operating systems (computers);parallel
processing;quality of service;MRAN;QoS;ROS;architecture design;cloud
robotic system;data management system;fair resource competence;hardware
limitation;multirobot autonomous negotiation module;networked
robotics;online colocalization;parallel computing
capability;poor-equipped robots;quality of service;robotic operating
system;shared network;twisted-based server-client platform;well-equipped
robot},
doi={10.1109/ROBIO.2012.6491284},
month={Dec},}
@INPROCEEDINGS{6481240,
author={R. Doriya and P. Chakraborty and G. C. Nandi},
booktitle={2012 International Symposium on Cloud and Services Computing},
title={Robotic Services in Cloud Computing Paradigm},
year={2012},
pages={80-83},
abstract={Cloud computing and service oriented architecture (SOA) are
the dominant computing paradigm. Since past few years Robotics
applications have also started to build around these paradigms. This
paper presents entrance of robotic services in SOA and cloud computing.
Where a client/user can opt for the robotic services present at the
cloud like navigation, map building, object recognition etc. Map-reduce
computing cluster is also facilitated at the cloud to process large
amount of data for the cloud robotic services. The whole system follows
the Web 2.0 standard. We also reported the simulation results for
service based speech controlled robots with visual programming language
(VPL) of Microsoft Development Robotics Studio (MDRS) and implementation
of map-reduce computing cluster in robotic cloud.},
keywords={cloud computing;mobile robots;pattern
clustering;service-oriented architecture;speech recognition;visual
programming;MDRS;Microsoft Development Robotics Studio;SOA;VPL;Web 2.0
standard;cloud computing paradigm;cloud robotic services;data
processing;map-reduce computing cluster;service oriented
architecture;service-based speech controlled robots;visual programming
language;Cloud computing;Computational modeling;Navigation;Robot
kinematics;Service-oriented architecture;Speech;Cloud Computing;Cloud
Robotics;Cloud Robots;Robot as a Service;Robotic Services},
doi={10.1109/ISCOS.2012.24},
month={Dec},}
@INPROCEEDINGS{6480950,
author={D. Lorencik and P. Sincak},
booktitle={2013 IEEE 11th International Symposium on Applied Machine
Intelligence and Informatics (SAMI)},
title={Cloud robotics: Current trends and possible use as a service},
year={2013},
pages={85-88},
abstract={Since the cloud computing became widely available, lots of
previously algorithms and systems previously thought of as very time
consuming became instantly viable. For robotics and AI especially this
means that if the power behind the cloud could be harnessed, it would be
possible to build smaller, more battery effective robots because there
would be no need to have a powerful computer on board, but the brain of
the robot can be in the cloud. The idea of remote brain is not a new
one, though. Nevertheless, it is possible to create one now. Centralised
cloud for robot means that the memory can be nearly infinite, and
instantly available to other robots, so the process of learning and
exchanging the knowledge can be simplified. Also, this approach will
allow for easy change and upgrade of the methods used regardless of
robot hardware.},
keywords={cloud computing;control engineering computing;knowledge
acquisition;learning (artificial intelligence);robots;AI;artificial
intelligence;battery effective robot;centralised cloud;cloud
computing;cloud robotics;knowledge exchange;knowledge learning;remote
brain;robot hardware;Artificial intelligence;Cloud
computing;Computational modeling;Computer architecture;Robot sensing
systems},
doi={10.1109/SAMI.2013.6480950},
month={Jan},}
@INPROCEEDINGS{6427281,
author={J. Furrer and K. Kamei and C. Sharma and T. Miyashita and N.
Hagita},
booktitle={2012 IEEE/SICE International Symposium on System Integration
(SII)},
title={UNR-PF: An open-source platform for cloud networked robotic
services},
year={2012},
pages={945-950},
abstract={This paper gives an overview of UNR-PF, an open-source,
standards-based platform to enable multi-location daily life support
services using abstracted functionality from a cloud of robots. Such
services require seamless ability-adapted physical support and
interaction across multiple areas, provided through collaboration of
multiple robots, ubiquitous sensors and smartphones over a network.
UNR-PF provides a common infrastructure that separates development of
service applications and hardware-based components and intelligently
mediates between them. First, we summarize the requirements and design
goals initiated in previous work and then describe how the platform was
integrated by outlining the implementation of the internal modules and
APIs. Finally, we explain how to use UNR-PF by the example of a simple
life support scenario and discuss practical aspects of component and
service development for integration of a robotic service.},
keywords={application program interfaces;cloud
computing;groupware;multi-robot systems;public domain
software;sensors;service robots;smart phones;ubiquitous
computing;API;UNR-PF;ability-adapted physical support;abstracted
functionality;cloud networked robotic services;hardware-based
components;interaction across multiple areas;multilocation daily life
support services;multiple robot collaboration;open-source
standards-based platform;robot cloud;robotic service integration;service
applications;service development;smartphones;ubiquitous
sensors;Abstracts;Cleaning;Computer architecture;Robot kinematics;Robot
sensing systems;Service robots},
doi={10.1109/SII.2012.6427281},
month={Dec},}
@INPROCEEDINGS{6359111,
author={L. Wang and M. Q. H. Meng},
booktitle={Proceedings of the 10th World Congress on Intelligent Control
and Automation},
title={A game theoretical bandwidth allocation mechanism for cloud
robotics},
year={2012},
pages={3828-3833},
abstract={Cloud robotics is currently driving interest in both academia
and industry, since it would allow robots to off-load computation
intensive tasks, combine with multiple robots and even download new
skills. Bandwidth allocation is the fundamental and dominant task for
resource sharing among users in cloud robotics. However, many technical
challenges are still outstanding, since incast congestion happens in
high-bandwidth and low-latency networks, when multiple synchronized
users send data to a same receiver in parallel [1]. In this paper, we
introduce a resource allocation framework for cloud robotics, and
propose a game-theoretic problem formulation and linear pricing scheme
of bandwidth allocation, we also implement a congestion control
algorithm by using optimal parameters derived from the game-theoretic
algorithm. Simulation results demonstrate that the proposed mechanism
achieves better performance of bandwidth allocation in cloud robotics
scenarios.},
keywords={bandwidth allocation;cloud computing;control engineering
computing;game theory;multi-robot systems;resource
allocation;telecommunication congestion control;telecommunication
network management;cloud robotics;computation intensive tasks;congestion
control algorithm;game theoretical bandwidth allocation
mechanism;game-theoretic algorithm;game-theoretic problem
formulation;high-bandwidth networks;incast congestion;linear pricing
scheme;low-latency networks;multiple robots;multiple synchronized
users;optimal parameters;resource allocation framework;resource
sharing;Bandwidth;Channel allocation;Pricing;Resource management;Service
robots;Throughput;Cloud Robotics;Complete Information Pricing
Scheme;Congestion Control;Resource Allocation},
doi={10.1109/WCICA.2012.6359111},
month={July},}
@INPROCEEDINGS{6343054,
author={L. Wang and M. Liu and M. Q. H. Meng and R. Siegwart},
booktitle={2012 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Towards real-time multi-sensor information retrieval in Cloud
Robotic System},
year={2012},
pages={21-26},
abstract={Cloud Robotics is currently driving interest in both academia
and industry. It allows different types of robots to share information
and develop new skills even without specific sensors. They can also
perform intensive tasks by combining multiple robots with a cooperative
manner. Multi-sensor data retrieval is one of the fundamental tasks for
resource sharing demanded by Cloud Robotic system. However, many
technical challenges persist, for example Multi-Sensor Data Retrieval
(MSDR) is particularly difficult when Cloud Cluster Hosts accommodate
unpredictable data requested by multi robots in parallel. Moreover, the
synchronization of multi-sensor data mostly requires near real-time
response of different message types. In this paper, we describe a MSDR
framework which is comprised of priority scheduling method and buffer
management scheme. It is validated by assessing the quality of service
(QoS) model in the sense of facilitating data retrieval management.
Experiments show that the proposed framework achieves better performance
in typical Cloud Robotics scenarios.},
keywords={buffer storage;cloud computing;control engineering
computing;multi-robot systems;quality of service;scheduling;sensor
fusion;QoS model;buffer management scheme;cloud cluster host;cloud
robotic system;multiple robot;multisensor data retrieval;multisensor
information retrieval;priority scheduling method;quality of service
model;resource sharing;Bandwidth;Databases;Protocols;Quality of
service;Resource management;Robot sensing systems},
doi={10.1109/MFI.2012.6343054},
month={Sept},}
@INPROCEEDINGS{6339517,
author={I. J. Rudas},
booktitle={2012 IEEE 10th Jubilee International Symposium on Intelligent
Systems and Informatics},
title={Cloud computing in intelligent robotics},
year={2012},
pages={15-15},
abstract={Summary form only given. The presentation summarizes the
basics of cloud computing, namely the main idea, the definition, the
cloud model composed of essential characteristics, service models and
deployment models. In the second d part of the presentation the possible
applications of cloud computing in robotics are outlined with special
emphases to robots as a service in cloud computing. Finally some cloud
robotics projects, including the European project RoboEarth are
discussed.},
keywords={cloud computing;control engineering computing;intelligent
robots;European project RoboEarth;cloud computing;cloud model;cloud
robotics projects;deployment models;intelligent robotics;robots as a
service;service models;Abstracts;Cloud computing;Computational
modeling;Educational institutions;Informatics;Intelligent systems;Robots},
doi={10.1109/SISY.2012.6339517},
ISSN={1949-047X},
month={Sept},}
@INPROCEEDINGS{6215727,
author={W. Adiprawita and A. R. Ibrahim},
booktitle={2012 International Conference on Cloud Computing and Social
Networking (ICCCSN)},
title={Service oriented architecture in robotic as a platform for cloud
robotic (Case study: Human gesture based teleoperation for upper part of
humanoid robot)},
year={2012},
pages={1-4},
abstract={A service oriented architecture with Robot Operating System is
developed for upper part humanoid robot teleoperation with human gesture
recognition. This architecture enables the system to distributed in
nodes do physically do not change the humanoid weight.},
keywords={cloud computing;control engineering computing;gesture
recognition;humanoid robots;operating systems
(computers);service-oriented architecture;telerobotics;cloud robotic
platform;for upper part humanoid robot teleoperation;human gesture based
teleoperation;human gesture recognition;robot operating system;service
oriented architecture;Humanoid robots;Humans;Kinematics;Operating
systems;Robot sensing systems;Service oriented architecture;Aldebaran
NAO;Humanoid Robot;Kinect;ROS;Robot Operating System;Service Oriented
Architecture for Robotics;Teleoperation},
doi={10.1109/ICCCSN.2012.6215727},
month={April},}
@ARTICLE{6201212,
author={G. Hu and W. P. Tay and Y. Wen},
journal={IEEE Network},
title={Cloud robotics: architecture, challenges and applications},
year={2012},
volume={26},
number={3},
pages={21-28},
abstract={We extend the computation and information sharing capabilities
of networked robotics by proposing a cloud robotic architecture. The
cloud robotic architecture leverages the combination of an ad-hoc cloud
formed by machine-to-machine (M2M) communications among participating
robots, and an infrastructure cloud enabled by machine-to-cloud (M2C)
communications. Cloud robotics utilizes an elastic computing model, in
which resources are dynamically allocated from a shared resource pool in
the ubiquitous cloud, to support task offloading and information sharing
in robotic applications. We propose and evaluate communication
protocols, and several elastic computing models to handle different
applications. We discuss the technical challenges in computation,
communications and security, and illustrate the potential benefits of
cloud robotics in different applications.},
keywords={cloud computing;control engineering computing;mobile
robots;multi-robot systems;resource allocation;software
architecture;ubiquitous computing;M2C communications;M2M
communications;ad-hoc cloud;cloud robotic architecture;communication
protocol evaluation;computation capabilities;dynamic shared resource
pool allocation;elastic computing model;information sharing
capabilities;infrastructure cloud;machine-to-cloud
communications;machine-to-machine communications;networked robotics;task
offloading;ubiquitous cloud;Cloud computing;Networked control
systems;Robot kinematics;Robot sensing systems},
doi={10.1109/MNET.2012.6201212},
ISSN={0890-8044},
month={May},}
@ARTICLE{6201213,
author={K. Kamei and S. Nishio and N. Hagita and M. Sato},
journal={IEEE Network},
title={Cloud networked robotics},
year={2012},
volume={26},
number={3},
pages={28-34},
abstract={This article proposes a new field of research called Cloud
Networked Robotics, which tackles the issues for supporting daily
activity, especially for the elderly and the disabled, throughout
various locations in a continuously and seamless manner by abstracting
robotic devices and providing a means for utilizing them as a cloud of
robots. With recent advances in robotic development environments and in
integrated multi-robot systems, robots are acquiring richer
functionalities and robotic systems are becoming much easier to develop.
However, such stand-alone robotic services are not enough for
continuously and seamlessly supporting daily activity. We examine the
requirements in typical daily supporting services through example
scenarios that target senior citizens and the disabled. Based on these
requirements, we discuss the key research issues in cloud network
robotics. As a case study, a field experiment in a shopping mall shows
how our proposed prototype infrastructure of cloud networked robotics
enables multi-location robotic services for life support.},
keywords={cloud computing;multi-robot systems;service robots;abstracting
robotic device;cloud networked robotics;daily activity;integrated
multirobot system;life support;multilocation robotic services;robotic
development environment;robotic system;robots;stand-alone robotic
services;Cloud computing;Mobile robots;Multirobot systems;Networked
control systems;Robot kinematics;Service robots;Cloud networked robotics},
doi={10.1109/MNET.2012.6201213},
ISSN={0890-8044},
month={May},}
@INPROCEEDINGS{6118734,
author={L. Agostinho and L. Olivi and G. Feliciano and F. Paolieri and
D. Rodrigues and E. Cardozo and E. Guimaraes},
booktitle={2011 IEEE Ninth International Conference on Dependable,
Autonomic and Secure Computing},
title={A Cloud Computing Environment for Supporting Networked Robotics
Applications},
year={2011},
pages={1110-1116},
abstract={This paper presents the design, implementation, and evaluation
of a cloud computing environment for networked robotics. The environment
aims to support distributed robotics applications that require large
shares of computing resources and very low communication delays.
Networked robotics applications are built above these services and run
on virtualized environments deployed on processing nodes directly
connected to the robotic resources manipulated by the applications. The
cloud computing environment offers the REALabs platform according to the
Platform as a Service (PaaS) model of cloud computing. Also, we show an
approach to evaluate robotic experiments by submitting tasks in a
workflow cloud system.},
keywords={cloud computing;control engineering computing;robots;PaaS
model;REALabs platform;cloud computing;communication delay;distributed
robotics application;networked robotics;platform as a service;workflow
cloud system;Cloud computing;Protocols;Quality of service;Robot sensing
systems;Servers;Cloud Computing;Cloud Robotics;Cloud Workflow;Networked
Robotics;Platform as a Service},
doi={10.1109/DASC.2011.181},
month={Dec},}
@INPROCEEDINGS{6114210,
author={F. Abidi},
booktitle={2011 World Congress on Sustainable Technologies (WCST)},
title={Cloud computing and its effects on healthcare, robotics, and
piracy},
year={2011},
pages={135-140},
abstract={Cloud computing, in spite of a few issues associated with it,
is being proposed as the panacea of many IT challenges. The various
delivery models of it provide a unique way of sharing resources which
was never the case before. The adoption of the various delivery models
will in turn help in reducing piracy levels. The health industry which,
at present, is plagued by issues like high cost etc is bound to benefit
the most. The robotics industry as well can make use of cloud computing
in making the robots cheaper, faster, and lighter.},
keywords={cloud computing;computer crime;health care;resource
allocation;robots;cloud computing;health
industry;healthcare;piracy;resource sharing;robotics
industry;Computers;Irrigation;Robots},
month={Nov},}
@INPROCEEDINGS{6005269,
author={K. Petersen and K. Fukui and Z. Lin and N. Endo and E. Kazuki
and H. Ishii and M. Zecca and A. Takanishi and T. Asfour and R. Dillmann},
booktitle={2011 RO-MAN},
title={Towards high-level, cloud-distributed robotic telepresence:
Concept introduction and preliminary experiments},
year={2011},
pages={131-136},
abstract={In this paper we propose the basic concept of a tele-presence
system for two (or more) anthropomorphic robots located in remote
locations. As one robot interacts with a user, it acquires knowledge
about the user's behavior and transfers this knowledge to the network.
The robot in the remote location accesses this knowledge and according
to this information emulates the behavior of the remote user when
interacting with its partner. The behavioral patterns are grouped into
macro-behavior units (maBUs) and mirco-behavior units (miBUs). maBUs
carry information that is specific to a person-to-person communication.
miBUs are commonly used behavior patterns within a certain cultural
environment (e.g. handshake, bow etc). maBUs are usually chains of
miBUs. miBUs are chains of expression actions (e.g. gesture, facial
expression etc.). The idea behind this is, that human communication
contains several levels or layers of information exchange. This to be
emulated by implementing the concept of miBUs and maBUs. We present a
preliminary application of this concept to a musical context. The
rhythmic motion of a drum-stick during the performance of a drum rhythm
by a musician is recorded by a inertial measurements unit and
transmitted between two far distance locations (Waseda University in
Japan and Karlsruhe Institute of Technology in Germany) using three
different transmission methods: direct raw data transmission, miBU based
transmission and maBU-based transmission. We present experimental
results that show quantitative data to evaluate the suitability of our
approach, with the overall goal to implement a telepresence system of
larger scale.},
keywords={human-robot interaction;music;telerobotics;anthropomorphic
robot;behavior emulating;cloud-distributed robotic telepresence;direct
raw data transmission;drum-stick;inertial measurements unit;information
exchange;macrobehavior units-based transmission;mircobehavior
units-based transmission;musical context;person-to-person
communication;remote user;rhythmic motion;robot interaction;Data
communication;Delay;Humans;Libraries;Receivers;Rhythm;Robots},
doi={10.1109/ROMAN.2011.6005269},
ISSN={1944-9445},
month={July},}
@INPROCEEDINGS{5675289,
author={P. Curtis and P. Payeur},
booktitle={2010 IEEE International Workshop on Robotic and Sensors
Environments},
title={A method to segment a 3D surface point cloud for selective
sensing in robotic exploration},
year={2010},
pages={1-6},
abstract={Autonomous robotic exploration in a 3D environment requires
the acquisition of 3D data to create a consistent internal model of the
environment from which objects can be recognized for the robot to
interact with. As the acquisition of 3D data with stereo vision or a
laser range finder can be a relatively long process, selective sensing
is desired to optimize the amount of data collected to accurately
represent the environment in a minimal amount of time. In order to
perform selective sensing, a coarse acquisition of the environment first
needs to take place. Regions of interest, such as edges and other
boundaries, can then be identified so that an acquisition with higher
spatial resolution can occur over bounded regions. For that purpose a
segmentation method of the coarse data is proposed so that regions can
be efficiently distinguished from each other. The method takes a raw 3D
surface profile point cloud of varying point densities, organizes it
into a mesh, and then segments the surfaces present in this point cloud,
producing a segmented mesh, as well as an octree of labeled voxels
corresponding to the segmentation. This mesh and octree may then be used
for sensory selection to drive a robot exploration task. The method is
demonstrated on actual datasets collected in a laboratory environment.},
keywords={computer graphics;image segmentation;octrees;robot vision;3D
surface point cloud;autonomous robotic exploration;octree;robot
exploration task;segmentation method;selective sensing;sensory
selection;Image edge detection;Merging;Octrees;Robot sensing
systems;Three dimensional displays;3D modeling;octrees;robotic
exploration;segmentation;selective sensing},
doi={10.1109/ROSE.2010.5675289},
month={Oct},}
@INPROCEEDINGS{1301423,
author={N. Lomenie},
booktitle={First Canadian Conference on Computer and Robot Vision, 2004.
Proceedings.},
title={A generic methodology for partitioning unorganised 3D point
clouds for robotic vision},
year={2004},
pages={64-71},
abstract={Range image segmentation has many applications in computer
vision areas such as computer graphics and robotic vision. A generic
methodology for 3D point set analysis in which planar structures play an
important role is defined. It consists mainly of a specific K-means
algorithm which is able to process different shapes in cluster. At the
same time, within geometric and topologic considerations, a set of
application-driven heuristics is designed. This helps to find out the
right number of structures in point sets in order to give a good
visualization and representation of a large scale environment without a
priori models. Our aim is to propose a simple and generic frame for 3D
scene understanding. Tests were realised on different types of
environment data: natural and man-made. This research project has been
realized with EADS (French Air Space Society).},
keywords={Application software;Clouds;Clustering algorithms;Computer
vision;Image reconstruction;Image segmentation;Intelligent robots;Robot
kinematics;Robot vision systems;Shape},
doi={10.1109/CCCRV.2004.1301423},
month={May},}
@ARTICLE{7057679,
author={J. Civera and M. Ciocarlie and A. Aydemir and K. Bekris and S.
Sarma},
journal={IEEE Transactions on Automation Science and Engineering},
title={Guest Editorial: Special Issue on Cloud Robotics and Automation},
year={2015},
volume={12},
number={2},
pages={396-397},
abstract={The articles in this special section focus on the use of cloud
computing in the robotics industry. The Internet and the availability of
vast computational resources, ever-growing data and storage capacity
have the potential to define a new paradigm for robotics and automation.
An intelligent system connected to the Internet can expand its onboard
local data, computation and sensors with huge data repositories from
similar and very different domains, massive parallel computation from
server farms and sensor/actuator streams from other robots and automata.
It is the potential and also the research challenges of the field that
become the focus on this special section. The goal is to group together
and to show the state-of-the-art of this newly emerged field, identify
the relevant advances and topics, point out the current lines of
research and potential applications, and discuss the main research
challenges and future work directions.},
keywords={Cloud computing;Crowdsourcing;Knowledge based systems;Learning
systems;Robot sensing systems;Special issues and sections},
doi={10.1109/TASE.2015.2409511},
ISSN={1545-5955},
month={April},}
@INPROCEEDINGS{7876208,
author={M. Horton and Lei Chen and B. Samanta},
booktitle={2017 International Conference on Computing, Networking and
Communications (ICNC)},
title={Enhancing the security of IoT enabled robotics: Protecting
TurtleBot file system and communication},
year={2017},
pages={662-666},
abstract={The recently emerged Internet of Things (IoT) has already
become largely prevalent in today's society. However, strong and
fundamental security practices have not been applied to fully protect
these systems, partially negating the benefits of IoT. The concept has
grown so much in recent years that this lack of a security
infrastructure can no longer be ignored. Recent developments in the
field of IoT and robotics have proven invaluable in building a solid
framework for what lies ahead. This paper focuses on the examination and
enhancement of security between IoT enabled robots, specifically in this
project, TurtleBots, and the cloud infrastructure supporting them by
providing a combined set of security best practices for robotic file
systems and communications.},
keywords={Internet of Things;Linux;cloud computing;robot
programming;security of data;Internet of Things;IoT enabled robotics
security;Linux;TurtleBot file system protection;cloud
infrastructure;communication protection;security enhancement;security
infrastructure;Cloud Robotics;IoT;Linux;ROS;Security;TurtleBot;UFW;Ubuntu},
doi={10.1109/ICCNC.2017.7876208},
month={Jan},}
@INPROCEEDINGS{7813429,
author={A. Rahman and J. Jin and Y. W. Wong and K. S. Lam},
booktitle={2016 International Conference on Advanced Mechatronic Systems
(ICAMechS)},
title={Development of a cloud-enhanced investigative mobile robot},
year={2016},
pages={104-109},
abstract={In recent times the cloud based applications for robotics has
become an emerging topic of discussion. Both individual as well as
networked robotic operation can be benefitted with the support of cloud
computing paradigm. The different applications of the cloud enhance its
integration with various aspects of robotic operation. However, it
depends on the scope of the applications as well as the setup of the
robotic system. This has led to our work in this paper that includes a
hardware implementation of a robot with cloud-aided applications of face
detection and identification for the purpose of interaction and
investigation. We explain our face recognition algorithm that we have
implemented as well as our methods of using it for context-aware
offloading to the cloud. In addition to that, we present an autonomous
platform switching algorithm that is network and context aware in terms
of task allocation between resources. Our practical implementation and
demonstrations analyse the impact of cloud in the performance of this
application. From the results, we can suggest that cloud-based approach
is beneficial for performance and longevity of the system.},
keywords={cloud computing;face recognition;mobile robots;autonomous
platform switching algorithm;cloud computing paradigm;cloud-enhanced
investigative mobile robot;context-aware offloading;face detection;face
recognition algorithm;Cloud computing;Face detection;Face
recognition;Hardware;Robot sensing systems;Switches;Cloud
Computing;Cloud Robotics;Context-Aware Offloading;Face
Recognition;Investigation;Platform Switching Algorithm},
doi={10.1109/ICAMechS.2016.7813429},
month={Nov},}
@INPROCEEDINGS{7776601,
author={M. Bottone and F. Palumbo and G. Primiero and F. Raimondi and R.
Stocker},
booktitle={2016 5th IEEE International Conference on Cloud Networking
(Cloudnet)},
title={Implementing Virtual Pheromones in BDI Robots Using MQTT and
Jason (Short Paper)},
year={2016},
pages={196-199},
abstract={Robotic coordination is a crucial issue in the development of
many applications in swarm robotics, ranging from mapping unknown and
potentially dangerous areas to the synthesis of plans to achieve complex
tasks such as moving goods between locations under resource constraints.
In this context, stigmergy is a widely employed approach to robotic
coordination based on the idea of interacting with the environment by
means of markers called pheromones. Pheromones do not need to be
"physical marks", and a number of works have investigated the use of
digital, virtual pheromones. In this paper, we show how the concept of
virtual pheromones can be implemented in Jason, a Java-based interpreter
for an extended version of AgentSpeak, providing a high-level modelling
and execution environment for multi-agent systems. We also exploit MQTT,
a messaging infrastructure for the Internet-of-Things. This allows the
implementation of stigmergic algorithms in a high-level declarative
language, building on top of low-level infrastructures typically used
only for controlling sensors and actuators.},
keywords={Internet of Things;Java;multi-robot systems;program
interpreters;AgentSpeak;BDI robots;Internet-of-Things;Jason;Java-based
interpreter;MQTT;actuator control;digital-virtual pheromone
implementation;execution environment;high-level declarative
language;high-level modelling;low-level infrastructures;messaging
infrastructure;multiagent systems;pheromone markers;physical
marks;resource constraints;robotic coordination;sensor
control;stigmergic algorithms;swarm robotics;Cloud computing;Multi-agent
systems;Planning;Quality of service;Robot kinematics;Sensors;cloud
robotics;multi-agent systems;stigmergy},
doi={10.1109/CloudNet.2016.22},
month={Oct},}
@INPROCEEDINGS{7734049,
author={D. M. Lofaro},
booktitle={2016 13th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI)},
title={Secure robotics},
year={2016},
pages={311-313},
abstract={Security is an under-studied problem within robotics and
Internet of Things. Part of the reason for this is that currently most
robots and IoT devices remain in the lab at all times. Recent trends
show more robots and IoT devices moving “out into the wild” with no
humans to protect them. This creates vulnerabilities beyond the well
known and well studied network/internet based threat. These threats
include external network, local network, software, physical access,
tricking the artificial intelligence, and intellectual property theft.
This document discribes the above and shows our current work towards
detection and mitigation.},
keywords={Internet of Things;cloud computing;control engineering
computing;robots;security of data;Internet of Things;IoT;cloud
robotics;secure robotics;Reliability;Robots;Surges;Cloud
Robotics;Real-Time;Secure Robotics},
doi={10.1109/URAI.2016.7734049},
month={Aug},}
@INPROCEEDINGS{7718216,
author={N. Yoshikane and T. Sato and Y. Isaji and C. Shao and T. Marco
and S. Okamoto and T. Miyazawa and T. Ohshima and C. Yokoyama and Y.
Sumida and H. Sugiyama and M. Miyabe and T. Katagiri and N. Kakegawa and
S. Matsumoto and Y. Ohara and I. Satou and A. Nakamura and S. Yoshida
and K. Ishii and S. Kametani and J. Nicho and J. Meyer and S. Edwards
and P. Evans and T. Tsuritani and H. Harai and M. Razo and D. Hicks and
A. Fumagalli and N. Yamanaka},
booktitle={2016 21st OptoElectronics and Communications Conference
(OECC) held jointly with 2016 International Conference on Photonics in
Switching (PS)},
title={First demonstration of geographically unconstrained control of an
industrial robot by jointly employing SDN-based optical transport
networks and edge compute},
year={2016},
pages={1-3},
abstract={Geographically unconstrained remote control of an industrial
robot for surface blending over multi-domain SDN-based optical transport
networks is demonstrated, showing that cloud/edge computing technology
improves controllability of the industrial robot.},
keywords={Cloud computing;Optical fiber networks;Optical surface
waves;Robot control;Servers;Service robots;Cloud robotics;Cloud/Edge
computing;Industrial robot;Optical transport network;Software-Defined
Networking},
month={July},}
@INPROCEEDINGS{7398709,
author={Y. Hagiwara},
booktitle={2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)},
title={Cloud based VR system with immersive interfaces to collect
multimodal data in human-robot interaction},
year={2015},
pages={256-259},
abstract={This paper presents a cloud based VR system with immersive
interfaces to collect multimodal data in human-robot interaction and its
applications. The proposed system enables a subject to log in to the VR
space as an avatar and to naturally interact with a virtual robot by
immersive interfaces. A head mounted display and a motion capture device
provide immersive visualization and natural motion control in the VR
system, respectively. The proposed VR system can simultaneously perform
natural human-robot interaction in a VR space and collect visual,
physical, and voice data during human-robot interaction by the immersive
interfaces. Two application experiments to learn object's attributes and
to learn communication protocol demonstrate the availability of the
proposed system.},
keywords={avatars;cloud computing;data acquisition;data
visualisation;graphical user interfaces;helmet mounted
displays;human-robot interaction;VR space;avatar;cloud-based VR
system;head mounted display;human-robot interaction;immersive
interface;immersive visualization;motion capture device;multimodal data
collection;natural motion control;virtual robot;Aerospace
electronics;Avatars;Human-robot interaction;Protocols;Robot sensing
systems;Shape;cloud robotics;human-robot interaction;multimodal
data;virtual reality},
doi={10.1109/GCCE.2015.7398709},
month={Oct},}
@INPROCEEDINGS{7329755,
author={P. Sinčák and E. Novotná and T. Cádrik and G. Magyar and M. Mach
and F. Cavallo and M. Bonaccorsi},
booktitle={2015 IEEE 19th International Conference on Intelligent
Engineering Systems (INES)},
title={Cloud-based Wizard of Oz as a service},
year={2015},
pages={445-448},
abstract={The paper deals with theoretical and experimental issues of an
idea towards a cloud-based Wizard of Oz in the Microsoft Azure cloud
environment. Wizard of Oz is a common tool in social robotics and
especially in specific applications like mental illness treatment,
ambient assisted living, and many others. The final goal is to create a
system with the ability to learn and replace a human wizard by an
intelligent software agent, which simulates the behavior of the human.},
keywords={cloud computing;intelligent robots;software agents;Microsoft
Azure cloud environment;WoOz;ambient assisted living;cloud-based Wizard
of Oz;intelligent robotics;intelligent software agent;mental illness
treatment;social robotics;Artificial intelligence;Cloud
computing;Protocols;Robots;Servers;Standards;Testing;Software as a
Service;Wizard of Oz;cloud computing;cloud robotics;human
behavior;intelligent agent},
doi={10.1109/INES.2015.7329755},
month={Sept},}
@INPROCEEDINGS{7325508,
author={L. Giuliano and M. E. Kaouk Ng and M. L. Lupetti and C. Germak},
booktitle={2015 7th International Conference on Intelligent Technologies
for Interactive Entertainment (INTETAIN)},
title={Virgil, robot for museum experience: Study on the opportunity
given by robot capability to integrate the actual museum visit},
year={2015},
pages={222-223},
abstract={Robotics platforms are becoming more and more present in
people everyday life. A reflection on multimedia technologies that are
currently used in the museum experiences has been made. In this paper,
we present and describe a robotic system, called “Virgil” and a remote
tele-operation application used as a support instrument for the museum
guide in order to enhance the museum experience and increase the
cultural value of the territorial heritage. The application is based on
a Cloud Robotic infrastructure that contains ROS nodes and exposes a set
of APIs to the user.},
keywords={application program interfaces;history;intelligent
robots;mobile robots;museums;telerobotics;API;ROS node;Virgil;cloud
robotic infrastructure;multimedia technology;museum;remote teleoperation
application;robotic system;territorial heritage;Cloning;Cloud
computing;Cultural differences;Mobile robots;Multimedia
communication;Robot sensing systems;Cloud Robotics Platform;Human
Robotic Interaction;Intelligence Robots;Robot mobile Telepresence},
month={June},}
@INPROCEEDINGS{7295801,
author={S. Skibinski and J. H. Terhorst and F. Weichert and H. Müller},
booktitle={2015 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Large-scale fusion of collective, areal vehicle data},
year={2015},
pages={152-159},
abstract={In this paper we present a novel approach to the large-scale
fusion of collaboratively acquired areal sensor data from a vehicle
fleet, like temperatures, illuminations, frictions, traffic densities,
signal strengths, air qualities, etc., which are intended for the
incorporation into next-generation comfort functions and driver
assistance systems. Our algorithm is based on the interpolation via
RBFNs, but is extended, so that it fulfills the additional demands of
the automotive industry. Therefore, we examine how to tackle
large-scale, integrate timestamp-dependent weights, incorporate elliptic
basis functions and realize an incremental computation. Finally, we
investigate how our incremental approach to interpolation can be further
improved by incorporating the Fast Gauss Transform, resulting in a
reduction of the computation time by a factor of ten, if a batch insert
or update (e. g. in the case of temporal reweighting) has to be applied.
Our suggested approach to double-staged interpolation is applied
exemplary to friction data, but can be also applied to other kinds of
areal data as enumerated prior. Further, we present, that the first
stage of the double-staged interpolation can be utilized for the
determination of lane geometries, which can be consecutively used to
annotate fused areal data with appropriate lane affiliations.},
keywords={Gaussian distribution;automobiles;driver information
systems;geometry;interpolation;radial basis function networks;sensor
fusion;transforms;RBFN;automotive industry;comfort
function;double-staged interpolation;driver assistance system;fast Gauss
transform;lane geometry determination;radial basis function
network;vehicle data
fusion;Geometry;Histograms;Interpolation;Mathematical
model;Transforms;Uncertainty;Vehicles;Cloud Robotics;Determination of
Lane Geometries;Fast Gauss Transform;Large-Scale Fusion of Areal
Data;Temporal Weighting},
doi={10.1109/MFI.2015.7295801},
month={Sept},}
@INPROCEEDINGS{7279535,
author={G. Tian and H. Chen and F. Lu},
booktitle={2015 IEEE International Conference on Information and
Automation},
title={Cloud computing platform based on intelligent space for service
robot},
year={2015},
pages={1562-1566},
abstract={In order to develop a cloud computing platform which provides
a compute cluster built with commodity hardware exposing some robotic
algorithms and share data co-operatively across the robotic ecosystem.
This paper proposes a computing platform that provides the scalability
and parallelism advantages of cloud computing for service robots in
actual environments. The platform is implemented such a system around
the intelligent space. This platform integrates sensor dates and human
knowledge. Knowledge and information is extracted, processed and
restored in separate storage modules of the platform. The computing
environments allow the robots in different locations to easily access
the knowledge library remotely in the cloud layer. It helps robots to
offload heavy computation by providing secured customizable computing
environments in the cloud. By utilizing this platform, the robot is able
to perceive the environment more comprehensive, study from each other's
experience and provide better service to the people.},
keywords={cloud computing;control engineering computing;intelligent
robots;knowledge acquisition;service robots;telerobotics;cloud computing
platform;information extraction;intelligent space;knowledge
extraction;knowledge library;robotic algorithms;sensor dates;service
robot;storage modules;Cloud computing;Computer
architecture;Libraries;Robot kinematics;Robot sensing systems;Service
robots;Cloud robotics;Computing platform;Intelligent space;Micro cloud
layer},
doi={10.1109/ICInfA.2015.7279535},
month={Aug},}
@INPROCEEDINGS{7280891,
author={K. Al-Mutib},
booktitle={2014 5th International Conference on Intelligent Systems,
Modelling and Simulation},
title={Smart Stereovision Based Gaze Control for Navigation in
Low-Feature Unknown Indoor Environments},
year={2014},
pages={121-126},
abstract={State-of-the-art mobile robot navigation systems usually
consider Gaze control and locomotion planning to be independent modules.
We present an integrated gaze control strategy that maximizes the
collection of relevant 3D measurements of environment. The presented
work results in a more reliable environment representation compared to
state-of-the-art approaches for unknown, dynamic indoor environment. In
our particular approach, Grid based mapping is employed for path
planning purposes. Such an approach demands a sufficient amount of 3D
information from stereovision sensors for effective navigation,
especially in environments with relatively less number of features such
as plain walls and floors. We conduct several experiments in such
environments and compare our results with contemporary stereovision
based mapping and path planning approaches. We present results of
multiple experiments, to show the effectiveness of our system to be able
to navigate in low-feature environments using solely, stereovision based
gaze control strategy for path planning.},
keywords={mobile robots;navigation;path planning;robot vision;stereo
image processing;gaze control;grid based mapping;locomotion
planning;low-feature unknown indoor environments;mobile
robot;navigation;path planning;smart stereovision;stereovision
sensors;Cameras;Collision avoidance;Navigation;Robot kinematics;Robot
vision systems;component; robotics; navigation; Stereo-vision; gaze
control; point cloud filtering; multi-baseline ; pathplanning},
doi={10.1109/ISMS.2014.163},
ISSN={2166-0662},
month={Jan},}
@INPROCEEDINGS{7161824,
author={G. Li and H. Wang and X. Ying and J. Liu},
booktitle={The 27th Chinese Control and Decision Conference (2015 CCDC)},
title={A proxy-based cloud infrastructure for home service robots},
year={2015},
pages={5718-5723},
abstract={With the emerging of cloud computing, a growing body of work
is focused on cloud robotics. In this paper, robots and the other
devices in home are regarded as service supplier or service consumer and
are packaged as RaaS (Robot as a Service) units. We design the RaaS Unit
Module and propose a proxy-based cloud infrastructure for the home
service robots. In addition, by modeling the disconnection times as a
Poisson Process, a simple offloading strategy about whether it is
suitable to offload a task to cloud is analyzed. To verify the
feasibility of this infrastructure, a specific instruction set is
developed to call cloud robot's service via Lua Programming Language. By
using iFLY voice cloud which is a third application, speech recognition
is implemented to allow users to call robot service by voice input,
which illustrates that this cloud robot can utilize cloud resources.},
keywords={cloud computing;home automation;programming languages;service
robots;speech recognition;stochastic processes;Lua programming
language;Poisson process;RaaS unit module;cloud computing;cloud
robotics;home service robot;iFLY voice cloud;proxy-based cloud
infrastructure;robot-as-a-service;service consumer;service
supplier;speech recognition;Cloud computing;Robot sensing systems;Sensor
phenomena and characterization;Service robots;Temperature sensors;Cloud
Robotics;Lua Programming Language;Offloading Strategy;Proxy-based
Model;RaaS Unit Module;Speech Recognition;XiaoNan Home Service Robot},
doi={10.1109/CCDC.2015.7161824},
ISSN={1948-9439},
month={May},}
@INPROCEEDINGS{7151907,
author={P. Benavidez and M. Kumar and S. Agaian and M. Jamshidi},
booktitle={2015 10th System of Systems Engineering Conference (SoSE)},
title={Design of a home multi-robot system for the elderly and disabled},
year={2015},
pages={392-397},
abstract={Home-based assistive robotic care for the elderly and disabled
has long been a goal of robotics researchers. Unfortunately, no single
group has solved the problem of making robots that will perform a set of
tasks sufficient enough to warrant the cost to the end consumer.
Numerous advances and improvements in computing, communication and
related robotic technologies have been paving the way towards cheaper,
more capable robots. We propose a home robot system consisting of a set
of heterogeneous robots with different task spaces, cloud computing to
enhance the abilities of the system, integration with existing home
infrastructure, and compatibility with mobile technology. A high level
of integration with the open source software of the Robot Operating
System (ROS) is proposed to accelerate the design process. For the exact
types of robots, we propose to use an enhanced floor cleaning robot and
a mobility and vision assistance robot in the form of an improved
rollator walker.},
keywords={assisted living;cloud computing;handicapped aids;mobile
robots;multi-robot systems;operating systems (computers);public domain
software;service robots;ROS;cloud computing;design process;floor
cleaning robot;heterogeneous robots;home infrastructure;home multirobot
system design;home-based assistive robotic care;improved rollator
walker;mobile technology;open source software;robot operating
system;robotic technology;vision assistance robot;Cloud computing;Mobile
robots;Portals;Robot sensing systems;Systems engineering and
theory;ROS;assistive robotics;cloud robotics;indoor robot;service
robot;vSLAM},
doi={10.1109/SYSOSE.2015.7151907},
month={May},}
@INPROCEEDINGS{7148524,
author={R. Doriya and S. Mishra and S. Gupta},
booktitle={International Conference on Computing, Communication
Automation},
title={A brief survey and analysis of multi-robot communication and
coordination},
year={2015},
pages={1014-1021},
abstract={Robot navigation is one of the basic problem in robotics. In
the field of multi robot systems, it has become an interesting area for
researchers to design algorithms that co-ordinate the movement and
control the communication of multiple robots. A big challenge in this
area is to design a suitable framework which makes multiple robots to
work as a team to perform their task and reach their goal. In this
paper, we present a brief survey of various researches being carried out
in the field of multi-robot coordination and communication. We also
present a framework for multi robot coordination and communication with
the help of cloud. In this framework, Particle Swarm Optimization (PSO)
has been used for coordination whereas Cluster Head Gateway Switch
Routing (CGSR) protocol is used for communication among the robots.},
keywords={cloud computing;control engineering
computing;internetworking;mobile robots;multi-robot systems;particle
swarm optimisation;path planning;routing protocols;CGSR
protocol;PSO;cloud;cluster head gateway switch routing protocol;movement
coordination;multirobot communication analysis;multirobot
coordination;particle swarm optimization;robot navigation;Cloud
computing;Collision avoidance;Robot kinematics;Robot sensing
systems;Routing;Routing protocols;Cloud Computing;Cloud
Robotics;Multi-robot communication;Multi-robot
coordination;PSO;coordination and communication;swarm robotics},
doi={10.1109/CCAA.2015.7148524},
month={May},}
@ARTICLE{7014315,
author={C. E. Agüero and N. Koenig and I. Chen and H. Boyer and S.
Peters and J. Hsu and B. Gerkey and S. Paepcke and J. L. Rivero and J.
Manzo and E. Krotkov and G. Pratt},
journal={IEEE Transactions on Automation Science and Engineering},
title={Inside the Virtual Robotics Challenge: Simulating Real-Time
Robotic Disaster Response},
year={2015},
volume={12},
number={2},
pages={494-506},
abstract={This paper presents the software framework established to
facilitate cloud-hosted robot simulation. The framework addresses the
challenges associated with conducting a task-oriented and real-time
robot competition, the Defense Advanced Research Projects Agency (DARPA)
Virtual Robotics Challenge (VRC), designed to mimic reality. The core of
the framework is the Gazebo simulator, a platform to simulate robots,
objects, and environments, as well as the enhancements made for the VRC
to maintain a high fidelity simulation using a high degree of freedom
and multisensor robot. The other major component used is the CloudSim
tool, designed to enhance the automation of robotics simulation using
existing cloud technologies. The results from the VRC and a discussion
are also detailed in this work. Note to Practitioners - Advances in
robot simulation, cloud hosted infrastructure, and web technology have
made it possible to accurately and efficiently simulate complex robots
and environments on remote servers while providing realistic data
streams for human-in-the-loop robot control. This paper presents the
software and hardware frameworks established to facilitate cloud-hosted
robot simulation, and addresses the challenges associated with
conducting a task-oriented robot competition designed to mimic reality.
The competition that spurred this innovation was the VRC, a precursor to
the DARPA Robotics Challenge, in which teams from around the world
utilized custom human-robot interfaces and control code to solve
disaster response-related tasks in simulation. Winners of the VRC
received both funding and access to Atlas, a humanoid robot developed by
Boston Dynamics. The Gazebo simulator, an open source and high fidelity
robot simulator, was improved upon to met the needs of the VRC
competition. Additionally, CloudSim was created to act as an interface
between users and the cloud-hosted simulations. As a result of this
work, we have achieved automated deployment of cloud resources f- r
robotic simulations, near real-time simulation performance, and
simulation accuracy that closely mimics real hardware. These tools have
been released under open source licenses and are freely available, and
can be used to help reduce robot and algorithm design and development
time, and increase robot software robustness.},
keywords={cloud computing;control engineering computing;digital
simulation;human-robot interaction;humanoid robots;public domain
software;rescue robots;Atlas humanoid robot;CloudSim tool;DARPA;Defense
Advanced Research Projects Agency;Gazebo simulator;VRC;Web
technology;cloud-hosted robot simulation;human-in-the-loop robot
control;human-robot interfaces;multisensor robot;open source robot
simulator;real-time robot competition;real-time robotic disaster
response;software framework;task-oriented robot competition;virtual
robotics challenge;Computational modeling;Computer
architecture;Computers;Real-time systems;Robot sensing
systems;Servers;Cloud robotics;real-time robot simulation;robotic
disaster response},
doi={10.1109/TASE.2014.2368997},
ISSN={1545-5955},
month={April},}
@ARTICLE{7057681,
author={G. Mohanarajah and V. Usenko and M. Singh and R. D'Andrea and M.
Waibel},
journal={IEEE Transactions on Automation Science and Engineering},
title={Cloud-Based Collaborative 3D Mapping in Real-Time With Low-Cost
Robots},
year={2015},
volume={12},
number={2},
pages={423-431},
abstract={This paper presents an architecture, protocol, and parallel
algorithms for collaborative 3D mapping in the cloud with low-cost
robots. The robots run a dense visual odometry algorithm on a
smartphone-class processor. Key-frames from the visual odometry are sent
to the cloud for parallel optimization and merging with maps produced by
other robots. After optimization the cloud pushes the updated poses of
the local key-frames back to the robots. All processes are managed by
Rapyuta, a cloud robotics framework that runs in a commercial data
center. This paper includes qualitative visualization of collaboratively
built maps, as well as quantitative evaluation of localization accuracy,
bandwidth usage, processing speeds, and map storage.},
keywords={cartography;cloud computing;computer centres;control
engineering computing;distance measurement;mobile robots;Rapyuta;cloud
robotics framework;cloud-based collaborative 3D mapping;commercial data
center;dense visual odometry algorithm;low-cost robots;parallel
algorithms;parallel optimization;real-time robots;smartphone-class
processor;Cloning;Optimization;Robot kinematics;Robot sensing
systems;Three-dimensional displays;Visualization;Cloud
robotics;cloud-based mapping;dense visual odometry;platform-as-a-Service},
doi={10.1109/TASE.2015.2408456},
ISSN={1545-5955},
month={April},}
@ARTICLE{7052418,
author={J. Salmerón-Garcı´a and P. Íñigo-Blasco and F. Dı´az-del-Rı´o
and D. Cagigas-Muñiz},
journal={IEEE Transactions on Automation Science and Engineering},
title={A Tradeoff Analysis of a Cloud-Based Robot Navigation Assistant
Using Stereo Image Processing},
year={2015},
volume={12},
number={2},
pages={444-454},
abstract={The use of Cloud Computing for computation offloading in the
robotics area has become a field of interest today. The aim of this work
is to demonstrate the viability of cloud offloading in a low level and
intensive computing task: a vision-based navigation assistance of a
service mobile robot. In order to do so, a prototype, running over a
ROS-based mobile robot (Erratic by Videre Design LLC) is presented. The
information extracted from on-board stereo cameras will be used by a
private cloud platform consisting of five bare-metal nodes with AMD
Phenom 965 × 4 CPU, with the cloud middleware Openstack Havana. The
actual task is the shared control of the robot teleoperation, that is,
the smooth filtering of the teleoperated commands with the detected
obstacles to prevent collisions. All the possible offloading models for
this case are presented and analyzed. Several performance results using
different communication technologies and offloading models are explained
as well. In addition to this, a real navigation case in a domestic
circuit was done. The tests demonstrate that offloading computation to
the Cloud improves the performance and navigation results with respect
to the case where all processing is done by the robot.},
keywords={cloud computing;collision avoidance;control engineering
computing;image filtering;image retrieval;image
sensors;middleware;mobile robots;object detection;robot vision;service
robots;stereo image processing;telerobotics;AMD Phenom 965 × 4 CPU;LLC
Videre Design;Openstack Havana;ROS-based mobile robot;bare-metal
nodes;cloud computing;cloud middleware;cloud-based robot navigation
assistant;collision prevention;communication technologies;computation
offloading;domestic circuit;information extraction;intensive computing
task;obstacle detection;on-board stereo cameras;private cloud
platform;robot teleoperation;robotics area;service mobile robot;smooth
filtering;stereo image processing;teleoperated commands;tradeoff
analysis;vision-based navigation assistance;Cameras;Mobile
robots;Navigation;Robot vision systems;Stereo vision;Three-dimensional
displays;Cloud offloading;cloud robotics;image point cloud;mobile
robots;navigation assistance;shared control},
doi={10.1109/TASE.2015.2403593},
ISSN={1545-5955},
month={April},}
@ARTICLE{6923491,
author={B. Kehoe and D. Warrier and S. Patil and K. Goldberg},
journal={IEEE Transactions on Automation Science and Engineering},
title={Cloud-Based Grasp Analysis and Planning for Toleranced Parts
Using Parallelized Monte Carlo Sampling},
year={2015},
volume={12},
number={2},
pages={455-470},
abstract={This paper considers grasp planning in the presence of shape
uncertainty and explores how cloud computing can facilitate parallel
Monte Carlo sampling of combination actions and shape perturbations to
estimate a lower bound on the probability of achieving force closure. We
focus on parallel-jaw push grasping for the class of parts that can be
modeled as extruded 2-D polygons with statistical tolerancing. We
describe an extension to model part slip and experimental results with
an adaptive sampling algorithm that can reduce sample size by 90%. We
show how the algorithm can also bound part tolerance for a given grasp
quality level and report a sensitivity analysis on algorithm parameters.
We test a cloud-based implementation with varying numbers of nodes,
obtaining a 515 × speedup with 500 nodes in one case, suggesting the
algorithm can scale linearly when all nodes are reliable. Code and data
are available at: http://automation.berkeley.edu/cloud-based-grasping.},
keywords={Monte Carlo methods;cloud computing;control engineering
computing;force control;grippers;industrial manipulators;materials
handling;probability;sampling methods;sensitivity analysis;adaptive
sampling algorithm;algorithm parameter sensitivity analysis;cloud
computing;cloud-based grasp analysis;cloud-based implementation;extruded
2D polygons;force closure;grasp planning;grasp quality;parallel-jaw push
grasping;parallelized Monte Carlo sampling;part slip
modeling;probability;shape perturbation;shape uncertainty;statistical
tolerancing;toleranced parts;Algorithm design and
analysis;Force;Grasping;Grippers;Monte Carlo
methods;Planning;Shape;Cloud automation;Monte Carlo sampling;cloud
computing;cloud robotics;grasping},
doi={10.1109/TASE.2014.2356451},
ISSN={1545-5955},
month={April},}
@INPROCEEDINGS{7020689,
author={C. Muhammad and B. Samanta},
booktitle={2014 IEEE Symposium on Computational Intelligence, Cognitive
Algorithms, Mind, and Brain (CCMB)},
title={Neuromodulation based control of autonomous robots in ROS
environment},
year={2014},
pages={16-23},
abstract={The paper presents a control approach based on vertebrate
neuromodulation and its implementation on autonomous robots in the
open-source, open-access environment of robot operating system (ROS)
within a cloud computing framework. A spiking neural network (SNN) is
used to model the neuromodulatory function for generating context based
behavioral responses of the robots to sensory input signals. The neural
network incorporates three types of neurons- cholinergic and
noradrenergic (ACh/NE) neurons for attention focusing and action
selection, dopaminergic (DA) neurons for rewards- and curiosity-seeking,
and serotonergic (5-HT) neurons for risk aversion behaviors. The model
depicts description of neuron activity that is biologically realistic
but computationally efficient to allow for large-scale simulation of
thousands of neurons. The model is implemented using graphics processing
units (GPUs) for parallel computing in real-time using the ROS
environment. The model is implemented to study the risk-taking,
risk-aversive, and distracted behaviors of the neuromodulated robots in
single- and multi-robot configurations. The entire process is
implemented in a distributed computing framework using ROS where the
robots communicate wirelessly with the computing nodes through the
on-board laptops. Results are presented for both single- and multi-robot
configurations demonstrating interesting behaviors.},
keywords={cloud computing;control engineering computing;graphics
processing units;multi-robot systems;neurocontrollers;operating systems
(computers);parallel processing;GPU;ROS environment;SNN;action
selection;attention focusing;autonomous robots;cholinergic neuron;cloud
computing framework;context based behavioral response;control
approach;curiosity-seeking behavior;dopaminergic neuron;graphics
processing unit;multi-robot configuration;neuromodulation based
control;neuromodulatory function;neuron activity;noradrenergic
neuron;parallel computing;rewards-seeking behavior;risk aversion
behavior;robot operating system;serotonergic neuron;single-robot
configuration;spiking neural network;vertebrate
neuromodulation;Biological neural networks;Collision
avoidance;Computational modeling;Neurons;Portable computers;Robot
sensing systems;Artificial neural networks;CUDA;GPU;Izhikevich spiking
neuron;cloud robotics;neuromodulation;neurorobotics;parallel
computing;robot operating system;spiking neural networks},
doi={10.1109/CCMB.2014.7020689},
month={Dec},}
@INPROCEEDINGS{6996193,
author={S. S. Prabha and A. J. P. Antony and M. J. Meena and S. R.
Pandian},
booktitle={2014 International Conference on Recent Trends in Information
Technology},
title={Smart cloud robot using raspberry Pi},
year={2014},
pages={1-5},
abstract={Cloud robotics is an emerging field that is centered on the
benefits of converged infrastructure and shared services of a cloud
computing environment. In this paper, a system is designed with an
autonomous robot to sense environmental data such as temperature,
humidity, and air quality, along with GPS coordinates and store them on
the cloud. The mobile robot is controlled using an Arduino
microcontroller and communicates with the cloud via a Raspberry Pi. A
private cloud is set up using OpenStack that provides Infrastructure as
a Service. The collected data are stored in a cloud server which could
be viewed through a web browser and can be used to create awareness
about the environmental changes of the location under study. A
proof-of-concept prototype has been developed to illustrate the
effectiveness of the proposed system.},
keywords={Global Positioning System;air quality;cloud computing;control
engineering computing;environmental science computing;humidity
sensors;microcontrollers;mobile robots;service robots;temperature
sensors;Arduino microcontroller;GPS coordinates;OpenStack;Raspberry
Pi;Web browser;air quality;autonomous robot;cloud computing
environment;cloud robotics;cloud server;environmental
changes;environmental data sensing;humidity;infrastructure as a
service;mobile robot;private cloud;shared services;smart cloud
robot;system design;temperature;Cloud computing;Clouds;Mobile
robots;Robot kinematics;Robot sensing systems;Cloud Robotics;GPS;Mobile
Robot;Open source software;Raspberry Pi},
doi={10.1109/ICRTIT.2014.6996193},
month={April},}
@INPROCEEDINGS{6921886,
author={P. J. S. Gonçalves and F. M. S. Santos and P. M. B. Torres},
booktitle={2014 Sixth World Congress on Nature and Biologically Inspired
Computing (NaBIC 2014)},
title={Towards a low-cost framework for Intelligent Robots},
year={2014},
pages={244-249},
abstract={Intelligent Robots can take advantage of a distributed,
web-based information system deployed in the cloud to perform high level
tasks. This paper proposes a robotic control framework suited to be used
by low-cost robots, performing teloperated and/or autonomous tasks. The
first part is dedicated to the development of an Android based robot
control framework. This framework connects to specific low-level
controllers that were developed for multicopters, wheeled/tracked mobile
robots. The second part is dedicated to “place” the Android based robot
in the cloud. There, the robot can perform Cloud based highly automated
cognitive tasks in order to optimize their use and take best advantage
of previous knowledge models, e.g., objects databases or 3D world
models. Also, the robot can be controlled remotely using a classical
teleoperation mode, using wifi networks. First experiments are presented
when a tracked robot is performing surveillance tasks, while its state
can be changed to teleoperation/videoconferencing mode, while
interacting with a reasoning engine in the Cloud.},
keywords={cloud computing;control engineering computing;inference
mechanisms;intelligent robots;mobile robots;telerobotics;wireless
LAN;Android based robot control framework;Wifi networks;cloud
platform;intelligent robots;low-cost framework;mobile robots;reasoning
engine;teleoperation mode;Bluetooth;Databases;Europe;IEEE 802.11
Standards;Robot sensing systems;Three-dimensional displays;Cloud
Robotics;Human-Computer Interaction;Intelligent Robots;Intelligent
Systems;Knowledge Technologies},
doi={10.1109/NaBIC.2014.6921886},
month={July},}
@INPROCEEDINGS{6700526,
author={M. Narita and S. Okabe and Y. Kato and Y. Murakwa and K.
Okabayashi and S. Kanda},
booktitle={IECON 2013 - 39th Annual Conference of the IEEE Industrial
Electronics Society},
title={Reliable cloud-based robot services},
year={2013},
pages={8317-8322},
abstract={Internet and cloud-based robot services and their platforms
are becoming attractive. Many related works have been also done, such as
DAvinCi, ROS on Android, and RoboEarth. However, when robot services are
provided via computer networks, very reliable services are required to
deal with short/long-term disconnection between services and robots due
to wireless LAN disconnection, robot service problems, system error on
robots, and so on. In this paper, we adopt Robot Service Network
Protocol (RSNP) as a robot service platform in order to integrate robot
services with Internet services. In addition, we propose a method and
architecture to realize reliable cloud-based robot services for RSNP in
a communication view point.},
keywords={cloud computing;computer network reliability;protocols;service
robots;telecommunication computing;wireless LAN;DAvinCi;Internet
services;ROS on Android;RSNP;RoboEarth;cloud-based robot
services;communication view point;computer networks;long-term
disconnection;robot service network protocol;robot service
platform;short-term disconnection;system error on robots;wireless LAN
disconnection;Libraries;Protocols;Robots;Servers;Software
reliability;Web services;cloud robotics;reliability;robot services},
doi={10.1109/IECON.2013.6700526},
ISSN={1553-572X},
month={Nov},}
@INPROCEEDINGS{6483540,
author={E. Hidago-Peña and L. F. Marin-Urias and F. Montes-González and
A. Marín-Hernández and H. V. Ríos-Figueroa},
booktitle={2013 8th ACM/IEEE International Conference on Human-Robot
Interaction (HRI)},
title={Learning from the Web: Recognition method based on object
appearance from Internet images},
year={2013},
pages={139-140},
abstract={In this work an Object Learning and Recognition method for a
Humanoid is presented. This method aims to take advantage of the Cloud
Resources, since it is based on image web search in order to build
training sets for learning objects' appearance. In case Internet access
is unavailable, the robot asks human to show the objects and acquires
the images using its camera. Through this technique, our method aims to
be a flexible and natural framework for Human-Robot Interaction and to
provide as much autonomy as possible to the robot.},
keywords={Internet;human-robot interaction;image sensors;object
recognition;robot vision;Internet images;Web learning;camera;cloud
resources;human-robot interaction;image Web search;object
appearance;object learning method;object recognition method;recognition
method;Computational modeling;Internet;Media;Principal component
analysis;Robot kinematics;Training;Cloud Robotics;Humanoid
Learning;Object Recognition},
doi={10.1109/HRI.2013.6483540},
ISSN={2167-2121},
month={March},}
@INPROCEEDINGS{6200718,
author={V. Nagrath and F. Meriaudeau and A. S. Malik and O. Morel},
booktitle={2012 International Conference on Communication Systems and
Network Technologies},
title={Agent Relation Charts (ARCs) for Modeling Cloud Based Transactions},
year={2012},
pages={704-709},
abstract={Software development is the most central challenge in present
day robot system development. This challenge is even more demanding in
multi robot systems interacting via one or many connectivity clouds.
There is a demand for an overall improvement in the way the robotic
software development is done today. This demand is even more critical
for cloud based multi robot systems. This article describes multi-view
agent relation charts which are modeling and specification tools for
software development in cloud connected multi agent systems. ARCs are
part of a more elaborate model driven approach for cloud connected
agents currently being developed by the authors. The model is being
built with a focus on the way human transactions takes place. This
article also discusses implementational compatibility of ARCs with
object oriented programming methodology.},
keywords={cloud computing;mobile robots;multi-agent systems;multi-robot
systems;object-oriented programming;robot programming;software
engineering;transaction processing;ARC;agent relation charts;cloud
connected agents;cloud connected multiagent systems;cloud-based
multirobot systems;cloud-based transactions modelling;connectivity
clouds;human transactions;multiview agent relation
charts;object-oriented programming methodology;robot system
development;robotic software development;specification
tools;Computational modeling;Humans;Object oriented modeling;Service
robots;Software;Unified modeling language;Cloud Computing;Cloud
Robotics;Model-Driven Engineering;Robotic Software},
doi={10.1109/CSNT.2012.156},
month={May},}
@INPROCEEDINGS{6163637,
author={R. Budihal and N. Mohanan and S. A. Anand and S. S. Kamat},
booktitle={2011 Fifth IEEE International Conference on Advanced
Telecommunication Systems and Networks (ANTS)},
title={Exploration and implementation of a next generation Telepresence
System},
year={2011},
pages={1-6},
abstract={Human communication includes not only spoken language but also
non-verbal cues such as hand and body gestures, facial expressions,
etc., to communicate our thoughts and feelings and gather feedback.
Telepresence systems of today use a 2-way audio and video transmission
to transmit this non-verbal information. In this paper, we introduce a
novel Experiential Telepresence System, which possesses cognitive
intelligence and is also context-ware i.e., it is aware of the multiple
components of communication and ambience in which it communicates - both
verbal and non-verbal, making the telepresence experience far more
immersive when compared to its peers. This is achieved using a 3-tier
architecture comprising of a Humanoid Robot, a Cognitive Collective
Intelligence Platform on Cloud and an Experience Centre. Towards the
end, a performance analysis coupled with a qualitative analysis of user
perception which in otherwords is to measure the Quality of Experience
of the system - shows the acceptability and user experience of our
system is far higher when compared to with traditional telepresence and
video conferencing.},
keywords={humanoid robots;next generation
networks;teleconferencing;telecontrol;video communication;2-way audio
transmission;2-way video transmission;cognitive collective intelligence
platform;cognitive intelligence;experiential telepresence
system;feedback;human communication;humanoid robot;next generation
telepresence system;nonverbal information;video
conferencing;Cameras;Emotion recognition;Face recognition;Humanoid
robots;Navigation;Sensors;Affective Interfaces;Cloud Robotics;Collective
Intelligence on Cloud;Experiential Telepresence;Quality of Experience
(QoE, QoX);Quality of Service (QoS);SLAM;Tele-operation;User
Experience(UX);augmented reality;cognition;context-awareness;humanoid
robot},
doi={10.1109/ANTS.2011.6163637},
ISSN={2153-1676},
month={Dec},}
@INPROCEEDINGS{7801596,
author={M. Grabisch and S. G. Kong and K. Iwano and P. Sinčák},
booktitle={2016 Joint 8th International Conference on Soft Computing and
Intelligent Systems (SCIS) and 17th International Symposium on Advanced
Intelligent Systems (ISIS)},
title={Plenary Talks},
year={2016},
pages={xxvi-xxix},
abstract={These plenary talk discuss the following: Multicriteria
Decision Making with Interacting Criteria; Machine Learning for Computer
Vision; Reality 2.0 and Wisdom Computing - our vision toward the future;
Cloud Based Intelligent Robotics.},
keywords={cloud computing;computer vision;decision making;intelligent
robots;learning (artificial intelligence);ISIS;International Symposium
on Advanced Intelligent Systems;SCIS;cloud based intelligent
robotics;computer vision;interacting criteria;machine
learning;multicriteria decision making;reality 2.0;soft computing and
intelligent systems;wisdom computing},
doi={10.1109/SCIS-ISIS.2016.0009},
month={Aug},}
@INPROCEEDINGS{7746287,
author={S. Payandeh},
booktitle={2016 IEEE 7th Annual Information Technology, Electronics and
Mobile Communication Conference (IEMCON)},
title={Recursive Bayesian tracking for smart elderly living},
year={2016},
pages={1-7},
abstract={Being able to monitor movements and activities of the elderly
in a smart living environment can offer an approach for detecting any
on-set of anomalies. These smart living dwellings can be equipped with
various networked ambient sensors, wearable sensing technologies and
cloud-robotics. Detection of any such on-set of anomalies in elderly
movements and activities can further be used to determine the state of
mental health of the elderly for example related to dementia or
Alzheimer. There are two mains challenges associated with the deployment
such sensor network in the dwelling of elderly. The main challenge is in
the processing of the sensed information in order to ensure the privacy
of individuals. The next big challenge is the robustness of tracking
algorithm in the presence of lost or occluded sensed data. It has been
shown that the recursive Bayesian approach can offer a suitable
framework for tracking targets with the maneuvering trajectories which
follows a non-linear behavior and non-Gaussian distribution. This paper
presents an overview of recursive Bayesian framework which can be used
as a part of tracking environment in the smart living environment of
elderly. Through step-by-step development, the paper highlights various
features of the method which can be adapted in order to reduce various
computational issues associated with the implementation of this
framework.},
keywords={Bayes methods;biomedical equipment;diseases;gait
analysis;geriatrics;living systems;medical robotics;sensors;Alzheimer
disease;cloud-robotics;dementia;elderly activity;elderly
movements;maneuvering trajectories;mental health;networked ambient
sensors;nonGaussian distribution;nonlinear behavior;recursive Bayesian
tracking;smart elderly living;smart living dwellings;smart living
environment;tracking algorithm;Bayes methods;Cameras;Intelligent
sensors;Senior citizens;Time measurement;Tracking},
doi={10.1109/IEMCON.2016.7746287},
month={Oct},}
@INPROCEEDINGS{7743442,
author={J. Mahler and B. Hou and S. Niyaz and F. T. Pokorny and R.
Chandra and K. Goldberg},
booktitle={2016 IEEE International Conference on Automation Science and
Engineering (CASE)},
title={Privacy-preserving Grasp Planning in the Cloud},
year={2016},
pages={468-475},
abstract={To support industrial automation, systems such as GraspIt! and
Dex-Net 1.0 provide “Grasp Planning as a Service” (GPaaS). To assist
manufacturers setting up automated assembly lines, users can send part
geometry via the Internet to the service and receive a ranked set of
robust grasp configurations. As industrial users may be reluctant to
share proprietary details of product geometry with outside parties, this
paper proposes a privacy-preserving approach awhere a masked version of
the part boundary is uploaded, allowing proprietary aspects of the part
geometry to remain confidential. One challenge is the tradeoff between
grasp coverage and privacy: balancing the desire for a rich set of
alternative grasps based on analysis of graspable surfaces (coverage)
against the desire for privacy. We introduce a grasp coverage metric
based on dispersion from motion planning, and plot its relationship with
privacy (the fraction of the object surface that is masked). We
implement the algorithm using Dex-Net 1.0 and present case studies of
the privacy-coverage tradeoff on a set of 23 industrial parts. Results
suggest that masking the part using the convex hull of the proprietary
zone can provide grasp coverage with minor distortion to the object
similarity metric used to accelerate grasp planning in Dex-Net 1.0.
Code, data, and additional information can be found at
http://berkeleyautomation.io/privacy_preserving_grasping.},
keywords={cloud computing;control engineering computing;data
privacy;grippers;path planning;robust control;Dex-Net
1.0;GPaaS;GraspIt!;Internet;automated assembly lines;cloud based
robotics;grasp coverage metric;grasp planning as a service;industrial
automation;masked version;motion planning;object similarity
metric;object surface;privacy-coverage tradeoff;privacy-preserving grasp
planning;product geometry;robot gripper;robust grasp
configurations;Dispersion;Grippers;Measurement;Planning;Privacy;Robustness;Solid
modeling},
doi={10.1109/COASE.2016.7743442},
month={Aug},}
@INPROCEEDINGS{7487342,
author={J. Mahler and F. T. Pokorny and B. Hou and M. Roderick and M.
Laskey and M. Aubry and K. Kohlhoff and T. Kröger and J. Kuffner and K.
Goldberg},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Dex-Net 1.0: A cloud-based network of 3D objects for robust grasp
planning using a Multi-Armed Bandit model with correlated rewards},
year={2016},
pages={1957-1964},
abstract={This paper presents the Dexterity Network (Dex-Net) 1.0, a
dataset of 3D object models and a sampling-based planning algorithm to
explore how Cloud Robotics can be used for robust grasp planning. The
algorithm uses a Multi- Armed Bandit model with correlated rewards to
leverage prior grasps and 3D object models in a growing dataset that
currently includes over 10,000 unique 3D object models and 2.5 million
parallel-jaw grasps. Each grasp includes an estimate of the probability
of force closure under uncertainty in object and gripper pose and
friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks
(MV-CNNs), a new deep learning method for 3D object classification, to
provide a similarity metric between objects, and the Google Cloud
Platform to simultaneously run up to 1,500 virtual cores, reducing
experiment runtime by up to three orders of magnitude. Experiments
suggest that correlated bandit techniques can use a cloud-based network
of object models to significantly reduce the number of samples required
for robust grasp planning. We report on system sensitivity to variations
in similarity metrics and in uncertainty in pose and friction. Code and
updated information is available at
http://berkeleyautomation.github.io/dex-net/.},
keywords={cloud computing;control engineering computing;convolution;data
analysis;image classification;learning systems;neural nets;path
planning;robot vision;3D object models;Dex-Net 1.0;MV-CNN;cloud
robotics;cloud-based network;correlated rewards;dataset;deep learning
method;dexterity network 1.0;force closure;multi-armed bandit
model;multiview convolutional neural networks;object
classification;parallel-jaw grasps;robust grasp planning;similarity
metrics;Force;Friction;Measurement;Planning;Robustness;Solid
modeling;Three-dimensional displays},
doi={10.1109/ICRA.2016.7487342},
month={May},}
@ARTICLE{7470948,
author={M. Maier and M. Chowdhury and B. P. Rimal and D. P. Van},
journal={IEEE Communications Magazine},
title={The tactile internet: vision, recent progress, and open challenges},
year={2016},
volume={54},
number={5},
pages={138-145},
abstract={The advent of commercially available remote-presence robots
may be the precursor of an age of technological convergence, where
important tasks of our everyday life will be increasingly done by
robots. A very low roundtrip latency in conjunction with ultra-high
reliability and essentially guaranteed availability for control
communications has the potential to move today's mobile broadband
experience into the new world of the Tactile Internet for a race with
(rather than against) machines. To facilitate a better understanding of
the Tactile Internet, this article first elaborates on the commonalities
and subtle differences between the Tactile Internet and the Internet of
Things and 5G vision. After briefly reviewing its anticipated impact on
society and infrastructure requirements, we then provide an up-to-date
survey of recent progress and enabling technologies proposed for the
Tactile Internet. Given that scaling up research in the area of future
wired and wireless access networks will be essential for the Tactile
Internet, we pay particular attention to the latency and reliability
performance gains of fiber-wireless (FiWi) enhanced LTE-Advanced
heterogeneous networks and their role for emerging cloudlets,
mobile-edge computing, and cloud robotics. Finally, we conclude by
outlining remaining open challenges for the Tactile Internet.},
keywords={5G mobile communication;Internet;Internet of Things;Long Term
Evolution;cloud computing;haptic interfaces;mobile robots;radio access
networks;5G vision;FiWi enhanced LTE-Advanced heterogeneous
networks;Internet of Things;cloud robotics;fiber-wireless enhanced
LTE-advanced heterogeneous networks;mobile broadband
experience;mobile-edge computing;remote-presence robots;tactile
Internet;technological convergence;ultra-high reliability;wired access
networks;wireless access networks;5G mobile communication;Cloud
computing;IEEE 802.11 Standard;Reliability;Robots},
doi={10.1109/MCOM.2016.7470948},
ISSN={0163-6804},
month={May},}
@INPROCEEDINGS{7441057,
author={E. F. Villaronga},
booktitle={eChallenges e-2015 Conference},
title={Legal and regulatory challenges for physical assistant robots},
year={2015},
pages={1-8},
abstract={This study pioneers the identification and examination of
concrete legal and ethical issues concerning a sub-type of Personal Care
Robots (PCR): Physical Assistant Robots (PAR). PCR are the core of the
progress of Information and Communication Technology (ICT) and are
likely to be introduced in many Healthcare facilities very soon.
Nonetheless, appropriate and specific legal regulations regarding PAR
are missing and several problems need to be carefully considered: from
technical issues, e.g. cloud robotics and security; to legal problems,
e.g. privacy, liability, user rights, etc.; to ethical ones, e.g. user
acceptance, dependence on the device, etc. Despite the recent advances,
there is still a long way ahead and further research is needed to
overcome such problems. This is the first version of a regulatory (law
and ethics) framework for PAR more concrete and useful than current
legal and ethical general principles, which includes its contents and
challenges.},
keywords={data privacy;ethical aspects;health care;law;medical
robotics;ICT;PAR;PCR;cloud robotics;concrete ethical issue
examination;concrete legal issue examination;healthcare
facilities;information-and-communication technology;liability;personal
care robots;physical assistant robots;privacy;regulatory
challenges;technical issues;user acceptance;user rights;Law;Robot
sensing systems;Safety;Senior citizens},
doi={10.1109/eCHALLENGES.2015.7441057},
month={Nov},}
@ARTICLE{7432173,
author={A. Manzalini and N. Crespi},
journal={IEEE Communications Magazine},
title={An edge operating system enabling anything-as -a-service},
year={2016},
volume={54},
number={3},
pages={62-67},
abstract={This article argues that SDN and NFV, together with cloud and
edge-fog computing, can be seen as different facets of a systemic
transformation of telecommunications and ICT, called softwarization. The
first impact will be at the edge of current telecommunications
infrastructures, which are becoming powerful network and service
platforms. The edge operating system (EOS) software architecture is
proposed as the means to get there. In fact, the main feature of EOS is
to bring several service domains, such as cloud robotics, Internet of
Things, and Tactile Internet, into convergence at the edge. The
development of EOS leverages available open source software. A use case
is described to validate the EOS with a proof-of-concept.},
keywords={Internet of Things;cloud computing;public domain
software;software defined networking;EOS software
architecture;ICT;Internet of Things;NFV;SDN;anything-as-a-service;cloud
computing;cloud robotics;edge operating system software
architecture;edge-fog computing;network function virtualization;open
source software;software defined networking;softwarization;tactile
Internet;telecommunications infrastructures;telecommunications systemic
transformation;Cloud computing;Earth Observing System;Operating
systems;Peer-to-peer computing;Robots;Semantics;Telecommunication
services},
doi={10.1109/MCOM.2016.7432173},
ISSN={0163-6804},
month={March},}
@INPROCEEDINGS{7334501,
author={M. Pavlić and D. Bratković},
booktitle={2015 57th International Symposium ELMAR (ELMAR)},
title={Comparison of different NGN aggregation networks scenarios},
year={2015},
pages={81-84},
abstract={The possibilities of communications are rapidly changing.
Providing the infrastructure to new production forms like distributed
manufacturing or cloud robotics, IoT (Internet of Things) or M2M
(Machine to Machine) becoming necessary for economic revitalization.
With the development of Internet technology, as well as increasing the
availability of the growing Internet users, live streaming services are
face increasing challenges for service providers who wish to multimedia
content and data transfer to as much as possible users. With the
unification of services such as Internet, TV, telecommunications
services, multimedia and video, live stream not only become desirable
but necessary. The development of many new telecommunication services
such as personals services, virtualized in - network - storage services,
video services, High Definition (HD+) IPTV service and multiplication of
user's terminals, leads to a significant increasing of bandwidth
demands. All this high bandwidth new services could lead to traffic
bottlenecks when the ratio of the capacity of output interface to the
sum of all input links' capacities is not enough. Such bottlenecks could
in turn lead to QoS degradations such as packet losses, excessive delay,
and jitter degradation. Most usually bottleneck is settled somewhere
inside access or aggregation networks.},
keywords={Internet;next generation networks;quality of service;IPTV
service;Internet;Internet of Things;IoT;M2M;NGN aggregation networks;QoS
degradations;aggregation networks;cloud robotics;data
transfer;delay;high definition;jitter degradation;machine to
machine;multimedia content;telecommunications
services;Bandwidth;Broadband communication;IP
networks;IPTV;Internet;Mobile communication;Streaming
media;Agreggation;Fixed;Moving;NGN;Network},
doi={10.1109/ELMAR.2015.7334501},
month={Sept},}
@ARTICLE{7224704,
author={M. Maier and B. P. Rimal},
journal={China Communications},
title={Invited paper: The audacity of fiber-wireless (FiWi) networks:
revisited for clouds and cloudlets},
year={2015},
volume={12},
number={8},
pages={33-45},
abstract={There is a growing awareness among industry players of reaping
the benefits of mobile-cloud convergence by extending today's unmodified
cloud to a decentralized two-level cloud-cloudlet architecture based on
emerging mobile-edge computing (MEC) capabilities. In light of future 5G
mobile networks moving toward decentralization based on cloudlets,
intelligent base stations, and MEC, the inherent distributed processing
and storage capabilities of radio-and-fiber (R&F) networks may be
exploited for new applications, e.g., cognitive assistance, augmented
reality, or cloud robotics. In this paper, we first revisit
fiber-wireless (FiWi) networks in the context of conventional clouds and
emerging cloudlets, thereby highlighting the limitations of conventional
radio-overfiber (RoF) networks such as China Mobile's centralized cloud
radio access network (C-RAN) to meet the aforementioned trends.
Furthermore, we pay close attention to the specific design challenges of
data center networks and revisit our switchless arrayed waveguide
grating (AWG) based network with efficient support of east-west flows
and enhanced scalability.},
keywords={5G mobile communication;optical fibre networks;5G mobile
networks;AWG;C-RAN;China mobile centralized cloud radio access
network;FiWi networks;MEC;R&F networks;augmented reality;cloud cloudlet
architecture;cloud robotics;cloudlets;cognitive assistance;distributed
processing;fiber wireless networks;intelligent base stations;mobile
cloud convergence;mobile edge computing;radio-and-fiber;switchless
arrayed waveguide grating;Mobile communication;Optical fibers;Optical
network units;Optical packet switching;Optical switches},
doi={10.1109/CC.2015.7224704},
ISSN={1673-5447},
month={August},}
@ARTICLE{7055942,
author={P. Pandey and D. Pompili and J. Yi},
journal={IEEE Transactions on Automation Science and Engineering},
title={Dynamic Collaboration Between Networked Robots and Clouds in
Resource-Constrained Environments},
year={2015},
volume={12},
number={2},
pages={471-480},
abstract={Underwater mobile sensor networks such as Autonomous
Underwater Vehicles (AUVs) or robots are envisioned to enable
applications for oceanographic data collection, environmental and
pollution monitoring, offshore exploration, and distributed tactical
surveillance. These applications require running compute- and
data-intensive algorithms that go beyond the capabilities of the
individual AUVs that are involved in a mission. To execute these
task-parallel algorithms in resource- and time-constrained environments,
dynamic and reliable collaboration between local networked robots (e.g.,
AUVs) and remote public Clouds is needed. To this end, the heterogeneous
sensing, computing, communication, and storage capabilities of local and
remote resources are exploited to form a “loosely coupled” mobile Cloud,
and a novel resource provisioning engine that dynamically takes
decisions on “what” and “where” the tasks should be executed in the
mobile Cloud is introduced. Comparison of benefits of collaboration
between local and Cloud resources with purely local and centralized
approaches are presented through exhaustive computer simulations.},
keywords={autonomous underwater vehicles;cloud computing;control
engineering computing;decision making;mobile robots;resource
allocation;wireless sensor networks;AUV;autonomous underwater
vehicle;cloud robotics;decision making;heterogeneous sensing;networked
robot;resource provisioning engine;resource-constrained
environment;underwater mobile sensor network;Algae;Cloud
computing;Collaboration;Mobile communication;Robot sensing
systems;Vehicles;Amazon EC2;autonomous underwater vehicles;cloud
computing;robot coordination},
doi={10.1109/TASE.2015.2406115},
ISSN={1545-5955},
month={April},}
@INPROCEEDINGS{6942764,
author={W. Hilton and D. M. Lofaro and Youngmoo Kim},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={A lightweight, cross-platform, multiuser robot visualization
using the cloud},
year={2014},
pages={1570-1575},
abstract={Cloud robotics emphasizes harnessing the power of the Web for
robotics. Modern mobile devices connect to the Web and are convenient
user interfaces. We decided to explore what was possible at the
intersection of robotics, the Web, and mobile devices by creating a
mobile web interface for a humanoid robot. This paper describes our
implementation of a monitoring interface for high degree of freedom
(DOF) robots that works with both desktop and mobile devices. Using only
standard web technologies, our application provides a rich 3D interface
that displays the robot's pose, orientation, and sensor data, and can
update at 30Hz. It is easy to use, because there is no software for the
user to install; it runs using the device's mobile browser. The web
interface can be deployed on a private or public cloud, and is designed
to scale to support hundreds or thousands of viewers by utilizing cloud
services. The system was successfully tested with two different robots
and on multiple browsers and mobile devices.},
keywords={cloud computing;control engineering computing;data
visualisation;humanoid robots;mobile handsets;online
front-ends;sensors;user interfaces;DOF robots;cloud
robotics;cross-platform;degree of freedom robots;desktop
devices;humanoid robot;mobile browser;mobile devices;mobile
interface;multiuser robot visualization;private cloud;public cloud;rich
3D interface;robot orientation;robot pose;sensor data;standard Web
technologies;user interfaces;Browsers;Cloud computing;Joints;Mobile
handsets;Robot sensing systems;Servers},
doi={10.1109/IROS.2014.6942764},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6882669,
author={T. G. Orphanoudakis and C. Matrakidis and A. Stavdas},
booktitle={2014 European Conference on Networks and Communications
(EuCNC)},
title={Next generation optical network architecture featuring
distributed aggregation, network processing and information routing},
year={2014},
pages={1-5},
abstract={The ontology of communications is rapidly changing, shifting
interest to machine-to-machine (M2M) interactions and the internet of
Things (IoT). These are becoming vital for sustainability of social life
and the revitalization of the economy providing the infrastructure to
new production forms like distributed manufacturing, cloud robotics
while becoming important to grid-based energy systems. Adding to them
the voracious needs for data of the traditional broadband users,
residential or business, together with the back/front hauling
requirements of mobile operators, one is expecting a significant strain
in the access. A multitude of heterogeneous access networks are emerging
and the integration of them in a single platform ensuring seamless
data-exchange with Data-Centres is of major importance. In this paper we
describe HYDRA (HYbriD long-Reach fiber Access network), a novel network
architecture that overcomes the limitations of both long-reach PONs as
well as mobile backhauling schemes, leading to significantly improved
cost and power consumption figures. The key concept is the introduction
of an Active Remote Node (ARN) that interfaces to end-users by means of
the lowest cost/power consumption technology (short-range xPON,
wireless, etc.) whilst on the core network side it employs adaptive
ultra-long reach links to bypass the Metropolitan Area Network. The
scheme leads to a higher degree of node consolidation, network
convergence and Access-Core integration. The proposed architecture can
enhance performance while supporting network virtualization and
efficient resource orchestration based on Software Defined Networking
(SDN) principles and open access networking models.},
keywords={Internet of Things;computer centres;computer network
performance evaluation;electronic data interchange;mobile
computing;ontologies (artificial intelligence);passive optical
networks;telecommunication network
routing;virtualisation;HYDRA;Internet-of-things;IoT;M2M
interactions;PON;SDN principles;access-core integration;active remote
node;back hauling requirement;cloud robotics;communication ontology;cost
consumption technology;data centres;data exchange;distributed
aggregation;distributed manufacturing;economy revitalization;front
hauling requirement;grid-based energy systems;heterogeneous access
networks;hybrid long-reach fiber access network;information
routing;machine-to-machine interactions;mobile backhauling scheme;mobile
operators;network convergence;network processing;network
virtualization;next generation optical network architecture;node
consolidation;open access networking models;passive optical
network;power consumption technology;production forms;resource
orchestration;short-range xPON;social life sustainability;software
defined networking principles;Computer architecture;Mobile
communication;Optical fibers;Passive optical networks;Wavelength
division multiplexing;Ubiquitous network infrastructure;network
convergence;optical networks;software defined networking},
doi={10.1109/EuCNC.2014.6882669},
month={June},}
@INPROCEEDINGS{7776559,
booktitle={2016 5th IEEE International Conference on Cloud Networking
(Cloudnet)},
title={[Title page i]},
year={2016},
pages={i-i},
abstract={The following topics are dealt with: cost-efficient
algorithms; critical resource allocation; cloud federations; virtual
network PaaS; 3GPP 4G; core network services; distributed genetic
algorithm; reliable application placement; hybrid clouds; wireless fog
computing; 5G networks; VM migration; openflow; resiliency mechanism;
IaaS migration; FELIX federated testbed; mobile cloud; ICN; smart
probabilistic fingerprinting; indoor localization; Fog computing;
multipath TCP; latency sensitive flows; mobile cloud storage; content
distribution; information aware mechanism; multiuser mobile cloud
offloading game; computing access point; virtual network functions;
joint power scaling; transparent optimisation; intervirtual network
function communication; open switch; energy aware placement; network
traffic flows; data center network topology; Hadoop MapReduce
performance; augmented data center infrastructure management system;
virtual machine allocation; security mobile cloud; mobile edge
computing; network softwarization; comlex bioinformatics; cloud
benchmarking; data management; federated cloud services; VPN endpoints;
COTS server hardware; edge computing system; Big Data applications;
cyber physical systems; cloud connected social robot; cloud agent
concept; BDI robots; human-robot control; Internet of robotic things-
cloud robotics teleoperation systems; DoS attacks; SDN networks;
middleware platform; distributed applications; Geo-distributed
collaborative cloud; performance debugging; SDN controllers; LISP
control plane: cloud networks; ERTMS railways systems; load management;
transparent virtual machine; P2P auctions and service oriented networks.},
keywords={5G mobile communication;Big Data;Internet;cloud
computing;computer centres;computer networks;distributed
processing;genetic algorithms;middleware;mobile computing;peer-to-peer
computing;resource allocation;telecommunication network
topology;telerobotics;transport protocols;virtual machines;3GPP 4G;5G
networks;BDI robots;Big Data applications;COTS server hardware;DoS
attacks;ERTMS railways systems;Fog computing;Geo-distributed
collaborative cloud;Hadoop MapReduce performance;ICN;IaaS
migration;Internet of robotic things;LISP control plane;P2P auctions;SDN
controllers;SDN networks;VM migration;VPN endpoints;augmented data
center infrastructure management system;cloud agent concept;cloud
benchmarking;cloud connected social robot;cloud federations;cloud
networks;cloud robotics teleoperation systems;comlex
bioinformatics;computing access point;content distribution;core network
services;cost-efficient algorithms;critical resource allocation;cyber
physical systems;data center network topology;data
management;distributed applications;distributed genetic algorithm;edge
computing system;energy aware placement;federated cloud
services;federated testbed;human-robot control;hybrid clouds;indoor
localization;information aware mechanism;intervirtual network function
communication;joint power scaling;latency sensitive flows;load
management;middleware platform;mobile cloud;mobile cloud storage;mobile
edge computing;multipath TCP;multiuser mobile cloud offloading
game;network softwarization;network traffic flows;open
switch;openflow;performance debugging;reliable application
placement;resiliency mechanism;security mobile cloud;service oriented
networks;smart probabilistic fingerprinting;transparent
optimisation;transparent virtual machine;virtual machine
allocation;virtual network PaaS;virtual network functions;wireless fog
computing},
doi={10.1109/CloudNet.2016.1},
month={Oct},}
@ARTICLE{5719709,
author={E. Guizzo},
journal={IEEE Spectrum},
title={Robots with their heads in the clouds},
year={2011},
volume={48},
number={3},
pages={16-18},
abstract={Several research groups are exploring the idea of robots that
rely on cloud computing infrastructure to access vast amounts of
processing power and data. This approach, which some are calling "cloud
robotics," would allow robots to off-load compute-intensive tasks like
image processing and voice recognition and even download new skills
instantly. A Google researcher argues that cloud computing could make
robots smaller, cheaper, and smarter.},
keywords={cloud computing;control engineering computing;robots;cloud
computing;cloud robotics;image processing;off-load compute-intensive
tasks;robots;voice recognition;Cloud computing;Robot programming;Robots},
doi={10.1109/MSPEC.2011.5719709},
ISSN={0018-9235},
month={March},}
@ARTICLE{7914582,
author={C. Kulatunga and L. Shalloo and W. Donnelly and E. Robson and S.
Ivanov},
journal={IT Professional},
title={Opportunistic Wireless Networking for Smart Dairy Farming},
year={2017},
volume={19},
number={2},
pages={16-23},
abstract={The integration of precision farming techniques with the
Internet of Things, cloud computing, and big data analytics is vital to
increase productivity in the challenging dairy industry. At the same
time, the energy-efficiency and cost-effectiveness of networking
solutions that bring big data from farms to the cloud should be
addressed as regards economic and environmental sustainability. This
article presents an opportunistic networking paradigm for a
pasture-based dairy farm in which cows voluntarily participating in
robotic milking are used for mulling delay-tolerant data to a fog
computing node at an Internet gateway.},
keywords={Cloud computing;Dairy products;Energy efficiency;Internet of
Things;Logic gates;Precision engineering;Robots;Wireless
communication;delay-tolerant big data networking;distributed
systems;energy-efficiency;fog computing;opportunistic forwarding;smart
dairy farming},
doi={10.1109/MITP.2017.28},
ISSN={1520-9202},
month={March},}
@ARTICLE{7911188,
author={M. Bilal and J. E. Nichol},
journal={IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing},
title={Evaluation of the NDVI-Based Pixel Selection Criteria of the
MODIS C6 Dark Target and Deep Blue Combined Aerosol Product},
year={2017},
volume={PP},
number={99},
pages={1-6},
abstract={The moderate resolution and imaging spectroradiometer (MODIS)
Collection 6 (C6) level 2 operational aerosol product (MOD04) contains
the Dark Target (DT) and Deep Blue (DB) combined aerosol optical depth
(AOD) observations (DTB) at 10 km resolution, which is generated using
the selection criteria based on the static normalized difference
vegetation index (NDVI) as follows: 1) the DT AOD data are used for NDVI
> 0.3; 2) the DB AOD data are used for NDVI < 0.2; and 3) the average of
both algorithms or AOD data with highest quality flag are used for ≤ 0.2
NDVI ≤ 0.3. The objective of this study is to evaluate the NDVI pixel
selection criteria used in the DTB AOD product. For this, the DT, the
DB, and the DTB AOD retrievals are evaluated using the Aerosol Robotic
Network (AERONET) level 2.0 cloud-screened and quality-controlled AOD
data over Beijing from 2002 to 2014, Lahore from 2007 to 2013, and Paris
from 2005 to 2014. The DT and DB AOD retrievals considered by the DTB
product are tabulated. For comparison purposes, the MODIS level 3
monthly NDVI product (MOD13A3) at 1 km resolution is also tabulated
indicating how the NDVI-based pixel selection criteria operate for the
DT and DB AOD retrievals used in the DTB product. Results show that the
DT AOD retrievals for NDVI ≤ 0.3 are used in the DTB product, and this
increases the mean bias and percentage of retrievals above the expected
error. These results conclude that the DTB AOD product must follow the
dynamic NDVI values for pixel selection criteria.},
keywords={Aerosols;Clouds;Earth;Image resolution;Land
surface;MODIS;Optical surface waves;Aerosol Robotic Network
(AERONET);Beijing;Dark Target (DT);Deep Blue (DB);Lahore;MOD04 C6;Paris},
doi={10.1109/JSTARS.2017.2693289},
ISSN={1939-1404},
month={},}
@INPROCEEDINGS{7906787,
author={D. Sawhney},
booktitle={2016 IEEE Region 10 Humanitarian Technology Conference
(R10-HTC)},
title={Technology integration in Indian schools using a value-stream
based framework},
year={2016},
pages={1-6},
abstract={Technology use in the Indian classrooms has increased
manifolds in the last five years or so. Even though most of the
technology initiatives thrive in the private sector or the autonomous
higher education institutes, many educational technology companies were
launched in the last decade. Smart classrooms, online tutoring,
personalised learning content, learning management systems, content
delivery through tablets and laptops, cloud storage and file management,
robotics, integrated approach to technology are a few things that are
being discussed in the education space in India. But the challenges are
many-lack of professional development of teachers, access and
availability, mind-set and attitude of staff and lack of digital
leadership at management level in schools being a few areas. Working
with teachers and principals in Indian private schools, the researcher
has developed a teacher professional development model, which aims to
provide support for integrating technology in a more meaningful way,
identifying personal value-streams as the base of technology integration.},
keywords={Conferences;Educational technology;Government;Information and
communication technology;Training;Urban areas;ICT (Information and
Communication Technology);Indian schools;connectome;teacher professional
development;technology integration;value-streams},
doi={10.1109/R10-HTC.2016.7906787},
month={Dec},}
@ARTICLE{7795205,
author={J. Cleveland and D. Thakur and P. Dames and C. Phillips and T.
Kientz and K. Daniilidis and J. Bergstrom and V. Kumar},
journal={IEEE Transactions on Automation Science and Engineering},
title={Automated System for Semantic Object Labeling With Soft-Object
Recognition and Dynamic Programming Segmentation},
year={2017},
volume={14},
number={2},
pages={820-833},
abstract={This paper presents an automated robotic system for generating
semantic maps of inventory in retail environments. In retail settings,
semantic maps are labeled maps of stores where each discrete section of
shelving is assigned a department label describing the types of products
on that shelf. Starting from a metric map of the store, the robot
autonomously extracts the shelf boundaries, generates a distance-optimal
tour of the store to view every shelf, and follows the tour while
avoiding unmapped clutter and moving people. The robot creates a point
cloud of the store using the data collected from this tour. We introduce
a novel soft-object assignment algorithm to create a virtual map and a
dynamic programming algorithm to segment this map. These algorithms use
a priori information about the products to boost data from laser and
camera sensors in order to recognize and semantically label objects. The
primary contribution of this paper is the integration of multiple
systems for automated path planning, navigation, object recognition, and
semantic mapping. This paper represents an important contribution toward
deploying mobile robots in dynamic human environments.},
keywords={collision avoidance;computer graphics;dynamic
programming;image segmentation;inventory management;mobile robots;object
recognition;retailing;robot vision;automated path planning;automated
robotic system;distance-optimal tour;dynamic human environments;dynamic
programming segmentation;inventory;labeled maps;metric map;mobile
robots;navigation;point cloud;retail environments;semantic maps
generation;semantic object labeling;shelf boundaries;soft-object
assignment algorithm;soft-object recognition;virtual map;Computer
interfaces;Labeling;Navigation;Robot vision
systems;Semantics;Automation;robot vision systems;robots},
doi={10.1109/TASE.2016.2631085},
ISSN={1545-5955},
month={April},}
@ARTICLE{7875417,
author={E. Luberto and Y. Wu and G. Santaera and M. Gabiccini and A.
Bicchi},
journal={IEEE Robotics and Automation Letters},
title={Enhancing Adaptive Grasping Through a Simple Sensor-Based Reflex
Mechanism},
year={2017},
volume={2},
number={3},
pages={1664-1671},
abstract={This paper presents an approach to achieve adaptive grasp of
unknown objects whose position is only approximately known via
point-cloud data. We exploit the adaptability of a soft robotic hand
which can autonomously conform to the shape of a grasped object if
properly approached. Once a grasp approach has been preliminarily
planned based only on rough estimates of the object position, the hand
is shaped to a pregrasp configuration. Before closing the hand, a
sensor-based algorithm is applied that corrects the relative hand-object
posture so as to enhance the probability that the object is uniformly
approached by all fingers, thus avoiding undesired premature contacts.
The algorithm minimizes the distance between the hand's fingerpads and
the object by continuously controlling both the wrist pose and
orientation and the hand closure. Experimental studies with a Kuka-LWR
arm and a Pisa/IIT Softhand illustrate the benefit of the developed
technique and the improvement in grasping performance with respect to
open-loop execution of grasps planned on the basis of prior RGB-D cues
only.},
keywords={approximation theory;image colour analysis;manipulators;open
loop systems;pose estimation;robot vision;Kuka-LWR arm;Pisa-IIT
Softhand;RGB-D cues;adaptive grasping enhancement;grasps open-loop
execution;hand closure;hand fingerpads;object position rough
estimation;point-cloud data;sensor-based reflex mechanism;soft robotic
hand;wrist pose;Databases;Grasping;Robot sensing
systems;Three-dimensional displays;Uncertainty;Grasping;dexterous
manipulation;sensor-based control},
doi={10.1109/LRA.2017.2681122},
ISSN={2377-3766},
month={July},}
@ARTICLE{7546861,
author={S. F. Atashzar and M. Shahbazi and M. Tavakoli and R. V. Patel},
journal={IEEE Transactions on Control Systems Technology},
title={A Passivity-Based Approach for Stable Patient #x2013;Robot
Interaction in Haptics-Enabled Rehabilitation Systems: Modulated
Time-Domain Passivity Control},
year={2017},
volume={25},
number={3},
pages={991-1006},
abstract={In this paper, a novel passivity-based technique is proposed
to 1) analyze and 2) guarantee the stability of haptics-enabled
robotic/telerobotic systems when there is a possibility of having a
source of nonpassivity (namely, a nonpassive environment) in addition to
the conventional nonpassive component in teleoperation systems (namely,
a delayed communication channel). The need for the proposed technique is
motivated by safe and optimal implementation of the haptics-enabled
robotic, cloud-based, and remote rehabilitation systems. The objective
of the controller proposed in this paper is to perform minimum
alteration to the system transparency, in a dynamic and patient-specific
manner, by utilizing quantifiable biomechanical capability of the user's
limb (i.e., excess of passivity) in dissipating interactive energies to
guaranteeing human-robot interaction safety, in the context of the
strong passivity theorem. The proposed controller is named modulated
time-domain passivity control (M-TDPC) approach and is a new member of
the family of the state-of-the-art TDPC techniques. Simulations and
experimental results are presented in support of the proposed technique
and the developed theory.},
keywords={biomechanics;cloud computing;haptic interfaces;human-robot
interaction;patient rehabilitation;telerobotics;M-TDPC;cloud-based
systems;conventional nonpassive component;delayed communication
channel;dynamic patient-specific manner;haptics-enabled rehabilitation
systems;haptics-enabled robotic/telerobotic system stability;human-robot
interaction safety;interactive energy;modulated time-domain passivity
control;nonpassivity source;passivity-based approach;quantifiable
biomechanical capability;remote rehabilitation systems;stable
patient-robot interaction;teleoperation systems;user
limb;Biomechanics;Context;Haptic interfaces;Medical
treatment;Robots;Stability analysis;Time-domain analysis;Excess of
passivity (EOP);haptics-enabled systems;patient–robot (P–R)
interaction;telerobotic rehabilitation;time-domain passivity control
(TDPC)},
doi={10.1109/TCST.2016.2594584},
ISSN={1063-6536},
month={May},}
@INPROCEEDINGS{7876188,
author={C. Moreno and Yilin Chen and M. Li},
booktitle={2017 International Conference on Computing, Networking and
Communications (ICNC)},
title={A dynamic compression technique for streaming kinect-based Point
Cloud data},
year={2017},
pages={550-555},
abstract={With the relative inexpensiveness of 3D sensors today, it has
become easier to collect 3D spatial data. This opens up the usability
for these data, called point clouds, in areas such as robotics,
telemedicine, and entertainment. However, a primary issue is the data
rate of point clouds being produced by sensors such as the Microsoft
Kinect. When transmitted, the size of a single point cloud coupled with
the high frame rate results in an impractical bandwidth requirement.
Therefore, compression is required. However, current compression
techniques offered by the Point Cloud Library (PCL) are static in terms
of available network bandwidth, meaning the techniques do not adjust as
the network changes. In this paper, we propose using a dynamic
compression technique that adjusts the compression ratio in response to
the network throughput. Experimental results under both static and
dynamic network traffic conditions show that dynamic compression is
promising in achieving higher and smoother frame rates compared with the
built-in compression algorithm in PCL.},
keywords={data compression;image sensors;telecommunication traffic;video
coding;video communication;video streaming;3D sensors;3D spatial
data;Kinect-based point cloud data streaming;Microsoft Kinect;PCL;data
usability;dynamic compression technique;dynamic network traffic
conditions;network bandwidth;point cloud library;static network traffic
conditions;video transmission;Filtering;Octrees;Real-time
systems;Sensors;Streaming media;Three-dimensional
displays;Throughput;Kinect;compression;point cloud;real-time;video
transmission},
doi={10.1109/ICCNC.2017.7876188},
month={Jan},}
@INPROCEEDINGS{7868418,
author={S. Ricardo and D. Bein and A. Panagadan},
booktitle={2017 IEEE 7th Annual Computing and Communication Workshop and
Conference (CCWC)},
title={Low-cost, real-time obstacle avoidance for mobile robots},
year={2017},
pages={1-7},
abstract={The goal of this project^1 is to advance the field of
automation and robotics by utilizing recently-released, low-cost sensors
and microprocessors to develop a mechanism that provides
depth-perception and autonomous obstacle avoidance in a plug-and-play
fashion. We describe the essential hardware components that can enable
such a low-cost solution and an algorithm to avoid static obstacles
present in the environment. The mechanism utilizes a novel single-point
LIDAR module that affords more robustness and invariance than popular
approaches, such as Neural Networks and Stereo. When this hardware is
coupled with the proposed efficient obstacle avoidance algorithm, this
mechanism is able to accurately represent environments through point
clouds and construct obstacle-free paths to a destination, in a small
timeframe. A prototype mechanism has been installed on a quadcopter for
visualization on how actual implementation may take place^2 . We
describe experimental results based on this prototype.},
keywords={collision avoidance;control engineering computing;mobile
robots;autonomous obstacle avoidance;depth-perception;hardware
components;low-cost sensors;microprocessors;mobile robots;obstacle
avoidance algorithm;obstacle-free paths;plug-and-play fashion;point
clouds;quadcopter;single-point LIDAR module;static obstacle
avoidance;Algorithm design and analysis;Collision
avoidance;Hardware;Laser radar;Robots;Sensors;Three-dimensional
displays;Lidar;UAV;microcontroller;path planning;quadcopter},
doi={10.1109/CCWC.2017.7868418},
month={Jan},}
@INPROCEEDINGS{7849757,
author={H. Y. Wang and W. K. Ling},
booktitle={2016 IEEE International Conference on Consumer
Electronics-China (ICCE-China)},
title={Robotic grasp detection using deep learning and geometry model of
soft hand},
year={2016},
pages={1-6},
abstract={This paper proposes a method using deep learning and geometry
model of soft hand for detecting the graspable positions and
orientations of the objects from clustered scenes, given a point cloud
from a single depth camera placed in the middle of the robot. With the
method this paper proposed, which is different with others, it takes the
collision problem into consideration without any segmentation and
recognition of the objects. Significantly, this paper also analyses the
computational complexity and the spacial complexity of CNN (a kind of
deep learning), which can provide some useful tips for engineer to
design the deep learning models. Firstly, a geometry model of soft hand
(more flexible than hard hand) will be designed for searching appreciate
closing cube in the 3D point clouds. A closing cube is regarded as a
grasp hypothesis, which should be whole contained in the geometry model
of soft hand without any collisions. And it includes all the parameters
of positions and orientations which can be used to robotic grasping.
Secondly, we use a deep learning method to classify and rank these grasp
hypotheses, so that we can find out the best handle. Notably, the
labeled training data set will be generated automatically with some
criteria. As to say, these criteria, with which to make decision whether
the grasp hypotheses are handles or not, will be instead of the trained
deep learning model.},
keywords={collision avoidance;computational complexity;geometry;learning
(artificial intelligence);manipulators;robot vision;3D point
clouds;CNN;clustered scenes;collision problem;computational
complexity;deep learning;geometry model;grasp hypotheses
classification;grasp hypothesis;graspable position detection;object
orientation detection;robotic grasp detection;single depth camera;soft
hand;spacial complexity;training data set;Conferences},
doi={10.1109/ICCE-China.2016.7849757},
month={Dec},}
@INPROCEEDINGS{7844019,
author={H. Sun and Z. Meng and X. Shen and M. H. Ang},
booktitle={2016 IEEE/SICE International Symposium on System Integration
(SII)},
title={Detection and state estimation of moving objects on a moving base
for indoor navigation},
year={2016},
pages={325-330},
abstract={This paper presents a method for detection and trajectory
estimation of moving objects such as walking humans to achieve efficient
indoor navigation of a mobile manipulator. Mobile manipulation requires
interactions of the robot with objects such as humans and robots, which
are usually in motion, thus it is important that a robot can detect
moving humans and robots, estimate their trajectories for collision
avoidance and path planning in order to improve safety and travel
efficiency. We introduce our omni-directional mobile robotics system
which is specially designed for efficient navigation in cluttered indoor
environments with narrow passages. To achieve detection and trajectory
estimation of moving objects while the robot base itself is also in
motion, relative position information between the base and the world is
necessary in our algorithm and it is obtained by a proposed robust fused
odometry. Moving objects are detected with their 3D bounding boxes by
integration of the fused odometry and a proposed algorithm using point
cloud captured by Kinect. At the same time, velocities and trajectories
of the moving objects are also estimated and can be used for path
planning and moving obstacle avoidance. The overall system has been
implemented on our robot base for real practice.},
keywords={collision avoidance;distance measurement;indoor
navigation;manipulators;mobile robots;object detection;robot
vision;state estimation;trajectory control;3D bounding
boxes;Kinect;collision avoidance;human-robot interactions;indoor
navigation;mobile manipulator;moving object detection;moving object
trajectory estimation;moving obstacle avoidance;omnidirectional mobile
robotics system;path planning;point cloud;robot object
interactions;robust fused odometry;state estimation;Collision
avoidance;Laser radar;Octrees;Robot kinematics;Three-dimensional
displays;Trajectory},
doi={10.1109/SII.2016.7844019},
month={Dec},}
@INPROCEEDINGS{7824221,
author={M. T. Sqalli and K. Tatsuno and K. Kurabe and H. Ando and H.
Obitsu and R. Itakura and T. Aoto and K. Yoshino},
booktitle={2016 International Symposium on Micro-NanoMechatronics and
Human Science (MHS)},
title={Improvement of a tele-presence robot autonomous navigation Using
SLAM algorithm},
year={2016},
pages={1-7},
abstract={As the processing power and the science of robotics advances,
robots are becoming more and more available to the masses and convenient
to use. One instance of the applications of robots are the moving
robots. These robots essentially move and explore the world they are
located in, mainly: indoor robots. In our laboratory we are currently
working on an indoor robot which serves the purpose of teleperesence.
One of the challenges in building such robot is the navigation, mainly
planning routes, and avoiding the obstacles that are in front of it. In
order to navigate, the robot needs to have a map of the environment
where it is located. Plus, it needs to locate itself in that map. This
paper discusses an improved obstacle avoidance algorithm to our
laboratory's experimental telepresence robot. The improved algorithm is
based on Simultaneous Localization and Mapping (SLAM) This new obstacle
avoidance algorithm uses a Kinect 2 to collect depth frames and RGB
images then synthesize them into Point Cloud files (PCL). These Point
clouds are processed and gathered into a 3D map and used as input to the
Simultaneous Localization and Mapping (SLAM) algorithm to help the robot
navigate as smooth as possible in its indoor environment. The robot
navigates by creating a map of its surroundings and constantly
localizing itself in that map.},
keywords={SLAM (robots);collision avoidance;image colour analysis;indoor
environment;indoor navigation;robot vision;telerobotics;3D map;PCL;RGB
images;SLAM algorithm;autonomous navigation;depth frames;indoor
environment;indoor robot;laboratory experimental telepresence
robot;obstacle avoidance;point cloud files;robot navigation;route
planning;simultaneous localization and mapping;Collision
avoidance;Navigation;Simultaneous localization and mapping;Software
algorithms;3D Mapping;Computer Vision;Obstacle Avoidance;Robotics;SLAM},
doi={10.1109/MHS.2016.7824221},
month={Nov},}
@ARTICLE{7820198,
author={U. Asif and M. Bennamoun and F. A. Sohel},
journal={IEEE Transactions on Robotics},
title={RGB-D Object Recognition and Grasp Detection Using Hierarchical
Cascaded Forests},
year={2017},
volume={PP},
number={99},
pages={1-18},
abstract={This paper presents an efficient framework to perform
recognition and grasp detection of objects from RGB-D images of real
scenes. The framework uses a novel architecture of hierarchical cascaded
forests, in which object-class and grasp-pose probabilities are computed
at different levels of an image hierarchy (e.g., patch and object
levels) and fused to infer the class and the grasp of unseen objects. We
introduce a novel training objective function that minimizes the
uncertainties of the class labels and the grasp ground truths at the
leaves of the forests, thereby enabling the framework to perform the
recognition and grasp detection of objects. Our objective function is
learned from features that are extracted from RGB-D point clouds of the
objects. For that, we propose a novel method to encode an RGB-D point
cloud into a representation that facilitates the use of large
convolution neural networks to extract discriminative features from
RGB-D images. We evaluate our framework on challenging object datasets,
where we demonstrate that our framework outperforms the state-of-the-art
methods in terms of object-recognition and grasp-detection accuracies.
We also show experiments by using live video streams from a Kinect
mounted on our in-house robotic platform.},
keywords={Feature extraction;Grasping;Image color analysis;Object
recognition;Robots;Three-dimensional displays;Training;Grasp
detection;RGB-D object recognition;robotic grasping},
doi={10.1109/TRO.2016.2638453},
ISSN={1552-3098},
month={},}
@ARTICLE{7482793,
author={L. Chen and Y. He and J. Chen and Q. Li and Q. Zou},
journal={IEEE Transactions on Intelligent Transportation Systems},
title={Transforming a 3-D LiDAR Point Cloud Into a 2-D Dense Depth Map
Through a Parameter Self-Adaptive Framework},
year={2017},
volume={18},
number={1},
pages={165-176},
abstract={The 3-D LiDAR scanner and the 2-D charge-coupled device (CCD)
camera are two typical types of sensors for surrounding-environment
perceiving in robotics or autonomous driving. Commonly, they are jointly
used to improve perception accuracy by simultaneously recording the
distances of surrounding objects, as well as the color and shape
information. In this paper, we use the correspondence between a 3-D
LiDAR scanner and a CCD camera to rearrange the captured LiDAR point
cloud into a dense depth map, in which each 3-D point corresponds to a
pixel at the same location in the RGB image. In this paper, we assume
that the LiDAR scanner and the CCD camera are accurately calibrated and
synchronized beforehand so that each 3-D LiDAR point cloud is aligned
with its corresponding RGB image. Each frame of the LiDAR point cloud is
then projected onto the RGB image plane to form a sparse depth map.
Then, a self-adaptive method is proposed to upsample the sparse depth
map into a dense depth map, in which the RGB image and the anisotropic
diffusion tensor are exploited to guide upsampling by reinforcing the
RGB-depth compactness. Finally, convex optimization is applied on the
dense depth map for global enhancement. Experiments on the KITTI and
Middlebury data sets demonstrate that the proposed method outperforms
several other relevant state-of-the-art methods in terms of visual
comparison and root-mean-square error measurement.},
keywords={CCD image sensors;charge-coupled devices;convex
programming;image capture;image colour analysis;image
enhancement;intelligent transportation systems;optical radar;tensors;2D
charge-coupled device camera;2D dense depth map;3D LiDAR point cloud
capture;CCD camera;RGB image plane;anisotropic diffusion tensor;color
information;convex optimization;image enhancement;intelligent
vehicle;parameter self-adaptive framework;shape information;Anisotropic
magnetoresistance;Cameras;Image color analysis;Laser
radar;Measurement;Tensile stress;Three-dimensional displays;3D-2D
conversion;Intelligent vehicle;dense depth map;global
enhancement;upsampling},
doi={10.1109/TITS.2016.2564640},
ISSN={1524-9050},
month={Jan},}
@INPROCEEDINGS{7796986,
author={X. Huang and J. Zhang and Q. Wu and L. Fan and C. Yuan},
booktitle={2016 International Conference on Digital Image Computing:
Techniques and Applications (DICTA)},
title={A Coarse-to-Fine Algorithm for Registration in 3D Street-View
Cross-Source Point Clouds},
year={2016},
pages={1-6},
abstract={With the development of numerous 3D sensing technologies,
object registration on cross-source point cloud has aroused researchers'
interests. When the point clouds are captured from different kinds of
sensors, there are large and different kinds of variations. In this
study, we address an even more challenging case in which the
differently-source point clouds are acquired from a real street view.
One is produced directly by the LiDAR system and the other is generated
by using VSFM software on image sequence captured from RGB cameras. When
it confronts to large scale point clouds, previous methods mostly focus
on point-to-point level registration, and the methods have many
limitations.The reason is that the least mean error strategy shows poor
ability in registering large variable cross-source point clouds. In this
paper, different from previous ICP-based methods, and from a statistic
view, we propose a effective coarse-to-fine algorithm to detect and
register a small scale SFM point cloud in a large scale Lidar point
cloud. Seen from the experimental results, the model can successfully
run on LiDAR and SFM point clouds, hence it can make a contribution to
many applications, such as robotics and smart city development.},
keywords={image sequences;least mean squares methods;optical radar;smart
cities;3D street-view cross-source point clouds;ICP-based methods;LiDAR
system;RGB cameras;SFM point cloud;VSFM software;coarse-to-fine
algorithm;cross-source point cloud;image sequence;least mean error
strategy;numerous 3D sensing technologies;object
registration;point-to-point level registration;Complexity theory;Laser
radar;Robot sensing systems;Shape;Smart cities;Three-dimensional displays},
doi={10.1109/DICTA.2016.7796986},
month={Nov},}
@INPROCEEDINGS{7792930,
booktitle={IECON 2016 - 42nd Annual Conference of the IEEE Industrial
Electronics Society},
title={Proceedings},
year={2016},
pages={1-263},
abstract={The following topics are dealt with: Industrial Internet of
Things; cyber-physical systems; motion control; electrical machine
drives; power electronics; HVDC technologies; power converter; machine
control; fault diagnosis; mechatronics; robotics and haptics; image
processing; artificial intelligence; renewable energy system; energy
storage; photovoltaic system; transportation electrification; vehicle
system; cloud computing; Big Data; industrial informatics; MEMS;
nanotechnologies; smart metering; smart power grids; energy Internet;
fuel cell power system; wind turbines; smart cities; data centers;
radiocommunication; induction heating; fusion plasma facilities; and
power transformers.},
keywords={Big Data;HVDC power transmission;Internet;Internet of
Things;artificial intelligence;cloud computing;computer centres;fault
diagnosis;fuel cells;haptic interfaces;image processing;induction
heating;machine control;mechatronics;micromechanical devices;motion
control;motor drives;photovoltaic power systems;plasma
applications;power convertors;power
transformers;radiocommunication;renewable energy sources;robots;wind
turbines;Big Data;HVDC technologies;IEEE industrial electronics
society;MEMS;artificial intelligence;cloud computing;cyber-physical
systems;data centers;electrical machine drives;energy Internet;energy
storage;fault diagnosis;fuel cell power system;fusion plasma
facilities;haptics;image processing;induction heating;industrial
Internet of Things;industrial informatics;machine
control;mechatronics;motion control;nanotechnologies;photovoltaic
system;power converter;power electronics;power
transformers;radiocommunication;renewable energy system;robotics;smart
cities;smart metering;smart power grids;transportation
electrification;vehicle system;wind turbines},
doi={10.1109/IECON.2016.7792930},
month={Oct},}
@INPROCEEDINGS{7789476,
author={C. A. Garcia-Perez and P. Merino},
booktitle={2016 IEEE 1st International Workshops on Foundations and
Applications of Self* Systems (FAS*W)},
title={Enabling Low Latency Services on LTE Networks},
year={2016},
pages={248-255},
abstract={The upcoming 5G technologies promise to enable ultra low
latency services such as remote robotics, augmented reality or vehicle
to vehicle communications. Fog and MEC computing can enable low latency
services for many different scenarios by moving the cloud and some
network functions closer to the user. In the case of mobile networks
this improvement is traditionally introduced in the reference point that
defines the limit of the operator domain. In this paper we explore the
introduction of an intermediate component in the LTE standard
architecture, describing its functionality and providing experimental
results on its use. This component, called Fog Gateway, can process the
data plane for specific services to prevent all the traffic reaching the
core network. The gateway analyzes the GTP traffic inner destination IP
in order to determine whether to route the packet to the fog network or
forward it to the destination SGW. The solution is compliant with
standard LTE equipment all along the path (UE, eNodeB, EPC), so it can
be implemented in current networks, and the preliminary figures,
obtained combining emulated and COTS equipment, show an improvement of
up to the 78% in terms of latency reduction.},
keywords={5G mobile communication;Long Term Evolution;cloud
computing;internetworking;5G technology;COTS equipment;EPC;GTP traffic
inner destination IP;LTE standard architecture;MEC
computing;UE;augmented reality;data plane;destination SGW;eNodeB;fog
computing;fog gateway;fog network;latency reduction;mobile
networks;network function;remote robotics;standard LTE
networks;ultralow-latency services;vehicle-to-vehicle
communications;Computer architecture;Delays;Long Term Evolution;Mobile
communication;Mobile computing;Proposals;Vehicles;5G;Fog
Computing;LTE;Low Latency;Mobile Edge Computing},
doi={10.1109/FAS-W.2016.59},
month={Sept},}
@INPROCEEDINGS{7783535,
author={A. T. Angonese and P. F. F. Rosa},
booktitle={2016 XIII Latin American Robotics Symposium and IV Brazilian
Robotics Symposium (LARS/SBR)},
title={Integration of People Detection and Simultaneous Localization and
Mapping Systems for an Autonomous Robotic Platform},
year={2016},
pages={251-256},
abstract={This paper presents the implementation of a people detection
system for a robotic platform able to perform Simultaneous Localization
and Mapping (SLAM), allowing the exploration and navigation of the robot
considering people detection interaction. The robotic platform consists
of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200
sensor laser and a computer using the robot operating system ROS. The
idea is to integrate the people detection system to the simultaneous
localization and mapping (SLAM) system of the robot using ROS.
Furthermore, this paper presents an evaluation of two different
approaches for the people detection system. The first one uses a manual
feature extraction technique, and the other one is based on deep
learning methods. The manual feature extraction method in the first
approach is based on HOG (Histogram of Oriented Gradients) detectors.
The accuracy of the techniques was evaluated using two different
libraries. The PCL library (Point Cloud Library) implemented in C ++ and
the VLFeat MatLab library with two HOG variants, the original one, and
the DPM (Deformable Part Model) variant. The second approaches are based
on a Deep Convolutional Neural Network (CNN), and it was implemented
using the MatLab MatConvNet library. Tests were made objecting the
evaluation of losses and false positives in the people's detection
process in both approaches. It allowed us to evaluate the people
detection system during the navigation and exploration of the robot,
considering the real time interaction of people recognition in a
semi-structured environment.},
keywords={SLAM (robots);feature extraction;gradient methods;image
sensors;neurocontrollers;path planning;robot vision;C ++;CNN;DPM;HOG
variants;MatLab MatConvNet library;PCL library;Pioneer 3DX robot;RGB-D
camera;SLAM;Sick Lms200 sensor laser;VLFeat MatLab library;autonomous
robotic platform;deep convolutional neural network;deformable part model
variant;histogram of oriented gradients detectors;manual feature
extraction technique;mapping systems;people detection integration;people
detection interaction;people detection process;people detection
system;point cloud library;robot navigation;robot operating system
ROS;robotic platform;simultaneous localization;simultaneous localization
and mapping;Feature extraction;Histograms;Libraries;MATLAB;Mathematical
model;Simultaneous localization and mapping;CNN;Deep Learning;HOG;People
Detection;Simultaneous Localization and Mapping (SLAM);robot operating
system ROS},
doi={10.1109/LARS-SBR.2016.49},
month={Oct},}
@INPROCEEDINGS{7776603,
author={P. Simoens and C. Mahieu and F. Ongenae and F. D. Backere and S.
D. Pestel and J. Nelis and F. D. Turck and S. A. Elprama and K. Kilpi
and C. Jewell and A. Jacobs},
booktitle={2016 5th IEEE International Conference on Cloud Networking
(Cloudnet)},
title={Internet of Robotic Things: Context-Aware and Personalized
Interventions of Assistive Social Robots (Short Paper)},
year={2016},
pages={204-207},
abstract={Assistive service and companion robots are versatile and
dexterous actuators that operate in our daily living environment. These
robots are able to manipulate physical objects, to displace themselves
and to engage in conversations. Human behavior is dynamic and oftentimes
unpredictable, therefore it is crucial for such robotic systems to be
assisted by a cloud-backend which: i) analyzes data from sensor and
wearables ii) determines which robotic tasks need to be executed and
iii) provides the necessary support for the execution of these tasks in
our daily living environment. In this paper, we present our
Internet-of-Robotic-Things system architecture design for a case study
on personal interactions by a companion robot to alleviate behavioral
disturbances of people with dementia.},
keywords={Internet of Things;data analysis;dexterous
manipulators;human-robot interaction;service robots;Internet of Robotic
Things;architecture design;assistive service-and-companion
robots;assistive social robots;daily living environment;data
analysis;dexterous actuators;Actuators;Context;Dementia;Legged
locomotion;Robot kinematics;Robot sensing systems;IoT;intervention;robot},
doi={10.1109/CloudNet.2016.27},
month={Oct},}
@INPROCEEDINGS{7776602,
author={P. Grefen and I. Vanderfeesten and G. Boultadakis},
booktitle={2016 5th IEEE International Conference on Cloud Networking
(Cloudnet)},
title={Supporting Hybrid Manufacturing: Bringing Process and Human/Robot
Control to the Cloud (Short Paper)},
year={2016},
pages={200-203},
abstract={In this paper, we outline the architecture design of the HORSE
system for support of hybrid manufacturing processes. The HORSE project
aims at providing integrated end-to-end manufacturing process management
and advanced control of hybrid manufacturing work cells. In these hybrid
cells, human and robotic actors collaborate flexibly and safely. As the
HORSE system should support SME organizations, we map part of the
architecture to a cloud environment to avoid costly and complex software
installations for small organizations. We show how well-structured
architecture design is the basis for a modular software structure of a
complex cyber-physical system with partial cloud support.},
keywords={cellular manufacturing;cloud computing;control engineering
computing;industrial robots;production engineering
computing;small-to-medium enterprises;HORSE project;HORSE system
architecture design;SME organizations;cloud environment;cyber-physical
system;end-to-end manufacturing process management;human/robot
control;hybrid manufacturing processes;hybrid manufacturing work cell
advanced control;modular software structure;Cloud computing;Computer
architecture;Manufacturing processes;Robots;Stakeholders;cloud
computing;cyber-physical system;manufacturing system;process
support;robot control},
doi={10.1109/CloudNet.2016.39},
month={Oct},}
@INPROCEEDINGS{7776599,
author={A. G. Thallas and K. Panayiotou and E. Tsardoulias and A. L.
Symeonidis and P. A. Mitkas and G. G. Karagiannis},
booktitle={2016 5th IEEE International Conference on Cloud Networking
(Cloudnet)},
title={Relieving Robots from Their Burdens: The Cloud Agent Concept
(Short Paper)},
year={2016},
pages={188-191},
abstract={The consumer robotics concept has already invaded our everyday
lives, however two major drawbacks have become apparent both for the
roboticists and the consumers. The first is that these robots are
pre-programmed to perform specific tasks and usually their software is
proprietary, thus not open to "interventions". The second is that even
if their software is open source, low-cost robots usually lack
sufficient resources such as CPU power or memory capabilities, thus
forbidding advanced algorithms to be executed in-robot. Within the
context of RAPP (Robotic Applications for Delivering Smart User
Empowering Applications) we treat robots as platforms, where
applications can be downloaded and automatically deployed. Furthermore,
we propose and implement a novel multi-agent architecture, empowering
robots to offload computations in entities denoted as Cloud Agents. This
paper discusses the respective architecture in detail.},
keywords={cloud computing;multi-agent systems;public domain
software;robot programming;service robots;software agents;RAPP;cloud
agent concept;consumer robotics concept;low-cost robots;multiagent
architecture;open source software;robot preprogramming;robotic
applications for delivering smart user empowering applications;Cloud
computing;Computer architecture;Containers;Robots;Sockets;Web servers},
doi={10.1109/CloudNet.2016.38},
month={Oct},}
@INPROCEEDINGS{7759050,
author={I. Bogoslavskyi and C. Stachniss},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Fast range image-based segmentation of sparse 3D laser scans for
online operation},
year={2016},
pages={163-169},
abstract={Object segmentation from 3D range data is an important topic
in mobile robotics. A robot navigating in a dynamic environment needs to
be aware of objects that might change or move. A segmentation of the
laser scans into individual objects is typically the first processing
step before a further analysis is performed. In this paper, we present a
fast method that segments 3D range data into different objects, runs
online, and has small computational demands. Our approach avoids the
explicit computation of the 3D point cloud and performs all computations
directly on a 2D range image, which enables a fast segmentation for each
scan. A further relevant aspect of our method is that we can segment
objects even if the 3D data is sparse. This is important for scanners
such as the new Velodyne Puck. We implemented our approach in C++ and
ROS and thoroughly tested it using different 3D scanners. Our method can
operate at over 100 Hz for the 64-beam Velodyne scanner on a single core
of a mobile CPU while producing high quality segmentation results. In
addition to this, we make the source code for the approach available.},
keywords={image segmentation;mobile robots;optical radar;2D range
image;3D point cloud;3D range data;3D range data
segmentation;C++;ROS;Velodyne scanner;fast range image-based
segmentation;mobile CPU;mobile robotics;object segmentation;online
operation;robot navigation;sparse 3D laser scans;Automobiles;Image
segmentation;Laser beams;Measurement by laser
beam;Robots;Sensors;Three-dimensional displays},
doi={10.1109/IROS.2016.7759050},
month={Oct},}
@INPROCEEDINGS{7761098,
author={A. Inzartsev and A. Pavin and A. Kleschev and V. Gribova and G.
Eliseenko},
booktitle={OCEANS 2016 MTS/IEEE Monterey},
title={Application of artificial intelligence techniques for fault
diagnostics of autonomous underwater vehicles},
year={2016},
pages={1-6},
abstract={Autonomous underwater robotic vehicles (AUVs) are equipped
with monitoring and emergency systems (MESs) to increase the mission
success rate. The MES ensures the AUV's safety in water and the fault
tolerance of its subsystems. The signals produced by the self-test
functionality of robot subsystems as well as the parameters measured by
sensors are the source information for the MES. Nowadays, MES actions in
the AUVs designed by the Institute for Marine Technology Problems (IMTP)
are based upon the hypothesis of the isolation of signals. As a result,
the robot's reaction to several quasi-simultaneous signals is not always
adequate. It is possible to overcome the disadvantages of the existing
version of MES with the help of an ontological approach to be used for
the implementation of an intelligent MES (IMES). An IMES functions as a
tactic-level agent of the information and control system can carry out
diagnostic actions (submissions) if necessary. The compiler forms the
IMES with the help of a preset knowledge base and description of the
robotic configuration with the application of the cloud platform
IACPaaS. Knowledge is a semantic network. The architecture of the AUV's
IMES consists of three basic blocks: a block for the formation of
knowledge on AUV fault diagnostics, a translator that translates
knowledge into program code, and the AUV MES itself. Extensive testing
of IMES as part of AUV is planned soon.},
keywords={autonomous underwater vehicles;cloud computing;fault
diagnosis;knowledge based systems;ontologies (artificial
intelligence);sensors;signal processing;AUV IMES function;AUV fault
diagnostics;AUV safety;IACPaaS;IMES functions;IMTP;Institute for Marine
Technology Problems;MES actions;artificial intelligence
techniques;autonomous underwater robotic vehicles;cloud
platform;diagnostic actions;fault diagnostics;fault
tolerance;intelligent MES;monitoring and emergency systems;ontological
approach;quasisimultaneous signals;robot subsystems;robotic
configuration;self-test functionality;Circuit faults;Knowledge based
systems;Noise measurement;Robot sensing systems;Vehicles;autonomous
underwater vehicle;intelligent monitoring and emergency system;knowledge
base;mission replanning;ontology of diagnostic situation},
doi={10.1109/OCEANS.2016.7761098},
month={Sept},}
@INPROCEEDINGS{7759112,
author={S. Caccamo and Y. Bekiroglu and C. H. Ek and D. Kragic},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Active exploration using Gaussian Random Fields and Gaussian
Process Implicit Surfaces},
year={2016},
pages={582-589},
abstract={In this work we study the problem of exploring surfaces and
building compact 3D representations of the environment surrounding a
robot through active perception. We propose an online probabilistic
framework that merges visual and tactile measurements using Gaussian
Random Field and Gaussian Process Implicit Surfaces. The system
investigates incomplete point clouds in order to find a small set of
regions of interest which are then physically explored with a robotic
arm equipped with tactile sensors. We show experimental results obtained
using a PrimeSense camera, a Kinova Jaco2 robotic arm and Optoforce
sensors on different scenarios. We then demostrate how to use the online
framework for object detection and terrain classification.},
keywords={Gaussian processes;cameras;computer graphics;image
classification;image representation;manipulators;object
detection;probability;random processes;robot vision;tactile
sensors;Gaussian process implicit surfaces;Gaussian random fields;Kinova
Jaco2 robotic arm;Optoforce sensors;PrimeSense camera;active
exploration;active perception;compact 3D representations;object
detection;online probabilistic framework;point clouds;tactile
measurements;tactile sensors;terrain classification;visual
measurements;Gaussian processes;Robot sensing systems;Shape;Surface
reconstruction;Surface treatment;Three-dimensional displays;Active
perception;Gaussian process;Implicit surface;Random field;Surface
reconstruction;Tactile exploration},
doi={10.1109/IROS.2016.7759112},
month={Oct},}
@INPROCEEDINGS{7759436,
author={L. Twardon and H. Ritter},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Active Boundary Component Models for robotic dressing assistance},
year={2016},
pages={2811-2818},
abstract={The dynamics of deformable objects, especially that of highly
flexible articles of clothing, is difficult to model. This is due to
their vast number of degrees of freedom in addition to the noisy and
incomplete measurements robots have to cope with. Therefore, we suggest
focusing on the structures and object parts which are relevant to the
task at hand. The openings (e.g., at the waist, leg or sleeve ends)
characterize garments surprisingly well, not only from a topological
perspective, but also in terms of their inherent function, namely
dressing. We model openings as closed, oriented chains of movable points
which we refer to as Active Boundary Component Models (ABCMs). Compared
with the hardly predictable motions of an overall piece of clothing,
relatively strict assumptions regarding the dynamics of these contour
models can be made. We express these assumptions through position-based
constraints which drastically restrict the degrees of freedom. In the
present paper, we show how ABCMs can be initialized exploiting geometric
prior knowledge of garments, and how they can be tracked visually using
3D point cloud data. Additionally, we consider the task of sliding a rod
through a pant leg as a first step toward robotic dressing assistance
for physically handicapped persons.},
keywords={clothing;handicapped aids;object tracking;robot vision;3D
point cloud data;ABCMs;active boundary component models;clothing;contour
models;deformable objects dynamics;garments;physically handicapped
persons;position-based constraints;robotic dressing assistance;visual
tracking;Clothing;Legged locomotion;Robot
kinematics;Skeleton;Three-dimensional displays;Topology},
doi={10.1109/IROS.2016.7759436},
month={Oct},}
@INPROCEEDINGS{7759724,
author={M. M. Zhang and M. D. Kennedy and M. A. Hsieh and K. Daniilidis},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={A triangle histogram for object classification by tactile sensing},
year={2016},
pages={4931-4938},
abstract={We present a new descriptor for tactile 3D object
classification. It is invariant to object movement and simple to
construct, using only the relative geometry of points on the object
surface. We demonstrate successful classification of 185 objects in 10
categories, at sparse to dense surface sampling rate in point cloud
simulation, with an accuracy of 77.5% at the sparsest and 90.1% at the
densest. In a physics-based simulation, we show that contact clouds
resembling the object shape can be obtained by a series of gripper
closures using a robotic hand equipped with sparse tactile arrays.
Despite sparser sampling of the object's surface, classification still
performs well, at 74.7%. On a real robot, we show the ability of the
descriptor to discriminate among different object instances, using data
collected by a tactile hand.},
keywords={grippers;image classification;robot vision;tactile
sensors;contact clouds;dense surface sampling rate;gripper
closures;object classification;object instances;object
movement;physics-based simulation;point cloud simulation;robotic
hand;sparse tactile arrays;tactile 3D object classification;tactile
hand;tactile sensing;triangle histogram;Histograms;Robot sensing
systems;Robustness;Shape;Three-dimensional displays},
doi={10.1109/IROS.2016.7759724},
month={Oct},}
@INPROCEEDINGS{7745232,
author={M. Nejati and B. D. Argall},
booktitle={2016 25th IEEE International Symposium on Robot and Human
Interactive Communication (RO-MAN)},
title={Automated incline detection for assistive powered wheelchairs},
year={2016},
pages={1007-1012},
abstract={This work presents an algorithm for automated real-time ramp
detection using 3D point cloud data in the context of shared-control
powered wheelchairs. Limitations in the interfaces available to those
with severe motor impairments can make basic maneuvering tasks with
powered wheelchairs difficult. Although a significant amount of work has
been done on obstacle detection and avoidance, much less attention has
been given to algorithms for the safe and reliable detection of ramps
and inclines; even though navigating these structures is an important
part of urban life. We provide an algorithmic solution for accurately
detecting traversable inclines for applications with powered wheelchairs
using the Point Cloud Library (PCL) within the Robotics Operating System
(ROS) framework. All algorithms are implemented first in simulation and
later evaluated on data obtained from indoor and outdoor urban
environments. We measure the performance of our algorithm with
systematic testing on several different ramp structures, observed from
varied viewpoints. Results show that our algorithm is successful in
detecting the orientation, slope, and width of traversable ramps with up
to 100% accuracy and an average detection accuracy of 88%.},
keywords={control engineering computing;handicapped aids;mobile
robots;object detection;operating systems (computers);robot
vision;wheelchairs;3D point cloud data;PCL;ROS framework;assistive
powered wheelchairs;automated incline detection;automated real-time ramp
detection;basic maneuvering tasks;point cloud library;robotics operating
system;severe motor impairments;shared-control powered
wheelchairs;traversable incline detection;traversable ramp
detection;Image edge detection;Mobile robots;Navigation;Robot sensing
systems;Three-dimensional displays;Wheelchairs},
doi={10.1109/ROMAN.2016.7745232},
month={Aug},}
@INPROCEEDINGS{7733689,
author={A. Tellaeche and I. Maurtua and A. Ibarguren},
booktitle={2016 IEEE 21st International Conference on Emerging
Technologies and Factory Automation (ETFA)},
title={Use of machine vision in collaborative robotics: An industrial
case.},
year={2016},
pages={1-6},
abstract={Robotic applications are evolving to a paradigm of
collaborative robotics, where human workers and compliant robots work
together to solve complex tasks, until now done fully manually. These
tasks also present another challenging issue for the use of robotics,
variability in part and tool positions, in robot placement and even in
the targets for the robot operation. To comply with all this
uncertainties while solving the work efficiently, the robot needs to be
equipped with sensors that allow it perceive the environment. For this,
machine vision techniques in all its variants (2D, 3D, point clouds)
becomes fundamental. This paper outlines a real industrial case of
collaborative robotics, and the details of use of machine vision
techniques to cope with variability and uncertainties. The industrial
case presented has been developed as part of the EuRoC European project,
under the 7th European Framework.},
keywords={industrial robots;robot vision;uncertain systems;2D clouds;3D
clouds;7th European Framework;EuRoC European project;collaborative
robotics;compliant robots;human workers;machine vision techniques;point
clouds;real industrial case;robot operation;tool
positions;uncertainties;Cameras;Fasteners;Machine vision;Robot sensing
systems;Service robots;Three-dimensional displays;collaborative
robotics;human robot interaction;machine vision},
doi={10.1109/ETFA.2016.7733689},
month={Sept},}
@ARTICLE{7588245,
author={M. S. Net and I. D. Portillo and E. Crawley and B. Cameron},
journal={IEEE/OSA Journal of Optical Communications and Networking},
title={Approximation methods for estimating the availability of optical
ground networks},
year={2016},
volume={8},
number={10},
pages={800-812},
abstract={Optical communications are a key technology enabler to return
increasing amounts of data from space exploration platforms such as
robotic spacecraft in Earth orbit or across the solar system. However,
several challenges have hindered the deployment and utilization of this
technology in an operational context, most notably its sensitivity to
atmospheric impairments such as cloud coverage. To mitigate this
problem, building a network of interconnected and geographically
disperse ground stations has been proposed as a possible solution to
ensure that, at any point in time, at least one space-to-ground optical
link is available to contact the space-based platforms. In this paper,
we present a new approach for quantifying the availability of an optical
ground network that is both computationally inexpensive and suitable for
high-level architectural concept studies. Based on the cloud fraction
data set, several approximation methods are used to estimate the
probability of having a certain number of space-to-ground links fail due
to cloud coverage. They are developed in order to capture increasingly
complex atmospheric factors, from sites with independent weather
conditions, to stations that are both temporally and spatially
correlated. Then, the proposed approximation methods are benchmarked and
recommendations on how to utilize and implement them are finally
summarized.},
keywords={atmospheric optics;clouds;optical links;space communication
links;Earth orbit;approximation method;atmospheric impairments;cloud
coverage;cloud fraction data set;complex atmospheric
factors;geographically disperse ground stations;high-level architectural
concept studies;independent weather conditions;interconnected ground
stations;optical communication;optical ground networks;robotic
spacecraft;solar system;space exploration platforms;space-to-ground
optical link;Approximation methods;Atmospheric modeling;Clouds;Optical
fiber networks;Random variables;Space vehicles;Cloud fraction; Optical
ground stationdiversity; Space-to-ground optical communications},
doi={10.1364/JOCN.8.000800},
ISSN={1943-0620},
month={Oct},}
@ARTICLE{7350228,
author={M. Petit and T. Fischer and Y. Demiris},
journal={IEEE Transactions on Cognitive and Developmental Systems},
title={Lifelong Augmentation of Multimodal Streaming Autobiographical
Memories},
year={2016},
volume={8},
number={3},
pages={201-213},
abstract={Robot systems that interact with humans over extended periods
of time will benefit from storing and recalling large amounts of
accumulated sensorimotor and interaction data. We provide a principled
framework for the cumulative organization of streaming autobiographical
data so that data can be continuously processed and augmented as the
processing and reasoning abilities of the agent develop and further
interactions with humans take place. As an example, we show how a
kinematic structure learning algorithm reasons a-posteriori about the
skeleton of a human hand. A partner can be asked to provide feedback
about the augmented memories, which can in turn be supplied to the
reasoning processes in order to adapt their parameters. We employ
active, multimodal remembering, so the robot as well as humans can gain
insights of both the original and augmented memories. Our framework is
capable of storing discrete and continuous data in real-time. The data
can cover multiple modalities and several layers of abstraction (e.g.,
from raw sound signals over sentences to extracted meanings). We show a
typical interaction with a human partner using an iCub humanoid robot.
The framework is implemented in a platform-independent manner. In
particular, we validate its multi platform capabilities using the iCub,
Baxter and NAO robots. We also provide an interface to cloud based
services, which allow automatic annotation of episodes. Our framework is
geared towards the developmental robotics community, as it: 1) provides
a variety of interfaces for other modules; 2) unifies previous works on
autobiographical memory; and 3) is licensed as open source software.},
keywords={cloud computing;control engineering computing;human-robot
interaction;inference mechanisms;learning (artificial
intelligence);public domain software;Baxter;NAO robots;autobiographical
data;cloud based services;developmental robotics community;human hand
skeleton;iCub;interaction data;kinematic structure learning
algorithm;lifelong augmentation;multimodal remembering;multimodal
streaming autobiographical memories;open source software;reasoning
abilities;robot systems;sensorimotor data;Cognition;Data
visualization;Kinematics;Real-time
systems;Robots;Skeleton;Visualization;Autobiographical
memory;developmental robotics;human
feedback;reasoning;remembering;robotics},
doi={10.1109/TAMD.2015.2507439},
ISSN={2379-8920},
month={Sept},}
@INPROCEEDINGS{7559123,
author={M. Julia and M. Holloway and O. Reinoso and P. R. N. Childs},
booktitle={Proceedings of ISR 2016: 47st International Symposium on
Robotics},
title={Autonomous Surveying of Underfloor Voids},
year={2016},
pages={1-7},
abstract={In this paper, a novel robotic system that solves the problem
of autonomous mapping an underfloor void is presented. The approach is
based on a 3D laser scanner. A real time navigation system and a new
high level planner that selects the next best scanning position controls
the motion of the robot. Multiple scans are aligned using ICP and graph
optimization techniques. Finally, a point cloud fusion algorithm creates
a global model of the environment from the aligned scans. The survey
robot has been successfully deployed in a commercial application for
scanning underfloor voids before and after the application of thermal
insulation. Using this system, the robot was successfully able to
autonomously map the controlled test scenario. For some applications the
quantity of rubble within the void caused the real time navigation to
fail and teleoperation and manual initialization of the ICP algorithm
was necessary.},
month={June},}
@INPROCEEDINGS{7559178,
author={A. Kuss and U. Schneider and T. Dietz and A. Verl},
booktitle={Proceedings of ISR 2016: 47st International Symposium on
Robotics},
title={Detection of Assembly Variations for Automatic Program Adaptation
in Robotic Welding Systems},
year={2016},
pages={1-6},
abstract={This paper proposes a novel approach for detection of assembly
variations to adapt the programming of a robotic welding system. A
matching process is performed between the different parts of an assembly
CAD model and a measured point cloud using an Iterative Closest Point
algorithm to determine assembly variations. Experimental validation is
performed with an industrial robot equipped with stereo camera and
welding gun. A method for accurate calibration of sensor and welding gun
is presented as it influences the quality of the welding process.
Experimental results show an increased weld seam quality and the
robustness of the proposed approach in industrial applications.},
month={June},}
@INPROCEEDINGS{7549316,
author={K. Salhi and A. M. Alimi and P. Gorce and M. M. Ben Khelifa},
booktitle={2016 IEEE Tenth International Conference on Research
Challenges in Information Science (RCIS)},
title={Navigation assistance to disabled persons with powered
wheelchairs using tracking system and cloud computing technologies},
year={2016},
pages={1-6},
abstract={This paper introduces a novel approach for secure navigation
of wheelchairs. The approach is based on a combination of robotic road
train based navigation and Cloud Computing technologies. The navigation
strategy is inspired from elephants. Marching trunk to tail, each
wheelchair in a platoon takes cues from the wheelchair just in front of
it. The leader of navigation is a person. The cloud computing
technologies are used to share wheelchairs positions and to send
notifications to caregiver. The role of the caregiver is to lead
navigation and to watch in real time the navigation progress and to
control the system navigation when problems occur. To improve tracking
process, two approaches of obstacle detection and avoidance were
developed. Some experimental results are given in the paper to
demonstrate the feasibility and performance of the developed system.},
keywords={cloud computing;collision avoidance;handicapped aids;mobile
robots;wheelchairs;cloud computing;disabled persons;elephants;obstacle
avoidance;obstacle detection;powered wheelchairs navigation
assistance;robotic road train based navigation;tracking system;Cloud
computing;Hardware;Navigation;Robots;Three-dimensional displays;Two
dimensional displays;Wheelchairs;assistive robotics;autonomous
navigaion;cloud computing;obstacle avoidance;obstacle detection;secure
navigation},
doi={10.1109/RCIS.2016.7549316},
month={June},}
@INPROCEEDINGS{7542940,
author={M. Mortimer and B. Horan and M. Joordens},
booktitle={2016 11th System of Systems Engineering Conference (SoSE)},
title={Kinect with ROS, interact with Oculus: Towards Dynamic User
Interfaces for robotic teleoperation},
year={2016},
pages={1-6},
abstract={Teleoperation remains an important aspect for robotic systems
especially when deployed in unstructured environments. While a range of
research strives for robots that are completely autonomous, many robotic
applications still require some level of human-in-the-loop control. In
any situation where teleoperation is required an effective User
Interface (UI) remains a key component within the systems design.
Current advancements in Virtual Reality (VR) software and hardware such
as the Oculus Rift, HTC Vive and Google Cardboard combined with greater
transparency to robotic systems afforded by middleware such as the Robot
Operating System (ROS) provides an opportunity to rapidly improve
traditional teleoperation interfaces. This paper uses a System of System
(SoS) approach to present the concept of a Virtual Reality Dynamic User
Interface (VRDUI) for the teleoperation of heterogeneous robots.
Different geometric virtual workspaces are discussed and a cylindrical
workspace aligned with interactive displays is presented as a virtual
control room. A presentation mode within the proposed VRDUI is also
detailed, this shows how point cloud information obtained from the
Microsoft Kinect can be incorporated within the proposed virtual
workspace. This point cloud data is successfully processed into an
OctoMap utilizing the octree data structure to create a voxelized
representation of the 3D scanned environment. The resulting OctoMap is
then displayed to an operator as a 3D point cloud using the Oculus Rift
Head Mounted Display (HMD).},
keywords={helmet mounted displays;human-robot
interaction;middleware;mobile robots;octrees;telerobotics;user
interfaces;virtual reality;3D point cloud;3D scanned environment;Google
cardboard;HTC Vive;OctoMap;Oculus Rift HMD;Oculus rift head mounted
display;ROS;SoS;VR hardware;VR software;VRDUI;autonomous robots;dynamic
user interfaces;human-in-the-loop control;interactive displays;microsoft
Kinect;middleware;octree data structure;point cloud data;point cloud
information;robot operating system;robotic systems;robotic
teleoperation;system-of-system;virtual control room;virtual reality
dynamic user interface;virtual reality hardware;virtual reality
software;voxelized representation;Cameras;Geometry;Robot vision
systems;Three-dimensional displays;Visualization;Google Cardboard;HTC
Vive;Kinect;Oculus Rift;ROS;Samsung Gear VR;augmented reality;point
cloud;teleoperation;virtual reality},
doi={10.1109/SYSOSE.2016.7542940},
month={June},}
@ARTICLE{7498096,
author={T. H. Szymanski},
journal={IEEE Communications Magazine},
title={Supporting consumer services in a deterministic industrial
internet core network},
year={2016},
volume={54},
number={6},
pages={110-117},
abstract={A convergence is occurring in the networking world. Industrial
networks currently provide deterministic services in robotic factories
and aircraft, while the best effort Internet of Things provides best
effort services for consumers. We argue that a convergence should occur,
and that a future converged Industrial Internet of Things (IIoT) should
support both best effort and deterministic services, with very low
latency and jitter. This article presents the design of a deterministic
IIoT core network consisting of many simple deterministic packet
switches configured by an SDN control plane. The use of deterministic
communications can reduce router buffer sizes by a factor of ≥ 1000, and
can reduce end-to-end latencies to the speed of light in fiber. A
speed-of-light deterministic core network can have a profound impact on
virtually all consumer services such as multimedia distribution,
e-Commerce, and cloud computing or gaming systems. Highly aggregated
video streams can be delivered over a deterministic virtual network with
very high link utilization (≤ 100 percent), very low packet jitter (≤ 10
μs), and zero congestion. In addition to improving consumer services, a
converged deterministic IIoT core network can save billions of dollars
per year as a result of significantly improved network utilization and
energy efficiency.},
keywords={Internet;Internet of Things;customer services;packet
switching;software defined networking;telecommunication network
routing;SDN control plane;Tactile Internet;aircraft;cloud
computing;consumer services;deterministic IIoT core
network;deterministic communications;deterministic industrial internet
core network;deterministic packet switches;deterministic virtual
network;e-commerce;energy efficiency;gaming systems;industrial Internet
of Things;link utilization;low packet jitter;multimedia
distribution;network utilization;robotic factories;router buffer size
reduction;software defined networking;speed-of-light deterministic core
network;video streams;zero congestion;Automation;IP
networks;Internet;Internet of Things;Packet switching;Service
robots;Streaming media},
doi={10.1109/MCOM.2016.7498096},
ISSN={0163-6804},
month={June},}
@INPROCEEDINGS{7492744,
author={K. Salhi and A. M. Alimi and M. M. Ben Khelifa and P. Gorce},
booktitle={2015 11th International Conference on Information Assurance
and Security (IAS)},
title={Improved secure navigation of wheelchairs using multi-robot
system and cloud computing technologies},
year={2015},
pages={50-54},
abstract={This paper introduces a novel approach for secure navigation
of wheelchairs. The approach is based on a combination of robotic road
train based navigation and Cloud Computing technologies. The navigation
strategy is inspired from elephants. Marching trunk to tail, each
wheelchair in a platoon takes cues from the wheelchair just in front of
it. The wheelchair also communicates directly with the leader in order
to anticipate any turns or braking action. The cloud computing
technologies are used to perform shared computations and remote
teleoperation by a caregiver. Caregivers are persons connected through
cloud interfaces. Their role is to watch in real time the navigation
progress and to control the system navigation when problems occur. Some
experimental results are given in the paper to demonstrate the
feasibility and performance of the developed system.},
keywords={Cloud computing;Collision avoidance;Mobile
robots;Navigation;Robot sensing systems;Wheelchairs;autonomous
navigaion;cloud computing;secure navigation},
doi={10.1109/ISIAS.2015.7492744},
month={Dec},}
@INPROCEEDINGS{7487749,
author={M. Beetz and D. Beßler and J. Winkler and J. H. Worch and F.
Bálint-Benczédi and G. Bartels and A. Billard and A. K. Bozcuoğlu and
Zhou Fang and N. Figueroa and A. Haidu and H. Langer and A. Maldonado
and A. L. P. Ureche and M. Tenorth and T. Wiedemeyer},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Open robotics research using web-based knowledge services},
year={2016},
pages={5380-5387},
abstract={In this paper we discuss how the combination of modern
technologies in “big data” storage and management, knowledge
representation and processing, cloud-based computation, and web
technology can help the robotics community to establish and strengthen
an open research discipline. We describe how we made the demonstrator of
a EU project review openly available to the research community.
Specifically, we recorded episodic memories with rich semantic
annotations during a pizza preparation experiment in autonomous robot
manipulation. Afterwards, we released them as an open knowledge base
using the cloud- and web-based robot knowledge service OPENEASE. We
discuss several ways on how this open data can be used to validate our
experimental reports and to tackle novel challenging research problems.},
keywords={Web services;cloud computing;control engineering
computing;knowledge representation;manipulators;mobile robots;Big Data
management;Big Data storage;EU project;OPENEASE;Web technology;Web-based
knowledge services;Web-based robot knowledge service;autonomous robot
manipulation;cloud-based computation;cloud-based robot knowledge
service;knowledge processing;knowledge representation;open knowledge
base;open research discipline;open robotics research;pizza preparation
experiment;robotics community;Big data;Data visualization;Knowledge
based systems;Knowledge engineering;Robot sensing systems;Semantics},
doi={10.1109/ICRA.2016.7487749},
month={May},}
@INPROCEEDINGS{7487092,
author={F. T. Pokorny and K. Goldberg and D. Kragic},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Topological trajectory clustering with relative persistent
homology},
year={2016},
pages={16-23},
abstract={Cloud Robotics techniques based on Learning from
Demonstrations suggest promising alternatives to manual programming of
robots and autonomous vehicles. One challenge is that demonstrated
trajectories may vary dramatically: it can be very difficult, if not
impossible, for a system to learn control policies unless the
trajectories are clustered into meaningful consistent subsets. Metric
clustering methods, based on a distance measure, require quadratic time
to compute a pairwise distance matrix and do not naturally distinguish
topologically distinct trajectories. This paper presents an algorithm
for topological clustering based on relative persistent homology, which,
for a fixed underlying simplicial representation and discretization of
trajectories, requires only linear time in the number of trajectories.
The algorithm incorporates global constraints formalized in terms of the
topology of sublevel or superlevel sets of a function and can be
extended to incorporate probabilistic motion models. In experiments with
real automobile and ship GPS trajectories as well as pedestrian
trajectories extracted from video, the algorithm clusters trajectories
into meaningful consistent subsets and, as we show in an experiment with
ship trajectories, results in a faster and more efficient clustering
than a metric clustering by Fréchet distance.},
keywords={pattern clustering;probability;topology;global
constraints;pedestrian trajectories;probabilistic motion models;relative
persistent homology;ship GPS trajectories;topological trajectory
clustering;Clustering algorithms;Global Positioning
System;Measurement;Probabilistic logic;Robots;Topology;Trajectory},
doi={10.1109/ICRA.2016.7487092},
month={May},}
@INPROCEEDINGS{7487348,
author={S. Jain and B. Argall},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Grasp detection for assistive robotic manipulation},
year={2016},
pages={2015-2021},
abstract={In this paper, we present a novel grasp detection algorithm
targeted towards assistive robotic manipulation systems. We consider the
problem of detecting robotic grasps using only the raw point cloud depth
data of a scene containing unknown objects, and apply a geometric
approach that categorizes objects into geometric shape primitives based
on an analysis of local surface properties. Grasps are detected without
a priori models, and the approach can generalize to any number of novel
objects that fall within the shape primitive categories. Our approach
generates multiple candidate object grasps, which moreover are
semantically meaningful and similar to what a human would generate when
teleoperating the robot-and thus should be suitable manipulation goals
for assistive robotic systems. An evaluation of our algorithm on 30
household objects includes a pilot user study, confirms the robustness
of the detected grasps and was conducted in real-world experiments using
an assistive robotic arm.},
keywords={assisted living;manipulators;service
robots;telerobotics;assistive robotic arm;assistive robotic
manipulation;geometric approach;geometric shape primitives;local surface
properties;raw point cloud depth data;robot teleoperation;robotic grasp
detection;shape primitive categories;Grasping;Rehabilitation
robotics;Robot kinematics;Robot sensing systems;Shape;Three-dimensional
displays},
doi={10.1109/ICRA.2016.7487348},
month={May},}
@INPROCEEDINGS{7487367,
author={D. Kappler and S. Schaal and J. Bohg},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Optimizing for what matters: The top grasp hypothesis},
year={2016},
pages={2167-2174},
abstract={In this paper, we consider the problem of robotic grasping of
objects when only partial and noisy sensor data of the environment is
available. We are specifically interested in the problem of reliably
selecting the best hypothesis from a whole set. This is commonly the
case when trying to grasp an object for which we can only observe a
partial point cloud from one viewpoint through noisy sensors. There will
be many possible ways to successfully grasp this object, and even more
which will fail. We propose a supervised learning method that is trained
with a ranking loss. This explicitly encourages that the top-ranked
training grasp in a hypothesis set is also positively labeled. We show
how we adapt the standard ranking loss to work with data that has binary
labels and explain the benefits of this formulation. Additionally, we
show how we can efficiently optimize this loss with stochastic gradient
descent. In quantitative experiments, we show that we can outperform
previous models by a large margin.},
keywords={gradient methods;learning systems;robots;binary labels;noisy
sensor data;partial point cloud;robotic grasping;standard ranking
loss;stochastic gradient descent;supervised learning method;top grasp
hypothesis;top-ranked training grasp;Grasping;Noise measurement;Robot
sensing systems;Standards;Three-dimensional displays;Training},
doi={10.1109/ICRA.2016.7487367},
month={May},}
@INPROCEEDINGS{7487201,
author={C. Graumann and B. Fuerst and C. Hennersperger and F. Bork and
N. Navab},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Robotic ultrasound trajectory planning for volume of interest
coverage},
year={2016},
pages={736-741},
abstract={Medical robotic ultrasound offers potential to assist
interventions, ease long-term monitoring and reduce operator dependency.
Various techniques for remote control of ultrasound probes through
telemanipulation systems have been presented in the past, however not
exploiting the potential of fully autonomous acquisitions directly
performed by robotic systems. In this paper, a trajectory planning
algorithm for automatic robotic ultrasound acquisition under expert
supervision is introduced. The objective is to compute a suitable path
for covering a volume of interest selected in diagnostic images, for
example by prior segmentation. A 3D patient surface point cloud is
acquired using a depth camera, which is the sole prerequisite besides
the volume delineation. An easily parameterizable path function
generates single or multiple parallel scan trajectories capable of
dealing with large target volumes. A spline is generated through the
preliminary path points and is transferred to a lightweight robot to
perform the ultrasound scan using an impedance control mode. The
proposed approach is validated via simulation as well as on phantoms and
on animal viscera.},
keywords={biomedical ultrasonics;cameras;medical image
processing;medical robotics;path planning;splines
(mathematics);telecontrol;ultrasonic imaging;3D patient surface point
cloud;automatic robotic ultrasound acquisition;autonomous
acquisitions;depth camera;diagnostic images;impedance control
mode;lightweight robot;medical robotic ultrasound trajectory planning
algorithm;parallel scan trajectories;parameterizable path
function;preliminary path points;remote control;spline;telemanipulation
systems;ultrasound probes;ultrasound scan;volume delineation;volume of
interest coverage;Probes;Robot kinematics;Three-dimensional
displays;Trajectory;Transducers;Ultrasonic imaging},
doi={10.1109/ICRA.2016.7487201},
month={May},}
@INPROCEEDINGS{7487230,
author={M. Tanner and P. Piniés and L. M. Paz and P. Newman},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={What lies behind: Recovering hidden shape in dense mapping},
year={2016},
pages={979-986},
abstract={In mobile robotics applications, generation of accurate static
maps is encumbered by the presence of ephemeral objects such as
vehicles, pedestrians, or bicycles. We propose a method to process a
sequence of laser point clouds and back-fill dense surfaces into gaps
caused by removing objects from the scene - a valuable tool in scenarios
where resource constraints permit only one mapping pass in a particular
region. Our method processes laser scans in a three-dimensional voxel
grid using the Truncated Signed Distance Function (TSDF) and then uses a
Total Variation (TV) regulariser with a Kernel Conditional Density
Estimation (KCDE) “soft” data term to interpolate missing surfaces.
Using four scenarios captured with a push-broom 2D laser, our technique
infills approximately 20 m2 of missing surface area for each removed
object. Our reconstruction's median error ranges between 5.64 cm - 9.24
cm with standard deviations between 4.57 cm - 6.08 cm.},
keywords={mobile robots;path planning;statistical analysis;KCDE;TSDF;TV
regulariser;back-fill dense surfaces;dense mapping;ephemeral
objects;hidden shape recovery;kernel conditional density
estimation;laser point clouds;mobile robotics applications;resource
constraints;static maps generation;three-dimensional voxel grid;total
variation regulariser;truncated signed distance function;Mathematical
model;Robots;Surface emitting lasers;Surface reconstruction;Surface
treatment;TV;Three-dimensional displays},
doi={10.1109/ICRA.2016.7487230},
month={May},}
@ARTICLE{7426833,
author={D. Frantz and A. Röder and M. Stellmes and J. Hill},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={An Operational Radiometric Landsat Preprocessing Framework for
Large-Area Time Series Applications},
year={2016},
volume={54},
number={7},
pages={3928-3943},
abstract={We developed a large-area preprocessing framework for
multisensor Landsat data, capable of processing large data volumes.
Cloud and cloud shadow detection is performed by a modified Fmask code.
Surface reflectance is inferred from Tanré's formulation of the
radiative transfer, including adjacency effect correction. A precompiled
MODIS water vapor database provides daily or climatological fallback
estimates. Aerosol optical depth (AOD) is estimated over dark objects
(DOs) that are identified in a combined database and image-based
approach, where information on their temporal persistency is utilized.
AOD is inferred with consideration of the actual target reflectance and
background contamination effect. In case of absent DOs in bright scenes,
a fallback approach with a modeled AOD climatology is used instead.
Topographic normalization is performed by a modified C-correction. The
data are projected into a single coordinate system and are organized in
a gridded data structure for simplified pixel-based access. We based the
assessment of the produced data set on an exhaustive analysis of
overlapping pixels: 98.8% of the redundant overlaps are in the range of
the expected ±2.5% overall radiometric algorithm accuracy. AOD is in
very good agreement with Aerosol Robotic Network sunphotometer data (R^2
: 0.72 to 0.79, low intercepts, and slopes near unity). The uncertainty
in using the water vapor fallback climatology is approximately ±2.8% for
the TM SWIR1 band in the wet season. The topographic correction was
considered successful by an investigation of the nonrelationship between
the illumination angle and the corrected radiance.},
keywords={aerosols;climatology;clouds;geophysical image
processing;radiative transfer;remote sensing;time series;AOD
climatology;Fmask code;MODIS water vapor database;Tanre
formulation;aerosol optical depth;aerosol robotic network sunphotometer
data;cloud shadow detection;data volume processing;large-area time
series application;multisensor Landsat data;operational radiometric
Landsat preprocessing;radiative transfer;radiometric algorithm
accuracy;water vapor fallback climatology;wet
season;Clouds;Earth;Radiometry;Remote sensing;Satellites;Surface
topography;Surface treatment;Atmospheric correction;Landsat;cloud
detection;large area;multitemporal;southern Africa;surface
reflectance;topographic correction},
doi={10.1109/TGRS.2016.2530856},
ISSN={0196-2892},
month={July},}
@ARTICLE{7473881,
author={Y. Hu and Z. Li and G. Li and P. Yuan and C. Yang and R. Song},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Development of Sensory-Motor Fusion-Based Manipulation and
Grasping Control for a Robotic Hand-Eye System},
year={2016},
volume={PP},
number={99},
pages={1-12},
abstract={In this paper, a sensory-motor fusion-based manipulation and
grasping control strategy has been developed for a robotic hand-eye
system. The proposed hierarchical control architecture has three
modules: 1) vision servoing; 2) surface electromyography (sEMG)-based
movement recognition; and 3) hybrid force and motion optimization for
manipulation and grasping. A stereo camera is used to obtain the 3-D
point cloud of a target object and provides the desired operational
position. The AdaBoost-based motion recognition is employed to
discriminate different movements based on sEMG of human upper limbs. The
operational space motion planning for bionic arm and force planning for
multifingered robotic hand can be both transformed as a convex
optimization problem with various constraints. A neural dynamics
optimization solution is proposed and implemented online. The proposed
formulation can achieve a substantial reduction of computational load.
The actual implementation includes a bionic arm with dextrous hand,
high-speed active vision, and an EMG sensors. A series of manipulation
tasks consisting of tracking/recogniting/grasping of an object are
implemented, and experiment results exhibit the responsiveness and
flexibility of the proposed sensory motion fusion approach.},
keywords={Cameras;Electromyography;Grasping;Planning;Robot sensing
systems;Bionic arm;multifingers hand;quadratic programming;surface
electromyography (sEMG);vision servoing},
doi={10.1109/TSMC.2016.2560530},
ISSN={2168-2216},
month={},}
@INPROCEEDINGS{7467680,
author={M. Attia and Y. Slama and M. A. Kamoun},
booktitle={2016 13th International Conference on Computer Graphics,
Imaging and Visualization (CGiV)},
title={On Performance Evaluation of Registration Algorithms for 3D Point
Clouds},
year={2016},
pages={45-50},
abstract={3D point Geometric alignment is a challenging task encountered
in many scientific applications related to different fields such as
robotics and computer vision. For this reason, the well-known 3D
registration problem has been extensively studied, and a lot of
efficient 3D registration algorithms (RA) exist. Even though many
surveys in the literature addressed RA's, none to our knowledge is
especially interested in their use in robotic fields and more precisely
in dimensional control of mechanical pieces. Our present work involving
both a theoretical and an experimental study compares some local and
non-rigid RAs, used to align large point clouds representing mechanical
pieces. This paper is two-fold and permits first to uncover the
similarities and differences between four known RAs which are ICP, NDT,
Softassign and RANSAC and then to establish an inter RAs comparative
performance evaluation based on accuracy, speed and other new specific
metrics we have defined.},
keywords={computational geometry;image registration;3D RA;3D point
clouds;3D point geometric alignment;3D registration
algorithms;ICP;NDT;RANSAC;Softassign;computer vision;performance
evaluation;robotic fields;Algorithm design and analysis;Computational
modeling;Gaussian distribution;Iterative closest point
algorithm;Measurement;Solid modeling;Three-dimensional displays;3D
registration algorithm;accuracy;dimensional control;robotics;speed},
doi={10.1109/CGiV.2016.18},
month={March},}
@ARTICLE{7165664,
author={Y. Wang and R. Tan and G. Xing and J. Wang and X. Tan and X. Liu
and X. Chang},
journal={IEEE Transactions on Mobile Computing},
title={Monitoring Aquatic Debris Using Smartphone-Based Robots},
year={2016},
volume={15},
number={6},
pages={1412-1426},
abstract={Monitoring aquatic debris is of great interest to the
ecosystems, marine life, human health, and water transport. This paper
presents the design and implementation of SOAR-a vision-based
surveillance robot system that integrates an off-the-shelf Android
smartphone and a gliding robotic fish for debris monitoring in
relatively calm waters. SOAR features real-time debris detection and
coverage-based rotation scheduling algorithms. The image processing
algorithms for debris detection are specifically designed to address the
unique challenges in aquatic environments. The rotation scheduling
algorithm provides effective coverage for sporadic debris arrivals
despite camera's limited angular view. Moreover, SOAR is able to
dynamically offload compute-intensive processing tasks to the cloud for
battery power conservation. We have implemented a SOAR prototype and
conducted extensive experimental evaluation. The results show that SOAR
can accurately detect debris in the presence of various environment and
system dynamics, and the rotation scheduling algorithm enables SOAR to
capture debris arrivals with reduced energy consumption.},
keywords={cloud computing;control engineering computing;environmental
monitoring (geophysics);marine control;mobile robots;robot
vision;scheduling;smart phones;surveillance;water pollution;SOAR
prototype;aquatic debris monitoring;battery power
conservation;cloud;compute-intensive processing tasks;coverage-based
rotation scheduling algorithm;gliding robotic fish;image processing
algorithms;off-the-shelf Android smartphone;real-time debris detection
algorithm;reduced energy consumption;smartphone-based robots;sporadic
debris arrivals;vision-based surveillance robot system;Cameras;Energy
consumption;Monitoring;Noise;Robot vision systems;Robotic sensor;aquatic
debris;computer vision;object detection;smartphone},
doi={10.1109/TMC.2015.2460240},
ISSN={1536-1233},
month={June},}
@INPROCEEDINGS{7443100,
author={J. Li and X. He and J. Li},
booktitle={2015 National Aerospace and Electronics Conference (NAECON)},
title={2D LiDAR and camera fusion in 3D modeling of indoor environment},
year={2015},
pages={379-383},
abstract={Detailed 3D modeling of indoor scene has become an important
topic in many research fields. It can provide extensive information
about the environment and boost various location based services, such as
interactive gaming and indoor navigation. This paper presents an indoor
scene construction approach using 2D line-scan LiDAR and entry-level
digital camera. Both devices are mounted rigidly on a robotic servo,
which sweeps vertically to cover the third dimension. Fiducial target
based extrinsic calibration is applied to acquire transformation
matrices between LiDAR and camera. Based on the transformation matrix,
we perform registration to fuse the color images from the camera with
the 3D points cloud from the LiDAR. The whole system setup has much
lower cost as compared to systems using 3D LiDAR and omnidirectional
camera. Using pre-calculated transformation matrices instead of feature
extraction techniques such as SIFT or SURF in registration gives better
fusion result and lower computational complexity. The experiments
carried out in office building environment show promising results of our
approach.},
keywords={computational complexity;computer graphics;image colour
analysis;image fusion;image registration;optical radar;radar imaging;2D
line-scan LiDAR;3D modeling;3D points cloud;camera fusion;color image
fusion;computational complexity;entry-level digital camera;fiducial
target based extrinsic calibration;image registration;indoor scene
construction approach;omnidirectional camera;robotic
servo;transformation matrices;Calibration;Cameras;Laser radar;Robot
sensing systems;Servomotors;Solid modeling;Three-dimensional displays;2D
line-scan LiDAR;3D indoor modeling;digital camera;extrinsic
calibration;sensor fusion},
doi={10.1109/NAECON.2015.7443100},
month={June},}
@INPROCEEDINGS{7441027,
author={A. G. Foina and R. Sengupta and P. Lerchi and Z. Liu and C.
Krainer},
booktitle={2015 Workshop on Research, Education and Development of
Unmanned Aerial Systems (RED-UAS)},
title={Drones in smart cities: Overcoming barriers through air traffic
control research},
year={2015},
pages={351-359},
abstract={Within the last decade, the recent automation of vehicles such
as cars and planes promise to fundamentally alter the microeconomics of
transporting people and goods. In this paper, we focus on the
self-flying planes (drones), which have been renamed Unmanned Aerial
System (UAS) by the US Federal Aviation Agency (FAA). The most
controversial operations envisaged by the UAS industry are small,
low-altitude UAS flights in densely populated cities - robotic aircraft
flying in the midst of public spaces to deliver goods and information.
This subset of robotic flight would be the most valuable to the nation's
economy, but we argue that it cannot happen without a new generation of
air traffic control and management services. This paper presents a cloud
based system for city-wide unmanned air traffic management, prototype
sensor systems required by city police to keep the city safe, and an
analysis of control systems for collision avoidance.},
keywords={aerospace computing;air traffic control;aircraft
control;autonomous aerial vehicles;cloud computing;collision
avoidance;control engineering computing;control system analysis;mobile
robots;FAA;Federal Aviation Agency;UAS industry;air traffic control
research;cars;city-wide unmanned air traffic management;cloud based
system;collision avoidance;control system analysis;drones;prototype
sensor systems;robotic aircraft;self-flying planes;smart cities;unmanned
aerial system;vehicle automation;Aircraft;Automobiles;Drones;FAA;Smart
cities},
doi={10.1109/RED-UAS.2015.7441027},
month={Nov},}
@INPROCEEDINGS{7435795,
author={A. P. Rao and S. Agarwal and K. Srinivas and B. K. Rani},
booktitle={2015 IEEE International Conference on Computational
Intelligence and Computing Research (ICCIC)},
title={Learning mechanism for RT task scheduling},
year={2015},
pages={1-4},
abstract={The fascinations of Internet of Things (IoT) necessitate a
large number of devices are to be integrated with the existing IoT.
These devices are very difficult to manage in a large distributed
environment without a careful management design. These location based
devices generate data at fixed intervals of time and need configure
these devices to software platform to analyze data and understand
environment in better way. So, learning capability should incorporate
within the system as the environment of system changes dynamically. As
the Internet of Things continues to develop, further potential is
estimated by a combination with related technology approaches and
concepts such as Cloud Computing, Future Internet, Big Data, Robotics
and Semantic Technologies. The idea is becomes now evident as those
related concepts have started to reveal synergies by combining them.},
keywords={Internet of Things;learning (artificial
intelligence);processor scheduling;real-time systems;Big Data;Internet
of Things;IoT;RT task scheduling;cloud computing;data analysis;future
Internet;learning mechanism;location based devices;real-time
system;robotics;semantic technologies;software platform;Classification
algorithms;Clustering algorithms;Computational modeling;Indexes;Mobile
handsets;Scheduling algorithms;Learning Capability;Processor
Cluster;Scheduling;Task Cluster;Task weight},
doi={10.1109/ICCIC.2015.7435795},
month={Dec},}
@INPROCEEDINGS{7434875,
author={A. Nayyar and V. Puri},
booktitle={2015 9th International Conference on Future Generation
Communication and Networking (FGCN)},
title={A Review of Beaglebone Smart Board's-A Linux/Android Powered Low
Cost Development Platform Based on ARM Technology},
year={2015},
pages={55-63},
abstract={In the last few years, there have been a rapid increase
towards single-board microcontrollers. These days, trend has shifted
towards development of full-fledged credit-card sized computer's like
Arduino Mega2560, Raspberry Pi, Orange Pi, Chip and even Beaglebone.
These boards are low cost, low power, easy deployable and has
user-friendly configurable options. Beaglebone technology is speeding up
and growing like anything as millions of pieces are sold worldwide till
date. Beaglebone boards are showing tremendous increase in adaptability
and implementation in diverse areas like Robotics, Drones, Smart Homes,
IoT devices, Linux and Cloud Computing Servers and even more. The aim
and objective of this research paper is to provide a comprehensive
review of Beaglebone Technology, various Beaglebone boards available
till date with their technical specifications along with various
research areas which can enable researchers and industry professionals
to take up Beaglebone Technology and develop wide range of ready to use
efficient and low cost products.},
keywords={Android (operating system);low-power
electronics;microcontrollers;ARM technology;Arduino Mega2560;Beaglebone
smart board;Beaglebone technology;Chip;Linux/Android powered low cost
development platform;Orange Pi;Raspberry Pi;full-fledged credit-card
sized computer;low cost boards;low power boards;single-board
microcontrollers;Computers;Linux;Operating systems;Ports
(Computers);Program processors;Random access memory;Universal Serial
Bus;ARM;Android;Beagle;Beaglebone;Beaglebone
Black;Drones;IoT;Linux;Sensors},
doi={10.1109/FGCN.2015.23},
month={Nov},}
@INPROCEEDINGS{7434427,
author={A. Sundaram and M. Gupta and V. Rathod and K. Chandrasekaran},
booktitle={2015 IEEE International Symposium on Nanoelectronic and
Information Systems},
title={Remote Surveillance Robot System -- A Robust Framework Using Cloud},
year={2015},
pages={213-218},
abstract={Today's technology provides a rather convenient way for
developer community to develop an integrated network environment for the
diversified applications of various robotic systems. Internet based
robotic systems have been gaining importance of late and we explore one
such application in the field of remote surveillance. Even though direct
control has potential difficulties due to unpredictable delay, recent
technological advances have reduced this to a large extent. This paper
describes a direct model of robot control. The system uses standard
protocol and a machine-human interface. Using a Web browser (thin
client), a remote user can control the mobile robot to navigate in an
enclosure with visual feedback via the Internet. The use of an intuitive
user interface permits Internet users to access and control the mobile
robot and perform useful tasks remotely, from a different location. The
direct mode is implemented with the help of event driven methods which
provide complete control over the robot. This paper proposes a model
architecture for direct control and discusses an implementation and
performance of a networked robot system with the help of Cloud computing.},
keywords={Internet;cloud computing;control engineering computing;mobile
robots;surveillance;Internet;Internet based robotic system;Web
browser;cloud computing;machine-human interface;mobile robot;remote
surveillance robot system;visual feedback;Cloud
computing;Instruments;Robots;Streaming media;Web servers;IOT;WebRTC},
doi={10.1109/iNIS.2015.47},
month={Dec},}
@INPROCEEDINGS{7432958,
author={S. Damodaran and A. P. Sudheer and T. K. S. Kumar},
booktitle={2015 International Conference on Control Communication
Computing India (ICCC)},
title={An evaluation of spatial mapping of indoor environment based on
point cloud registration using Kinect sensor},
year={2015},
pages={548-552},
abstract={Registration of 3D pointclouds obtained using depth sensor has
wide range of applications in robotics. Many different rigid 3D
registration algorithms have been proposed in literature, such as
Principal Component Analysis, Singular value decomposition, iterative
closest point (ICP) and its variants. The ICP is widely used algorithm
for registration of point clouds. It is accurate and fast for point
cloud registration. In this work, a performance evaluation of
point-to-point based ICP algorithm, integration of point-to-point with
random sampling and point-to-plane based ICP algorithm by using
Microsoft Kinect camera is conducted. Low-cost Microsoft Kinect sensor
provides a feasible and economical solution for such point cloud
generation. Root mean square error (RMSE) value is taken as the
measurement of precision of cloud registration. RMSE value is obtained
from the Euclidean distance between corresponding point pairs in both
point-clouds, used for the ICP registration. The ICP algorithm always
converges monotonically to the nearest local minimum of a mean square
distance metric. The results show that the convergence rate is fast
during initial iterations.},
keywords={SLAM (robots);cameras;image registration;image sampling;indoor
environment;iterative methods;mean square error methods;robot vision;3D
pointcloud registration;3D registration algorithm;Euclidean
distance;Microsoft Kinect camera;RMSE value;convergence;depth
sensor;indoor environment;iterative closest point;low-cost Microsoft
Kinect sensor;mean square distance metric;point cloud generation;point
cloud registration;point-to-point based ICP algorithm;random
sampling;robotics application;root mean square error;spatial
mapping;Algorithm design and analysis;Convergence;Filtering
algorithms;Iterative closest point algorithm;Memory management;Robot
sensing systems;Three-dimensional displays;localization;mapping;mapping
(SLAM);simultaneous localization},
doi={10.1109/ICCC.2015.7432958},
month={Nov},}
@ARTICLE{7403840,
author={M. Simsek and A. Aijaz and M. Dohler and J. Sachs and G. Fettweis},
journal={IEEE Journal on Selected Areas in Communications},
title={5G-Enabled Tactile Internet},
year={2016},
volume={34},
number={3},
pages={460-473},
abstract={The long-term ambition of the Tactile Internet is to enable a
democratization of skill, and how it is being delivered globally. An
integral part of this is to be able to transmit touch in perceived
real-time, which is enabled by suitable robotics and haptics equipment
at the edges, along with an unprecedented communications network. The
fifth generation (5G) mobile communications systems will underpin this
emerging Internet at the wireless edge. This paper presents the most
important technology concepts, which lay at the intersection of the
larger Tactile Internet and the emerging 5G systems. The paper outlines
the key technical requirements and architectural approaches for the
Tactile Internet, pertaining to wireless access protocols, radio
resource management aspects, next generation core networking
capabilities, edge-cloud, and edge-AI capabilities. The paper also
highlights the economic impact of the Tactile Internet as well as a
major shift in business models for the traditional telecommunications
ecosystem.},
keywords={5G mobile communication;Internet;access protocols;next
generation networks;5G mobile communications systems;5G-enabled tactile
Internet;architectural approach;edge-AI capability;edge-cloud
capability;fifth generation mobile communications systems;key technical
requirements;long-term ambition;next generation core networking
capability;radio resource management;wireless access protocols;5G mobile
communication;Internet;Real-time systems;Robot kinematics;Wireless
communication;5G;OFDM;Tactile Internet;edge intelligence;haptic
communications;massive connectivity;real-time communication;realtime
communication;ultra-high reliability;ultra-low latency;ultrahigh
reliability},
doi={10.1109/JSAC.2016.2525398},
ISSN={0733-8716},
month={March},}
@INPROCEEDINGS{7424579,
author={I. G. Raducu and V. C. Bojan and F. Pop and M. Mocanu and V.
Cristea},
booktitle={2015 10th International Conference on P2P, Parallel, Grid,
Cloud and Internet Computing (3PGCIC)},
title={Real-Time Alert Service for Cyber-Infrastructure Environments},
year={2015},
pages={296-303},
abstract={Smart environments represent a topic that gets more and more
attention nowadays, especially due to the progress made in supporting
fields such as Cloud computing, sensor networks, mobile computing and
robotics. Also, the recent development of Internet of Things has an
important contribution in increasing the desire to offer new solutions
for smart environments. The alerting service is an essential component
of any smart environment, that offers the user the possibility of being
informed of the latest events occurred in the monitored environment. It
is interconnected with other components of the smart environment, making
the automatic execution of actions based on alerts possible. This paper
presents a multi-layered architecture for the real time alert service,
the proposed solution being integrated within a specific smart
environment, namely "Smart farms". Besides describing each component of
the proposed system, the paper also shows how these components interact
with each other and how the system communicates with other external
components. The performance tests demonstrate that the service is able
to analyze large amounts of data coming from the large number of various
sensors used to monitor the farms.},
keywords={agricultural engineering;computerised monitoring;data
analysis;farming;intelligent sensors;Internet of Things;alerting
service;automatic actions execution;cyberinfrastructure
environments;farm monitoring;multilayered architecture;real time alert
service;real-time alert service;sensors;smart environments;smart
farms;Cloud computing;Electronic mail;Irrigation;Monitoring;Real-time
systems;Alert Service;Cyber-infrastructure;Farm Management System;Real
time analysis;Workflows},
doi={10.1109/3PGCIC.2015.122},
month={Nov},}
@INPROCEEDINGS{7418961,
author={X. Chen and C. G. Atkeson and Q. Huang},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={3D model based ladder tracking using vision and laser point cloud
data},
year={2015},
pages={1365-1370},
abstract={This paper presents 3D model based industrial ladder tracking
using vision and laser point cloud data for the ladder climbing task of
a humanoid robot ATLAS in the DARPA Robotics Challenge. A virtual visual
servoing algorithm with a moving edge detector is used for visual 3D
ladder tracking to obtain 6D pose of ladder relative to the robot. An
iterative closest point algorithm, which is suitable for 6D pose
recognition with laser point cloud data, is used for initialization and
failure recovery of the visual 3D tracking algorithm. For each loop of
the visual tracker, With 6D pose from previous image frame or from laser
point cloud data, a virtual image of the 3D ladder geometric model is
first generated by projective back-projection. Then, the moving edge
detector is applied to find the displacement of edge features in the
virtual and real image. Image Jacobian is calculated to obtain the
gradient of the 6D pose with respect to displacement of edges features.
Then, the visual virtual servoing algorithm is used to obtain the 6D
pose of the ladder iteratively according to the image Jacobian and
feature error. The iterative closest point algorithm with laser point
cloud data is executed to get the 6D pose globally if necessary for
reinitialization or recovering from tracking failure. The 3D ladder
tracker has been verified both in drcsim/Gazebo simulation environment
and with real data.},
keywords={digital simulation;edge detection;humanoid robots;iterative
methods;ladders;mobile robots;object tracking;pose estimation;robot
vision;visual servoing;3D ladder geometric model;3D model based
industrial ladder tracking;6D ladder pose;6D pose recognition;ATLAS
humanoid robot;DARPA robotics challenge;Gazebo simulation
environment;Jacobian image;drcsim simulation environment;iterative
closest point algorithm;ladder climbing task;laser point cloud
data;moving edge detector;projective back-projection;virtual visual
servoing algorithm;visual 3D ladder tracking;Data models;Image edge
detection;Solid modeling;Three-dimensional displays;Visual servoing},
doi={10.1109/ROBIO.2015.7418961},
month={Dec},}
@INPROCEEDINGS{7418771,
author={T. H. Yang and C. S. Ko and J. Y. Huang and W. P. Lee},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Configuring reusable robot services in a cloud environment},
year={2015},
pages={225-230},
abstract={Configuration of software services plays an important role in
the design of robots. However, due to the complexity and diversity of
the environments for developing robots, it is difficult to share and
reuse robot code created by different providers. In this study, we
present an ontology-based approach that exploits the standard web
interface to develop reusable robotic services. Our approach integrates
knowledge ontologies and several service functions for robot control,
and focuses on service configuration. In addition, a ROS-based cloud
environment is configured to allocate services and to perform service
configuration. The proposed approach has been implemented and evaluated,
and the results show that it can be used to build composite robotic
services successfully.},
keywords={cloud computing;control engineering computing;ontologies
(artificial intelligence);operating systems (computers);robots;ROS-based
cloud environment;Web interface;composite robotic services;knowledge
ontologies;ontology-based approach;reusable robot services;reusable
robotic services;robot control;robot design;service allocation;service
functions;software service configuration;Complexity
theory;Ontologies;Planning;Robots;Service-oriented architecture;HTN
planning;cloud robot;composite service;knowledge ontology;robotic
service;service reconfiguration},
doi={10.1109/ROBIO.2015.7418771},
month={Dec},}
@INPROCEEDINGS{7419127,
author={A. Ohsato and Y. Sasaki and H. Mizoguchi},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Real-time 6DoF localization for a mobile robot using pre-computed
3D laser likelihood field},
year={2015},
pages={2359-2364},
abstract={This paper proposes a real-time full-6DoF localization method
designed for a mobile robot moving in human-inhabited environments. The
field of robotics is expanding, and 6DoF pose estimation in the real
world is an important function for robots. The proposed method is based
on typical Monte-Carlo Localization algorithm, and we expanded it to
6DoF realtime pose estimation. For real-time calculation, the proposed
method uses 3D Laser Likelihood Field generated from pre-computed 3D
environment map. The 3D Laser Likelihood Field is represented voxels
including the distance between its center and the nearest neighbor
obstacle. To speed up the calculation of matching score between 3D
environmental map and 3D-LIDAR point cloud, we apply it to the
measurement process of Monte-Carlo Localization. The system is evaluated
with MoCap and demonstrated in human-inhabited environments. As these
experiments illustrated, the robot could estimate its self-pose robustly
in real-time with the algorithm running on an ordinary notebook PC.},
keywords={Monte Carlo methods;SLAM (robots);mobile robots;pose
estimation;robot vision;3D laser likelihood field;6DoF localization
method;6DoF pose estimation;Monte-Carlo localization
algorithm;human-inhabited environment;mobile robot;Atmospheric
measurements;Mobile robots;Monte Carlo methods;Real-time
systems;Sensors;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7419127},
month={Dec},}
@INPROCEEDINGS{7413855,
author={V. A. Ziparo and F. Cottefoglie and D. Calisi and F. Giannone
and G. Grisetti and B. Leibe and M. Proesmans and P. Salonia and L. Van
Gool and C. Ventura and C. Stachniss},
booktitle={2015 Digital Heritage},
title={A new approach to digitalization and data management of cultural
heritage sites},
year={2015},
volume={1},
pages={143-146},
abstract={In this paper, we describe a novel approach for acquiring and
managing digital models of archaeological sites. More in detail, we
present an approach to digitization based on a robotic platform and a
cloud-based information system. Our robot is the result of over two
years of efforts by a group of cultural heritage experts, computer
scientists and roboticists. Exploiting the large and heterogeneous
amount data provided by the robotic platform requires this data to be
managed, organized and analyzed. To this extent we developed ARIS
(ARchaeological Information System), a software that exploits modern
information retrieval and machine learning systems.},
keywords={archaeology;cloud computing;history;information
retrieval;intelligent robots;learning (artificial
intelligence);scientific information systems;ARIS;archaeological
information system;archaeological sites;cloud-based information
system;computer scientists;cultural heritage experts;cultural heritage
sites;data management;digital model acquisition;digital model
management;digitization approach;information retrieval;machine learning
systems;robotic platform;roboticists;Cultural differences;Information
systems;Robot sensing systems;Semantics;Three-dimensional displays},
doi={10.1109/DigitalHeritage.2015.7413855},
month={Sept},}
@INPROCEEDINGS{7414828,
author={M. K. Singh and K. S. Venkatesh and A. Dutta},
booktitle={2015 Third International Conference on Image Information
Processing (ICIIP)},
title={A new method for calibration of range sensor and terrain
classification},
year={2015},
pages={520-525},
abstract={Laser range scanner is an important modality in robotics for
the perception of an environment such as object classification, map
generation, navigation, etc. In this paper, we first present a new
method for calibration of Laser range scanner to generate the highly
accurate 3D model of the environment. Later, we propose a robust
segmentation method for objects and ground points separation of fused
range data which is obtained from a Laser range scanner. The Laser range
scanner gives the two range dataset from different orientation of the
same terrain. After fusion of these two range data using ICP algorithm,
the complete range data of terrain are obtained. In order to classify
objects and ground points of terrain, we exploit the properties of
statistical measures and local elevation of the range data. The proposed
algorithm is independent from range data format and resolution, i.e., it
works for point cloud and gridded data. The experimental results
presented in the paper have shown robustness of the proposed method for
explicit segmentation of objects from the ground points.},
keywords={calibration;image classification;image resolution;image
segmentation;iterative methods;laser ranging;optical scanners;optical
sensors;statistical analysis;terrain mapping;3D model;ICP
algorithm;fused range data;gridded data;ground point
separation;iterative closest point algorithm;laser range scanner;local
elevation;map generation;navigation;object classification;object
segmentation;object separation;point cloud;range data format;range data
resolution;range sensor calibration method;robust segmentation
method;statistical measures;terrain
classification;Robots;Robustness;Rotation measurement;Delaunay
Triangulation;Fusion;Laser Range Scanner;Segmentation;Sensor Calibration},
doi={10.1109/ICIIP.2015.7414828},
month={Dec},}
@INPROCEEDINGS{7419033,
author={R. Minowa and A. Namiki},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Real-time 3D recognition of manipulated object by robot hand
using 3D sensor},
year={2015},
pages={1798-1803},
abstract={In robotic manipulation, visual servoing is an important
technique for achieving dexterous and accurate handling of objects. One
of the most important problems in this task is occlusion by the robot's
body. In some situations, it is difficult to know the position and
orientation of the object during manipulation because it is hidden by
the robot itself. If its position and orientation could be estimated by
considering this occlusion, the robot would be able to manipulate it by
visual servoing more dexterously. In this paper, we propose an algorithm
for estimating the position and orientation of the manipulated object
during manipulation by using a 3-D sensor. 3-D information of the total
environment is observed by the 3-D sensor, and the observed 3-D
point-cloud information is classified into the manipulator, the object,
and the others based on their 3-D models. The proposed algorithm was
verified in an actual robot manipulation system.},
keywords={dexterous manipulators;estimation theory;object
recognition;position control;visual servoing;3D recognition;dexterous
robot;object manipulation;orientation estimation;position
estimation;robot hand;robotic manipulation;visual servoing;Iterative
closest point algorithm;Robot kinematics;Robot sensing systems;Solid
modeling;Three-dimensional displays;Visualization},
doi={10.1109/ROBIO.2015.7419033},
month={Dec},}
@INPROCEEDINGS{7414632,
author={N. Somani and A. Perzylo and C. Cai and M. Rickert and A. Knoll},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Object detection using boundary representations of primitive
shapes},
year={2015},
pages={108-113},
abstract={In this paper, an approach for matching of primitive shapes
detected from point clouds, to boundary representations of primitive
shapes contained in CAD models of objects/workpieces is presented. The
primary target application is object detection and pose estimation from
noisy RGBD sensor data. This approach can also be used to determine
incomplete object poses, including those of symmetrical objects.
Detection and reasoning about these under-specified object poses is
useful in several practical applications such as robotic manipulation,
which are also presented in this paper.},
keywords={CAD;image colour analysis;image matching;image
representation;image sensors;industrial manipulators;object
detection;object recognition;ontologies (artificial intelligence);pose
estimation;CAD models;boundary representations;noisy RGBD sensor
data;object detection;point clouds;pose estimation;primitive shape
matching;primitive shapes;robotic manipulation;symmetrical
objects;under-specified object poses;Design automation;Mathematical
model;Object detection;Shape;Solid modeling;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7414632},
month={Dec},}
@INPROCEEDINGS{7406423,
author={B. Busam and M. Esposito and S. Che'Rose and N. Navab and B.
Frisch},
booktitle={2015 IEEE International Conference on Computer Vision
Workshop (ICCVW)},
title={A Stereo Vision Approach for Cooperative Robotic Movement Therapy},
year={2015},
pages={519-527},
abstract={Movement therapy is an integrating part of stroke
rehabilitation. The positive influence of intensive, repetitive motion
training and the importance of active patient participation trigger the
development of cooperative robotic assistants. We suggest a device for
the re-education of upper limb movements in hemiparetic patients where a
light-weight robotic arm that supports the deficient arm is equipped
with a stereoscopic camera system. It follows the movements of the
healthy arm that wears a sleeve equipped with flat round reflective
markers detected by the cameras. We introduce an advanced robust and
real-time algorithm to provide the tracking information. It performs a
sparse marker based point cloud registration based on subpixel precision
contour fits to enable high accuracy pose estimates while being capable
of online model adjustments. The update rate of the tracking is 9 ms and
the precision of the system is measured to be 0.5 mm. Tests with healthy
subjects show that the system is able to accurately reproduce the
movement of the healthy arm on an impaired arm.},
keywords={cameras;image registration;medical robotics;patient
treatment;stereo image processing;active patient participation
trigger;cooperative robotic assistant;cooperative robotic movement
therapy;intensive motion training;light-weight robotic arm;online model
adjustment;repetitive motion training;sparse marker-based point cloud
registration;stereo vision approach;stereoscopic camera system;subpixel
precision contour fit;Cameras;Eigenvalues and eigenfunctions;Robot
kinematics;Robot vision systems;Three-dimensional displays;Tracking},
doi={10.1109/ICCVW.2015.74},
month={Dec},}
@ARTICLE{7378458,
author={J. Schneider and C. Stachniss and W. Förstner},
journal={IEEE Robotics and Automation Letters},
title={On the Accuracy of Dense Fisheye Stereo},
year={2016},
volume={1},
number={1},
pages={227-234},
abstract={Fisheye cameras offer a large field of view, which is
important for several robotics applications as a larger field of view
allows for covering a large area with a single image. In contrast to
classical cameras, however, fisheye cameras cannot be approximated well
using the pinhole camera model and this renders the computation of depth
information from fisheye stereo image pairs more complicated. In this
work, we analyze the combination of an epipolar rectification model for
fisheye stereo cameras with existing dense methods. This has the
advantage that existing dense stereo systems can be applied as a
black-box even with cameras that have field of view of more than 180° to
obtain dense disparity information. We thoroughly investigate the
accuracy potential of such fisheye stereo systems using image data from
our UAV. The empirical analysis is based on image pairs of a calibrated
fisheye stereo camera system and two state-of-the-art algorithms for
dense stereo applied to adequately rectified image pairs from fisheye
stereo cameras. The canonical stochastic model for sensor points assumes
homogeneous uncertainty and we generalize this model based on an
empirical analysis using a test scene consisting of mutually orthogonal
planes. We show that: (1) the combination of adequately rectified
fisheye image pairs and dense methods provides dense 3D point clouds at
6-7 Hz on our autonomous multicopter UAV; (2) the uncertainty of points
depends on their angular distance from the optical axis; (3) how to
estimate the variance component as a function of that distance; and (4)
how the improved stochastic model improves the accuracy of the scene
points.},
keywords={aircraft control;autonomous aerial
vehicles;cameras;helicopters;robot vision;stereo image
processing;stochastic processes;3D point clouds;UAV;angular
distance;autonomous multicopter UAV;canonical stochastic model;dense
fisheye stereo cameras;depth information computation;disparity
information;empirical analysis;epipolar rectification
model;field-of-view;fisheye stereo camera system;fisheye stereo image
pairs;homogeneous uncertainty;image pairs;mutually orthogonal
planes;optical axis;pinhole camera model;point uncertainty;rectified
fisheye image pairs;scene point accuracy improvement;stochastic
model;variance component estimation;Calibration;Cameras;Computational
modeling;Image reconstruction;Lenses;Robot vision
systems;Three-dimensional displays;Computer Vision for Other Robotic
Applications;Mapping;Omnidirectional Vision;Range Sensing},
doi={10.1109/LRA.2016.2516509},
ISSN={2377-3766},
month={Jan},}
@INPROCEEDINGS{7395157,
author={O. Alonso-Ramirez and Y. Aguas-Garcia and A. Marin-Hernandez and
H. V. Rios-Figueroa and M. Devy},
booktitle={2015 IEEE International Autumn Meeting on Power, Electronics
and Computing (ROPEC)},
title={An efficient alternative approach for home furniture detection
and localization by an autonomous mobile robot},
year={2015},
pages={1-6},
abstract={In order for service robots to help humans in daily home
tasks, they need to have a better understanding of their environment.
Therefore, detection and localization of home furniture becomes a very
important capability for them. This paper presents an approach to detect
typical home furniture by an RGB-D sensor mounted on a mobile robot. The
approach is based on the analysis of discriminant features extracted
from very easy to compute measures distributions. Over an offline
learning phase, each piece of furniture is modeled according to their
distributions of: height, color (H and S components) and normals. Then a
sequence of distributions analysis are applied to the scene for
selecting, according to learned models, the pieces of furniture with a
high probability of being present. The point cloud is segmented
according to model analysis and then segmented points are projected to
the floor plane for clustering and noise removal. According to previous
analysis and footprints of segmented points clouds, regions are then
classified as a possible piece of furniture. Finally, the orientation
and localization of the detected furnitures are obtained, using the
footprint and their neighbor regions. The proposed approach has been
proved to be very efficient in order to be incorporated on mobile
robotic platforms.},
keywords={SLAM (robots);feature extraction;furniture;image
classification;image colour analysis;image denoising;image
segmentation;mobile robots;robot vision;service robots;RGB-D sensor;SLAM
method;distribution analysis;feature extraction;home furniture
detection;home furniture localization;mobile robot;noise removal;point
segmentation;region classification;service robot;Irrigation;Legged
locomotion;Ontologies;Transforms},
doi={10.1109/ROPEC.2015.7395157},
month={Nov},}
@ARTICLE{7236897,
author={C. R. Marcos and R. Pedrós and J. L. Gómez-Amo and M. P.
Utrillas and J. A. Martínez-Lozano},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Analysis of Desert Dust Outbreaks Over Southern Europe Using
CALIOP Data and Ground-Based Measurements},
year={2016},
volume={54},
number={2},
pages={744-756},
abstract={Saharan dust outbreaks are one of the major aerosol sources in
Southern Europe. Dust affects the radiative balance of the
Earth-atmosphere system and impacts health. The assessment of Saharan
outbreaks requires information about the vertical distribution of dust
in the atmosphere, usually obtained from lidar measurements, such as the
ones performed by the spaceborne Cloud-Aerosol Lidar with Orthogonal
Polarization (CALIOP) sensor. In this paper, we have used CALIOP level-1
daytime data in combination with aerosol optical depth (AOD)
measurements from Aerosol Robotic Network (AERONET) ground-based
photometers to obtain aerosol profiles during 23 dust outbreaks over
Southern Europe. These profiles are used for two main purposes:
analyzing the lidar ratio values and vertical distribution of dust and
evaluating the performance of version 3 of CALIOP algorithms. The mean
lidar ratio obtained from the synergic method ranged between 47.7 and
49.0 sr, depending on the criterion used to average CALIOP signal. The
dust layers traveled at a wide range of altitudes, from surface level up
to 7.6 km. We found that the AOD values retrieved from CALIOP level 2
(version 3) and AERONET were not correlated (r <; 0.2). In addition, we
observed that the mean AOD obtained by satellite retrievals, between
0.16 and 0.18, was lower than the mean AERONET value, i.e., 0.33, by
50%. These differences in the AOD were found to be caused by two main
reasons: misclassification of dense dust layers as clouds by CALIOP
algorithms, mainly over 4 km, and limitations of CALIOP algorithms to
detect weak aerosol layers, particularly between 1.5 and 2.5 km.},
keywords={aerosols;atmospheric optics;optical radar;remote sensing by
radar;Aerosol Robotic Network;CALIOP Data;CALIOP level 2;aerosol
layers;aerosol profiles;desert dust outbreaks;ground-based
measurements;southern Europe;spaceborne Cloud-Aerosol Lidar with
Orthogonal Polarization;Aerosols;Atmosphere;Atmospheric
measurements;Backscatter;Clouds;Europe;Laser
radar;Atmosphere;Europe;laser radar},
doi={10.1109/TGRS.2015.2464701},
ISSN={0196-2892},
month={Feb},}
@INPROCEEDINGS{7370275,
author={N. Watthanawisuth and T. Maturos and A. Sappat and A. Tuantranont},
booktitle={2015 IEEE SENSORS},
title={The IoT wearable stretch sensor using 3D-Graphene foam},
year={2015},
pages={1-4},
abstract={In this work, we have developed flexible and wearable Stretch
Sensor based on the Internet of thing technology. These sensors were
realized using a 3D-Graphene foam amalgam with Polydimethylsiloxane
(PDMS). To demonstrate the 3D-graphene foam sensors, we constructed an
armband muscle measurement using such sensors and developed software
based on IoT for real-time muscle expansion and stretch tracking. Wi-Fi
was used to transfer data from the sensor to a cloud via web-socket
based on Node.js. The data are display expansion of muscle on a website.
This muscle stretch tracking is very useful in many contexts such as
workout performance measuring, rehabilitation and tele-robotics
application. The wearable stretch sensor is consisting of two pieces of
5 centimeters 3D-graphene foam strip and packed with clasped by
conductive epoxy. For accuracy, at the end of sensor edge are coated
with silver paste for better conductivity. Main CPU uses Intel Edison,
which made the sensor connect to the Internet easier. In order to deploy
this sensor with another application the ADXL335 was chosen as a 3-axis
accelerometer for tracking of gestures or fitness tracking application.
An accelerometer was attached to the down side of the Intel Edison main
CPU board and including battery and analog to digital converter circuit.},
keywords={Internet of Things;accelerometers;biomedical
measurement;biomedical transducers;coatings;computerised
instrumentation;conducting polymers;graphene;medical
computing;muscle;polymer foams;sensors;wireless LAN;3-axis
accelerometer;3D-graphene foam amalgam;3D-graphene foam
sensor;3D-graphene foam strip;ADXL335;C;CPU board;Intel Edison;Internet
of Thing technology;IoT wearable stretch sensor;Node.js
cloud;PDMS;Wi-Fi;analog to digital converter circuit;armband muscle
measurement;coating;conductive epoxy;muscle expansion;muscle stretch
tracking;polydimethylsiloxane;rehabilitation application;silver
paste;size 5 cm;telerobotics application;web-socket;Biomedical
monitoring;Graphene;Internet of things;Muscles;Real-time
systems;Software;3D-graphene foam;IoT;Stretch Sensor;wearable},
doi={10.1109/ICSENS.2015.7370275},
month={Nov},}
@INPROCEEDINGS{7367359,
author={B. Spinnewyn and B. Braem and S. Latré},
booktitle={2015 11th International Conference on Network and Service
Management (CNSM)},
title={Fault-tolerant application placement in heterogeneous cloud
environments},
year={2015},
pages={192-200},
abstract={The Internet of Things (IoT) has inspired a myriad of
real-time applications, such as robotics and human-machine interaction.
Many IoT applications have significant computational requirements, while
at the same time they demand very low latencies. The cloud can provide
the needed resources on-demand, however often fails to meet these timing
requirements. Low response time can only be realized by having
computational infrastructure in close vicinity. Therefore we investigate
to what extent the cloud can be extended in the direct wireless
surroundings of the IoT devices. This environment is highly
heterogeneous as it comprises a wide variety of devices, connected using
a plethora of technologies (both wired and wireless). A direct
implication is that, compared to traditional cloud infrastructure, many
of those nodes and links are likely to fail. We propose an application
placement that can overcome failure-related challenges. We demonstrate
that availability-awareness can increase the number of applications that
can be hosted simultaneously by 132%. Furthermore we find that an
additional increase of 54% can be realized through redundant
provisioning of resources.},
keywords={Internet of Things;cloud computing;Internet of
Things;IoT;fault-tolerant application placement;heterogeneous cloud
environment;Bandwidth;Cloud computing;Redundancy;Service-oriented
architecture;Substrates;Wireless communication;Wireless sensor networks},
doi={10.1109/CNSM.2015.7367359},
month={Nov},}
@ARTICLE{7348660,
author={D. Wolf and J. Prankl and M. Vincze},
journal={IEEE Robotics and Automation Letters},
title={Enhancing Semantic Segmentation for Robotics: The Power of 3-D
Entangled Forests},
year={2016},
volume={1},
number={1},
pages={49-56},
abstract={We present a novel, fast, and compact method to improve
semantic segmentation of three-dimensional (3-D) point clouds, which is
able to learn and exploit common contextual relations between observed
structures and objects. Introducing 3-D Entangled Forests (3-DEF), we
extend the concept of entangled features for decision trees to 3-D point
clouds, enabling the classifier not only to learn, which labels are
likely to occur close to each other, but also in which specific
geometric configuration. Operating on a plane-based representation of a
point cloud, our method does not require a final smoothing step and
achieves state-of-the-art results on the NYU Depth Dataset in a single
inference step. This compactness in turn allows for fast processing
times, a crucial factor to consider for online applications on robotic
platforms. In a thorough evaluation, we demonstrate the expressiveness
of our new 3-D entangled feature set and the importance of spatial
context in the scope of semantic segmentation.},
keywords={decision trees;image classification;image representation;image
segmentation;mobile robots;3-D entangled feature set;3-D entangled
forests;3-D point clouds;3-DEF;NYU Depth Dataset;classifier;decision
trees;entangled features;geometric configuration;plane-based point cloud
representation;robotic platforms;semantic segmentation;single inference
step;smoothing step;three-dimensional point clouds;Feature
extraction;Image color analysis;Radio
frequency;Semantics;Standards;Three-dimensional
displays;Training;Entangled Fores;Semantic Segmentation},
doi={10.1109/LRA.2015.2506118},
ISSN={2377-3766},
month={Jan},}
@INPROCEEDINGS{7363547,
author={T. Rodehutskors and M. Schwarz and S. Behnke},
booktitle={2015 IEEE-RAS 15th International Conference on Humanoid
Robots (Humanoids)},
title={Intuitive bimanual telemanipulation under communication
restrictions by immersive 3D visualization and motion tracking},
year={2015},
pages={276-283},
abstract={Robots which solve complex tasks in environments too dangerous
for humans to enter are desperately needed, e.g. for search and rescue
applications. As fully autonomous robots are not yet capable of
operating in highly unstructured real-world scenarios, teleoperation is
often used to embed the cognitive capabilities of human operators into
the robotic system. The many degrees of freedom of anthropomorphic
robots and communication restrictions pose challenges to the design of
teleoperation interfaces, though. In this work, we propose to combine
immersive 3D visualization and tracking of operator head and hand
motions to an intuitive interface for bimanual teleoperation. 3D point
clouds acquired from the robot are visualized together with a 3D robot
model and camera images using a tracked 3D head-mounted display. 6D
magnetic trackers capture the operator hand motions which are mapped to
the grippers of our two-armed robot Momaro. The proposed user interface
allows for solving complex manipulation tasks over degraded
communication links, as demonstrated at the DARPA Robotics Challenge
Finals and in lab experiments.},
keywords={cameras;data visualisation;dexterous
manipulators;grippers;helmet mounted displays;image capture;mobile
robots;telerobotics;user interfaces;3D point cloud visualization;3D
robot model;6D magnetic trackers;DARPA Robotics Challenge
Finals;bimanual teleoperation;camera images;communication
restrictions;complex manipulation tasks;degraded communication
links;grippers;immersive 3D visualization;intuitive bimanual
telemanipulation;intuitive interface;lab experiments;operator hand
motion capture;operator hand motion tracking;operator head motion
tracking;tracked 3D head-mounted display;two-armed robot
Momaro;Cameras;Mobile robots;Robot vision systems;Three-dimensional
displays;Tracking},
doi={10.1109/HUMANOIDS.2015.7363547},
month={Nov},}
@INPROCEEDINGS{7363506,
author={R. Cisneros and S. Kajita and T. Sakaguchi and S. Nakaoka and M.
Morisawa and K. Kaneko and F. Kanehiro},
booktitle={2015 IEEE-RAS 15th International Conference on Humanoid
Robots (Humanoids)},
title={Task-level teleoperated manipulation for the HRP-2Kai humanoid
robot},
year={2015},
pages={1102-1108},
abstract={This paper presents the strategy used by the team AIST-NEDO at
the DARPA Robotics Challenge to deal with the designated manipulation
tasks by means of a task-level teleoperation of the HRP-2Kai humanoid
robot, considering a disaster-hit scenario that is inherently
non-structured and a limited communication between the user and the
robot. The strategy, based on the information provided by a laser
rangefinder and a set of cameras installed at the head and at both
hands, consisted in the alignment of 3D models representing the desired
manipulation targets with a measured point cloud, in order to provide a
reference frame to describe the manipulation motion required for each
task. Each motion was carefully planned in advance by assuming minimum
information of the object representing the manipulation target. In order
to exemplify the before mentioned approach, two representative tasks of
the DARPA Robotics Challenge are described, as well as the corresponding
results obtained during the competition.},
keywords={disasters;humanoid robots;image representation;image
sensors;laser ranging;mobile robots;path planning;robot vision;solid
modelling;telerobotics;3D model alignment;AIST-NEDO team;DARPA Robotics
Challenge;HRP-2Kai humanoid robot;disaster-hit scenario;laser
rangefinder;task-level teleoperated manipulation;Cameras;Graphical user
interfaces;Humanoid robots;Legged locomotion;Robot vision
systems;Three-dimensional displays},
doi={10.1109/HUMANOIDS.2015.7363506},
month={Nov},}
@INPROCEEDINGS{7353539,
author={A. J. Rodríguez-Sánchez and S. Szedmak and J. Piater},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={SCurV: A 3D descriptor for object classification},
year={2015},
pages={1320-1327},
abstract={3D Object recognition is one of the big problems in Computer
Vision which has a direct impact in Robotics. There have been great
advances in the last decade thanks to point cloud descriptors. These
descriptors do very well at recognizing object instances in a wide
variety of situations. Of great interest is also to know how descriptors
perform in object classification tasks. With that idea in mind, we
introduce a descriptor designed for the representation of object
classes. Our descriptor, named SCurV, exploits 3D shape information and
is inspired by recent findings from neurophysiology. We compute and
incorporate surface curvatures and distributions of local surface point
projections that represent flatness, concavity and convexity in a 3D
object-centered and view-dependent descriptor. These different sources
of information are combined in a novel and simple, yet effective, way of
combining different features to improve classification results which can
be extended to the combination of any type of descriptor. Our
experimental setup compares SCurV with other recent descriptors on a
large classification task. Using a large and heterogeneous database of
3D objects, we perform our experiments both on a classical, flat
classification task and within a novel framework for hierarchical
classification. On both tasks, the SCurV descriptor outperformed all
other 3D descriptors tested.},
keywords={computer vision;image classification;object recognition;shape
recognition;3D descriptor;3D object recognition;3D object-centered
descriptor;3D shape information;SCurV;computer vision;flat
classification task;local surface point
projections;neurophysiology;object classification;object classification
tasks;point cloud descriptors;surface curvatures;view-dependent
descriptor;Computer
vision;Context;Databases;Histograms;Shape;Three-dimensional displays},
doi={10.1109/IROS.2015.7353539},
month={Sept},}
@INPROCEEDINGS{7353632,
author={Xiaolong Wang and Hong Zhang and Guohua Peng},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={3-DOF point cloud registration using congruent triangles},
year={2015},
pages={1943-1948},
abstract={In this paper, we present an efficient 3-DOF registration
method to align two overlapping point clouds captured by a range sensor
that experiences locally planar motion such as in many robotics
applications. The algorithm follows the RANSAC framework and is based on
finding a pair of congruent triangles in the source and target clouds.
With the assumption of planar sensor motion, corresponding vertices of
the congruent triangles must have similar elevation values, allowing our
algorithm to identify them efficiently. Specifically, given a triangle
in the source cloud, our algorithm first finds two vertices of a
candidate congruent triangle in the target cloud and then uses the third
vertex to verify, minimizing the complexity of the geometric base as
well as the expected time of sampling successfully a matching congruent
triangle. To improve the performance of our algorithm further, our
verification of a hypothetical alignment transformation proceeds first
locally by using only the points near the vertices of the congruent
triangles before involving the entire cloud. Our experimental results
show that the proposed algorithm outperforms state-of-the-art
registration algorithms in the case of 3-DOF planar sensor motion.},
keywords={computational geometry;iterative methods;path planning;random
processes;robots;3-DOF point cloud registration;RANSAC
framework;congruent triangles;planar sensor motion;range sensor;robotics
application;Complexity theory;Computational modeling;Mathematical
model;Nearest neighbor searches;Robot sensing systems;Three-dimensional
displays},
doi={10.1109/IROS.2015.7353632},
month={Sept},}
@INPROCEEDINGS{7353928,
author={A. Petit and V. Lippiello and B. Siciliano},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Real-time tracking of 3D elastic objects with an RGB-D sensor},
year={2015},
pages={3914-3921},
abstract={This paper presents a method to track in real-time a 3D
textureless object which undergoes large deformations such as elastic
ones, and rigid motions, using the point cloud data provided by an RGB-D
sensor. This solution is expected to be useful for enhanced manipulation
of humanoid robotic systems. Our framework relies on a prior visual
segmentation of the object in the image. The segmented point cloud is
registered first in a rigid manner and then by non-rigidly fitting the
mesh, based on the Finite Element Method to model elasticity, and on
geometrical point-to-point correspondences to compute external forces
exerted on the mesh. The real-time performance of the system is
demonstrated on synthetic and real data involving challenging
deformations and motions.},
keywords={finite element analysis;humanoid robots;image colour
analysis;image segmentation;image texture;manipulators;object
tracking;robot vision;3D elastic object;3D textureless object;RGB-D
sensor;enhanced manipulation;finite element method;geometrical
point-to-point correspondence;humanoid robotic system;model
elasticity;point cloud data;real-time performance;real-time
tracking;rigid motion;segmented point cloud;visual
segmentation;Computational modeling;Deformable models;Finite element
analysis;Image segmentation;Real-time systems;Robots;Three-dimensional
displays},
doi={10.1109/IROS.2015.7353928},
month={Sept},}
@INPROCEEDINGS{7353481,
author={D. Maturana and S. Scherer},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={VoxNet: A 3D Convolutional Neural Network for real-time object
recognition},
year={2015},
pages={922-928},
abstract={Robust object recognition is a crucial skill for robots
operating autonomously in real world environments. Range sensors such as
LiDAR and RGBD cameras are increasingly found in modern robotic systems,
providing a rich source of 3D information that can aid in this task.
However, many current systems do not fully utilize this information and
have trouble efficiently dealing with large amounts of point cloud data.
In this paper, we propose VoxNet, an architecture to tackle this problem
by integrating a volumetric Occupancy Grid representation with a
supervised 3D Convolutional Neural Network (3D CNN). We evaluate our
approach on publicly available benchmarks using LiDAR, RGBD, and CAD
data. VoxNet achieves accuracy beyond the state of the art while
labeling hundreds of instances per second.},
keywords={image representation;neurocontrollers;object
recognition;real-time systems;robot vision;3D convolutional neural
network;VoxNet;point cloud data;range sensors;real-time object
recognition;robots;robust object recognition;volumetric occupancy grid
representation;Feature extraction;Laser radar;Neural networks;Object
recognition;Robots;Sensors;Three-dimensional displays},
doi={10.1109/IROS.2015.7353481},
month={Sept},}
@INPROCEEDINGS{7353451,
author={A. Duda and J. Schwendner and C. Gaudig},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={SRSL: Monocular self-referenced line structured light},
year={2015},
pages={717-722},
abstract={Sensing of environment geometry and texture is a key
requirement for mobile robotic systems. In the underwater domain,
difficult environmental conditions restrict the applicability of many
existing methods for 3D sensing. A new method is proposed, which uses a
visible laser line projected onto a monocular camera image to perform 3D
scene reconstruction. The method fuses Structured Light with Structure
from Motion in an integrated process, which allows for the capturing of
dense 3D point clouds on moving systems in situations with low texture
and minimal scene structure. The system is evaluated in three experiment
scenarios and provides an average translation error of 2% in the KITTI
Benchmark for monocular visual odometry and an average model error of
1:7mm when compared with a tabletop structured light system.},
keywords={mobile robots;robot vision;underwater vehicles;3D scene
reconstruction;3D sensing;KITTI benchmark;SRSL;dense 3D point
clouds;environment geometry sensing;mobile robotic systems;monocular
self-referenced line structured light;monocular visual odometry;moving
systems;tabletop structured light system;texture sensing;underwater
domain;visible laser line;Adaptive optics;Cameras;Feature
extraction;Optical imaging;Robot sensing systems;Three-dimensional
displays;Visualization},
doi={10.1109/IROS.2015.7353451},
month={Sept},}
@INPROCEEDINGS{7353737,
author={D. Hu and Y. Gong and B. Hannaford and E. J. Seibel},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Path planning for semi-automated simulated robotic neurosurgery},
year={2015},
pages={2639-2645},
abstract={This paper considers the semi-automated robotic surgical
procedure for removing the brain tumor margins, where the manual
operation is a tedious and time-consuming task for surgeons. We present
robust path planning methods for robotic ablation of tumor residues in
various shapes, which are represented in point-clouds instead of
analytical geometry. Along with the path plans, corresponding metrics
are also delivered to the surgeon for selecting the optimal candidate in
the automated robotic ablation. The selected path plan is then executed
and tested on RAVEN™ II surgical robot platform as part of the
semi-automated robotic brain tumor ablation surgery in a simulated
tissue phantom.},
keywords={medical robotics;neurophysiology;path planning;robust
control;surgery;tumours;RAVEN II surgical robot platform;analytical
geometry;automated robotic ablation;brain tumor margins;path
planning;point-clouds;robust path planning methods;semiautomated robotic
brain tumor ablation surgery;semiautomated simulated robotic
neurosurgery;simulated tissue phantom;tumor residues;Cavity
resonators;Planning;Robots;Shape;Surgery;Three-dimensional
displays;Tumors},
doi={10.1109/IROS.2015.7353737},
month={Sept},}
@INPROCEEDINGS{7346889,
author={J. L. Espinosa-Aranda and N. Vallez and C. Sanchez-Bueno and D.
Aguado-Araujo and G. Bueno and O. Deniz},
booktitle={2015 IEEE Conference on Communications and Network Security
(CNS)},
title={Pulga, a tiny open-source MQTT broker for flexible and secure IoT
deployments},
year={2015},
pages={690-694},
abstract={The Eyes of Things (EoT) EU H2020 project envisages a computer
vision platform that can be used both standalone and embedded into more
complex artifacts, particularly for wearable applications, robotics,
home products, surveillance etc. The core hardware will be based on a
number of technologies and components that have been designed for
maximum performance of the always-demanding vision applications while
keeping the lowest energy consumption. An important functionality is to
be able to communicate with other devices that we use everyday (say,
configuring and controlling the EoT device from a tablet). Apart from
low-power hardware components, an efficient protocol is necessary.
Text-oriented protocols like HTTP are not appropriate in this context.
Instead, the lightweight publish/subscribe MQTT protocol was selected.
Still, the typical scenario is that of a device that sends/receives
messages, the messages being forwarded by a cloud-based message broker.
In this paper we propose a novel approach in which each EoT device acts
as an MQTT broker instead of the typical cloud-based architecture. This
eliminates the need for an external Internet server, which not only
makes the whole deployment more affordable and simpler but also more
secure by default.},
keywords={Internet of Things;cloud computing;computer network
security;computer vision;energy consumption;telecommunication power
management;transport protocols;EoT EU H2020 project;Eyes of Things EU
H2020 project;Internet;cloud-based message broker;computer vision
platform;energy consumption;secure IoT deployment;text-oriented
protocol;tiny open-source MQTT broker;Ad hoc networks;Cloud
computing;Computers;IEEE 802.11 Standard;Protocols;Security;Servers},
doi={10.1109/CNS.2015.7346889},
month={Sept},}
@INPROCEEDINGS{7329648,
author={G. Magyar and M. Vircikova},
booktitle={2015 IEEE 19th International Conference on Intelligent
Engineering Systems (INES)},
title={Proposal of a cloud-based agent for social human-robot
interaction that learns from the human experimenters},
year={2015},
pages={107-111},
abstract={The field of human-robot interaction usually uses an
experimental technique called Wizard of Oz where a human operator (the
experimenter or a confederate) remotely controls the behavior of the
system. Per contra, if robots are autonomous during the interaction,
they have a limited pre-programmed set of behaviors. We propose to use
reinforcement learning for adaptation of autonomous robotic behavior
during the interaction and to benefit from the advantages that brings
the field of cloud computing. The overall goal is to design robotic
behaviors less boring and more effective and thus, to prepare robots for
a long-term human-robot interaction.},
keywords={cloud computing;human-robot interaction;learning (artificial
intelligence);mobile robots;telecontrol;autonomous robotic
behavior;cloud computing;cloud-based agent;human operator;reinforcement
learning;remote control;social human-robot interaction;Cloud
computing;Educational robots;Human-robot interaction;Learning
(artificial intelligence);Libraries},
doi={10.1109/INES.2015.7329648},
month={Sept},}
@INPROCEEDINGS{7334089,
author={A. Kolb and C. Meaclem and X. Chen and R. Parker and S.
Gutschmidt and B. Milne},
booktitle={2015 IEEE 10th Conference on Industrial Electronics and
Applications (ICIEA)},
title={Tree trunk detection system using LiDAR for a semi-autonomous
tree felling robot},
year={2015},
pages={84-89},
abstract={To successfully fell a tree, semi-autonomous robotic devices
must be capable of identifying the position, size and orientation of
trees in the environment of a plantation forest. An approach is proposed
to identify the aforementioned tree properties using data gathered from
a Light Detection And Ranging (LiDAR) scanner. Trees are detected from
the gathered point cloud by searching for horizontally directed normal
vectors, then formed into clusters on which cylinder fitting is
performed. This approach was verified using three different forest
scenes to test its effectiveness.},
keywords={curve fitting;forestry;manipulators;mobile robots;object
detection;optical radar;robot vision;vegetation;LiDAR scanner;Light
Detection And Ranging;cylinder fitting;forest scene;horizontally
directed normal vectors;plantation forest;point cloud;semiautonomous
robotic device;semiautonomous tree felling robot;tree orientation
identification;tree position identification;tree property;tree size
identification;tree trunk detection system;Biological system
modeling;Gravity;Laser radar;Object recognition;Robots;Three-dimensional
displays;Vegetation},
doi={10.1109/ICIEA.2015.7334089},
month={June},}
@INPROCEEDINGS{7326249,
author={Y. Che and Y. Xue and H. Xu and R. Mikusauskas and L. She},
booktitle={2015 IEEE International Geoscience and Remote Sensing
Symposium (IGARSS)},
title={The inter-comparison of AATSR aerosol optical depth retrievals
from various algorithms},
year={2015},
pages={2230-2233},
abstract={The project aerosol-CCI as part of European Space Agency (ESA)
Climate Change Initiative (CCI) has provided three aerosol retrieval
algorithms for the Advanced Along-Track Scanning Radiometer (AATSR)
aboard on ENVISAT. For the purpose of estimating different performance
of these three algorithms in Asia, in this paper we compared the Aerosol
Optical Depth (AOD) of L2 data (10km×10km) including FMI AATSR Dual-view
ADV algorithm, the Oxford RAL Aerosol and Cloud retrieval (ORAC)
algorithm and the Swansea University AATSR retrieval (SU) algorithm with
the AErosol RObotic NETwork (AERONET) and the China Aerosol Remote
Sensing Network (CARSNET) data separately. The result shows that the
algorithms of ADV and SU have good performance on the retrieval of AOD,
and the ORAC algorithm has relative lower precision than other two
algorithms.},
keywords={aerosols;atmospheric optics;radiometry;remote sensing;AATSR
aerosol optical depth retrieval;Aerosol Robotic Network;Asia;CARSNET
data;China Aerosol Remote Sensing Network;Climate Change
Initiative;Environmental Satellite;European Space Agency;FMI AATSR
Dual-view ADV algorithm;ORAC algorithm;Oxford RAL Aerosol and Cloud
retrieval;Swansea University AATSR retrieval algorithm;advanced
along-track scanning radiometer;aerosol-CCI
project;Accuracy;Aerosols;Atmospheric measurements;Optical
sensors;Remote sensing;Satellite broadcasting;Satellites;AATSR;aerosol
optical depth (AOD);satellite remote sensing;validation},
doi={10.1109/IGARSS.2015.7326249},
ISSN={2153-6996},
month={July},}
@INPROCEEDINGS{7314200,
author={M. Miknis and R. Davies and P. Plassmann and A. Ware},
booktitle={2015 International Conference on Systems, Signals and Image
Processing (IWSSIP)},
title={Near real-time point cloud processing using the PCL},
year={2015},
pages={153-156},
abstract={Real-time 3D data processing is important in robotics, video
games, environmental mapping, medical and many other fields. In this
paper we propose a novel optimisation approach for the open source Point
Cloud Library (PCL) that is frequently used for processing 3D data.
Three aspects of the PCL are discussed: point cloud creation from
disparity of colour image pairs, voxel grid downsample filtering to
simplify point clouds and passthrough filtering to adjust the size of
the point cloud. Additionally, rendering is examined. An optimisation
technique based on CPU cycle measurement is proposed and applied in
order to optimise those parts of the processing chain where measured
performance is worst. The PCL modules thus optimised show on average an
improvement in speed of 2.4x for point cloud creation, 91x for voxel
grid filtering and 7.8x for the passthrough filter.},
keywords={filtering theory;image colour analysis;solid modelling;3D data
processing;CPU cycle measurement;PCL;colour image pairs;near realtime
point cloud processing;open source point cloud library;optimisation
technique;pass-through filtering;point cloud creation;voxel grid
downsample filtering;Filtering;Image color
analysis;Libraries;Optimization;Real-time systems;Rendering (computer
graphics);Three-dimensional displays;PCL;Point clouds;Real-time},
doi={10.1109/IWSSIP.2015.7314200},
ISSN={2157-8672},
month={Sept},}
@INPROCEEDINGS{7301605,
author={E. Rohmer and P. Pinheiro and E. Cardozo and M. Bellone and G.
Reina},
booktitle={2015 IEEE 20th Conference on Emerging Technologies Factory
Automation (ETFA)},
title={Laser based driving assistance for smart robotic wheelchairs},
year={2015},
pages={1-4},
abstract={This paper is presenting the ongoing work toward a novel
driving assistance system of a robotic wheelchair, for people paralyzed
from down the neck. The user's head posture is tracked, to accordingly
project a colored spot on the ground ahead, with a pan-tilt mounted
laser. The laser dot on the ground represents a potential close range
destination the operator wants to reach autonomously. The wheelchair is
equipped with a low cost depth-camera (Kinect sensor) that models a
traversability map in order to define if the designated destination is
reachable or not by the chair. If reachable, the red laser dot turns
green, and the operator can validate the wheelchair destination via an
Electromyogram (EMG) device, detecting a specific group of muscle's
contraction. This validating action triggers the calculation of a path
toward the laser pointed target, based on the traversability map. The
wheelchair is then controlled to follow this path autonomously. In the
future, the stream of 3D point cloud acquired during the process will be
used to map and self localize the wheelchair in the environment, to be
able to correct the estimate of the pose derived from the wheel's
encoders.},
keywords={control engineering computing;electromyography;handicapped
aids;medical robotics;wheelchairs;3D point cloud;EMG device;Kinect
sensor;depth-camera;electromyogram;head posture;laser based driving
assistance;paralyzed people;smart robotic wheelchairs;Magnetic
heads;Mobile robots;Navigation;Robot sensing systems;Three-dimensional
displays;Wheelchairs},
doi={10.1109/ETFA.2015.7301605},
ISSN={1946-0740},
month={Sept},}
@INPROCEEDINGS{7294188,
author={E. C. H. Cheung and J. Wong and J. Chan and J. Pan},
booktitle={2015 IEEE International Conference on Automation Science and
Engineering (CASE)},
title={Optimization-based automatic parameter tuning for stereo vision},
year={2015},
pages={855-861},
abstract={Stereo vision is an important 3D sensing technique for
producing dense point clouds required for robotic navigation and
manipulation. It can provide excellent depth resolution at high frame
rates and is potentially smaller, cheaper and consumes less power than
systems using active sensor devices, due to its use of standard imaging
components like cameras. However, stereo vision system can generate high
quality point clouds only when its parameters are appropriately tuned.
To tune these parameters manually is not only tedious but also
challenging, due to the large number of parameters and their non-linear
effect on the depth map quality. In this paper, we present an
optimization-based method to automatically tune the stereo parameters.
In particular, we first adjust the disparity range to ensure the entire
scene can be covered in the resultant depth map, and then use non-linear
optimization to refine other parameters for the optimal depth map
quality. Our tuning process is efficient and can update adaptively
according to changing environment. Experiments on the teleoperation
tasks using the Atlas robot validate our approach, and demonstrate the
improvement it brings for the teleoperation effectiveness.},
keywords={image resolution;manipulators;mobile
robots;navigation;optimisation;robot vision;stereo image processing;3D
sensing technique;automatic parameter tuning;dense point clouds;depth
resolution;manipulation;nonlinear optimization;robotic navigation;stereo
vision;Cameras;Optimization;Robot sensing systems;Stereo
vision;Three-dimensional displays;Tuning},
doi={10.1109/CoASE.2015.7294188},
ISSN={2161-8070},
month={Aug},}
@INPROCEEDINGS{7275877,
author={D. Sunehra and A. Bano and S. Yandrathi},
booktitle={2015 International Conference on Advances in Computing,
Communications and Informatics (ICACCI)},
title={Remote monitoring and control of a mobile robot system with
obstacle avoidance capability},
year={2015},
pages={1803-1809},
abstract={With the advancements in technology, the field of robotics and
automation has gained tremendous popularity. Mobile Robots are being
widely used in a number of places including production plants,
warehouses, airports, agriculture, medical, military, and in hazardous
environments to reduce human efforts. In this paper, we present the
design and implementation of a mobile robot system with obstacle
avoidance capability for remote sensing and monitoring. The proposed
system enables the user (base station) to send necessary commands to the
remote station (mobile robot) using Dual-Tone Multi-Frequency (DTMF)
signals for robot teleoperation. Global Positioning System (GPS) and
Global System for Mobile communication (GSM) technologies are used,
which provide user with mobile robot location in the form of a Google
map link. The system also provides the user with real time video
monitoring of the remote area by using an internet enabled device. The
user can also save the images and record the videos captured by the
mobile robot IP webcam at the remote location, which can be stored in a
public cloud for later use.},
keywords={collision avoidance;computerised monitoring;control
engineering computing;mobile robots;telerobotics;DTMF signals;GPS;GSM
technology;Global Positioning System;Google map link;Internet enabled
device;base station;dual-tone multifrequency signals;global system for
mobile communication;mobile robot system;obstacle avoidance
capability;real-time video monitoring;remote control;remote
monitoring;remote sensing;remote station;robot IP Webcam;robot
teleoperation;DC motors;GSM;Global Positioning
System;Microcontrollers;Mobile robots;Robot sensing systems;DTMF
decoder;GSM Modem;Global Positioning System;IP webcam;Mobile
Robot;Ultrasonic sensors},
doi={10.1109/ICACCI.2015.7275877},
month={Aug},}
@ARTICLE{7165667,
author={C. H. Park and E. S. Ryu and A. M. Howard},
journal={IEEE Transactions on Haptics},
title={Telerobotic Haptic Exploration in Art Galleries and Museums for
Individuals with Visual Impairments},
year={2015},
volume={8},
number={3},
pages={327-338},
abstract={This paper presents a haptic telepresence system that enables
visually impaired users to explore locations with rich visual
observation such as art galleries and museums by using a telepresence
robot, a RGB-D sensor (color and depth camera), and a haptic interface.
The recent improvement on RGB-D sensors has enabled real-time access to
3D spatial information in the form of point clouds. However, the
real-time representation of this data in the form of tangible haptic
experience has not been challenged enough, especially in the case of
telepresence for individuals with visual impairments. Thus, the proposed
system addresses the real-time haptic exploration of remote 3D
information through video encoding and real-time 3D haptic rendering of
the remote real-world environment. This paper investigates two scenarios
in haptic telepresence, i.e., mobile navigation and object exploration
in a remote environment. Participants with and without visual
impairments participated in our experiments based on the two scenarios,
and the system performance was validated. In conclusion, the proposed
framework provides a new methodology of haptic telepresence for
individuals with visual impairments by providing an enhanced interactive
experience where they can remotely access public places (art galleries
and museums) with the aid of haptic modality and robotic telepresence.},
keywords={handicapped aids;haptic interfaces;image sensors;mobile
robots;museums;robot vision;telerobotics;3D spatial information;RGB-D
sensor;art galleries;haptic interface;haptic telepresence system;mobile
navigation;museums;object exploration;point clouds;public
places;real-time 3D haptic rendering;remote environment;rich visual
observation;robotic telepresence;tangible haptic experience;telepresence
robot;telerobotic haptic exploration;visual impairments;Haptic
interfaces;Image color analysis;Robot sensing systems;Streaming
media;Three-dimensional displays;Visualization;3D haptic rendering;3D
haptic rendering,;Haptic telepresence;assistive robotics;depth
sensors;multimedia streaming;video encoding;visual impairment;0},
doi={10.1109/TOH.2015.2460253},
ISSN={1939-1412},
month={July},}
@INPROCEEDINGS{7253926,
author={S. K. Balabhadruni},
booktitle={2015 International Conference on Electrical, Electronics,
Signals, Communication and Optimization (EESCO)},
title={Intelligent traffic with connected vehicles: intelligent and
connected traffic systems},
year={2015},
pages={1-6},
abstract={This paper describes the structured approach involved in the
development of an Intelligent Autonomous (self-driving, unmanned,
driverless or robotic) Vehicles. In which autopilot with artificial
intelligence are critical subsystems whose development requires
multidisciplinary approach along with concurrent engineering to create a
better, safer and reliable future. We have studied and implemented a
miniature scale model with outcome of satisfactory results of supporting
realistic vehicular mobility simulation using concepts of swarm
technology discussed in this paper. Our Model must be equipped with a
variety of instrumentation and controls depending upon the mission of
the target vehicle. Mechatronics, Systems Engineering (SE), Control
Systems (CS), Swarm Technology, Artificial Intelligence, Image
Processing Cloud Computing, Virtualization with caching, Fuzzy Logic and
Neural Networks has a potential scope of design for the prototype needed
to be developed that will navigate to a desired location with obstacle
avoidance. In this design of autonomous vehicles have access to
information about their surroundings gathered from its several sensors
such as Radar, GPS including a very important component of this system
Infrastructure Unit which is connected virtually with Vehicle's
Operating System, mapping and direction system is discussed broadly.
Here, Infrastructure Unit plays a major role in routing the traffic to
maintain free flow and accident avoidance, by provides information such
as Routes, Traffic, Time, Directions to Vehicles and maintain constant
speed for all vehicles to achieve an efficient autonomous transportation
reducing accidents to zero. To improve the response time and storage of
V2I Communication a new approach of caching and virtualization are
encapsulated with a better and faster hardware such as Solid State
Technology. This study has various applications in Space Science,
Oceanography, and Automation in Traffic control which can effortlessly
mee- the necessity, scalability of future Generation.},
keywords={3D Imaging;AI;Automation;Cache;Cloud;Flashcache;OSI
Model;Robotics;Smart Cars;Solid State;Swarm Technology;VLC;Vehicular
Communications;Virtualization},
doi={10.1109/EESCO.2015.7253926},
month={Jan},}
@INPROCEEDINGS{7219710,
author={A. Vick and V. Vonásek and R. Pěnička and J. Krüger},
booktitle={2015 10th International Workshop on Robot Motion and Control
(RoMoCo)},
title={Robot control as a service #x2014; Towards cloud-based motion
planning and control for industrial robots},
year={2015},
pages={33-39},
abstract={This paper introduces a new concept for flexible motion
planning and control of industrial robots. Instead of a closed
monolithic architecture, an open service-based framework is proposed.
The services can be run hardware-independent on a decentralized IT
infrastructure (Cloud) allowing a fast reconfiguration of control
modules and their multiple usage in different tasks. The services can be
run on a dedicated PC or on individual virtual machines inside a single
computing cluster. The proposed service-based framework was implemented
and tested for the motion planning of a robotic manipulator. Effects on
control performance, availability and scalability are investigated and
documented.},
keywords={cloud computing;industrial manipulators;motion control;path
planning;virtual machines;PC;RobotControl;cloud-based flexible motion
planning;cloud-based flexibleflexible motion control;computing
cluster;control modules reconfiguration;decentralized IT
infrastructure;hardware-independent services;industrial robots;open
service-based framework;robotic manipulator;service-based
framework;virtual machines;Collision
avoidance;Interpolation;Planning;Robot kinematics;Robot sensing
systems;Service robots},
doi={10.1109/RoMoCo.2015.7219710},
month={July},}
@INPROCEEDINGS{7208265,
author={J. Žáček and E. Volná and M. Janošek},
booktitle={2015 IEEE 10th Jubilee International Symposium on Applied
Computational Intelligence and Informatics},
title={Modular neural network in the cloud},
year={2015},
pages={553-557},
abstract={The paper proposes a new approach to implement common neural
network algorithms in the cloud. First part of the paper defines the
context and describes a usage of cloud computing concept in the
robotics. Next part describes layers of the typical cloud service and
shows the connection between cloud and legacy robot. Then we introduce a
modular neural network concept and explain the topology of the neural
network and connection between the physical and the logical
architecture. Following part proposes a new architecture of the cloud
service that implements a neural network algorithm. Last part shows a
case study migration of the typical neural network topology into the
cloud with respect to logical and physical topology.},
keywords={cloud computing;mobile robots;neural nets;topology;cloud
computing;cloud service;legacy robot;logical architecture;mobile
robot;modular neural network topology;Cloud computing;Computer
architecture;Network topology;Neural networks;Robot sensing
systems;Topology},
doi={10.1109/SACI.2015.7208265},
month={May},}
@INPROCEEDINGS{7193238,
author={Aadhityan A},
booktitle={2015 International Conference on Innovations in Information,
Embedded and Communication Systems (ICIIECS)},
title={A novel method for implementing Artificial Intelligence, Cloud
and Internet of Things in Robots},
year={2015},
pages={1-4},
abstract={This paper describe about a new methodology to implement
Artificial Intelligence, Cloud and Internet of Things in Robots. Now a
days, Artificial Intelligence take a main part in the world into
robotics. Almost all industries use robots for various works. They were
using co-operative robots to make different kind of works. But there was
some problem to make robot for multi tasks. So there is a need to create
new methodology to made multi tasking robots. It will be done only by
artificial intelligence and internet of things. Also connected to cloud
can reduce cost. This paper describe the process to make a robot in a
simplified manner.},
keywords={Internet of Things;artificial intelligence;cloud
computing;control engineering computing;multi-robot systems;Internet of
Things;artificial intelligence;cloud computing;cooperative robots;cost
reduction;multitasking robots;robots;Artificial intelligence;Cloud
computing;Robot kinematics;Robot sensing systems;Service
robots;Artificial intelligence;Cloud;Internet of things;Robotics;Robots},
doi={10.1109/ICIIECS.2015.7193238},
month={March},}
@INPROCEEDINGS{7166229,
author={A. Kumar and A. Mishra and P. Makula and K. Karan and V. K.
Mittal},
booktitle={2015 IEEE Region 10 Symposium},
title={Smart Robotic Assistant},
year={2015},
pages={25-28},
abstract={Smart robotic assistants help human beings in reducing the
manual efforts in day-to-day tasks and the risk to precious human lives
in hazardous situations. In this paper, we develop a smart robotic
assistant that operates on human voice commands, given remotely by using
an Android platform based smart IoT device. The real-time signal
processing of the voice commands is carried out using a cloud server.
The speech command signal converted to text form is then communicated to
the robotic assistant over a Bluetooth network. The robotic assistant is
developed on an Arduino micro-controller based platform. The robotic
assistant is capable of performing different movements such as turns and
start/stop operations, and picking up an object from one location and
placing it at another location. The effectiveness of the voice commands
given over the network is measured through several experiments.
Performance evaluation results are encouraging. Potential applications
of this smart robotic assistant in homes, hospitals and industries are
discussed.},
keywords={Android (operating system);Bluetooth;Internet of Things;audio
signal processing;audio user interfaces;cloud computing;human-robot
interaction;intelligent robots;microcontrollers;mobile computing;risk
management;service robots;speech processing;Android platform based smart
IoT device;Arduino microcontroller based platform;Bluetooth
network;cloud server;day-to-day tasks;hazardous situations;human
lives;human voice commands;performance evaluation;real-time signal
processing;smart robotic assistants;speech command signal;start-stop
operations;Angular velocity;Bluetooth;Performance evaluation;Service
robots;Speech;Torque;Android based smart IoT devices;Bluetooth
network;smart robotic assistant;voice commands},
doi={10.1109/TENSYMP.2015.8},
month={May},}
@INPROCEEDINGS{7153196,
author={D. Lang and S. Friedmann and J. Hedrich and D. Paulus},
booktitle={2015 14th IAPR International Conference on Machine Vision
Applications (MVA)},
title={Semantic mapping for mobile outdoor robots},
year={2015},
pages={325-328},
abstract={In this paper we present the concept and realization of a
semantic mapping system for a mobile outdoor robot. Semantic maps aim to
give robots the ability to gather semantic information about their
environment, to store it, represent it for the user, and to perform
high-level tasks based on the semantic information. The map is build by
a system integrating the combination of object classification and
common-sense knowledge. We validate the proposed semantic map
representation on a real-world 3D point cloud dataset. The presented
classification approach achieves an overall precision about 96 %. The
semantic maps result into a data structure which offers the opportunity
to solve complex task settings and can be integrated onto real robotic
systems.},
keywords={control engineering computing;data structures;mobile
robots;path planning;pattern classification;classification
approach;common-sense knowledge;complex task settings;data
structure;high-level tasks;mobile outdoor robots;object
classification;real-world 3D point cloud dataset;robotic
systems;semantic information;semantic map representation;semantic
mapping system;Algorithm design and analysis;Buildings;Data
structures;Robots;Search problems;Semantics;Three-dimensional displays},
doi={10.1109/MVA.2015.7153196},
month={May},}
@INPROCEEDINGS{7151926,
author={P. Benavidez and M. Kumar and B. Erol and M. Jamshidi and S.
Agaian},
booktitle={2015 10th System of Systems Engineering Conference (SoSE)},
title={Software interface design for home-based assistive multi-robot
system},
year={2015},
pages={404-409},
abstract={In many assistive robotic systems, the interface to the user
is simply a tablet computer or a monitor attached to a single robot.
Missing from approaches are the system extensibility made possible with
a tablet computer and a division of work between multiple agents. In
this paper we present the design for a software interface to connect
users to an assistive robot system for the disabled and elderly. The
system is comprised of heterogeneous low-cost assistive robots, a home
management portal and a cloud computing backend. The system is designed
with the premise that all components do not need to be present for the
system to function, but it will be improved when expanded by addition of
robots and expanded computing capabilities. This paper focuses on
developing the interfaces necessary to connect the user to these systems
in a simple and easy to comprehend manner for the target user population.},
keywords={assisted living;cloud computing;handicapped aids;human-robot
interaction;multi-robot systems;notebook computers;service robots;cloud
computing;computing capabilities;disabled users;elderly
users;heterogeneous low-cost assistive robots;home management
portal;home-based assistive multirobot system;software interface
design;tablet computer;user interface;Cloud computing;Graphical user
interfaces;Mobile communication;Robots;Systems engineering and
theory;Usability;Robot Operating System;Robotics;assistive robot;cloud
computing;user interface design},
doi={10.1109/SYSOSE.2015.7151926},
month={May},}
@INPROCEEDINGS{7150798,
author={A. Mishra and P. Makula and A. Kumar and K. Karan and V. K.
Mittal},
booktitle={2015 International Conference on Industrial Instrumentation
and Control (ICIC)},
title={A voice-controlled personal assistant robot},
year={2015},
pages={523-528},
abstract={Personal robotic assistants help reducing the manual efforts
being put by humans in their day-to-day tasks. In this paper, we develop
a voice-controlled personal assistant robot. The human voice commands
are given to the robotic assistant remotely, by using a smart mobile
phone. The robot can perform different movements, turns, start/stop
operations and relocate an object from one place to another. The voice
commands are processed in real-time, using an online cloud server. The
speech signal commands converted to text form are communicated to the
robot over a Bluetooth network. The personal assistant robot is
developed on a micro-controller based platform and can be aware of its
current location. The effectiveness of the voice control communicated
over a distance is measured through several experiments. Performance
evaluation is carried out with encouraging results of the initial
experiments. Possible improvements are also discussed towards potential
applications in home, hospitals and industries.},
keywords={Bluetooth;cloud computing;control engineering
computing;microcontrollers;mobile computing;mobile handsets;service
robots;speech recognition;Bluetooth network;day-to-day tasks;human voice
commands;microcontroller based platform;online cloud server;personal
robotic assistants;smart mobile phone;speech signal commands;start-stop
operations;voice-controlled personal assistant robot;Angular
velocity;Bluetooth;Robot kinematics;Service robots;Smart
phones;Speech;Android based smart devices;Bluetooth;control over
voice;smart assistant robot},
doi={10.1109/IIC.2015.7150798},
month={May},}
@INPROCEEDINGS{7139871,
author={R. Ramakrishnan and J. Nieto and S. Scheding},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Shadow compensation for outdoor perception},
year={2015},
pages={4835-4842},
abstract={Outdoor robotic systems rely on perception modules that must
be robust to variations in environmental conditions. In particular,
vision-based perception systems are affected by illumination variations
caused by occlusions. We propose an approach to calculate the lighting
distribution of outdoor scenes. The new approach enables us to
compensate for shadows and therefore obtain images which are invariant
to the sun position and scene geometry, while also retaining the
dimensionality of the original data. The method combines images with
geometric information provided by range sensors to infer shadows. We
select a pair of points on a shadow boundary from a single material and
estimate a terrestrial sunlight-skylight ratio. Individual scaling
factors are then calculated for all points based on their orientation
and incident illumination sources. The result is a coloured point cloud
that is independent of illumination variation due to occlusions and
geometry. To demonstrate the effectiveness and generalisation of the
approach, we present evaluations using two datasets with different
cameras. The first uses a hyperspectral sensor that allows us to analyse
the results for a large number of wavelengths, while the second dataset
uses a standard RGB camera. The approach is shown to consistently
provide good illumination compensation in both scenarios.},
keywords={compensation;environmental factors;geometry;robot
vision;sensors;environmental conditions;hyperspectral
sensor;illumination compensation;incident illumination sources;lighting
distribution;outdoor robotic systems;range sensors;scene geometry;shadow
compensation;standard RGB camera;sun position;terrestrial
sunlight-skylight ratio;vision-based perception systems;Geometry;Image
color analysis;Lighting;Robots;Sensors;Sun;Three-dimensional displays},
doi={10.1109/ICRA.2015.7139871},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139439,
author={R. Zhang and S. A. Candra and K. Vetter and A. Zakhor},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Sensor fusion for semantic segmentation of urban scenes},
year={2015},
pages={1850-1857},
abstract={Semantic understanding of environments is an important problem
in robotics in general and intelligent autonomous systems in particular.
In this paper, we propose a semantic segmentation algorithm which
effectively fuses information from images and 3D point clouds. The
proposed method incorporates information from multiple scales in an
intuitive and effective manner. A late-fusion architecture is proposed
to maximally leverage the training data in each modality. Finally, a
pairwise Conditional Random Field (CRF) is used as a post-processing
step to enforce spatial consistency in the structured prediction. The
proposed algorithm is evaluated on the publicly available KITTI dataset
[1] [2], augmented with additional pixel and point-wise semantic labels
for building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist,
sign/pole, and fence regions. A per-pixel accuracy of 89.3% and average
class accuracy of 65.4% is achieved, well above current state-of-the-art
[3].},
keywords={image fusion;image segmentation;3D point clouds;KITTI
dataset;conditional random field;late-fusion architecture;pairwise
CRF;point-wise semantic labels;sensor fusion;urban scenes semantic
segmentation;Feature extraction;Image
segmentation;Labeling;Semantics;Three-dimensional
displays;Training;Training data},
doi={10.1109/ICRA.2015.7139439},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139459,
author={H. Zhang and C. Reardon and Chi Zhang and L. E. Parker},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Adaptive human-centered representation for activity recognition
of multiple individuals from 3D point cloud sequences},
year={2015},
pages={1991-1998},
abstract={Activity recognition of multi-individuals (ARMI) within a
group, which is essential to practical human-centered robotics
applications such as childhood education, is a particularly challenging
and previously not well studied problem. We present a novel adaptive
human-centered (AdHuC) representation based on local spatio-temporal
features (LST) to address ARMI in a sequence of 3D point clouds. Our
human-centered detector constructs affiliation regions to associate LST
features with humans by mining depth data and using a cascade of
rejectors to localize humans in 3D space. Then, features are detected
within each affiliation region, which avoids extracting irrelevant
features from dynamic background clutter and addresses moving cameras on
mobile robots. Our feature descriptor is able to adapt its support
region to linear perspective view variations and encode multi-channel
information (i.e., color and depth) to construct the final
representation. Empirical studies validate that the AdHuC representation
obtains promising performance on ARMI using a Meka humanoid robot to
play multi-people Simon Says games. Experiments on benchmark datasets
further demonstrate that our adaptive human-centered representation
outperforms previous approaches for activity recognition from
color-depth data.},
keywords={data mining;feature extraction;humanoid robots;image colour
analysis;image representation;image sensors;image sequences;mobile
robots;object recognition;robot vision;3D point cloud
sequences;ARMI;AdHuC representation;LST features;Meka humanoid
robot;activity recognition;adaptive human-centered
representation;affiliation regions;childhood education;depth data
mining;dynamic background clutter;human localization;human-centered
detector;human-centered robotics applications;linear perspective view
variations;local spatio-temporal features;mobile robots;moving
cameras;multichannel information encoding;multipeople Simon Says
games;multiple individuals;Cameras;Clutter;Feature extraction;Image
color analysis;Robot vision systems;Three-dimensional displays},
doi={10.1109/ICRA.2015.7139459},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139472,
author={B. Steder and M. Ruhnke and R. Kümmerle and W. Burgard},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Maximum likelihood remission calibration for groups of
heterogeneous laser scanners},
year={2015},
pages={2078-2083},
abstract={Laser range scanners are commonly used in mobile robotics to
enable a robot to sense the spatial configuration of its environment. In
addition to the range measurements, most scanners provide remission
values, representing the intensity of the returned light pulse. These
values add a visual component to the measurement and can be used to
improve reasoning on the data. Unfortunately, a remission value does not
directly tell us how bright a measured surface is in the infrared
spectrum. Rather, it varies with respect to the incidence angle and the
range at which it was measured. In addition, multiple scanners typically
do not agree upon the values of a certain surface. In this paper, we
present a calibration method for remission values of multiple laser
scanners considering dependencies in range, incidence angle of the
measured surface, and the respective scanner unit. Our system learns the
calibration parameters based on a set of registered point clouds. It
uses a graph optimization scheme to minimize the error between different
measurements, so that all involved scanners yield consistent reflection
values, independent of the perspective from which the corresponding
surface is observed.},
keywords={calibration;graph theory;laser ranging;maximum likelihood
estimation;mobile robots;optical scanners;optimisation;robot
vision;calibration parameters;error minimization;graph optimization
scheme;heterogeneous laser scanner group;incidence angle;infrared
spectrum;laser range scanners;maximum likelihood remission
calibration;mobile robotics;multiple laser scanners;range
measurements;reasoning improvement;reflection values;registered point
cloud set;remission values;returned light pulse intensity;scanner
unit;spatial configuration;visual component;Calibration;Laser
modes;Measurement by laser beam;Optimization;Robots;Surface emitting
lasers;Three-dimensional displays},
doi={10.1109/ICRA.2015.7139472},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7101633,
author={N. Pavón-Pulido and J. A. López-Riquelme and J. J.
Pinuaga-Cascales and J. Ferruz-Melero and R. M. d. Santos},
booktitle={2015 IEEE International Conference on Autonomous Robot
Systems and Competitions},
title={Cybi: A Smart Companion Robot for Elderly People: Improving
Teleoperation and Telepresence Skills by Combining Cloud Computing
Technologies and Fuzzy Logic},
year={2015},
pages={198-203},
abstract={This paper describes Cybi, an inexpensive smart companion
mobile robot for elderly and disabled people. The software architecture
that enables users to perform telepresence and teleoperation tasks with
Cybi is based on combining Cloud Computing technologies and Fuzzy Logic.
On one hand, the robot can be remotely teleoperated from a thin client
by executing a specific module of a distributed software component that
uses ROS, WebRTC and Google App Engine. On the other hand, Cybi is
capable of understanding simple spoken orders that allow it to navigate
through an unknown domestic indoor environment. In both cases, a fuzzy
reactive navigation software component is used with the aim of making
the teleoperation procedure as easy and safe as possible for both
elders' caregivers and elderly users themselves. Finally, several real
tests are also presented, demonstrating that Cybi could be considered as
a suitable robotic companion, since the proposed methods enhance the
performance of teleoperation and telepresence tasks.},
keywords={cloud computing;control engineering computing;fuzzy
logic;handicapped aids;mobile robots;operating systems
(computers);telerobotics;Cybi robot;Google App Engine;ROS;WebRTC;cloud
computing technology;disabled people;distributed software
component;elderly people;fuzzy logic;realtime operating system;smart
companion mobile robot;teleoperation skill;telepresence skill;Collision
avoidance;Navigation;Robot sensing systems;Senior
citizens;Software;Fuzzy Logic;Smart robot;disabled people;elderly
people;teleoperation;telepresence},
doi={10.1109/ICARSC.2015.40},
month={April},}
@INPROCEEDINGS{7101630,
author={J. Gavilanes and R. Fernández and H. Montes and J. Sarria and P.
G. d. Santos and M. Armada},
booktitle={2015 IEEE International Conference on Autonomous Robot
Systems and Competitions},
title={Instrumented Scanning Manipulator for Landmines Detection Tasks},
year={2015},
pages={180-185},
abstract={This paper presents an instrumented robotic arm for landmines
detection tasks during humanitarian demining activities. The manipulator
has 5 DOF and it is endowed with a metal detector for landmines
detection and a mini-TOF camera for mapping the terrain that has to be
scanned. The mini-TOF camera provides a point cloud of the terrain that
allows keeping the metal detector at a constant height above the ground
and performing an efficient scanning of the contaminated terrain.},
keywords={image sensors;landmine detection;manipulators;robot
vision;contaminated terrain scanning;humanitarian demining
activities;instrumented robotic arm;instrumented scanning
manipulator;landmines detection tasks;metal detector;mini-TOF
camera;terrain mapping;Cameras;Detectors;Metals;Robot vision
systems;humanitarian demining;landmines;manipulator;robotic
arm;scanning;time-of-flight camera},
doi={10.1109/ICARSC.2015.36},
month={April},}
@INPROCEEDINGS{7090625,
author={M. Häselich and B. Jöbgen and F. Neuhaus and D. Lang and D.
Paulus},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Markov random field terrain classification of large-scale 3D maps},
year={2014},
pages={1970-1975},
abstract={Simultaneous localization and mapping, drivability
classification of the terrain and path planning represent three major
research areas in the field of autonomous outdoor robotics. Especially
unstructured environments require a careful examination as they are
unknown, continuous and the number of possible actions for the robot are
infinite. We present an approach to create a semantic 3D map with
drivability information for wheeled robots using a terrain
classification algorithm. Our robot is equipped with a 3D laser range
finder, a Velodyne HDL-64E, as primary sensor. For the registration of
the point clouds, we use a featureless 3D correlative scan matching
algorithm which is an adaption of the 2D algorithm presented by Olson.
Every 3D laser scan is additionally classified with a Markov random
field based terrain classification algorithm. Our data structure for the
terrain classification approach is a 2D grid whose cells carry
information extracted from the laser range finder data. All cells within
the grid are classified and their surface is analyzed regarding its
drivability for wheeled robots. The main contribution of this work is
the novel combination of these two algorithms which yields classified 3D
maps with obstacle and drivability information. Thereby, the newly
created semantic map is perfectly tailored for generic path planning
applications for all kinds of wheeled robots. We evaluate our algorithms
on large datasets with more than 137 million annotated 3D points that
were labeled by multiple human experts. All datasets are published
online and are provided for the community.},
keywords={Global Positioning System;Markov processes;SLAM
(robots);collision avoidance;inertial navigation;laser ranging;mobile
robots;terrain mapping;2D grid;3D laser range finder;Markov random field
terrain classification;Velodyne HDL-64E;autonomous outdoor robotics;data
structure;drivability classification;drivability information;featureless
3D correlative scan matching algorithm;generic path planning
applications;large-scale 3D maps;laser range finder data;obstacle
information;path planning;semantic 3D map;simultaneous localization and
mapping;terrain planning;wheeled robots;Mobile
robots;Roads;Semantics;Simultaneous localization and
mapping;Three-dimensional displays},
doi={10.1109/ROBIO.2014.7090625},
month={Dec},}
@INPROCEEDINGS{7053067,
author={Z. Li and Z. Xia and Y. Hu and J. Zhang},
booktitle={Proceeding of the 11th World Congress on Intelligent Control
and Automation},
title={An improved framework for robotic door-opening task using Kinect
and tactile information},
year={2014},
pages={2219-2224},
abstract={Based on our previous work, this paper proposes an improved
robotic door-opening framework. The improved framework has three
components, grasping pattern, door-opening pattern, and semantic
monitoring and exception handling. (1) Grasping pattern contains various
grasping points and ways for different types of door knob. The point
cloud of the target area is captured using Kinect sensor, and the door
edge and knob is then extracted. (2) Determination of door-opening
pattern includes sub operations and the corresponding trajectories. (3)
By using semantic monitoring and exception handling, the tactile data
from the gripper during the task execution is monitored based on which
exceptional situation is assessed and handled. The proposed framework
was validated by two normal door-opening experiments and two exception
handling experiments using KUKA prototype.},
keywords={computer graphics;control engineering computing;exception
handling;grippers;image sensors;manipulators;tactile sensors;KUKA
prototype;Kinect sensor;door-opening pattern;exception handling
experiment;exceptional situation;grasping pattern;grasping
point;gripper;normal door-opening experiment;point cloud;robotic
door-opening framework;robotic door-opening task;semantic
monitoring;tactile data;tactile information;task
execution;Force;Grasping;Grippers;Monitoring;Tactile
sensors;Three-dimensional displays;Door-opening task;Exception
monitoring and handling;Kinect;Robotic manipulation;Tactile},
doi={10.1109/WCICA.2014.7053067},
month={June},}
@INPROCEEDINGS{7049637,
author={F. F. Sales and D. Portugal and R. P. Rocha},
booktitle={2014 11th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={Real-time people detection and mapping system for a mobile robot
using a RGB-D sensor},
year={2014},
volume={02},
pages={467-474},
abstract={In this paper, we present a robotic system capable of mapping
indoor, cluttered environments and, simultaneously, detecting people and
localizing them with respect to the map, in real-time, using solely a
Red-Green-Blue and Depth (RGB-D) sensor, the Microsoft Kinect, mounted
on top of a mobile robotic platform running Robot Operating System
(ROS). The system projects depth measures in a plane for mapping
purposes, using a grid-based Simultaneous Localization and Mapping
(SLAM) approach, and pre-processes the sensor's point cloud to lower the
computational load of people detection, which is performed using a
classical technique based on Histogram of Oriented Gradients (HOG)
features, and a linear Support Vector Machine (SVM) classifier. Results
show the effectiveness of the approach and the potential to use the
Kinect in real world scenarios.},
keywords={Cameras;Feature extraction;Mobile robots;Simultaneous
localization and mapping;Three-dimensional displays;Mapping;Mobile
Robot;People Detection;RGB-D Sensor;ROS},
doi={10.5220/0005060604670474},
month={Sept},}
@INPROCEEDINGS{7045722,
author={S. R. Rupanagudi and Ranjani B. S. and P. Nagaraj and V. G. Bhat
and Thippeswamy G},
booktitle={2015 International Conference on Communication, Information
Computing Technology (ICCICT)},
title={A novel cloud computing based smart farming system for early
detection of borer insects in tomatoes},
year={2015},
pages={1-6},
abstract={Every year farmers experience huge losses due to pest
infestation in crops & this inturn impacts his livelihood. In this paper
we discuss a novel approach to solve this problem by constantly
monitoring crops using video processing, cloud computing and robotics.
The paper concentrates in methodologies to detect pests in one of the
most popular fruits in the world - the tomato. An insight into how the
idea of the Internet of Things can also be conceptualized in this
project has been elaborated.},
keywords={Internet of Things;cloud computing;computerised
monitoring;farming;video signal processing;Internet of Things;cloud
computing based smart farming system;crop monitoring;farmer
experience;pest infestation;tomato borer insect early detection;video
processing;Agriculture;Algorithm design and analysis;Cameras;Cloud
computing;Image color analysis;Insects;Robots;Internet of Things
(IOT);borer insect;cloud computing;early pest detection;fruit
detection;fruit plucking;fruit worm;growth stages;smart farming;tomato
detection;video processing},
doi={10.1109/ICCICT.2015.7045722},
month={Jan},}
@INPROCEEDINGS{7044895,
author={T. Cádrik and M. Mach and P. Sinčák},
booktitle={2014 Joint 7th International Conference on Soft Computing and
Intelligent Systems (SCIS) and 15th International Symposium on Advanced
Intelligent Systems (ISIS)},
title={Interference of waves based usage of an optimization algorithm
for finding rules in an agent system},
year={2014},
pages={1068-1072},
abstract={There are many controllers using algorithms based on neural
networks, evolutionary classifier systems and others, which are saving
the discovered data in the form of a model characteristic to this
algorithm. If we want to use it in the agent domain, we need to have
this method in the agent to interpret the data. With the extension of
using cloud with a combination of robotics, it is not very effective to
use this approach. So we need to build a method which can extract the
data in the form of rules from the algorithms and then we can use only
the extracted rules. This paper introduces a method which extracts rules
from the interaction of the learned model of Zeroth Level Classifier
System with the environment. The new method uses an interference of
waves inspired approach based on a simple evolutionary algorithm.},
keywords={evolutionary computation;learning (artificial
intelligence);multi-agent systems;optimisation;pattern
classification;Zeroth Level Classifier System;agent system;data
extraction;evolutionary classifier systems;learned model;model
characteristic;neural networks;optimization algorithm;rule finding;wave
based usage interference;Classification algorithms;Evolutionary
computation;Interference;Robots;Sociology;Statistics;Zero current
switching;Animai problem;Evolutionary algorithm;Inference of waves;Rule
extractor;ZCS},
doi={10.1109/SCIS-ISIS.2014.7044895},
month={Dec},}
@INPROCEEDINGS{7041374,
author={A. Stumpf and S. Kohlbrecher and D. C. Conner and O. von Stryk},
booktitle={2014 IEEE-RAS International Conference on Humanoid Robots},
title={Supervised footstep planning for humanoid robots in rough terrain
tasks using a black box walking controller},
year={2014},
pages={287-294},
abstract={In recent years, the numbers of life-size humanoids as well as
their mobility capabilities have steadily grown. Stable walking motion
and control for humanoid robots are already well investigated research
topics. This raises the question how navigation problems in complex and
unstructured environments can be solved utilizing a given black box
walking controller with proper perception and modeling of the
environment provided. In this paper we present a complete system for
supervised footstep planning including perception, world modeling, 3D
planner and operator interface to enable a humanoid robot to perform
sequences of steps to traverse uneven terrain. A proper height map and
surface normal estimation are directly obtained from point cloud data. A
search-based planning approach (ARA*) is extended to sequences of
footsteps in full 3D space (6 DoF). The planner utilizes a black box
walking controller without knowledge of its implementation details.
Results are presented for an Atlas humanoid robot during participation
of Team ViGIR in the 2013 DARPA Robotics Challenge Trials.},
keywords={gait analysis;humanoid robots;mobile robots;motion
control;path planning;planning (artificial intelligence);2013 DARPA
Robotics Challenge Trials;3D space;6 DoF;ARA;Atlas humanoid robot;Team
ViGIR;black box walking controller;environment modeling;environment
perception;height map;life-size humanoid robot;mobility
capabilities;navigation problems;point cloud data;rough terrain
tasks;search-based planning approach;stable walking motion;supervised
footstep planning;surface normal estimation;Cost function;Foot;Humanoid
robots;Legged locomotion;Planning;Three-dimensional displays},
doi={10.1109/HUMANOIDS.2014.7041374},
ISSN={2164-0572},
month={Nov},}
@INPROCEEDINGS{7041355,
author={J. G. Victores and S. Morante and A. Jardón and C. Balaguer},
booktitle={2014 IEEE-RAS International Conference on Humanoid Robots},
title={On using humanoid robot imagination to perform the Shortened
Token Test},
year={2014},
pages={172-172},
abstract={Summary form only given. Mental models allow us to imagine how
something may be, only by its description. This video presents a
use-case demonstration of a high-dimensional geometrical system for
acquiring mental models for robots, called Robot Imagination System
(RIS). RIS generates models of objects based on their descriptive words,
even prior to their perception. This is achieved by using an inference
algorithm that computes the fusion of features corresponding to
descriptive words, allowing to imagine an object whose description has
never been presented before. As shown in the video, there is a previous
training process where visual data is combined with semantic
information. Each keyword creates an n-dimensional instance of the
object in the feature space. Feature inference is treated as the
intersection of hyperplanes generated from the keywords in the feature
space. These hyperplanes extend the meaning of keywords. With a previous
basic algorithm, we explored the basis for robotic imagination through
mental models. Now, the extended algorithm allows context detection by
introducing a previous clustering step combined with cluster bigram
co-occurrences, and an analysis of keyword point cloud principal
components. In the video, a experiment is performed using the Shortened
Token Test.},
keywords={control engineering computing;humanoid robots;inference
mechanisms;pattern clustering;principal component analysis;sensor
fusion;RIS;cluster bigram co-occurrence;clustering step;descriptive
words;feature fusion;feature inference;high-dimensional geometrical
system;humanoid robot imagination;hyperplane generation;inference
algorithm;keyword point cloud principal component analysis;mental
models;robot imagination system;semantic information;shortened token
test;visual data;Algorithm design and analysis;Clustering
algorithms;Cognitive science;Computational modeling;Humanoid
robots;Inference algorithms},
doi={10.1109/HUMANOIDS.2014.7041355},
ISSN={2164-0572},
month={Nov},}
@INPROCEEDINGS{7033707,
author={F. Lemic and J. Büsch and M. Chwalisz and V. Handziski and A.
Wolisz},
booktitle={2014 Ubiquitous Positioning Indoor Navigation and Location
Based Service (UPINLBS)},
title={Infrastructure for benchmarking RF-based indoor localization
under controlled interference},
year={2014},
pages={26-35},
abstract={The proliferation of RF-based indoor localization solutions
raises the need for testing systems that enable objective evaluation of
their functional and non functional properties. We introduce a testbed
and cloud infrastructure for supporting automatized benchmarking of
RF-based indoor localization solutions under controlled interference.
For evaluating the impact of RF interference on the performance of
benchmarked solution, the infrastructure leverages various interference
generation and monitoring devices. The infrastructure obtains location
estimates from the System Under Test (SUT) using a well defined
interface, and the estimates are subsequently processed in a dedicated
metrics computation engine and stored in the dedicated engine for
storing the results of benchmarking experiments. The infrastructure
further includes a robotic mobility platform which serves as a reference
localization system and can transport the localized device of the
evaluated indoor localization solution in an autonomous and repeatable
manner. We present the accuracy of our autonomous mobility platform in
two different setups, showing that, due to the high accuracy, the
location estimation provided by the platform can be considered as the
reference localization system for benchmarking of RF-based indoor
localization solutions. The results, as well as the raw data from the
benchmarking experiments, can be stored into the dedicated publicly
available services which gives the opportunity of reusing the same data
for benchmarking different solutions. Finally, we present the
capabilities of the testbed and cloud infrastructure on the use-case of
benchmarking of an example WiFi fingerprinting-based indoor localization
solution in four different interference scenarios.},
keywords={indoor communication;mobile robots;mobility management (mobile
radio);radiofrequency interference;wireless LAN;RF interference;RF-based
indoor localization;WiFi fingerprinting;autonomous mobility
platform;benchmarked solution;cloud infrastructure;controlled
interference;dedicated metrics computation engine;interference
generation;location estimation;monitoring devices;robotic mobility
platform;system under test;testbed infrastructure;Accuracy;Benchmark
testing;Electromagnetic interference;Measurement;Monitoring;Robots;Radio
Frequency;benchmarking;cloud
services;evaluation;experimentation;generation;indoor
localization;interference;mobility;monitoring;testbed infrastructure},
doi={10.1109/UPINLBS.2014.7033707},
month={Nov},}
@INPROCEEDINGS{7033055,
author={B. Jost and M. Ketterl and R. Budde and T. Leimbach},
booktitle={2014 IEEE International Symposium on Multimedia},
title={Graphical Programming Environments for Educational Robots: Open
Roberta - Yet Another One?},
year={2014},
pages={381-386},
abstract={In recent years, an increasing number of school children is
beginning to learn about robotics in the classroom in order to stir
their interest in STEM professions. Teachers rely on simple educational
robots and intuitive programming environments and graphical programming
environments have become a frequent starting point for young robotics
new bies. However, currently available tools do often not sufficiently
support teachers and students in the classroom. In this study, we
evaluate programming environments for educational robots, our results
point to the need of lowering the complexity of tools as well as of
incorporating combinations of web and cloud technologies, embedded
systems and communication concepts into these environments. The
technical part of this work presents Open Roberta - an open source based
addition to commercial educational robot environments that addresses
these needs.},
keywords={cloud computing;computer aided instruction;computer science
education;control engineering computing;control engineering
education;educational robots;embedded systems;Open Roberta;STEM
professions;Web technologies;cloud technologies;commercial educational
robot environments;communication concepts;educational robots;embedded
systems;graphical programming environments;intuitive programming
environments;school children;young robotics newbies;Educational
institutions;Educational robots;Programming environments;Programming
profession;Software;cloud computing;e-learning;education;embedded
systems;graphical programming;roberta;robotic;visual programming;web
2.0;web technology},
doi={10.1109/ISM.2014.24},
month={Dec},}
@INPROCEEDINGS{7024272,
author={R. L. Klaser and F. S. Osório and D. Wolf},
booktitle={2014 Joint Conference on Robotics: SBR-LARS Robotics
Symposium and Robocontrol},
title={Vision-Based Autonomous Navigation with a Probabilistic Occupancy
Map on Unstructured Scenarios},
year={2014},
pages={146-150},
abstract={Vision-based robotics perception still have a great focus of
attention on building systems because of its common availability and low
cost. The 3D data produced by the disparity calculation methods in
stereo cameras are inaccurate and presents substantial noise. We present
here our method to deal with the noisy 3D point cloud produced by stereo
camera to build a navigation map and mark obstacles with a probabilistic
occupancy map approach. The objective is to integrate continuously the
sensor readings marking occupied and free space based on some certainty
and accumulate it over time. The output is a navigability map we use to
plan a trajectory path. Our main focus is applications like agricultural
fields. We have modeled and tested the system fully in simulation and
validated it with our real vehicle platform Carina I on unstructured
scenarios.},
keywords={SLAM (robots);cameras;collision avoidance;mobile
robots;navigation;probability;robot vision;stereo image
processing;trajectory control;3D data;Carina I;agricultural
field;continuous sensor reading integration;disparity calculation
method;free space;navigability map;navigation map building;noisy 3D
point cloud;obstacle marking;occupied space;probabilistic occupancy map
approach;stereo cameras;trajectory path planning;unstructured
scenario;vision-based autonomous navigation;vision-based robotics
perception;Cameras;Navigation;Probabilistic logic;Robot sensing
systems;Three-dimensional displays;Vehicles;autonomous
vehicle;octomap;stereo vision},
doi={10.1109/SBR.LARS.Robocontrol.2014.13},
month={Oct},}
@INPROCEEDINGS{7024274,
author={V. M. Utino and D. F. Wolf and F. S. Osório},
booktitle={2014 Joint Conference on Robotics: SBR-LARS Robotics
Symposium and Robocontrol},
title={Data Fusion Obtained from Multiple Images Aiming the Navigation
of Autonomous Intelligent Vehicles in Agricultural Environment},
year={2014},
pages={157-162},
abstract={Visual navigation is an important research field in robotics
due to low cost of cameras and the good results that these systems
usually achieve. This paper presents monocular and stereo vision-based
detection methods. The obstacles are detected and fused through the
Dempster-Shafer theory for generating a cloud of points that contains
the probability of the existence of obstacles in the environment and its
distance from the autonomous vehicle. The experiments were performed in
a real rural environment to evaluate and validate the approach. The
proposed system has shown to be a promising approach for obstacle
detection aimed at navigating an autonomous vehicle in rural and
agricultural environments.},
keywords={agricultural machinery;collision avoidance;inference
mechanisms;mobile robots;navigation;robot vision;sensor fusion;stereo
image processing;uncertainty handling;Dempster-Shafer
theory;agricultural environment;autonomous intelligent vehicle
navigation;autonomous vehicle;data fusion;monocular vision-based
detection methods;obstacle detection;robotics;rural environment;stereo
vision-based detection methods;visual navigation;Cameras;Data
integration;Image color
analysis;Navigation;Robots;Vehicles;Visualization;Agricultural
environment;Data fusion;Monocular vision;Obstacle detection;Stereo vision},
doi={10.1109/SBR.LARS.Robocontrol.2014.21},
month={Oct},}
@ARTICLE{6994118,
author={C. Beasley and E. Parsons and M. Pierce and S. Baumgarten and A.
Neviera and S. Banerjee and T. Snow and C. Diana and N. Nakadate and S.
Satsangi},
journal={Computer},
title={Passionate Projects: This Is What I Made},
year={2014},
volume={47},
number={12},
pages={42-54},
abstract={Passion for their work--whether in fashion, printing for the
blind, water conservation, or Lego robotics--drives makers to render
their individual visions in tangible forms that can benefit others. In
"Slow Fashion," by Crystal Beasley, "Emmie's Lunchbox," by Emmeline
Parsons, "Filmmaking's Transformation in the 21st Century," by Melissa
Pierce, "Dismissrr: A Line-Manage Service for Schools," by Sam
Baumgarten, "Helping Startup Communities around the World: World Startup
Report," by Andrius Neviera, "Braigo: Building for Social Good," by
Shubham Banerjee, "WÜF: A Wearable Device for Dogs," by Tyesha Snow,
"Experiments in 3D-Printed Design and Distribution," by Carla Diana,
"Reusing Gray Water: Becoming a Maker through Open Source Hardware," by
Shruti Satsangi, and "Builtbot," by Nicholas Nakadate, 10 makers share
what they're making. The first Web extra at http://youtu.be/8mQkO1DXtuw
is an audio recording of guest editor Brian David Johnson interviewing
Crystal Beasley about how the Maker Movement is changing the way fashion
is made with QCut offering womens jeans in 400 sizes. The second Web
extra at http://youtu.be/DefazBEQOXA is an audio recording of guest
editor Brian David Johnson interviewing author Sam Baumgarten, a high
school student from Palo Alto, Calif., who has founded multiple
companies of his own, including Dismissrr, an iOS device- and
cloud-based system that helps schools manage how students are dismissed
for pickup after school, based on when their rides arrive. The third Web
extra at http://youtu.be/YzTuglXVbMc is an audio recording of guest
editor Brian David Johnson interviewing Tyesha Snow about WÜF, a
wearable device for dogs. The fourth Web extra at
http://youtu.be/GQ6ooHBb7fk is an audio recording of guest editor Brian
David Johnson interviewing author Carla Diana about making, creativity,
art, and her experiments in 3D-printed design and distribution. The
fifth Web extra at http://youtu.be/Cwikv247mfs is an audio recordi- g of
guest editor Brian David Johnson interviewing author Nicholas Nakadate
about harnessing kids' imagination and creativity to build robots and
learn about engineering by using 3D printing.},
keywords={Internet;cloud computing;cognition;educational
institutions;entertainment;humanities;operating systems
(computers);public domain software;robots;three-dimensional
printing;water conservation;wearable computers;3D-printed
design;3D-printed distribution;Braigo;Calif;Dismissrr;Emmie
lunchbox;Lego robotics;Maker Movement;Palo Alto;QCut;WÜF;Web extra;audio
recording;cloud-based system;filmmaking transformation;gray water
reuse;high school student;iOS device-based system;kid creativity;kid
imagination;line-manage school service;open source hardware;passionate
projects;slow-fashion;social good;startup communities;water
conservation;wearable device-for-dogs;women jeans sizes;work
passion;world startup report;Forecasting;Motion pictures;Product
development;Production facilities;Robotics;Technological
innovation;Technology forecasting;Maker Movement;history of
computing;makerspaces},
doi={10.1109/MC.2014.364},
ISSN={0018-9162},
month={Dec},}
@INPROCEEDINGS{6991019,
author={A. N. Ravari and H. D. Taghirad},
booktitle={2014 Second RSI/ISM International Conference on Robotics and
Mechatronics (ICRoM)},
title={Transformation invariant 3D object recognition based on
information complexity},
year={2014},
pages={902-907},
abstract={The 3D representation of objects and scenes as a point cloud
or range image has been made simple by means of sensors such as
Microsoft Kinect, stereo camera or laser scanner. Various tasks, such as
recognition, modeling and classification can not be performed on raw
measurements because of the curse of high dimensionality, computational
and algorithm complexity. Non Uniform Rational Basis Splines (NURBS) are
a widely used representation technique for 3D objects in various
robotics and Computer Aided Design (CAD) applications. In this paper, a
similarity measurement from information theory is employed in order to
recognize an object sample from a set of objects. From a NURBS model
fitted to the observed point cloud, a complexity based representation is
derived which is transformation invariant in the sense of Kolmogorov
complexity. Experimental results on a set of 3D objects grabbed by a
Kinect sensor indicates the applicability of the proposed method for
object recognition tasks. Furthermore, the results of the proposed
method is compared to that of some state of the art algorithms.},
keywords={computational complexity;image representation;information
theory;object recognition;splines (mathematics);3D object
representation;3D scene representation;Kinect sensor;Kolmogorov
complexity;Microsoft Kinect;NURBS;algorithm complexity;computational
complexity;information complexity;information theory;laser
scanner;nonuniform rational basis splines;stereo camera;transformation
invariant 3D object recognition;Complexity theory;Computational
modeling;Object recognition;Splines (mathematics);Surface
reconstruction;Surface topography;Three-dimensional displays},
doi={10.1109/ICRoM.2014.6991019},
month={Oct},}
@INPROCEEDINGS{6985354,
author={S. Satorres Martínez and J. de la Casa Cárdenas and J. Gámez
García and J. Gómez Ortega},
booktitle={IEEE SENSORS 2014 Proceedings},
title={Position predictive control of an anthropomorphic robotic arm
using a time-of-flight camera},
year={2014},
pages={1718-1721},
abstract={This paper introduces a predictive control strategy for visual
servoing an anthropomorphic robotic arm in an object tracking task.
Robot workspace constraints and the target location are addressed
through a time-of-flight camera. We present a methodology to process the
vision information to explicitly identify and remove obstacles and
objects from the point cloud. Later, this information is used by a
predictive-control strategy, achieving convergence during the robotic
arm motion and avoiding obstacles that are included in the cost function
as constraints. The proposal validation was successfully tested on a
real robotic set-up.},
keywords={cameras;collision avoidance;dexterous manipulators;motion
control;object tracking;predictive control;robot vision;visual
servoing;anthropomorphic robotic arm;object tracking task;obstacle
avoidance;obstacle identification;point cloud;position predictive
control strategy;robot workspace constraints;target
location;time-of-flight camera;vision information processing;visual
servoing;Cameras;Predictive control;Robot kinematics;Robot vision
systems;Shape;Three-dimensional displays},
doi={10.1109/ICSENS.2014.6985354},
ISSN={1930-0395},
month={Nov},}
@INPROCEEDINGS{6977442,
author={F. Husain and L. Dellen and C. Torras},
booktitle={2014 22nd International Conference on Pattern Recognition},
title={Recognizing Point Clouds Using Conditional Random Fields},
year={2014},
pages={4257-4262},
abstract={Detecting objects in cluttered scenes is a necessary step for
many robotic tasks and facilitates the interaction of the robot with its
environment. Because of the availability of efficient 3D sensing devices
as the Kinect, methods for the recognition of objects in 3D point clouds
have gained importance during the last years. In this paper, we propose
a new supervised learning approach for the recognition of objects from
3D point clouds using Conditional Random Fields, a type of
discriminative, undirected probabilistic graphical model. The various
features and contextual relations of the objects are described by the
potential functions in the graph. Our method allows for learning and
inference from unorganized point clouds of arbitrary sizes and shows
significant benefit in terms of computational speed during prediction
when compared to a state-of-the-art approach based on constrained
optimization.},
keywords={interactive devices;learning (artificial intelligence);mobile
robots;object detection;object
recognition;optimisation;probability;robot vision;3D sensing
devices;Kinect;conditional random fields;constrained optimization;object
detection;object recognition;point cloud recognition;probabilistic
graphical model;robotic tasks;supervised learning;Graphical models;Laser
radar;Mathematical model;Object recognition;Three-dimensional
displays;Training;Vectors},
doi={10.1109/ICPR.2014.730},
ISSN={1051-4651},
month={Aug},}
@INPROCEEDINGS{6973743,
author={J. Inman and G. Grider and H. B. Chen},
booktitle={2014 IEEE 7th International Conference on Cloud Computing},
title={Cost of Tape versus Disk for Archival Storage},
year={2014},
pages={208-215},
abstract={For archiving large datasets in high-performance computing
facilities, tape technology has a long history of providing inexpensive
capacity. However, as the memory-size of supercomputers continues to
grow geometrically, the cost of tape bandwidth is becoming more
important. The projected costs for tape-drives, robotics, and
maintenance, are creating challenges for tape-based archives. The advent
of erasure-coded object storage, driven by the "cloud storage" industry,
might make it practical to implement archives using disks, or hybrid
disk-and-tape systems. We used linear optimization techniques to
investigate when and how this transition might best be made, taking into
consideration our significant investment in tape technology. Our models
introduce a technique to systematically relax constraints on the
relationship between tape-capacity and tape-bandwidth, which governs a
trade-off between cost and performance. We ran parameter studies that
support some preliminary conclusions about paths forward for archive
infrastructure at LANL.},
keywords={disc storage;information retrieval
systems;optimisation;LANL;archival storage;archive
infrastructure;disk;linear optimization techniques;tape
technology;tape-bandwidth;tape-capacity;Bandwidth;Computational
modeling;Equations;Maintenance engineering;Mathematical
model;Media;Robots;archive;cloud;cost;disk;erasure-code;linear-optimization;tape},

doi={10.1109/CLOUD.2014.37},
ISSN={2159-6182},
month={June},}
@INPROCEEDINGS{6974309,
author={A. Noyvirt and R. Setchi and B. Albert},
booktitle={2014 IEEE International Conference on Systems, Man, and
Cybernetics (SMC)},
title={GLAICP: A global-local optimization algorithm for robust human
pose tracking from depth data},
year={2014},
pages={2541-2546},
abstract={Due to its high efficiency, the Iterative Closest Point (ICP)
algorithm has become a popular choice in computer vision and robotics
for the registration of point cloud data sets when the point-to-point
correspondences are unknown. Its generalization for articulated
structures, although possible through a joint optimization of all pose
parameters, is challenging as it is necessary to solve a non-closed
form. It also suffers heavily from the local minima problem. A number of
proposed Articulated ICP (AICP) algorithms circumvent the problem of the
non-closed form solution and offer an efficient alternative. However,
they still exhibit an increased tendency, caused by the local minima, to
converge to an incorrect pose. Typically, the above problem manifests
itself after a transient disturbance in the convergence, such as an
occlusion which causes an increase in the point-to-point association
distances between the model and the data. In this paper, we propose an
extension to the AICP algorithm that benefits from the efficiency of ICP
as well as avoids its problems by using global pose optimization
elements to guide the convergence process to the correct pose. The
proposed approach is to merge adaptively the joint adjustments computed
by AICP with the adjustments needed for a number of key points to reach
their respective target positions, identified by a local feature
descriptor search. Experiments show that the proposed Global-Local
Articulated ICP algorithm exhibits improved robustness to transient
disturbances, like occlusions, in comparison with the AICP algorithm.},
keywords={computer vision;iterative methods;minimisation;object
tracking;pose estimation;robots;GLAICP;computer vision;convergence
process;depth data;global pose optimization element;global-local
articulated ICP algorithm;global-local optimization algorithm;iterative
closest point algorithm;local feature descriptor search;occlusion;point
cloud data sets;point-to-point association distance;robotics;robust
human pose tracking;Biological system modeling;Convergence;Iterative
closest point algorithm;Joints;Kinematics;Optimization;Three-dimensional
displays;Articulated ICP;Human pose tracking;ICP;RBG_D;global pose
optimization;point cloud data;service robotics},
doi={10.1109/SMC.2014.6974309},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{6964471,
author={C. L. Tsui and D. Schipf and K. R. Lin and J. Leang and F. J.
Hsieh and W. C. Wang},
booktitle={OCEANS 2014 - TAIPEI},
title={Using a Time of Flight method for underwater 3-dimensional depth
measurements and point cloud imaging},
year={2014},
pages={1-6},
abstract={Underwater depth measurements and point cloud images can be
used for robotic navigation and haptic feedback. Structured Light and
Time of Flight (ToF) depth camera systems were tested to demonstrate
depth measurements and point cloud imaging underwater. A commercial ToF
depth camera was modified to include a movable external light source.
Images from depth measurements and constructed point cloud images from
underwater tests are shown. A comparison of depth images captured while
objects were positioned ~0.10-1.80 m from the camera is presented.
Calibration of ToF cameras augmented with movable external light sources
for underwater use is discussed.},
keywords={bathymetry;calibration;cameras;image processing;oceanographic
techniques;underwater equipment;calibration;commercial ToF depth
camera;haptic feedback;movable external light source;point cloud
imaging;robotic navigation;time of flight method;underwater 3D depth
measurements;Cameras;Containers;Light sources;Metals;Three-dimensional
displays;Water resources;Depth Camera;Haptics;Kinect;Point
Cloud;Time-of-Flight;Underwater Optics},
doi={10.1109/OCEANS-TAIPEI.2014.6964471},
month={April},}
@INPROCEEDINGS{6949960,
author={Geetha M and Rakendu R},
booktitle={2014 International Conference on Communication and Signal
Processing},
title={An improved method for segmentation of point cloud using Minimum
Spanning Tree},
year={2014},
pages={833-837},
abstract={With the development of low-cost 3D sensing hardware such as
the Kinect, three dimensional digital images have become popular in
medical diagnosis, robotics etc. One of the difficult task in image
processing is image segmentation. The problem become simpler if we add
the depth channel along with height and width. The proposed algorithm
uses Minimum Spanning Tree (MST) for the segmentation of point cloud. As
a pre processing step, first level clustering is done which gives group
of cluttered objects. Each of this cluttered group is subjected to more
finite level of segmentation using MST based on distance and normal. In
our method, we build a weighted planar graph of each of the clustered
cloud and construct the MST of the corresponding graph. By taking the
advantage of normal, we can separate the surface from the object. The
proposed method is applied to different 3D scenes and the results are
discussed.},
keywords={image segmentation;natural scenes;pattern clustering;trees
(mathematics);3D scene;3D sensing hardware;MST;cluttered object
group;depth channel;image processing;image segmentation;level
clustering;minimum spanning tree;point cloud segmentation;weighted
planar graph;Floors;Image segmentation;Sensors;Minimum Spanning
Tree;Normal;Principal Curvature;RGBD Images;Segmentation},
doi={10.1109/ICCSP.2014.6949960},
month={April},}
@INPROCEEDINGS{6943050,
author={L. Zhang and S. L. Lee and G. Z. Yang and G. P. Mylonas},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Semi-autonomous navigation for robot assisted tele-echography
using generalized shape models and co-registered RGB-D cameras},
year={2014},
pages={3496-3502},
abstract={This paper proposes a semi-autonomous navigated master-slave
system, for robot assisted remote echography for early trauma
assessment. Two RGB-D sensors are used to capture real-time 3D
information of the scene at the slave side where the patient is located.
A 3D statistical shape model is built and used to generate a customized
patient model based on the point cloud generated by the RGB-D sensors.
The customized patient model can be updated and adaptively fitted to the
patient. The model is also used to generate a trajectory to navigate a
KUKA robotic arm and safely conduct the ultrasound examination.
Extensive validation of the proposed system shows promising results in
terms of accuracy and robustness.},
keywords={biomedical ultrasonics;image sensors;manipulators;medical
robotics;3D statistical shape model;KUKA robotic arm;coregistered RGB-D
cameras;customized patient model;early trauma assessment;generalized
shape models;master-slave system;point cloud;real-time 3D
information;robot assisted remote echography;robot assisted
tele-echography;semiautonomous navigation;ultrasound examination;Robot
kinematics;Robot sensing systems;Shape;Three-dimensional displays;Torso},
doi={10.1109/IROS.2014.6943050},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6943088,
author={W. J. Beksi and N. Papanikolopoulos},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Point cloud culling for robot vision tasks under communication
constraints},
year={2014},
pages={3747-3752},
abstract={In this paper, we present two real-time methods for
controlling data transmission in a robotic network that utilizes a
remote computing infrastructure. The proposed algorithms use information
and communication theory concepts to perform a highly efficient transfer
of RGB-D data from a client (robot) to a server (cloud). We show that
this approach makes it possible to conserve bandwidth and reduce network
latency while allowing a mobile robot to perform vision tasks.},
keywords={data communication;mobile robots;robot vision;RGB-D
data;communication constraints;data transmission control;information and
communication theory concepts;mobile robot vision tasks;network latency
reduction;point cloud culling;remote computing infrastructure;robotic
network;Entropy;Robot sensing systems;Servers;Three-dimensional
displays;Water heating},
doi={10.1109/IROS.2014.6943088},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6931476,
author={Y. Wang and Q. Zhang and Y. Zhou},
booktitle={2014 9th IEEE Conference on Industrial Electronics and
Applications},
title={RGB-D mapping for indoor environment},
year={2014},
pages={1888-1892},
abstract={RGB-D sensors provide RGB images along with pre-pixel depth
information, the richness of their data and recent development of
low-cost sensors have made them more popular in mobile robotics
research. In this paper, we introduce a framework for real-time mapping
in indoor environment by using a RGB-D sensor and present RGB-D mapping,
a 3D mapping system that utilizes 3D point clouds available for RGB-D
cameras combining local position of the robot computed by a visual
odometry. Thereinto, SURF features have been extracted and matched to
estimate the poses of robot combining with a nonlinear least-squares
solver. A sliding window Sparse Bundle Adjustment (SBA) has been used to
refine both the robot poses and landmarks, then 3D point clouds were
projected into global map with a 3D pose of the robot. At last,
Experimental results have validated the feasibility and effectiveness of
this system.},
keywords={computer graphics;feature extraction;image colour
analysis;least squares approximations;mobile robots;pose
estimation;real-time systems;3D mapping system;3D point clouds;RGB
images;RGB-D cameras;RGB-D mapping;RGB-D sensors;SBA;SURF feature
extraction;global map;indoor environment;low-cost sensors;mobile
robotics research;nonlinear least-squares solver;pose
estimation;pre-pixel depth information;real-time mapping;sliding window
sparse bundle adjustment;visual odometry;Cameras;Feature
extraction;Real-time systems;Robots;Sensors;Three-dimensional
displays;Visualization;3D point clouds;RGB-D mapping;SBA;SURF;visual
odometry},
doi={10.1109/ICIEA.2014.6931476},
ISSN={2156-2318},
month={June},}
@INPROCEEDINGS{6920723,
author={D. von Söhsten and S. Murilo},
booktitle={2013 13th International Conference on Intellient Systems
Design and Applications},
title={Multiple face recognition in real-time using cloud computing,
Emgu CV and Windows Azure},
year={2013},
pages={137-140},
abstract={Multiple face recognition has several applications, such as in
the areas of security and robotics. Recognition and classification
techniques have been developed in recent years, through different
programming languages and approaches. However, the level of detailing
often requires a high processing power. This article proposes the use of
cloud computing - more specifically, Windows Azure platform - to
identify possible performance gains while testing EmguCV framework.},
keywords={C++ language;cloud computing;face recognition;programming
languages;visual languages;Emgu CV;Windows Azure;cloud
computing;multiple face recognition;programming
languages;Cameras;Cryptography;Databases;Face;Face recognition;Image
recognition;Servers;cloud computing;eigenfaces;emgu cv;face
recognition;windows azure},
doi={10.1109/ISDA.2013.6920723},
ISSN={2164-7143},
month={Dec},}
@INPROCEEDINGS{6916180,
author={G. Kurz and U. D. Hanebeck},
booktitle={17th International Conference on Information Fusion (FUSION)},
title={2D and 3D image stabilization for robotic beating heart surgery},
year={2014},
pages={1-7},
abstract={Image stabilization is relevant for various industrial and
medical applications. In particular, we consider the use of image
stabilization in robotic beating heart surgery. A robot, which is
remotely controlled by the surgeon, can automatically compensate for the
motion of the beating heart. To give the surgeon the illusion of
operating on a stationary heart, a stabilized image of the beating heart
is shown to the surgeon. Image stabilization cancels the unwanted motion
of the heart, but retains changes to color and texture, for example cuts
on the heart surface. In this paper, stabilization is first considered
as a 2D image transformation problem. Subsequently, it is extended to
stabilization of a 3D point cloud or surface. The proposed algorithms
are evaluated in both ex-vivo and in-vivo experiments. In the
evaluation, the stabilization quality achievable with several common
interpolation functions is compared.},
keywords={cardiology;image colour analysis;image
texture;interpolation;medical image processing;medical robotics;motion
estimation;surgery;2D image stabilization;2D image transformation
problem;3D image stabilization;3D point cloud;3D surface;heart
surface;image color;image texture;interpolation functions;motion
compensation;robotic beating heart surgery;stabilization
quality;Cameras;Heart;Interpolation;Splines (mathematics);Surface
reconstruction;Surgery;Three-dimensional displays;coronary artery bypass
graft;interpolation;motion cancellation;point cloud;surface
reconstruction},
month={July},}
@INPROCEEDINGS{6907520,
author={D. Kent and M. Behrooz and S. Chernova},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Crowdsourcing the construction of a 3D object recognition
database for robotic grasping},
year={2014},
pages={4526-4531},
abstract={Object recognition and manipulation are critical in enabling
robots to interact with objects in a household environment. Construction
of 3D object recognition databases is time and resource intensive, often
requiring specialized equipment, and is therefore difficult to apply to
robots in the field. We present a system for constructing object models
for 3D object recognition and manipulation made possible by advances in
web robotics. The database consists of point clouds generated using a
novel iterative point cloud registration algorithm, which includes the
potential to encode manipulation data and usability characteristics. We
validate the system with a crowdsourcing user study and object
recognition system designed to work with our object recognition database.},
keywords={iterative methods;manipulators;object recognition;robot
vision;3D object recognition database;construction
crowdsourcing;crowdsourcing user study;household environment;iterative
point cloud registration algorithm;manipulation data;robotic
grasping;usability characteristics;web
robotics;Crowdsourcing;Databases;Image color analysis;Object
recognition;Robots;Solid modeling;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907520},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907312,
author={K. Li and M. Meng},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Robotic object manipulation with multilevel part-based model in
RGB-D data},
year={2014},
pages={3151-3156},
abstract={The performance of robotic object manipulation relies heavily
on the selection of object model. In this article, we develop a
multilevel part-based object model by applying latent support vector
machine to training a hierarchical object structure. We implement our
method with a robot arm and a depth sensor in Robot Operating System,
and then we compare the recognition performance of this model with
established methods on a point cloud data set and show the manipulation
performance of our model on three practical tasks. The result
demonstrates that our robot recognizes and manipulates objects more
accurately with this multilevel part-based object model.},
keywords={manipulators;object detection;robot vision;support vector
machines;RGB-D data;depth sensor;hierarchical object structure;latent
support vector machine;multilevel part-based object model;point cloud
data set;robot arm;robot operating system;robotic object
manipulation;Data models;Feature extraction;Robot kinematics;Robot
sensing systems;Support vector machines;Vectors},
doi={10.1109/ICRA.2014.6907312},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907140,
author={R. Tedrake and M. Fallon and S. Karumanchi and S. Kuindersma and
M. Antone and T. Schneider and T. Howard and M. Walter and H. Dai and R.
Deits and M. Fleder and D. Fourie and R. Hammoud and S. Hemachandra and
P. Ilardi and C. Pérez-D'Arpino and S. Pillai and A. Valenzuela and C.
Cantu and C. Dolan and I. Evans and S. Jorgensen and J. Kristeller and
J. A. Shah and K. Iagnemma and S. Teller},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={A summary of team MIT's approach to the virtual robotics challenge},
year={2014},
pages={2087-2087},
abstract={The paper describes the system developed by researchers from
MIT for the Defense Advanced Research Projects Agency's (DARPA) Virtual
Robotics Challenge (VRC), held in June 2013. The VRC was the first
competition in the DARPA Robotics Challenge (DRC), a program that aims
to “develop ground robotic capabilities to execute complex tasks in
dangerous, degraded, human-engineered environments”. The VRC required
teams to guide a model of Boston Dynamics' humanoid robot, Atlas,
through driving, walking, and manipulation tasks in simulation. Team
MIT's user interface, the Viewer, provided the operator with a unified
representation of all available information. A 3D rendering of the robot
depicted its most recently estimated body state with respect to the
surrounding environment, represented by point clouds and texture-mapped
meshes as sensed by on-board LIDAR and fused over time.},
keywords={control engineering computing;humanoid robots;rendering
(computer graphics);user interfaces;3D rendering;Boston Dynamics
humanoid robot;DARPA VRC;DARPA robotics
challenge;LIDAR;MIT;Massachusettes Institute of Technology;Viewer user
interface;driving task;ground robotic capabilities;light detection and
ranging;manipulation task;point clouds;texture-mapped meshes;virtual
robotics challenge;walking task;Kinematics;Legged
locomotion;Navigation;Planning;Robot sensing systems;Three-dimensional
displays},
doi={10.1109/ICRA.2014.6907140},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6906903,
author={A. Singh and J. Sha and K. S. Narayan and T. Achim and P. Abbeel},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={BigBIRD: A large-scale 3D database of object instances},
year={2014},
pages={509-516},
abstract={The state of the art in computer vision has rapidly advanced
over the past decade largely aided by shared image datasets. However,
most of these datasets tend to consist of assorted collections of images
from the web that do not include 3D information or pose information.
Furthermore, they target the problem of object category recognition -
whereas solving the problem of object instance recognition might be
sufficient for many robotic tasks. To address these issues, we present a
high-quality, large-scale dataset of 3D object instances, with accurate
calibration information for every image. We anticipate that “solving”
this dataset will effectively remove many perception-related problems
for mobile, sensing-based robots. The contributions of this work consist
of: (1) BigBIRD, a dataset of 100 objects (and growing), composed of,
for each object, 600 3D point clouds and 600 high-resolution (12 MP)
images spanning all views, (2) a method for jointly calibrating a
multi-camera system, (3) details of our data collection system, which
collects all required data for a single object in under 6 minutes with
minimal human effort, and (4) multiple software components (made
available in open source), used to automate multi-sensor calibration and
the data collection process. All code and data are available at
http://rll.eecs.berkeley.edu/bigbird.},
keywords={calibration;cameras;control engineering computing;data
handling;mobile robots;object recognition;robot vision;3D
information;BigBIRD;computer vision;data collection system;image
collection;large-scale 3D database;mobile robots;multicamera
system;multisensor calibration;object category recognition;object
instance recognition;perception-related problems;pose
information;robotic tasks;sensing-based robots;shared image
dataset;software components;Calibration;Cameras;Image color
analysis;Robots;Sensor systems;Three-dimensional displays},
doi={10.1109/ICRA.2014.6906903},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907321,
author={S. C. Stein and F. Wörgötter and M. Schoeler and J. Papon and T.
Kulvicius},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Convexity based object partitioning for robot applications},
year={2014},
pages={3213-3220},
abstract={The idea that connected convex surfaces, separated by concave
boundaries, play an important role for the perception of objects and
their decomposition into parts has been discussed for a long time. Based
on this idea, we present a new bottom-up approach for the segmentation
of 3D point clouds into object parts. The algorithm approximates a scene
using an adjacency-graph of spatially connected surface patches. Edges
in the graph are then classified as either convex or concave using a
novel, strictly local criterion. Region growing is employed to identify
locally convex connected subgraphs, which represent the object parts. We
show quantitatively that our algorithm, although conceptually easy to
graph and fast to compute, produces results that are comparable to far
more complex state-of-the-art methods which use classification, learning
and model fitting. This suggests that convexity/concavity is a powerful
feature for object partitioning using 3D data. Furthermore we
demonstrate that for many objects a natural decomposition into “handle
and body” emerges when employing our method. We exploit this property in
a robotic application enabling a robot to automatically grasp objects by
their handles.},
keywords={graph theory;image classification;image segmentation;robot
vision;3D data;3D point cloud segmentation;adjacency-graph;bottom-up
approach;concave boundaries;connected convex surfaces;convexity based
object partitioning;locally convex connected subgraphs;model
fitting;object perception;region growing;robotic application;spatially
connected surface patches;strictly local criterion;Benchmark
testing;Image segmentation;Object segmentation;Partitioning
algorithms;Robots;Shape;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907321},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907584,
author={E. Potapova and K. M. Varadarajan and A. Richtsfeld and M.
Zillich and M. Vincze},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Attention-driven object detection and segmentation of cluttered
table scenes using 2.5D symmetry},
year={2014},
pages={4946-4952},
abstract={The task of searching and grasping objects in cluttered
scenes, typical of robotic applications in domestic environments
requires fast object detection and segmentation. Attentional mechanisms
provide a means to detect and prioritize processing of objects of
interest. In this work, we combine a saliency operator based on symmetry
with a segmentation method based on clustering locally planar surface
patches, both operating on 2.5D point clouds (RGB-D images) as input
data to yield a novel approach to table-top scene segmentation.
Evaluation on indoor table-top scenes containing man-made objects
clustered in piles and dumped in a box show that our approach to
selection of attention points significantly improves performance of
state-of-the-art attention-based segmentation methods.},
keywords={image colour analysis;image segmentation;object
detection;pattern clustering;robot vision;2.5D point clouds;2.5D
symmetry;RGB-D images;attention-driven object detection;attention-driven
object segmentation;attentional mechanisms;cluttered table
scenes;domestic environments;indoor table-top scenes;locally planar
surface patch clustering;man-made object clustering;object
grasping;object searching;robotic applications;saliency
operator;table-top scene segmentation;Databases;Image color
analysis;Image segmentation;Object detection;Object
segmentation;Robots;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907584},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907776,
author={C. Feng and Y. Taguchi and V. R. Kamat},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Fast plane extraction in organized point clouds using
agglomerative hierarchical clustering},
year={2014},
pages={6218-6225},
abstract={Real-time plane extraction in 3D point clouds is crucial to
many robotics applications. We present a novel algorithm for reliably
detecting multiple planes in real time in organized point clouds
obtained from devices such as Kinect sensors. By uniformly dividing such
a point cloud into non-overlapping groups of points in the image space,
we first construct a graph whose node and edge represent a group of
points and their neighborhood respectively. We then perform an
agglomerative hierarchical clustering on this graph to systematically
merge nodes belonging to the same plane until the plane fitting mean
squared error exceeds a threshold. Finally we refine the extracted
planes using pixel-wise region growing. Our experiments demonstrate that
the proposed algorithm can reliably detect all major planes in the scene
at a frame rate of more than 35Hz for 640×480 point clouds, which to the
best of our knowledge is much faster than state-of-the-art algorithms.},
keywords={computer graphics;feature extraction;image sensors;mean square
error methods;object detection;pattern clustering;3D point clouds;Kinect
sensors;agglomerative hierarchical clustering;image space;nonoverlapping
groups;pixel-wise region growing;plane fitting mean squared error;planes
detection;real-time plane extraction;robotics applications;Clustering
algorithms;Image segmentation;Merging;Real-time
systems;Robots;Sensors;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907776},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6878213,
author={A. J. Reina and J. L. Martínez and A. Mandow and J. Morales and
A. García-Cerezo},
booktitle={2014 IEEE/ASME International Conference on Advanced
Intelligent Mechatronics},
title={Collapsible cubes: Removing overhangs from 3D point clouds to
build local navigable elevation maps},
year={2014},
pages={1012-1017},
abstract={Elevation maps offer a compact 2½ dimensional model of terrain
surface for navigation in field mobile robotics. However, building these
maps from 3D raw point clouds containing overhangs, such as tree canopy
or tunnels, can produce useless results. This paper proposes a simple
processing of a ground-based point cloud that identifies and removes
overhang points that do not constitute an obstacle for navigation while
keeping vertical structures such as walls or tree trunks. The procedure
uses efficient data structures to collapse unsupported 3D cubes down to
the ground. This method has been successfully applied to 3D laser scans
taken from a mobile robot in outdoor environments in order to build
local elevation maps for navigation. Computation times show an
improvement with respect to a previous point-based solution to this
problem.},
keywords={cartography;control engineering computing;data
structures;mobile robots;path planning;solid modelling;3D laser scans;3D
point clouds;collapsible cubes;data structures;field mobile
robotics;ground-based point cloud processing;local navigable elevation
maps;mobile robot;navigation;point-based solution;terrain surface
model;vertical structures;Data structures;Indexes;Mobile
robots;Navigation;Three-dimensional displays;Vegetation},
doi={10.1109/AIM.2014.6878213},
ISSN={2159-6247},
month={July},}
@INPROCEEDINGS{6869141,
author={R. Toris and C. Shue and S. Chernova},
booktitle={2014 IEEE International Conference on Technologies for
Practical Robot Applications (TePRA)},
title={Message authentication codes for secure remote non-native client
connections to ROS enabled robots},
year={2014},
pages={1-6},
abstract={Recent work in the robotics community has lead to the
emergence of cloud-based solutions and remote clients. Such work allows
robots to effectively distribute complex computations across multiple
machines, and allows remote clients, both human and automata, to control
robots across the globe. With the increasing use and importance of such
ideas, it is crucial not to overlook the critical issue of security in
these systems. In this work, we discuss the use of web tokens for
achieving secure authentication for remote, non-native clients in the
widely-used Robot Operating System (ROS) middleware. Written in a
system-independent manner, we demonstrate its use with an application
for securing clients within the popular rosbridge protocol.},
keywords={cloud computing;message authentication;middleware;operating
systems (computers);protocols;robot programming;ROS enabled robots;ROS
middleware;cloud-based solution;control robot;message authentication
codes;remote client;robotics community;rosbridge protocol;secure
authentication;secure remote nonnative client
connection;system-independent manner;web tokens;widely-used robot
operating
system;Authentication;Libraries;Protocols;Robots;Servers;Virtual private
networks},
doi={10.1109/TePRA.2014.6869141},
ISSN={2325-0526},
month={April},}
@INPROCEEDINGS{6869159,
author={C. Rasmussen and K. Sohn and K. Yuvraj and P. Oh},
booktitle={2014 IEEE International Conference on Technologies for
Practical Robot Applications (TePRA)},
title={Early phases of humanoid vehicle ingress using depth cameras},
year={2014},
pages={1-6},
abstract={This paper presents work on integrating perception and
motion-planning for a humanoid robot to ingress, or enter, a small
utility vehicle as a precursor to driving, a stage in the recent DARPA
Robotics Challenge (DRC). Using a Hubo 2+ robot platform and a pair of
RGB-D cameras, we describe a set of approaches to and present results on
the first four phases of ingress: (1) visually search for the vehicle's
doorway as a target to walk toward, (2) plan and execute a
collision-free approach to the doorway via walking using visual
odometry, (3) make visually-guided fine positioning adjustments near the
entry door during docking, and (4) step from the ground up to the floor
of the vehicle. All recognition is done on 3-D point clouds derived from
the depth cameras without appearance information. Some further ingress
results from the DRC-Hubo making use of grasping are also shown.},
keywords={cameras;collision avoidance;control engineering
computing;distance measurement;humanoid robots;image colour
analysis;robot vision;solid modelling;vehicles;3D point clouds;DARPA
Robotics Challenge;DRC;Hubo 2+ robot platform;RGB-D
cameras;collision-free approach;depth cameras;docking;driving;entry
door;humanoid robot;humanoid vehicle ingress;motion-planning;small
utility vehicle;vehicle doorway;visual odometry;visually-guided fine
positioning adjustments;walking;Cameras;Legged locomotion;Robot
kinematics;Sensors;Trajectory;Vehicles},
doi={10.1109/TePRA.2014.6869159},
ISSN={2325-0526},
month={April},}
@INPROCEEDINGS{6869142,
author={R. Hartanto and M. Eich},
booktitle={2014 IEEE International Conference on Technologies for
Practical Robot Applications (TePRA)},
title={Reliable, cloud-based communication for multi-robot systems},
year={2014},
pages={1-8},
abstract={In contrast to single robotic agent, multi-robot systems are
highly dependent on reliable communication. Robots have to synchronize
tasks or to share poses and sensor readings with other agents,
especially for co-operative mapping task where local sensor readings are
incorporated into a global map. The drawback of existing communication
frameworks is that most are based on a central component which has to be
constantly within reach. Additionally, they do not prevent data loss
between robots if a failure occurs in the communication link. During a
distributed mapping task, loss of data is critical because it will
corrupt the global map. In this work, we propose a cloud-based
publish/subscribe mechanism which enables reliable communication between
agents during a cooperative mission using the Data Distribution Service
(DDS) as a transport layer. The usability of our approach is verified by
several experiments taking into account complete temporary communication
loss.},
keywords={cloud computing;message passing;mobile robots;multi-robot
systems;telecommunication links;telerobotics;DDS;cloud-based
communication;cloud-based publish mechanism;cloud-based subscribe
mechanism;communication link;cooperative mapping task;data distribution
service;data loss;distributed mapping task;global map;multirobot
systems;reliable communication;sensor readings;single robotic
agent;transport layer;Couplings;OWL;Observers;Reliability;Robot sensing
systems},
doi={10.1109/TePRA.2014.6869142},
ISSN={2325-0526},
month={April},}
@INPROCEEDINGS{6846737,
author={Y. Wang and R. Tan and G. Xing and J. Wang and X. Tan and X. Liu
and X. Chang},
booktitle={IPSN-14 Proceedings of the 13th International Symposium on
Information Processing in Sensor Networks},
title={Aquatic debris monitoring using smartphone-based robotic sensors},
year={2014},
pages={13-24},
abstract={Monitoring aquatic debris is of great interest to the
ecosystems, marine life, human health, and water transport. This paper
presents the design and implementation of SOAR - a vision-based
surveillance robot system that integrates an off-the-shelf Android
smartphone and a gliding robotic fish for debris monitoring. SOAR
features real-time debris detection and coverage-based rotation
scheduling algorithms. The image processing algorithms for debris
detection are specifically designed to address the unique challenges in
aquatic environments. The rotation scheduling algorithm provides
effective coverage of sporadic debris arrivals despite camera's limited
angular view. Moreover, SOAR is able to dynamically offload
computation-intensive processing tasks to the cloud for battery power
conservation. We have implemented a SOAR prototype and conducted
extensive experimental evaluation. The results show that SOAR can
accurately detect debris in the presence of various environment and
system dynamics, and the rotation scheduling algorithm enables SOAR to
capture debris arrivals with reduced energy consumption.},
keywords={Android (operating system);cameras;computerised
monitoring;control engineering computing;geophysical image
processing;image sensors;mobile computing;mobile robots;oceanographic
techniques;scheduling;smart phones;video surveillance;SOAR;aquatic
debris monitoring;battery power conservation;camera;coverage-based
rotation scheduling algorithm;dynamically offload computation-intensive
processing;ecosystem;energy consumption;gliding robotic fish;human
health;image processing algorithm;marine life;off-the-shelf Android
smart phone;real-time debris detection;rotation scheduling
algorithm;smartphone-based robotic sensor;vision-based surveillance
robot system;water transport;Cameras;Energy consumption;Monitoring;Robot
vision systems;Robotic sensor;aquatic debris;computer vision;object
detection;smartphone},
doi={10.1109/IPSN.2014.6846737},
month={April},}
@ARTICLE{6767131,
author={C. McKinnon and J. A. Marshall},
journal={IEEE Transactions on Automation Science and Engineering},
title={Automatic Identification of Large Fragments in a Pile of Broken
Rock Using a Time-of-Flight Camera},
year={2014},
volume={11},
number={3},
pages={935-942},
abstract={This paper presents a solution to part of the problem of
making robotic or semi-robotic digging equipment less dependant on human
supervision. A method is described for identifying rocks of a certain
size that may affect digging efficiency or require special handling. The
process involves three main steps. First, by using range and intensity
data from a time-of-flight (TOF) camera, a feature descriptor is used to
rank points and separate regions surrounding high scoring points. This
allows a wide range of rocks to be recognized because features can
represent a whole or just part of a rock. Second, these points are
filtered to extract only points thought to belong to the large object.
Finally, a check is carried out to verify that the resultant point cloud
actually represents a rock. Results are presented from field testing on
piles of fragmented rock.},
keywords={cameras;foundations;geotechnical engineering;industrial
robots;object detection;robot vision;rocks;structural engineering;TOF
camera;broken rock pile;fragments identification;intensity data;points
extraction;range data;robotic digging equipment;scoring
points;semi-robotic digging equipment;time-of-flight
camera;Bridges;Cameras;Feature extraction;Noise;Robot vision
systems;Rocks;Shape;Field robotics;mining and construction
automation;object identification;point cloud data processing},
doi={10.1109/TASE.2014.2308011},
ISSN={1545-5955},
month={July},}
@INPROCEEDINGS{6840156,
author={J. Montoyo and V. Morell and M. Cazorla and J. Garcia-Rodriguez
and S. Orts Escolano},
booktitle={ISR/Robotik 2014; 41st International Symposium on Robotics},
title={Registration methods for RGB-D cameras accelerated on GPUs},
year={2014},
pages={1-8},
abstract={Registration of point clouds obtained from RGB-D sensors is a
challenging and time demanding problem. In this paper we present the GPU
implementation of different dense registration methods. Algorithms have
been redesigned and optimized to maximize their performance on GPUs. A
deep study has been developed in order to automatically estimate the
best parameters to achieve a high speed-up and accurate results. A
performance comparison with different GPUs is also provided. Accelerated
registration methods can be applied to time constrain tasks like
location and mapping in robotics field.},
month={June},}
@INPROCEEDINGS{6825357,
author={D. Habermann and A. Hata and D. Wolf and F. S. Osório},
booktitle={2013 III Brazilian Symposium on Computing Systems Engineering},
title={3D Point Clouds Segmentation for Autonomous Ground Vehicle},
year={2013},
pages={143-148},
abstract={Point clouds segmentation is an essential step to improve the
performance of obstacle detection and classification in areas of
autonomous ground vehicles and mobile robotics. This paper presents a
study and comparison of the performance of segmentation methods using
point clouds coming from a 3D laser sensor, more specifically obtained
from a Velodyne HDL32.},
keywords={collision avoidance;control engineering computing;mobile
robots;optical sensors;telerobotics;3D laser sensor;3D point clouds
segmentation;Velodyne HDL32;autonomous ground vehicle;mobile
robotics;obstacle detection;segmentation methods;Image segmentation;Land
vehicles;Laser radar;Lasers;Robot sensing systems;Three-dimensional
displays;3D Lidar;autonomous ground vehicle;point clouds segmentation},
doi={10.1109/SBESC.2013.43},
ISSN={2324-7886},
month={Dec},}
@INPROCEEDINGS{6739515,
author={S. Oishi and Y. Jeong and R. Kurazume and Y. Iwashita and T.
Hasegawa},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={ND voxel localization using large-scale 3D environmental map and
RGB-D camera},
year={2013},
pages={538-545},
abstract={We propose an efficient 3D global localization and tracking
technique for a mobile robot in a large-scale environment using 3D
geometrical map and a RGB-D camera. With the rapid development of
high-resolution 3D range sensors, high-speed processing of a large
amount of 3D data is becoming an urgent challenge in robotic
applications such as localization. To tackle this problem, the proposed
technique utilizes a ND (Normal Distributions) voxel representation.
Firstly, a 3D geometrical map represented by point-clouds is converted
to a number of ND voxels, and local features are extracted and stored as
an environmental map. In addition, range data captured by a RGB-D camera
is also converted to the ND voxels, and local features are calculated.
For global localization and tracking, the similarity of ND voxels
between the environmental map and the sensory data is examined according
to the local features or Kullback-Leibler divergence, and optimum
positions are determined in a framework of a particle filter.
Experimental results show that the proposed technique is robust for the
similarity in a 3D environmental map and converges more stable than a
standard voxel-based scan matching technique.},
keywords={feature extraction;image colour analysis;image matching;image
resolution;mobile robots;normal distribution;particle filtering
(numerical methods);3D geometrical map;3D global localization and
tracking technique;Kullback-Leibler divergence;ND voxel localization;ND
voxel representation;RGB-D camera;feature extraction;high-resolution 3D
range sensors;high-speed processing;large-scale 3D environmental
map;large-scale environment;mobile robot;normal distribution voxel
representation;particle filter;point-cloud;sensory data;voxel-based scan
matching technique;Atmospheric measurements;Cameras;Particle
measurements;Robot vision systems;Three-dimensional displays},
doi={10.1109/ROBIO.2013.6739515},
month={Dec},}
@INPROCEEDINGS{6739868,
author={X. Li and W. Guo and M. Li and L. Sun},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Combining two point clouds generated from depth camera},
year={2013},
pages={2620-2625},
abstract={Depth Cameras are widely used in mobile robotics recently.
Combining two frames is the key step to construct a complete 3D map, and
the relative pose between two views w.r.t two frames is needed. 3D point
pairs are necessary for computing 6DOF pose transformation. In this
paper, a method for finding correspondences of two point clouds is
proposed, and the advantages of depth camera are taken fully. The idea
of this approach comes from the fact that people pay more attentions to
some key points when they watch an environment. Point pairs are
generated from just points around the feature points, and these pairs
are corresponded accurately and the number of pairs is sufficient to
compute relative pose. Firstly, the algorithm for matching features from
intensity image is proposed. Secondly, 3D point pairs are obtained
according to the feature positions. Lastly, two point clouds are
registered based on the relative pose computed. The experiments
demonstrate the efficiency and the effectiveness of the approach.},
keywords={cameras;feature extraction;image matching;image
segmentation;three-dimensional displays;3D map;3D point pairs;6DOF pose
transformation;depth cameras;feature points;feature positions;intensity
image;matching features;mobile robotics;point clouds
correspondences;relative pose;Cameras;Feature extraction;Iterative
closest point algorithm;Robot vision systems;Three-dimensional displays},
doi={10.1109/ROBIO.2013.6739868},
month={Dec},}
@INPROCEEDINGS{6766553,
author={P. Drews and L. J. Manso and S. C. da Silva Filho and P. Núñez},
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)},
title={Improving change detection using Vertical Surface Normal
Histograms and Gaussian Mixture Models in structured environments},
year={2013},
pages={1-7},
abstract={The interest in change detection techniques for autonomous
robots has increased considerably during recent years. This is partly
due to the fact that changes in robot's working environments are
relevant for most tasks and robotics applications. Changes or Novelties
are usually detected by comparing the current data acquired by the robot
with its previous knowledge (a map or any other model of its
surroundings). Gaussian Mixture Models (GMM) have been satisfactorily
used for detecting changes in 3D point clouds. However, these methods
have drawbacks such as a long computational times and strong dependence
on the parameters of the algorithms. In structured environments like
offices or homes, it is possible to reduce the number of points to be
processed by filtering unlikely-to-change regions of the scene. This
paper introduces the concept of Vertical Surface Normal Histogram
(VSNH). VSNH provides a method for removing from the point clouds
acquired those points associated to the main planes: ceiling, walls and
floor. Removing these points decreases the size of the problem and
improves the segmentation of the environment into Gaussian Mixture
Models. The experimental results demonstrate that the proposed method
based on GMM and VSNH achieves change detection in structured indoor
environments faster and more accurately than previous approaches.},
keywords={Gaussian processes;mixture models;mobile robots;object
detection;3D point cloud;GMM;Gaussian mixture model;VSNH;autonomous
robot;change detection;structured environment;vertical surface normal
histogram;Change detection algorithms;Gaussian mixture
model;Histograms;Measurement;Robots;Three-dimensional displays;Vectors},
doi={10.1109/ICAR.2013.6766553},
month={Nov},}
@INPROCEEDINGS{6766552,
author={R. V. Aroca and V. P. Torres and L. M. G. Gonçalves and A.
Negreiros and A. Burlamaqui},
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)},
title={Cloud based low cost educational robot},
year={2013},
pages={1-6},
abstract={This paper presents a novel Internet and Telephony cloud based
system for low cost educational robotics. A webserver provides a web
based on-line application that allows students to write block based or
textual programs for a robot. When the program is executed, this server
calls a standard mobile phone using the public switched telephony
network and uses the telephony audio channel to send actuator commands
and read sensors through audio tones. The proposed architecture allows
the construction of low cost educational robots that can be programmed
and controlled from the web via any mobile phone that can receive calls
and has standard earphone connectors.},
keywords={Internet telephony;actuators;cloud computing;educational
robots;mobile handsets;sensors;Internet;Web server;actuator
commands;audio tones;block based programs;earphone connectors;low cost
educational robot;mobile phone;public switched telephony
network;sensors;telephony audio channel;telephony cloud based
system;textual programs;Actuators;Logic gates;Mobile communication;Robot
sensing systems;Servers},
doi={10.1109/ICAR.2013.6766552},
month={Nov},}
@INPROCEEDINGS{6766566,
author={T. Wiemann and H. Annuth and K. Lingemann and J. Hertzberg},
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)},
title={An evaluation of open source surface reconstruction software for
robotic applications},
year={2013},
pages={1-7},
abstract={With the raising popularity of 3D sensors in robotic
applications, e.g., RGB-D cameras or laser scanners, the demand for fast
and reliable methods for surface reconstruction from 3D point clouds
increases. Currently, several freely available implementations of such
algorithms exist. This paper presents an evaluation of the usability of
different open source software packages for polygonal map generation in
robotic contexts.},
keywords={control engineering computing;image reconstruction;public
domain software;robot vision;sensors;software packages;3D point
clouds;3D sensors;open source software package usability;open source
surface reconstruction software evaluation;polygonal map
generation;robotic applications;Approximation
methods;Lasers;Robots;Sensors;Shape;Surface
reconstruction;Three-dimensional displays},
doi={10.1109/ICAR.2013.6766566},
month={Nov},}
@INPROCEEDINGS{6758591,
author={Y. M. Srinivasa and S. Foong and D. Madhavan and U. X. Tan and
L. Hu and X. Fu and Y. L. Lo},
booktitle={2013 6th IEEE Conference on Robotics, Automation and
Mechatronics (RAM)},
title={Non-contact parametric estimation and localization of human head
for transcranial magnetic stimulation (TMS)},
year={2013},
pages={241-246},
abstract={Transcranial magnetic stimulation (TMS) has been gaining
popularity in various neurological treatments and an automated platform
with better precision is desired. This paper proposes a dexterous
articulated robotic arm with TMS coils as the end-effectors. Most
robotic arm based positioning systems utilize vision feedback from
stationary cameras for dynamic positioning, but these are not suitable
for clinics with limited space. In this paper, a compact approach to
identify the size, position and orientation of the patient's head
relative to a robotic arm using a non-vision, range data based system is
proposed. An accurate distance measurement sensor is used in tandem with
a robotic arm to scan the patient's head and produce a growing 3D point
cloud. An efficient surface fitting algorithm, taking into consideration
the similarity of the human head to an ellipsoid, is presented to
simultaneously extract position, orientation and geometrical information
of the target head. Simulations and experiments are conducted and
promising results are obtained to demonstrate its capability to identify
the location and orientation of the patient's head.},
keywords={attitude control;biomedical measurement;brain;cloud
computing;distance measurement;medical
robotics;neurophysiology;parameter estimation;patient treatment;position
control;transcranial magnetic stimulation;3D point cloud;TMS
coils;automated platform;dexterous articulated robotic arm;distance
measurement sensor;end-effectors;geometrical information;human
head;neurological treatments;noncontact parametric estimation;noncontact
parametric localization;orientation extraction;patient head
orientation;patient head position;patient head size;position
extraction;range data based system;robotic arm based positioning
systems;surface fitting algorithm;transcranial magnetic
stimulation;Automation;Conferences;Decision support
systems;Mechatronics;Random access memory;Robots},
doi={10.1109/RAM.2013.6758591},
ISSN={2158-2181},
month={Nov},}
@INPROCEEDINGS{6758588,
author={A. Nguyen and B. Le},
booktitle={2013 6th IEEE Conference on Robotics, Automation and
Mechatronics (RAM)},
title={3D point cloud segmentation: A survey},
year={2013},
pages={225-230},
abstract={3D point cloud segmentation is the process of classifying
point clouds into multiple homogeneous regions, the points in the same
region will have the same properties. The segmentation is challenging
because of high redundancy, uneven sampling density, and lack explicit
structure of point cloud data. This problem has many applications in
robotics such as intelligent vehicles, autonomous mapping and
navigation. Many authors have introduced different approaches and
algorithms. In this survey, we examine methods that have been proposed
to segment 3D point clouds. The advantages, disadvantages, and design
mechanisms of these methods are analyzed and discussed. Finally, we
outline the promising future research directions.},
keywords={image classification;image segmentation;robot vision;3D point
cloud segmentation;autonomous mapping;autonomous navigation;intelligent
vehicles;point cloud classification;robotics;Feature extraction;Image
edge detection;Image
segmentation;Robots;Robustness;Shape;Three-dimensional displays},
doi={10.1109/RAM.2013.6758588},
ISSN={2158-2181},
month={Nov},}
@INPROCEEDINGS{6747555,
author={Zhe Ji and Fan Zhou and Xiang Tian and Rongxin Jiang and Y. Chen},
booktitle={2013 IEEE Third International Conference on Information
Science and Technology (ICIST)},
title={Probabilistic 3D ICP algorithm based on ORB feature},
year={2013},
pages={300-304},
abstract={Aligning the 3D point clouds is considered as a crucial step
to build consistent maps from unknown environment in SLAM problem of
mobile robotics. However, ICP algorithms for aligning the point clouds
usually ignore the valuable visual information contained in the point
clouds and only model surface structure from the “model” scan. This
paper presents a new ICP algorithm incorporating the visual ORB feature.
This approach is based on a probabilistic ICP algorithm that takes into
account both scans for RGB images along with the depth information from
Kinect sensor. Experiments are carried out on real world scenes and
results show that the new approach improves the accuracy and saves time
of the registration.},
keywords={Cameras;Feature extraction;Iterative closest point
algorithm;Probabilistic logic;Robustness;Three-dimensional
displays;Visualization},
doi={10.1109/ICIST.2013.6747555},
ISSN={2164-4357},
month={March},}
@INPROCEEDINGS{6733746,
author={A. Bhaumik and K. Kabiraj and L. Dunn},
booktitle={2013 International Conference on Control, Automation,
Robotics and Embedded Systems (CARE)},
title={An empathizing robot, development of #x2018;human-like emotions
and augmented dynamics #x2019; (H.E.A.D) and it's Emotion Cloud},
year={2013},
pages={1-8},
abstract={This paper discusses the planning and development of a robotic
head which empathizes with the user and reflects their emotions with
concerted response. It is structured using keywords and expressions
heuristics (C.H.E.E and Mood) which embrace a non-predictable and a
layered and weighted approach to emotions: `Emotion Cloud', which
encompasses various inputs from the user. The paper details the design
of the hardware and the software for H.E.A.D.},
keywords={emotion recognition;human-robot interaction;natural language
processing;path planning;H.E.A.D;emotion cloud;empathizing
robot;expressions heuristics;hardware design;human like emotions and
augmented dynamics;keywords;robotic head development;robotic head
planning;software design;weighted
approach;History;Keyboards;Loading;Magnetic
heads;Mood;Robots;Semantics;human robot interaction;natural language
processing;social robotics},
doi={10.1109/CARE.2013.6733746},
month={Dec},}
@INPROCEEDINGS{6726744,
author={K. Ayush and N. K. Agarwal},
booktitle={2013 Fourth International Conference on Computing,
Communications and Networking Technologies (ICCCNT)},
title={Real time visual SLAM using cloud computing},
year={2013},
pages={1-7},
abstract={Simultaneous localization and map-building (SLAM) continues to
draw considerable attention in the robotics community due to the
advantages it can offer in building autonomous robots. However, few
approaches to this problem scale up to handle a large number of
landmarks present in real environments. The processing resource
requirement to carry out SLAM in real time can be quite high. In this
paper we present a novel system which employs resources provided by
infrastructure as a service (IaaS) and parallelism for effective
processing. Using private cloud infrastructure employing virtualized
resources based on MPI the task of global map building is performed in
real time simultaneously carrying out loop detection and bundle
adjustment for indoor environments. Through implementation in various
challenging environments with moving obstacles, visually homogeneous
areas having few features, regions with large changes in lighting and
relatively fast camera motion we demonstrate our system to be one which
is effective as well as robust.},
keywords={SLAM (robots);application program interfaces;cloud
computing;message passing;robot vision;IaaS;MPI;VSLAM system;autonomous
robots;cloud computing;indoor environments;infrastructure as a
service;loop detection;private cloud infrastructure;real-time visual
SLAM system;robotics community;simultaneous localization and
map-building;virtualized resources;Cameras;Cloud computing;Feature
extraction;Real-time systems;Simultaneous localization and
mapping;Three-dimensional displays;Tracking;Cloud Computing;Cloud based
Robots;Real Time Mapping;Visual SLAM},
doi={10.1109/ICCCNT.2013.6726744},
month={July},}
@INPROCEEDINGS{6722362,
author={M. Volkhardt and F. Schneemann and H. M. Gross},
booktitle={2013 IEEE International Conference on Systems, Man, and
Cybernetics},
title={Fallen Person Detection for Mobile Robots Using 3D Depth Data},
year={2013},
pages={3573-3578},
abstract={Falling down and not managing to get up again is one of the
main concerns of elderly people living alone in their home. Robotic
assistance for the elderly promises to have a great potential of
detecting these critical situations and calling for help. This paper
presents a feature-based method to detect fallen people on the ground by
a mobile robot equipped with a Kinect sensor. Point clouds are
segmented, layered and classified to detect fallen people, even under
occlusions by parts of their body or furniture. Different features,
originally from pedestrian and object detection in depth data, and
different classifiers are evaluated. Evaluation was done using data of
12 people lying on the floor. Negative samples were collected from
objects similar to persons, two tall dogs, and five real apartments of
elderly people. The best feature-classifier combination is selected to
built a robust system to detect fallen people.},
keywords={assisted living;geriatrics;image classification;image
segmentation;mobile robots;object detection;service robots;3D depth
data;Kinect sensor;elderly people;fallen person detection;feature-based
method;feature-classifier combination;mobile robots;occlusions;point
clouds classification;point clouds layering;point clouds
segmentation;robotic assistance;Feature extraction;Mobile robots;Radio
frequency;Robot sensing systems;Support vector
machines;Three-dimensional displays;3D depth data;Fallen person
detection;Kinect;mobile robot},
doi={10.1109/SMC.2013.609},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{6720347,
author={X. Li and W. Guo and M. Li and C. Chen and L. Sun},
booktitle={2013 IEEE International Conference on Information and
Automation (ICIA)},
title={Generating colored point cloud under the calibration between TOF
and RGB cameras},
year={2013},
pages={483-488},
abstract={3D point cloud is the original format of three dimensional map
for the navigation of mobile robotics. Time of Flight camera can
directly return 3D points and grayscale image simultaneously, but the
color information is lost. In this paper, 3D points are colored by
calibrating PMD camera with RGB camera, and then the RGB image is
generated with respect to a frame of TOF camera. Point cloud form only
one frame is not large enough for navigating the robot or observing the
environment, an approach for combining two continuous frames is also
proposed. Scale invariant features are detected from the RGB images
generated, and features from two images are matched accurately.
Iterative Closest Point(ICP) algorithm is used to estimate the relative
pose transformation of TOF camera, the initial pose is computed from
corresponded 3D feature point pairs. The experiments demonstrate the
effectiveness of the approach proposed in the paper.},
keywords={calibration;cameras;image processing;image processing
equipment;solid modelling;3D point cloud;PMD camera;RGB camera;TOF
camera;color information;colored point cloud generation;grayscale
image;iterative closest point algorithm;mobile robot navigation;three
dimensional map;time of flight camera;Calibration;Cameras;Feature
extraction;Gray-scale;Image color analysis;Robot vision
systems;Three-dimensional displays;TOF camera;calibration;point
cloud;relative pose},
doi={10.1109/ICInfA.2013.6720347},
month={Aug},}
@INPROCEEDINGS{6722442,
author={P. J. Sanz and A. Peñalver and J. Sales and D. Fornas and J. J.
Fernández and J. Pérez and J. Bernabé},
booktitle={2013 IEEE International Conference on Systems, Man, and
Cybernetics},
title={GRASPER: A Multisensory Based Manipulation System for Underwater
Operations},
year={2013},
pages={4036-4041},
abstract={This paper presents the progress that has been made recently
in the TRITON project. The TRITON project is an on going research
project being carried out in Spain which has as principal objective the
production of an AUV capable of autonomous underwater interventions. The
GRASPER sub-project focuses on developing the necessary manipulation
skills. Currently, a lot of research in the underwater robotics context
is developing increasing levels of autonomy for all kinds of
intervention operations, which always require some kind of physical
interaction. However, if autonomous robotic manipulation on land remains
a relatively undeveloped field, the situations is at an even more
primitive stage in underwater scenarios where currently the systems are
tele-operated by an expert user from a surface vessel. Only very few
underwater systems have the capacity to carry out manipulation without
any kind of umbilical cables teleoperating these actions. In particular,
this work introduces a new approach for increasing the autonomy levels
of an underwater manipulation system, discussing also preliminary
results. In order to test this concept, different objects, without
predefined models, are approached and recovered from the bottom in water
tank conditions. To achieve this purpose, a scan of the scene is
performed using a structured laser beam attached to the forearm of the
manipulator. At the same time, a digital video camera is used to capture
the scene with the laser beam projected onto the object. The laser
stripes are triangulated to obtain a 3D point cloud. Moreover, the
underwater robot gripper is provided with strain gauge tactile sensors,
which enable the execution of a more reliable grasp. On the other hand,
the process is shown inside an underwater simulator previously
developed, named UWSim, acting in this case as a virtual representation
of the real environment. This virtual representation allows the user to
specify the grasp, highlighting how the virtual grasp will be - efined
for the selected target. The feasibility and reliability of the
underwater manipulation system is demonstrated though the experimental
results.},
keywords={autonomous underwater vehicles;manipulators;strain
gauges;tactile sensors;tanks (containers);water storage;3D point
cloud;AUV production;GRASPER;TRITON project;UWSim;autonomous robotic
manipulation;autonomous underwater interventions;digital video
camera;laser stripes;manipulation skills;manipulator;multisensory based
manipulation system;scene capturing;strain gauge tactile
sensors;structured laser beam;underwater manipulation system;underwater
operations;underwater robot gripper;underwater robotics;underwater
simulator;virtual representation;water tank
conditions;Grasping;Lasers;Solid modeling;Tactile
sensors;Three-dimensional displays;3D reconstruction;Underwater
intervention mission;semiautonomous grasping;underwater object recovery},
doi={10.1109/SMC.2013.689},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{6719374,
author={N. Levi and G. Kovelman and A. Geynis and A. Sintov and A.
Shapiro},
booktitle={2013 IEEE International Symposium on Safety, Security, and
Rescue Robotics (SSRR)},
title={The DARPA virtual robotics challenge experience},
year={2013},
pages={1-6},
abstract={Destructive results of natural disasters like the earthquake
in Japan are not subject to control. Fukushima catastrophe has shown
that for now, robots do not have human capabilities when it comes to
search and rescue. At time of writing this paper a robotics competition
takes place whose goal is promoting research on issues related to
operation of robots in a human's environment. This so in future,
disasters like Fukushima's catastrophe would be handled better by
robots. In this paper we present the perspective of the dexterity group
that worked as part of the Israeli team “ROBIL”. The team participated
in DARPA's Virtual Robotic Challenge (VRC), a competition where teams
competing for the best score. The competition was performed on a
cloud-based simulator that monitored the physical behaviors of a
humanoid in an unknown simulated environment. A detailed description of
dexterity modules like motion planning and grasping will be presented
with regard to theory and implementation using ROS nodes framework. In
addition we present the integration between modules of the dexterity and
vision workgroups. The integration was conducted by creating a common
language within the workgroups defining the motion of objects in space.
Finally we discuss the results and conclusions we drew from the
dexterity implementation and integration during the three main project
phases: planning, qualification and competition.},
keywords={control engineering computing;dexterous
manipulators;disasters;earthquakes;emergency management;humanoid
robots;path planning;rescue robots;robot vision;DARPA virtual robotics
challenge experience;Fukushima catastrophe;Israeli team ROBIL;Japan;ROS
nodes framework;VRC;cloud-based simulator;dexterity group;dexterity
modules;dexterity workgroups;earthquake;grasping;human
environment;humanoid;motion planning;natural disasters;objects
motion;physical behaviors monitoring;qualification project
phases;robotics competition;search and rescue;vision
workgroups;Grasping;Hoses;Joints;Legged locomotion;Pelvis;Trajectory},
doi={10.1109/SSRR.2013.6719374},
ISSN={2374-3247},
month={Oct},}
@INPROCEEDINGS{6696484,
author={A. Mkhitaryan and D. Burschka},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={RGB-D sensor data correction and enhancement by introduction of
an additional RGB view},
year={2013},
pages={1077-1083},
abstract={RGB-D sensors are becoming more and more vital to robotics.
Sensors such as the Microsoft Kinect and time of flight cameras provide
3D colored point-clouds in real time can play a crucial role in Robot
Vision. However these sensors suffer from precision deficiencies, and
often the density of the point-clouds they provide is insufficient. In
this paper, we present a multi-camera system for correction and
enhancement of the data acquired from an RGB-D sensor. Our system
consists of two sensors, the RGB-D sensor (main sensor) and a regular
RGB camera (auxiliary sensor). We perform the correction and the
enhancement of the data acquired from the RGB-D sensor by placing the
auxiliary sensor in a close proximity to the target object and taking
advantage of the established epipolar geometry. We have managed to
reduce the relative error of the raw point-cloud from a Microsoft Kinect
RGB-D sensor by 74.5 % and increase its density up to 2.5 times.},
keywords={cameras;image colour analysis;image enhancement;image
sensors;robot vision;3D colored point-clouds;Microsoft Kinect;RGB
view;RGB-D sensor data;auxiliary sensor;data correction;data
enhancement;multi-camera system;red-green-blue-depth;regular RGB
camera;robot vision;robotics;time-of-flight
cameras;Calibration;Cameras;Estimation;Geometry;Reliability;Robot
sensing systems;Three-dimensional displays},
doi={10.1109/IROS.2013.6696484},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6696889,
author={J. Ryde and V. Dhiman and R. Platt},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Voxel planes: Rapid visualization and meshification of point
cloud ensembles},
year={2013},
pages={3731-3737},
abstract={Conversion of unorganized point clouds to surface
reconstructions is increasingly required in the mobile robotics
perception processing pipeline, particularly with the rapid adoption of
RGB-D (color and depth) image sensors. Many contemporary methods stem
from the work in the computer graphics community in order to handle the
point clouds generated by tabletop scanners in a batch-like manner. The
requirements for mobile robotics are different and include support for
real-time processing, incremental update, localization, mapping, path
planning, obstacle avoidance, ray-tracing, terrain traversability
assessment, grasping/manipulation and visualization for effective
human-robot interaction. We carry out a quantitative comparison of
Greedy Projection and Marching cubes along with our voxel planes method.
The execution speed, error, compression and visualization appearance of
these are assessed. Our voxel planes approach first computes the PCA
over the points inside a voxel, combining these PCA results across 2×2×2
voxel neighborhoods in a sliding window. Second, the smallest
eigenvector and voxel centroid define a plane which is intersected with
the voxel to reconstruct the surface patch (3-6 sided convex polygon)
within that voxel. By nature of their construction these surface patches
tessellate to produce a surface representation of the underlying points.
In experiments on public datasets the voxel planes method is 3 times
faster than marching cubes, offers 300 times better compression than
Greedy Projection, 10 fold lower error than marching cubes whilst
allowing incremental map updates.},
keywords={collision avoidance;computational geometry;data
visualisation;eigenvalues and eigenfunctions;image colour
analysis;mobile robots;principal component analysis;ray
tracing;real-time systems;surface reconstruction;2×2×2 voxel
neighborhoods;PCA;RGB-D image sensors;computer graphics community;convex
polygon;eigenvector;grasping;greedy projection;human-robot
interaction;incremental map updates;incremental
update;localization;manipulation;mapping;marching
cubes;meshification;mobile robotics perception processing
pipeline;obstacle avoidance;path planning;point cloud ensembles;public
datasets;rapid visualization;ray-tracing;real-time
processing;reconstruct the surface patch;sliding window;surface
patches;surface reconstructions;surface representation;tabletop
scanners;terrain traversability assessment;unorganized point
clouds;visualization appearance;voxel centroid;voxel planes
method;Eigenvalues and eigenfunctions;Image reconstruction;Principal
component analysis;Robot sensing systems;Surface reconstruction},
doi={10.1109/IROS.2013.6696889},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6696886,
author={J. Papon and T. Kulvicius and E. E. Aksoy and F. Wörgötter},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Point cloud video object segmentation using a persistent
supervoxel world-model},
year={2013},
pages={3712-3718},
abstract={Robust visual tracking is an essential precursor to
understanding and replicating human actions in robotic systems. In order
to accurately evaluate the semantic meaning of a sequence of video
frames, or to replicate an action contained therein, one must be able to
coherently track and segment all observed agents and objects. This work
proposes a novel online point cloud based algorithm which simultaneously
tracks 6DoF pose and determines spatial extent of all entities in indoor
scenarios. This is accomplished using a persistent supervoxel
world-model which is updated, rather than replaced, as new frames of
data arrive. Maintenance of a world model enables general object
permanence, permitting successful tracking through full occlusions.
Object models are tracked using a bank of independent adaptive particle
filters which use a supervoxel observation model to give rough estimates
of object state. These are united using a novel multi-model RANSAC-like
approach, which seeks to minimize a global energy function associating
world-model supervoxels to predicted states. We present results on a
standard robotic assembly benchmark for two application scenarios -
human trajectory imitation and semantic action understanding -
demonstrating the usefulness of the tracking in intelligent robotic
systems.},
keywords={adaptive filters;image segmentation;image
sequences;intelligent robots;iterative methods;object tracking;particle
filtering (numerical methods);pose estimation;robotic assembly;video
signal processing;6DoF pose estimation;full occlusions;general object
permanence;global energy function;human actions;human trajectory
imitation;independent adaptive particle filters;intelligent robotic
systems;novel multimodel RANSAC-like approach;novel online point cloud
based algorithm;object tracking;persistent supervoxel world-model;point
cloud video object segmentation;robotic systems;robust visual
tracking;semantic action understanding;spatial extent;standard robotic
assembly benchmark;supervoxel observation model;video frame
sequence;Image segmentation;Octrees;Robots;Target
tracking;Three-dimensional displays;Trajectory;Visualization},
doi={10.1109/IROS.2013.6696886},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6696809,
author={C. Rink and Z. C. Marton and D. Seth and T. Bodenmüller and M.
Suppa},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Feature based particle filter registration of 3D surface models
and its application in robotics},
year={2013},
pages={3187-3194},
abstract={This work is focused on global registration of surface models
such as homogeneous triangle meshes and point clouds. The investigated
approach utilizes feature descriptors in order to assign correspondences
between the data sets and to reduce complexity by considering only
characteristic feature points. It is based on the decomposability of
rigid motions into a rotation and a translation. The space of rotations
is searched with a particle filter and scoring is performed by looking
for clusters in the resulting sets of translations. We use features
computed from homogeneous triangle meshes and point clouds that require
low computation time. A major advantage of the approach proves to be the
possible consideration of prior knowledge about the relative
orientation. This is especially important when high noise levels produce
deteriorated features that are hard to match correctly. Comparisons to
existing algorithms show the method's competitiveness, and results in
robotic applications with different sensor types are presented.},
keywords={computational complexity;computer graphics;image
registration;particle filtering (numerical methods);robots;3D surface
models;complexity;feature based particle filter registration;feature
descriptors;global registration;homogeneous triangle meshes;point
clouds;rigid motions;robotic applications;Estimation;Feature
extraction;Image edge detection;Robots;Robustness;Solid
modeling;Three-dimensional displays},
doi={10.1109/IROS.2013.6696809},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6696617,
author={J. G. Petersen and F. R. Baena},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={A dynamic active constraints approach for hands-on robotic surgery},
year={2013},
pages={1966-1971},
abstract={Toward the goal of developing a hands-on robotic surgery
control strategy which simultaneously utilizes the various strengths of
both the surgeon and robot, we present a dynamic active constraint
approach tailored for hands-on surgery. Forbidden region active
constraints are used to prevent motion into areas which have been deemed
dangerous by the surgeon, helping to overcome some of the disadvantages
of fully active systems such as loss of tactile feedback, limited
workspace, and limited field-of-view. The computer graphics technique of
metaballs is used to represent point cloud data from an imaging system
with an analytical, differentiable surface and a dynamics-based
controller is proposed which controls the robot to lie on the zero set
of the generated time-varying implicit function for which the motion is
either known or unknown. This controller has been incorporated into a
recursive null-space approach to allow for unimpeded motion along the
surface and for further extension to joint optimization in the future.
This methodology is demonstrated in simulation and on a lightweight,
seven-degree-of-freedom serial manipulator.},
keywords={computer graphics;manipulator dynamics;medical robotics;motion
control;optimisation;surgery;time-varying systems;analytical
differentiable surface;computer graphics technique;dynamic active
constraint approach;dynamic-based controller;forbidden region active
constraints;hands-on robotic surgery control strategy;imaging
system;joint optimization;lightweight seven-degree-of-freedom serial
manipulator;metaballs;point cloud data representation;recursive
null-space approach;time-varying implicit function;unimpeded
motion;Aerospace electronics;Dynamics;Jacobian matrices;Level
set;Robots;Surface impedance;Surgery},
doi={10.1109/IROS.2013.6696617},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6698837,
author={L. Ma and T. Whelan and E. Bondarev and P. H. N. de With and J.
McDonald},
booktitle={2013 European Conference on Mobile Robots},
title={Planar simplification and texturing of dense point cloud maps},
year={2013},
pages={164-171},
abstract={Dense RGB-D based SLAM techniques and high-fidelity LIDAR
scanners are examples from an abundant set of systems capable of
providing multi-million point datasets. These large datasets quickly
become difficult to process and work with due to the sheer volume of
data, which typically contains significant redundant information, such
as the representation of planar surfaces with hundreds of thousands of
points. In order to exploit the richness of information provided by
dense methods in real-time robotics, techniques are required to reduce
the inherent redundancy of the data. In this paper we present a method
for efficient triangulation and texturing of planar surfaces in large
point clouds. Experimental results show that our algorithm removes more
than 90% of the input planar points, leading to a triangulation with
only 10% of the original amount of triangles per planar segment,
improving upon an existing planar simplification algorithm. Despite the
large reduction in vertex count, the principal geometric features of
each segment are well preserved. In addition to this, our texture
generation algorithm preserves all colour information contained within
planar segments, resulting in a visually appealing and geometrically
accurate simplified representation.},
keywords={computational geometry;image texture;mesh generation;real-time
systems;redundancy;robots;colour information;data redundancy;dense RGB-D
based SLAM techniques;dense point cloud maps;high-fidelity LIDAR
scanner;input planar points;planar segments;planar simplification
algorithm;planar surface texturing;planar surface
triangulation;principal geometric features;real-time robotics;texture
generation algorithm;vertex count;Approximation algorithms;Image color
analysis;Laser radar;Redundancy;Robots;Shape;Three-dimensional displays},
doi={10.1109/ECMR.2013.6698837},
month={Sept},}
@INPROCEEDINGS{6696883,
author={K. O. Rinnewitz and T. Wiemann and K. Lingemann and J. Hertzberg},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Automatic creation and application of texture patterns to 3D
polygon maps},
year={2013},
pages={3691-3696},
abstract={Textured polygon meshes are becoming more and more important
for robotic applications. In this paper we present an approach to
automatically extract textures from colored 3D point cloud data and
apply them to a polygonal reconstruction of the scene. The extracted
textures are analyzed for existing patters and reused if several
instances appear. Emphasis of this work is on minimizing the number of
used pixels while maintaining a realistic impression of the scanned
environment.},
keywords={feature extraction;image colour analysis;image
reconstruction;image texture;mesh generation;robot vision;3D polygon
maps;colored 3D point cloud data;robotic applications;scene polygonal
reconstruction;texture extraction;texture pattern application;texture
pattern automatic creation;textured polygon mesh;Correlation;Feature
extraction;Histograms;Image color analysis;Image
reconstruction;Robots;Three-dimensional displays},
doi={10.1109/IROS.2013.6696883},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6698825,
author={M. Dubois and P. K. Rozo and A. Gepperth and O. F. A. González
and D. Filliat},
booktitle={2013 European Conference on Mobile Robots},
title={A comparison of geometric and energy-based point cloud semantic
segmentation methods},
year={2013},
pages={88-93},
abstract={The recent availability of inexpensive RGB-D cameras, such as
the Microsoft Kinect, has raised interest in the robotics community for
point cloud segmentation. We are interested in the semantic segmentation
task in which the goal is to find some relevant classes for navigation,
wall, ground, objects, etc. Several effective solutions have been
proposed, mainly based on the recursive decomposition of the point cloud
into planes. We compare such a solution to a non-associative MRF method
inspired by some recent work in computer vision. The MRF yields
interesting results that are however less good than those of a carefully
tuned geometric method. Nevertheless, MRF still has some advantages and
we suggest some improvements.},
keywords={cameras;computational geometry;image colour analysis;image
segmentation;mobile robots;robot vision;RGB-D cameras;computer
vision;energy-based point cloud semantic segmentation method;geometric
point cloud semantic segmentation method;nonassociative MRF
method;recursive point cloud decomposition;Databases;Image color
analysis;Image segmentation;Robots;Semantics;Shape;Three-dimensional
displays},
doi={10.1109/ECMR.2013.6698825},
month={Sept},}
@INPROCEEDINGS{6697119,
author={D. Um and M. A. Gutiérrez and P. Bustos and S. Kang},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Simultaneous planning and mapping (SPAM) for a manipulator by
best next move in unknown environments},
year={2013},
pages={5273-5278},
abstract={In this paper, we propose a SPAM (Simultaneous Planning and
Mapping) technique for a manipulator type robot working in an uncertain
environment via a Best Next Move algorithm. Demands for a smart decision
to move a manipulator such as humanoid arms in uncertain or crowded
environments call for a simultaneous planning and mapping technique. We
assume no a priori knowledge of either the obstacles or the rest of the
environment exits. For rapid map building and path planning, we use a
skin type setup based on 3D depth camera sensors that completely
encompass the entire body of a manipulator. The 3D sensors capture the
point clouds used to create an instantaneous c-space map whereby a Best
Next Move algorithm directs the motion of the manipulator. The Best Next
Move algorithm utilizes the gradient of the density distribution of the
k-nearest-neighborhood sets in c-space. It has tendency to travel along
the direction by which the point clouds spread in space, thus rendering
faster mapping of c-space obstacles. The proposed algorithm is compared
with several sensor based algorithms for performance measurement such as
map completion rate, distribution of samples, total nodes, etc. Some
improved performances are reported for the proposed algorithm. Several
possible applications include semi-autonomous tele-robotics planning,
humanoid arm path planning, among others.},
keywords={image sensors;manipulators;path planning;3D depth camera
sensors;SPAM;best next move algorithm;c-space obstacles;humanoid
arms;instantaneous c-space map;k-nearest-neighborhood density
distribution gradient;manipulator type robot;map building;map completion
rate;path planning;performance measurement;point clouds;sample
distribution;sensor based algorithms;simultaneous planning and mapping
technique;skin type setup;total nodes;unknown
environments;Manipulators;Path planning;Planning;Robot sensing
systems;Three-dimensional displays},
doi={10.1109/IROS.2013.6697119},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6671778,
author={Jiaolong Yang and Y. Dai and H. Li and H. Gardner and Yunde Jia},
booktitle={2013 IEEE International Symposium on Mixed and Augmented
Reality (ISMAR)},
title={Single-shot extrinsic calibration of a generically configured
RGB-D camera rig from scene constraints},
year={2013},
pages={181-188},
abstract={With the increasing use of commodity RGB-D cameras for
computer vision, robotics, mixed and augmented reality and other areas,
it is of significant practical interest to calibrate the relative pose
between a depth (D) camera and an RGB camera in these types of setups.
In this paper, we propose a new single-shot, correspondence-free method
to extrinsically calibrate a generically configured RGB-D camera rig. We
formulate the extrinsic calibration problem as one of geometric 2D-3D
registration which exploits scene constraints to achieve single-shot
extrinsic calibration. Our method first reconstructs sparse point clouds
from a single-view 2D image. These sparse point clouds are then
registered with dense point clouds from the depth camera. Finally, we
directly optimize the warping quality by evaluating scene constraints in
3D point clouds. Our single-shot extrinsic calibration method does not
require correspondences across multiple color images or across different
modalities and it is more flexible than existing methods. The scene
constraints can be very simple and we demonstrate that a scene
containing three sheets of paper is sufficient to obtain reliable
calibration and with a lower geometric error than existing methods.},
keywords={calibration;cameras;computer graphics;image colour
analysis;image registration;3D point cloud;RGB camera;RGB-D camera
rig;augmented reality;color images;commodity RGB-D cameras;computer
vision;dense point clouds;depth camera;extrinsic calibration
problem;geometric 2D-3D registration;geometric error;mixed
reality;robotics;scene constraints;single-shot extrinsic calibration
method;single-view 2D image;sparse point clouds;warping
quality;Calibration;Cameras;Color;Educational institutions;Image color
analysis;Robot vision systems;Three-dimensional displays},
doi={10.1109/ISMAR.2013.6671778},
month={Oct},}
@INPROCEEDINGS{6685533,
author={V. S. Zaborovskiy and A. A. Lukashin and S. G. Popov and A. V.
Vostrov},
booktitle={2013 13th International Conference on ITS Telecommunications
(ITST)},
title={Adage mobile services for ITS infrastructure},
year={2013},
pages={127-132},
abstract={Modern Intelligent Transport Systems (ITS) are based on
specific services that are hosted at the edge of network infrastructure.
These services are created using the vehicle embedded computing
appliances, private cloud resources and global available information
services. Within proposed approach all vehicles are considered as mobile
part of low level operation network that providing low latency and
requested QoS characteristics for ITS communication and information
systems. More over some of the discussed decisions support multiprotocol
interactions and predictable real time performance so can be used for
different kind of industrial, transport and robotics applications. ITS
supplemented by low level operation network expands opportunity for
practical implementation of emerging Internet of Things (IoT) concept.
Thanks to wide functional ability proposed approach is well positioned
for big data and on demand high performance applications. Some aspects
of these services develops ideas of CISCO' Fog Computing [1] and may be
seamlessly supported into existing ITS cloud infrastructure using
virtual firewall appliances which provides bilateral access control
between vehicles that belongs to MESH network and IaaS segments
resources of which support high performance computing or even
supercomputers services [2].},
keywords={Internet of Things;cloud computing;intelligent transportation
systems;mobile computing;ITS infrastructure;IaaS segments
resources;Internet of Things concept;MESH network;adage mobile
services;big data;global available information services;high performance
applications;intelligent transport systems;multiprotocol
interactions;predictable real time performance;private cloud
resources;vehicle embedded computing appliances;Cloud computing;Mesh
networks;Mobile communication;Protocols;Security;Vehicles;Virtual
machining;Cloud Services;Intelligent Transport
Systems;MESH;Multiprotocol Node;Security Services},
doi={10.1109/ITST.2013.6685533},
month={Nov},}
@INPROCEEDINGS{6669459,
author={T. Oliveira and P. Encarnação and A. P. Aguiar},
booktitle={2013 European Control Conference (ECC)},
title={Moving path following for autonomous robotic vehicles},
year={2013},
pages={3320-3325},
abstract={This paper introduces the moving path following (MPF) problem
for autonomous robotic vehicles, in which the vehicle is required to
converge to and follow a desired geometric moving path, without a
specific temporal specification. This case generalizes the classical
path following problem, where the given path is stationary. Possible
tasks that can be formulated as a MPF problem include terrain/air
vehicles target tracking and gas clouds monitoring, where the velocity
of the target/cloud specifies the motion of the path. Using the concept
of parallel-transport frame associated to the geometric path, we derive
the MPF kinematic-error dynamics for 3D paths with arbitrary motion
specified by its linear and angular velocity. An application is made to
the problem of tracking a target on the ground using an Unmanned Aerial
Vehicle. The control law is derived using Lyapunov methods. Formal
convergence results are provided and hardware in the loop simulations
demonstrate the effectiveness of the proposed method.},
keywords={autonomous aerial vehicles;mobile robots;path planning;target
tracking;Lyapunov methods;MPF kinematic-error dynamics;MPF problem;air
vehicle target tracking;angular velocity;autonomous robotic
vehicles;control law;gas clouds monitoring;geometric moving
path;geometric path;hardware in the loop simulations;linear
velocity;moving path following;parallel-transport frame;path following
problem;temporal specification;terrain vehicle target tracking;unmanned
aerial vehicle;Aircraft;Angular velocity;Atmospheric
modeling;Equations;Target tracking;Vectors;Vehicles},
month={July},}
@INPROCEEDINGS{6654062,
author={M. Rajaraman and M. Dawson-Haggerty and K. Shimada and D. Bourne},
booktitle={2013 IEEE International Conference on Automation Science and
Engineering (CASE)},
title={Automated workpiece localization for robotic welding},
year={2013},
pages={681-686},
abstract={Precise knowledge of a workpiece's position is essential to
robotic manufacturing. This often requires design and use of special
purpose fixtures and programming between manufacturing operations. Our
goal is to remove these requirements and automate the discovery of
position (localization) of the workpiece. For quick setup and
localization we have augmented the end-effector of the robot with a
laser projector and a laser displacement sensor. The laser projector
guides the worker in initial fixture and workpiece placement and the
laser displacement sensor acquires a 3D point cloud of the workspace.
The point cloud collected after scanning the workspace is processed to
provide a sparse outline of the workpiece. This outline is then compared
to the computer aided design (CAD) data of the workpiece to estimate a
transformation between the actual and planned position. This estimation
is made by using an iterative closest point algorithm. Multiple searches
are run using different seed points to improve the chance of finding the
best fit while maintaining low run times. In this paper, we have used
flat weldments as test cases. In our experiments we were able to
localize and weld workpieces with significant time savings against
current practices in manual welding.},
keywords={CAD;control engineering computing;end effectors;industrial
manipulators;iterative methods;position control;production engineering
computing;robotic welding;sensors;solid modelling;3D point cloud;CAD
data;automated workpiece localization;computer aided design;end
effector;flat weldments;iterative closest point algorithm;laser
displacement sensor;laser projector;manufacturing operations;robotic
manufacturing;robotic welding;seed points;special purpose fixtures;time
savings;workpiece position;workpiece position discovery;Design
automation;Fixtures;Inspection;Lasers;Robot sensing systems;Welding},
doi={10.1109/CoASE.2013.6654062},
ISSN={2161-8070},
month={Aug},}
@INPROCEEDINGS{6651552,
author={R. Bormann and J. Fischer and G. Arbeiter and A. Verl},
booktitle={2012 12th IEEE-RAS International Conference on Humanoid
Robots (Humanoids 2012)},
title={Adding rotational robustness to the Surface-Approximation
Polynomials descriptor},
year={2012},
pages={409-416},
abstract={The Surface-Approximation Polynomials (SAP) descriptor has
been shown to be an appropriate global surface descriptor for object
categorization tasks in robotic applications [1]. Nevertheless, in the
original formulation the SAP descriptor is not invariant against
rotations around the camera axis. This paper explains and evaluates two
methods which pre-process the input data to yield repeatably
well-aligned point clouds for the computation of the SAP descriptor. We
show that the SAP descriptor can be rendered robust against rotations
while retaining almost the full performance of the original approach
which is superior to GFPFH, GRSD and VFH.},
keywords={computer graphics;robot vision;SAP descriptor;camera
axis;global surface descriptor;object categorization tasks;robotic
applications;rotational robustness;surface-approximation polynomials
descriptor;well-aligned point clouds;Area measurement;Robots;Robustness},
doi={10.1109/HUMANOIDS.2012.6651552},
ISSN={2164-0572},
month={Nov},}
@INPROCEEDINGS{6648028,
author={H. Thamer and H. Kost and D. Weimer and B. Scholz-Reiter},
booktitle={2013 IEEE 18th Conference on Emerging Technologies Factory
Automation (ETFA)},
title={A 3D-robot vision system for automatic unloading of containers},
year={2013},
pages={1-7},
abstract={Unloading of standard containers within logistic processes is
mainly performed manually. Amongst gripping technology, the development
of a robot vision system for recognizing different shaped logistic goods
is a major technical obstacle for developing robotic systems for
automatic unloading of containers. Goods can be arbitrarily placed
inside a container and the resulting packaging scenarios usually have a
high degree of occlusion. Existing systems and approaches use range
information acquired by laser scanners for recognizing and localizing
goods inside of containers. They are restricted to a single shape class
of goods and often have limited size ranges for goods. This paper
presents a robot vision for recognizing and localizing differently
shaped and sized objects in piled packaging scenarios using range data
acquired by different kinds of range sensors. After a specific
segmentation step, different shaped partial surfaces are detected and
classified in point cloud data and combined to complete logistic goods.
The system is evaluated with real and simulated sensor data from
different packaging scenarios.},
keywords={containerisation;containers;goods
distribution;grippers;optical scanners;packaging;robot vision;shape
recognition;unloading;3D-robot vision system;automatic container
unloading;goods localization;gripping technology;laser scanners;logistic
goods recognition;logistic processes;object shap
recognition;occlusion;packaging scenarios;piled packaging
scenarios;point cloud data;range sensors;robotic systems;simulated
sensor data;Containers;Logistics;Packaging;Robots;Sensors;Shape;Surface
treatment},
doi={10.1109/ETFA.2013.6648028},
ISSN={1946-0740},
month={Sept},}
@INPROCEEDINGS{6643524,
author={J. Kim and H. H. Nguyen and Y. Lee and S. Lee},
booktitle={2013 IEEE International Symposium on Assembly and
Manufacturing (ISAM)},
title={Structured light camera base 3D visual perception and tracking
application system with robot grasping task},
year={2013},
pages={187-192},
abstract={3D vision-based recognition as well as grasping of complex
objects is required not only for detection and categorization but also
for pose estimation and robotic pick-and-place operations. In this
paper, we propose a structured light camera based 3D visual perception
and tracking system application with robot arm grasping for
manufacturing. In the first step, we use a geometric surface primitive
patch segmentation approach based on Hough transforms to obtain accurate
surface normal estimations from 3D point clouds for the identification
of patch primitives. The most relevant primitives for our application
include planar and cylindrical surface patches. We extract primitive
surface patches from automotive CAD models in DXF or 3DS format. The
models are then decomposed to simple entities such as planar polygons,
vertexes and lines. Our system takes advantage of the available CAD data
for both object recognition and for pose estimation. In the final step,
we propose point based KLT tracking method with dominant features from
object recognition results both raw data and CAD. Our experimental
results demonstrate that we can show application results from YASKAWA
MOTOMAN robots demonstration. Whole processes perform within few
seconds, an accurate pose and reasonable tracking.},
keywords={CAD;Hough transforms;automobile industry;image
sensors;industrial manipulators;object recognition;object tracking;pose
estimation;robot vision;3D point clouds;3D vision-based recognition;3DS
format;DXF;Hough transforms;YASKAWA MOTOMAN robots;automotive CAD
models;complex object grasping;cylindrical surface patches;geometric
surface primitive patch segmentation approach;manufacturing;object
recognition results;planar surface patches;point based KLT tracking
method;pose estimation;robot arm grasping;robot grasping task;robotic
pick-and-place operations;structured light camera base 3D visual
perception;surface normal estimations;tracking application system;Design
automation;Estimation;Robots;Solid modeling;Surface
treatment;Three-dimensional displays;Vectors;Hough Transform;KTL
Tracking;Object Recognition;Pose Estimation;Surface Patch Primitives
Segmentation},
doi={10.1109/ISAM.2013.6643524},
month={July},}
@INPROCEEDINGS{6641246,
author={O. S. Gedik and A. A. Alatan},
booktitle={Proceedings of the 16th International Conference on
Information Fusion},
title={Fusing 2D and 3D clues for 3D tracking using visual and range data},
year={2013},
pages={1966-1973},
abstract={3D tracking of rigid objects is required in many applications,
such as robotics or augmented reality (AR). The availability of accurate
pose estimates increases reliability in robotic applications and
decreases jitter in AR scenarios. Pure vision-based 3D trackers require
either manual initializations or offline training stages, whereas
trackers relying on pure depth sensors are not suitable for AR
applications. In this paper, an automated 3D tracking algorithm, which
is based on fusion of vision and depth sensors via Extended Kalman
Filter (EKF), which inherits a novel observation weighting method, is
proposed. Moreover, novel feature selection and tracking schemes based
on intensity and shape index map (SIM) data of 3D point cloud, increases
2D and 3D tracking performance significantly. The proposed method
requires neither manual initialization of pose nor offline training,
while enabling highly accurate 3D tracking. The accuracy of the proposed
method is tested against a number of conventional techniques and
superior performance is observed.},
keywords={Kalman filters;nonlinear filters;object tracking;sensor
fusion;2D-3D clues fusion;3D point cloud;EKF;SIM data;automated 3D
tracking algorithm;depth sensors;extended Kalman filter;feature
selection;intensity data;observation weighting method;range data;rigid
objects 3D tracking;shape index map;vision sensors;visual
data;Cameras;Estimation;Feature extraction;Noise;Sensors;Solid
modeling;Three-dimensional displays;3D tracking;EKF;sensor fusion},
month={July},}
@INPROCEEDINGS{6631176,
author={K. Duncan and S. Sarkar and R. Alqasemi and R. Dubey},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Multi-scale superquadric fitting for efficient shape and pose
recovery of unknown objects},
year={2013},
pages={4238-4243},
abstract={Rapidly acquiring the shape and pose information of unknown
objects is an essential characteristic of modern robotic systems in
order to perform efficient manipulation tasks. In this work, we present
a framework for 3D geometric shape recovery and pose estimation from
unorganized point cloud data. We propose a low latency multi-scale
voxelization strategy that rapidly fits superquadrics to single view 3D
point clouds. As a result, we are able to quickly and accurately
estimate the shape and pose parameters of relevant objects in a scene.
We evaluate our approach on two datasets of common household objects
collected using Microsoft's Kinect sensor. We also compare our work to
the state of the art and achieve comparable results in less
computational time. Our experimental results demonstrate the efficacy of
our approach.},
keywords={curve fitting;geometry;image sensors;object detection;pose
estimation;robot vision;shape recognition;3D geometric pose estimation
framework;3D geometric shape recovery framework;3D point
clouds;Microsoft Kinect sensor;computational time;household object
datasets;low latency multiscale voxelization strategy;manipulation
tasks;multiscale superquadric fitting;object pose parameter
estimation;object shape parameter estimation;robotic systems;unknown
object pose information acquisition;unknown object pose recovery;unknown
object shape information acquisition;unknown object shape
recovery;unorganized point cloud data;Fitting;Robot sensing
systems;Standards},
doi={10.1109/ICRA.2013.6631176},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630636,
author={D. Fischinger and M. Vincze and Y. Jiang},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Learning grasps for unknown objects in cluttered scenes},
year={2013},
pages={609-616},
abstract={In this paper, we propose a method for grasping unknown
objects from piles or cluttered scenes, given a point cloud from a
single depth camera. We introduce a shape-based method - Symmetry Height
Accumulated Features (SHAF) - that reduces the scene description
complexity such that the use of machine learning techniques becomes
feasible. We describe the basic Height Accumulated Features and the
Symmetry Features and investigate their quality using an F-score metric.
We discuss the gain from Symmetry Features for grasp classification and
demonstrate the expressive power of Height Accumulated Features by
comparing it to a simple height based learning method. In robotic
experiments of grasping single objects, we test 10 novel objects in 150
trials and show significant improvement of 34% over a state-of-the-art
method, achieving a success rate of 92%. An improvement of 29% over the
competitive method was achieved for a task of clearing a table with 5 to
10 objects and overall 90 trials. Furthermore we show that our approach
is easily adaptable for different manipulators by running our
experiments on a second platform.},
keywords={computational complexity;learning (artificial
intelligence);manipulators;pattern classification;F-score
metric;SHAF;cluttered scenes;depth camera;grasp classification;grasp
learning;height based learning method;manipulators;point cloud;robotic
grasping;scene description complexity reduction;shape-based
method;symmetry height accumulated features;unknown object
grasping;Accuracy;Grasping;Grippers;Manipulators;Support vector
machines;Training},
doi={10.1109/ICRA.2013.6630636},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630612,
author={D. Hunziker and M. Gajamohan and M. Waibel and R. D'Andrea},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Rapyuta: The RoboEarth Cloud Engine},
year={2013},
pages={438-444},
abstract={In this paper we present the design and implementation of
Rapyuta^1 , the RoboEarth Cloud Engine. Rapyuta is an open source
Platform-as-a-Service (PaaS) framework designed specifically for
robotics applications. Rapyuta helps robots to offload heavy computation
by providing secured customizable computing environments in the cloud.
The computing environments also allow robots to easily access the
RoboEarth knowledge repository. Furthermore, these computing
environments are tightly interconnected, paving the way for deployment
of robotic teams. We also describe specific use case configurations and
present some performance results.},
keywords={cloud computing;public domain software;robots;PaaS
framework;Rapyuta;RoboEarth cloud engine;RoboEarth knowledge
repository;open source platform-as-a-service framework;robotic
teams;robotics applications;Robot sensing systems},
doi={10.1109/ICRA.2013.6630612},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630896,
author={F. Rydén and H. J. Chizeck},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={A method for constraint-based six degree-of-freedom haptic
interaction with streaming point clouds},
year={2013},
pages={2353-2359},
abstract={This paper presents a constraint-based method for haptic
interaction between an arbitrary voxelized polygon tool and a streaming
point cloud derived from a depth sensor. Using the presented method, a
user can interact with both dynamic as well as static point cloud
representations of real objects captured in real-time. Every depth image
frame is filtered and surface normals are calculated in real-time. For
movement of the virtual tool, a `quasi-static' simulation is used. The
innovations of this work include the extension of haptic rendering from
streaming point clouds to six degrees of freedom. This is appropriate
for co-robotic tasks where haptic feedback to the user is combined with
remote control of a robot.},
keywords={haptic interfaces;rendering (computer graphics);software
agents;virtual reality;arbitrary voxelized polygon tool;constraint based
method;corobotic tasks;depth image frame;depth sensor;haptic
feedback;haptic interaction;haptic rendering;quasistatic
simulation;remote control;static point cloud representations;streaming
point clouds;surface normals;virtual tool;Acceleration;Force;Haptic
interfaces;Real-time systems;Rendering (computer graphics);Streaming
media;Vectors},
doi={10.1109/ICRA.2013.6630896},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6629942,
author={A. Sarkar and S. Srivastava and B. S. Manoj},
booktitle={2013 IEEE Global Humanitarian Technology Conference: South
Asia Satellite (GHTC-SAS)},
title={Elevation mapping using stereo vision enabled heterogenous
multi-agent robotic network},
year={2013},
pages={340-345},
abstract={Multi-agent systems and stereo vision navigation is a
challenging area in Robotic research. The task can be widely scalable
and optimized for a specific application. An efficient centralized
heterogeneous swarm robotic system is proposed and implemented for
digital elevation mapping of an unknown terrain using stereo vision. The
utility of vision sensors compared to other sensor systems is shown in
mapping and navigation application on planetary terrain. The cloud
computing paradigm is implemented in the communication module via a
private cloud over OLSR. The system utilizes real-time odometry,
disparity mapping, cloud computing, and network hazard recovery
algorithms for efficient terrain mapping. The map is stored in an octree
data structure. The system is designed keeping in perspective planetary
exploration mission, however it is widely scalable and can be modified
for various civilian and military purposes.},
keywords={aerospace robotics;cloud computing;digital elevation
models;image sensors;mobile robots;multi-robot systems;octrees;path
planning;planetary rovers;robot vision;routing protocols;stereo image
processing;terrain mapping;OLSR;centralized heterogeneous swarm robotic
system;cloud computing;communication module;digital elevation
mapping;disparity mapping;heterogenous multiagent robotic
network;network hazard recovery algorithms;octree data
structure;optimized link state routing protocol;planetary exploration
mission;planetary terrain;private cloud;real-time odometry;stereo
vision;terrain mapping;vision sensors;Ad hoc networks;Cameras;Cloud
computing;Hazards;Octrees;Robots;Stereo vision;SLAM;VANET;digital
elevation mapping;heterogeneous centralised swarm;network hazard
control;octree;path planning;private cloud;robotic computer
network;stereo vision;visual odometry},
doi={10.1109/GHTC-SAS.2013.6629942},
month={Aug},}
@INPROCEEDINGS{6623540,
author={D. Di Marco and O. Zweigle and P. Levi},
booktitle={2013 13th International Conference on Autonomous Robot Systems},
title={Base pose estimation using shared reachability maps for
manipulation tasks},
year={2013},
pages={1-6},
abstract={Manipulation tasks for robots with a robotic arm and a mobile
base are one of the most error-prone operations in service robotics.
Especially for low-cost robots with a small reachable workspace of the
arms, the selection of base poses for reaching tasks is a challenging
problem. In this paper, we focus on the estimation of base poses for
robots such that they are able to reach a given end effector target pose
using space-efficient discretizations of the robots workspace, so-called
reachability maps. We further present a method to share these
reachability maps among robots by storing them on a cloud-database,
annotated with a semantic description. These are then integrated in a
knowledge-based task execution system for service robots. Last, we
conclude with the results of an experiment showing the cooperation of
robots of different hardware making use of each other's reachability
maps.},
keywords={end effectors;knowledge based systems;mobile
robots;multi-robot systems;pose estimation;reachability analysis;robot
vision;service robots;base pose estimation;cloud database;end effector
target pose;error-prone operations;knowledge-based task execution
system;manipulation tasks;mobile base;robotic arm;robots
workspace;semantic description;service robotics;service robots;shared
reachability maps;space-efficient discretizations;target
pose;Databases;Estimation;Kernel;Manipulators;Robot kinematics;Semantics},
doi={10.1109/Robotica.2013.6623540},
month={April},}
@INPROCEEDINGS{6610027,
author={J. Fu and W. Hao and T. White and Y. Yan and M. Jones and Y. K.
Jan},
booktitle={2013 35th Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC)},
title={Capturing and analyzing wheelchair maneuvering patterns with
mobile cloud computing},
year={2013},
pages={2419-2422},
abstract={Power wheelchairs have been widely used to provide independent
mobility to people with disabilities. Despite great advancements in
power wheelchair technology, research shows that wheelchair related
accidents occur frequently. To ensure safe maneuverability, capturing
wheelchair maneuvering patterns is fundamental to enable other research,
such as safe robotic assistance for wheelchair users. In this study, we
propose to record, store, and analyze wheelchair maneuvering data by
means of mobile cloud computing. Specifically, the accelerometer and
gyroscope sensors in smart phones are used to record wheelchair
maneuvering data in real-time. Then, the recorded data are periodically
transmitted to the cloud for storage and analysis. The analyzed results
are then made available to various types of users, such as mobile phone
users, traditional desktop users, etc. The combination of mobile
computing and cloud computing leverages the advantages of both
techniques and extends the smart phone's capabilities of computing and
data storage via the Internet. We performed a case study to implement
the mobile cloud computing framework using Android smart phones and
Google App Engine, a popular cloud computing platform. Experimental
results demonstrated the feasibility of the proposed mobile cloud
computing framework.},
keywords={accelerometers;cloud computing;data analysis;data
recording;handicapped aids;medical information systems;mobile
computing;sensors;smart phones;telemedicine;wheelchairs;Android smart
phone;Google App Engine;Internet;accelerometer sensor;desktop
user;gyroscope sensor;mobile cloud computing;mobile phone user;robotic
assistance;wheelchair maneuvering data analysis;wheelchair maneuvering
data recording;wheelchair maneuvering data storage;wheelchair
maneuvering pattern analysis;wheelchair maneuvering pattern
capturing;wheelchair user;Accelerometers;Cloud
computing;Gyroscopes;Mobile communication;Sensors;Smart
phones;Wheelchairs},
doi={10.1109/EMBC.2013.6610027},
ISSN={1094-687X},
month={July},}
@INPROCEEDINGS{6610212,
author={K. Dermitzakis and A. Ioannides and H. t. Lin},
booktitle={2013 35th Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC)},
title={Robotic thumb grasp-based range of motion optimisation},
year={2013},
pages={3163-3166},
abstract={With the thumb serving an important role in the function of
the human hand, improving robotic prosthetic thumb functionality will
have a direct impact on the prosthesis itself. So far, no significant
work exists that examines the ranges of motion a prosthetic thumb should
exhibit; many myoelectric prostheses arbitrarily select them. We
question this design practice as we expect a significant functional
volume reduction for performing certain activities vs. the maximum
obtainable workspace. To this end, we compare and contrast four
anatomically-accurate thumb models. We quantify their angular ranges of
motion by generating point clouds of end-effector positions, and by
computing their alpha-shape bounded volumes. Examining the function of
the thumb for several grasps, we identify a 76% reduction of the
required workspace volume vis-a-vis the maximum volume of a “`generic'”
human thumb.},
keywords={biomechanics;medical robotics;prosthetics;alpha-shape bounded
volume;end-effector position;functional volume reduction;human hand
function;myoelectric prostheses;point cloud generation;range of motion
optimisation;robotic human thumb grasping;robotic prosthetic thumb
functionality;Joints;Kinematics;Prosthetics;Read only
memory;Robots;Shape;Thumb},
doi={10.1109/EMBC.2013.6610212},
ISSN={1094-687X},
month={July},}
@INPROCEEDINGS{6595932,
author={D. Palossi and F. Tombari and S. Salti and M. Ruggiero and L. Di
Stefano and L. Benini},
booktitle={2013 IEEE Conference on Computer Vision and Pattern
Recognition Workshops},
title={GPU-SHOT: Parallel Optimization for Real-Time 3D Local Description},
year={2013},
pages={584-591},
abstract={The fields of 3D computer vision, 3D robotic perception and
photogrammetry rely more and more heavily on matching 3D local
descriptors, computed on a small neighborhood of a point cloud or a
mesh, to carry out tasks such as point cloud registration, 3D object
recognition and pose estimation in clutter, SLAM, 3D object retrieval.
One major drawback of these applications is currently the high
computational cost of processing 3D point clouds, with the 3D descriptor
computation representing one of the main bottlenecks. In this paper we
explore the optimization for parallel architectures of the recently
proposed SHOT descriptor [22] and of its extension to RGB-D data [23].
Even though some steps of the original algorithm are not directly
suitable for parallel optimization, we are able to obtain notable
speed-ups with respect to the CPU implementation. We also show an
application of our optimization to 3D object recognition in clutter,
where the proposed parallel implementation allows for real-time 3D local
description.},
keywords={graphics processing units;object
recognition;optimisation;parallel architectures;3D computer vision;3D
descriptor computation;3D local descriptor matching;3D object
recognition;3D object retrieval;3D point clouds;3D robotic
perception;CPU implementation;GPU-SHOT;RGB-D data;SLAM;mesh;parallel
architecture optimization;photogrammetry;point cloud registration;pose
estimation;real-time 3D local description;Covariance matrices;Graphics
processing units;Histograms;Image color
analysis;Indexes;Optimization;Three-dimensional displays;3D
descriptor;3D feature;3D object recognition;GPU optimization;SHOT},
doi={10.1109/CVPRW.2013.88},
ISSN={2160-7508},
month={June},}
@ARTICLE{6578563,
author={O. S. Gedik and A. A. Alatan},
journal={IEEE Transactions on Cybernetics},
title={3-D Rigid Body Tracking Using Vision and Depth Sensors},
year={2013},
volume={43},
number={5},
pages={1395-1405},
abstract={In robotics and augmented reality applications, model-based
3-D tracking of rigid objects is generally required. With the help of
accurate pose estimates, it is required to increase reliability and
decrease jitter in total. Among many solutions of pose estimation in the
literature, pure vision-based 3-D trackers require either manual
initializations or offline training stages. On the other hand, trackers
relying on pure depth sensors are not suitable for AR applications. An
automated 3-D tracking algorithm, which is based on fusion of vision and
depth sensors via extended Kalman filter, is proposed in this paper. A
novel measurement-tracking scheme, which is based on estimation of
optical flow using intensity and shape index map data of 3-D point
cloud, increases 2-D, as well as 3-D, tracking performance
significantly. The proposed method requires neither manual
initialization of pose nor offline training, while enabling highly
accurate 3-D tracking. The accuracy of the proposed method is tested
against a number of conventional techniques, and a superior performance
is clearly observed in terms of both objectively via error metrics and
subjectively for the rendered scenes.},
keywords={Kalman filters;computer vision;image sensors;image
sequences;nonlinear filters;object tracking;pose estimation;rendering
(computer graphics);3D point cloud;3D rigid body tracking;automated 3D
tracking algorithm;depth sensors;error metrics;extended Kalman
filter;intensity data;manual initializations;measurement-tracking
scheme;model-based 3D rigid object tracking;offline training
stages;optical flow estimation;pose estimation;pure vision-based 3D
trackers;scene rendering;shape index map data;Cameras;Feature
extraction;Image edge detection;Optical sensors;Optical variables
measurement;Solid modeling;3-D tracking;RGBD data fusion;extended Kalman
filter;model-based tracking;Actigraphy;Algorithms;Artificial
Intelligence;Computer Peripherals;Computer Simulation;Computer
Systems;Humans;Image Enhancement;Imaging, Three-Dimensional;Pattern
Recognition, Automated;Transducers;Video Games;Whole Body Imaging},
doi={10.1109/TCYB.2013.2272735},
ISSN={2168-2267},
month={Oct},}
@INPROCEEDINGS{6577892,
author={H. M. Nguyen and B. C. Wünsche and P. Delmas and C. Lutteroth
and W. van der Mark},
booktitle={2013 6th International Conference on Human System
Interactions (HSI)},
title={High resolution 3D content creation using unconstrained and
uncalibrated cameras},
year={2013},
pages={637-644},
abstract={An increasing number of applications require 3D content.
However, its creation from real-world data either necessitates expensive
equipment, artistic skills, or is constrained, for example, by the range
of the utilized sensors. Image-based modeling is rapidly increasing in
popularity since cameras are very affordable, widely available, and have
a wide image acquisition range suitable for objects of vastly different
size. The technique is especially suitable for mobile robotics involving
low cost equipment and robots with a light payload, for example, small
UAVs. In this paper we describe a novel image-based modeling system,
which produces high-quality 3D content automatically from a collection
of unconstrained and uncalibrated 2D images. The system estimates camera
parameters and a 3D scene geometry using Structure-from-Motion (SfM) and
Bundle Adjustment techniques. The point cloud density of 3D scene
components is enhanced by exploiting silhouette information of the
scene. This hybrid approach dramatically improves the reconstruction of
objects with few visual features, for example, unicolored objects, and
improves surface smoothness. A high quality texture is created by
parameterizing the reconstructed objects using a segmentation and
charting approach which also works for objects which are not
homeomorphic to a sphere. The resulting parameter space contains one
chart for each surface segment. A texture map is created by back
projecting the best fitting input images onto each surface segment, and
smoothly fusing them together over the corresponding chart by using
graph-cut techniques.},
keywords={cameras;image fusion;image reconstruction;image
resolution;image segmentation;image texture;3D scene geometry;best
fitting input image projection;bundle adjustment techniques;charting
method;high quality 3D content;high quality texture;high resolution 3D
content creation;image based modeling system;image fusion;image
segmentation;real world data;reconstructed object;silhouette
information;structure from motion;surface segment;uncalibrated
camera;unconstrained camera;visual feature;Cameras;Geometry;Image
reconstruction;Image segmentation;Surface reconstruction;Surface
texture;Surface treatment},
doi={10.1109/HSI.2013.6577892},
ISSN={2158-2246},
month={June},}
@INPROCEEDINGS{6566641,
author={U. Asif and M. Bennamoun and F. Sohel},
booktitle={2013 IEEE 8th Conference on Industrial Electronics and
Applications (ICIEA)},
title={Real-time pose estimation of rigid objects using RGB-D imagery},
year={2013},
pages={1692-1699},
abstract={Using full scale (480×640) RGB-D imagery, we here present an
approach for tracking 6d pose of rigid objects at runtime frequency of
up to 15fps. This approach is useful for robotic perception systems to
efficiently track object's pose during camera movements in tabletop
manipulation tasks with high detection rate and real-time performance.
Specifically, appearance-based feature correspondences are used for
initial object detection. We make use of Oriented Brief (ORB) feature
key-points to perform fast segmentation of object candidates in the 3d
point cloud. The task of 6d pose estimation is handled in the Cartesian
space by finding an interest window around the segmented object and 3d
geometry operations. The interest window is later used for feature
extraction in the subsequent camera frames to speed up the object
detection process. This also allows for an efficient pose tracking of
scenes where there are significantly large false matches between feature
correspondences due to scene clutter. Our approach was tested using an
RGB-D dataset comprising of scenes from video sequences of tabletops
with multiple objects in household environments. Experiments show that
our approach is capable of performing 3d segmentation followed by 6d
pose tracking at higher frame rates compared to existing techniques.},
keywords={clutter;feature extraction;image segmentation;image
sequences;object detection;pose estimation;video signal processing;3d
point cloud;6d pose estimation;Cartesian space;ORB feature
key-points;RGB-D imagery;appearance-based feature correspondences;fast
object candidate segmentation;feature extraction;frame rates;household
environments;object detection process;oriented brief feature
key-points;real-time pose estimation;rigid objects;runtime
frequency;scene clutter;scene pose tracking;tabletops;video
sequences;Cameras;Estimation;Feature extraction;Image
segmentation;Object detection;Robots;Vectors;3d segmentation;feature
matching;object detection;robotic grasping;tabletop manipulation},
doi={10.1109/ICIEA.2013.6566641},
ISSN={2156-2318},
month={June},}
@INPROCEEDINGS{6556352,
author={S. P. Baker and R. W. Sadowski},
booktitle={2013 IEEE Conference on Technologies for Practical Robot
Applications (TePRA)},
title={GPU assisted processing of point cloud data sets for ground
segmentation in autonomous vehicles},
year={2013},
pages={1-6},
abstract={In autonomous ground systems, developing a clear model of the
surroundings is crucial for operating in any environment.
Three-dimensional light detection and ranging (LIDAR) sensors, such as
the Velodyne HDL-64E S2, are powerful tools for robotic perception.
However, these sensors generate large data sets exceeding one million
points per second that can be difficult to use on space, power, and
processing constrained platforms. We report on GPU assisted processing
within a Robotic Operating System (ROS) environment capable of achieving
greater than an order of magnitude reduction in point cloud ground
segmentation processing time using a gradient field algorithm with only
a small increase in power consumption.},
keywords={edge detection;graphics processing units;image
segmentation;optical radar;robot vision;GPU assisted processing;LIDAR
sensors;ROS environment;Velodyne HDL-64E S2;autonomous ground
systems;autonomous vehicles;gradient field algorithm;point cloud data
sets;point cloud ground segmentation;power consumption;robotic operating
system;robotic perception;three-dimensional light detection and
ranging;Distance measurement;Graphics processing units;Hardware;Image
resolution;Image segmentation;Portable computers;Robots;GPU;LIDAR;ground
segmentation;point cloud},
doi={10.1109/TePRA.2013.6556352},
ISSN={2325-0526},
month={April},}
@INPROCEEDINGS{6518558,
author={J. L. Martínez and A. J. Reina and J. Morales and A. Mandow and
A. J. García-Cerezo},
booktitle={2013 IEEE International Conference on Mechatronics (ICM)},
title={Using multicore processors to parallelize 3D point cloud
registration with the Coarse Binary Cubes method},
year={2013},
pages={335-340},
abstract={This paper pursues speeding up 3D point cloud matching, which
is crucial for mobile robotics. In previous work, we devised the Coarse
Binary Cubes (CBC) method for fast and accurate registration of 3D
scenes based on an integer objective function. Instead of point distance
calculations, the method optimizes the number of coincident binary cubes
between a pair of range images. In this paper, we propose taking
advantage of widespread multicore and multithreaded processors to
further speed-up CBC by parallel evaluation of prospective solutions in
a globalized Nelder-Mead search. A performance analysis on two types of
multicore processors is offered for indoor and outdoor scans from a 3D
laser rangefinder. The proposed solution achieves a computational time
gain close to the number of physical cores.},
keywords={computational complexity;control engineering computing;laser
ranging;mobile robots;multi-threading;multiprocessing systems;search
problems;3D laser rangefinder;3D point cloud registration;3D scenes;CBC
method;coarse 3D point cloud matching;coarse binary cubes
method;coincident binary cubes;computational time;globalized Nelder-Mead
search;indoor scans;integer objective function;mobile robotics;multicore
processors;multithreaded processors;outdoor scans;parallel
evaluation;performance analysis;physical cores;Estimation;Image
resolution;Linear programming;Multicore processing;Optimization;Program
processors;Three-dimensional displays},
doi={10.1109/ICMECH.2013.6518558},
month={Feb},}
@ARTICLE{6328266,
author={S. Li and L. Chen and X. Xiong and J. Tao and L. Su and D. Han
and Y. Liu},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Retrieval of the Haze Optical Thickness in North China Plain
Using MODIS Data},
year={2013},
volume={51},
number={5},
pages={2528-2540},
abstract={China's industrialized regions have seen increasing occurrence
of heavy haze caused by severe particle pollution. However, aerosol
retrieval under these circumstances is often excluded from NASA's
Moderate Resolution Imaging Spectrometer (MODIS) aerosol products due to
cloud mask and suspected high surface reflectance. An algorithm to
retrieve the haze aerosol optical thickness (HAOT) is developed using
MODIS data to supplement the current MODIS retrieval algorithm. This
method includes 1) haze identification, 2) the generation of a surface
reflectance database using MODIS data in hazy conditions, and 3) the
development of haze aerosol models with four aerosol components
simulated by a global 3-D atmospheric chemical transport model
(GEOS-Chem). This algorithm was used in combination with the MODIS dense
dark vegetation algorithm to retrieve 1 km HAOT over North China Plain
from March to September of 2008. The values of the retrieved HAOT values
are mostly between 0.7–3, with a correlation coefficient of 0.82 with
the Aerosol Robotic NETwork observations and a 19% mean relative
difference. Retrieval uncertainties associated with the errors in haze
detection, surface reflectance, and haze models were analyzed using
ground measurements.},
keywords={Aerosols;Air pollution;Algorithm design and
analysis;Atmospheric modeling;Land surface;MODIS;Reflectivity;Aerosol
model;GEOS-Chem;Moderate Resolution Imaging Spectrometer (MODIS);haze
aerosol optical thickness (HAOT);surface reflectance},
doi={10.1109/TGRS.2012.2214038},
ISSN={0196-2892},
month={May},}
@INPROCEEDINGS{6492470,
author={B. Tudjarov and J. Botzheim and N. Kubota},
booktitle={2012 International Symposium on Micro-NanoMechatronics and
Human Science (MHS)},
title={Facilitation of Cognitive Robotics by web based computational
intelligent models},
year={2012},
pages={144-148},
abstract={The aim of this research is to combine computational
intelligent techniques with web based modeling and simulation to assure
functional and E-learning means in the field of Cognitive Robotics. A
web based evolutionary computation experimental tool has been created
based on cloud computing approach and bacterial memetic algorithm. The
technique is applied for generating locomotion of a six legged robot.},
keywords={cloud computing;cognitive systems;digital
simulation;educational robots;evolutionary computation;legged
locomotion;E-learning;Web-based computational intelligent
model;Web-based evolutionary computation experimental tool;Web-based
modeling;Web-based simulation;bacterial memetic algorithm;cloud
computing;cognitive robotics;locomotion generation;six-legged robot},
doi={10.1109/MHS.2012.6492470},
month={Nov},}
@INPROCEEDINGS{6484587,
author={C. K. Wong and P. P. K. Lim},
booktitle={2012 19th International Conference on Mechatronics and
Machine Vision in Practice (M2VIP)},
title={Processing of point cloud data from ToF camera for the
localisation of ground-based crop},
year={2012},
pages={184-189},
abstract={This paper describes an approach that analyses point cloud
information from a Time-of-Flight (ToF) camera to identify the location
of foremost spring onions along the crop bed, for the intention of
robotic manipulation. The process uses a combination of 2D image
processing on the amplitude data, as well as depth analysis on the point
cloud information, extracted from the camera to locate the desired
object. Whilst the experimental results demonstrated the robustness of
this approach, further testing is required to determine the ability of
system to cope with different scenarios that exists in the naturally
varying environment. Finally, it is important to validate the accuracy
of the system in the physical world by integrating the localization
algorithm to a robotic manipulation system.},
keywords={crops;industrial manipulators;robot vision;2D image
processing;ToF camera;crop bed;ground-based crop localisation;location
identification;point cloud data processing;robotic manipulation;spring
onion;time-of-flight camera;Agriculture;Cameras;Histograms;Image
segmentation;Robot vision systems;Springs;3D object localisation;ToF
camera;point cloud data},
month={Nov},}
@INPROCEEDINGS{6473354,
author={A. Breitenmoser and R. Siegwart},
booktitle={2012 2nd International Conference on Applied Robotics for the
Power Industry (CARPI)},
title={Surface reconstruction and path planning for industrial
inspection with a climbing robot},
year={2012},
pages={22-27},
abstract={Inspection and maintenance tasks in the power industry often
require tools to be moved smoothly in direct contact or close proximity
over surfaces of curved tube-like structures. This paper addresses the
problem of navigating robotic devices, namely a wheeled climbing robot
carrying an inspection tool, on surfaces as they typically appear in
steam chests of power plants. The surfaces are modeled by triangle
meshes. Several state-of-the-art surface reconstruction methods are
evaluated, and meshes are constructed for 3D point clouds taken with
different laser scanners and for tube structures varying in shape,
dimension and surface property. Each mesh represents an embedded graph,
and thus directly enables graph search methods to plan paths of triangle
strips over the surface. Discrete path planning and continuous robot
control are combined to a hybrid system, which steers the robot smoothly
along the triangle strip to the target regions on the surface. The
navigation approach applies to robotic surface inspection, and is
verified in simulation experiments by moving a robot on triangle meshes
reconstructed from real data.},
keywords={graph theory;industrial robots;inspection;maintenance
engineering;mesh generation;mobile robots;optical radar;optical
scanners;path planning;pipes;search problems;steam power
stations;strips;surface reconstruction;3D point clouds;continuous robot
control;curved tube-like structures;discrete path planning;embedded
graph;graph search method;hybrid system;industrial inspection;inspection
tool;laser scanners;maintenance tasks;navigation approach;power
industry;power plants;robotic devices;robotic surface inspection;steam
chests;surface property;surface reconstruction method;target
regions;triangle mesh reconstruction;triangle strip;triangle strips;tube
structures;wheeled climbing robot;Inspection;Navigation;Robot sensing
systems;Service robots;Strips;Surface reconstruction},
doi={10.1109/CARPI.2012.6473354},
month={Sept},}
@ARTICLE{6375774,
author={J. O. Hamblen and G. M. E. van Bekkum},
journal={IEEE Transactions on Education},
title={An Embedded Systems Laboratory to Support Rapid Prototyping of
Robotics and the Internet of Things},
year={2013},
volume={56},
number={1},
pages={121-128},
abstract={This paper describes a new approach for a course and
laboratory designed to allow students to develop low-cost prototypes of
robotic and other embedded devices that feature Internet connectivity,
I/O, networking, a real-time operating system (RTOS), and
object-oriented C/C++. The application programming interface (API)
libraries provided permit students to work at a higher level of
abstraction. A low-cost 32-bit SOC RISC microcontroller module with
flash memory, numerous I/O interfaces, and on-chip networking hardware
is used to build prototypes. A cloud-based C/C++ compiler is used for
software development. All student files are stored on a server, and any
Web browser can be used for software development. Breadboards are used
in laboratory projects to rapidly build prototypes of robots and
embedded devices using the microcontroller, networking, and other I/O
subsystems on small breakout boards. The commercial breakout boards used
provide a large assortment of modern sensors, drivers, display ICs, and
external I/O connectors. Resources provided include eBooks, laboratory
assignments, and extensive Wiki pages with schematics and sample
microcontroller application code for each breakout board.},
keywords={C++ language;Internet;Internet of Things;application program
interfaces;cloud computing;computer aided instruction;control
engineering computing;educational courses;electrical engineering
computing;electrical engineering education;input-output
programs;microcontrollers;online front-ends;program
compilers;robots;software prototyping;system-on-chip;32-bit SOC RISC
microcontroller module;API;I-O interfaces;I-O subsystems;Internet
connectivity;Internet-of-things;RTOS;Web browser;Wiki pages;application
programming interface libraries;breadboards;breakout boards;cloud-based
C compiler;cloud-based C++ compiler;display IC;eBooks;embedded
devices;embedded systems laboratory;external I-O connectors;flash
memory;laboratory assignments;microcontroller application
code;object-oriented C;object-oriented C++;onchip networking
hardware;rapid prototyping support;real-time operating
system;robotics;software development;Electronic
publishing;Hardware;Information services;Internet;Laboratories;Program
processors;Robots;Design project;embedded
systems;mechatronics;microcontroller;microprocessor;networking;real-time
operating system (RTOS);robotics},
doi={10.1109/TE.2012.2227320},
ISSN={0018-9359},
month={Feb},}
@INPROCEEDINGS{6412246,
author={H. W. Keat and L. S. Ming},
booktitle={TENCON 2012 IEEE Region 10 Conference},
title={An investigation of the use of Kinect sensor for indoor navigation},
year={2012},
pages={1-5},
abstract={Robot navigation in an unknown and dynamic environment has
been a challenging research not only among researchers in the area of
Robotics, but also in many learning institutions. The prevailing school
of thought is to create more complex sensors to feed the robot with a
better vision. However, the trade-off for more complex sensors is cost.
Unfortunately cost constrain is unforgiving for university researchers
and robotics enthusiasts. This paper reports on an undertaking by the
authors on the implementation of robotic vision and navigation system
using Kinect sensor making it a low cost system for research purposes in
the area of robot navigation. For this system, the point cloud to laser
scan function was used to convert the point cloud data to laser scan.
The odometry and laser data were then passed on to the SLAM Gmapping
function to carry out mapping of the surrounding. Once the map was
generated, the navigation stack will plan and send command to achieve
autonomous navigation.},
keywords={indoor communication;remote consoles;robots;sensors;Kinect
sensor;SLAM Gmapping function;autonomous navigation;indoor
navigation;laser scan function;learning institutions;point cloud;robot
navigation;Brightness;Lasers;Mobile robots;Navigation;Simultaneous
localization and mapping;Gmapping;Kinect sensor;autonomous;indoor
navigation;laser scan;point cloud;robot vision},
doi={10.1109/TENCON.2012.6412246},
ISSN={2159-3442},
month={Nov},}
@INPROCEEDINGS{6398208,
author={R. Doriya and P. Chakraborty and G. C. Nandi},
booktitle={2012 International Conference on Communication, Information
Computing Technology (ICCICT)},
title={ #x2018;Robot-Cloud #x2019;: A framework to assist heterogeneous
low cost robots},
year={2012},
pages={1-5},
abstract={This paper gives a framework to offer assistance to low cost
ROS (Robot Operating System) supported heterogeneous robots in a large
environment through cloud. To build such a system `Robot-Cloud' is
prepared which extends the functionality of a robot. A `Robot-Cloud' is
designed and implemented with components like cloud controller, ROS
master node, storage unit, Map-reduce computing cluster and robotic
services. In this system every robot is facilitated with ROS capability
that helps to provide abstraction over hardware, heterogeneity and
communication over TCP/IP. All the robots communicate with a master node
present at the cloud controller to avail communication with other robots
and to request services from the cloud. Our system supports all three
basic service models i.e. Saas, PaaS and IaaS by incorporating typical
SOA (Service Oriented Architecture) functionality. This approach can
also be utilized to achieve a common goal for the networked robots.
Finally, some simulation results are given to support the claimed
framework.},
keywords={cloud computing;control engineering
computing;robots;service-oriented architecture;ROS master
node;SOA;assist heterogeneous low cost robot framework;cloud
controller;map reduce computing cluster services;map reduce computing
robotic services;robot cloud;robot operating system;service oriented
architecture;storage unit;Cloud computing;Computational
modeling;Computers;Navigation;Operating systems;Robots;Service oriented
architecture;Cloud Computing;Cloud Robots;Low Cost Robots;Networked
Robots;ROS;SOA},
doi={10.1109/ICCICT.2012.6398208},
month={Oct},}
@INPROCEEDINGS{6390816,
author={Z. Siqi and C. Rongxin and X. Demin},
booktitle={Proceedings of the 31st Chinese Control Conference},
title={Effectiveness of Infotaxis algorithm for searching in dilute
conditions},
year={2012},
pages={5048-5053},
abstract={Both tracking scents and locating odor sources are challenges
in robotics. The odor plume is not a continuous cloud but consists of
intermittent odor patches dispersed by the wind. Far from the source,
the probability of encountering one of these patches vanishes. In such
dilute conditions, the Infotaxis algorithm is first `explore' the
environment and gather information, then `exploit' current knowledge and
direct toward the estimated source location. We quantitatively analyze
its success probability, mean path length, percentage of
oriented-movements, and average search time with different initial
distances from the source. Through comparison with the Dung Beetle
algorithm, we show the effectiveness of Infotaxis algorithm for the
scenario with a single source and a single searcher in a dilute
condition.},
keywords={electronic noses;mobile robots;search
problems;telerobotics;Dung Beetle algorithm;Infotaxis
Algorithm;autonomous mobile robots;average search time;dilute condition
search;intermittent odor patches;mean path length;odor source
localization;oriented-movement percentage;plume odor;robotics;scent
tracking;success probability;turbulent flow
environments;Animals;Chemicals;Entropy;Lattices;Robots;Trajectory;Chemical
plume;Infotaxis;Olfactory search;Robot},
ISSN={1934-1768},
month={July},}
@INPROCEEDINGS{6386005,
author={K. M. Varadarajan and E. Potapova and M. Vincze},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Attention driven grasping for clearing a heap of objects},
year={2012},
pages={2035-2042},
abstract={Generation of grasps for automated object manipulation in
cluttered scenarios presents major challenges for various modules of the
pipeline such as 2D/3D visual processing, 3D modeling, grasp hypothesis
generation, grasp planning and path planning. In this paper, we present
a solution framework for solving a complex instance of the problem -
represented by a heap of unknown and unstructured objects in a bounded
environment - in our case, a box; with the goal being removing all
objects in the box using an attention driven object modeling approach to
cognitive grasp planning. The focus of the algorithm delves on Grasping
by Components (GBC), with a prioritization scheme derived from scene
based attention and attention driven segmentation. In order to overcome
the traditional challenge of segmentation performing poorly in cluttered
scenes, we employ a novel active segmentation approach suited to our
scenario. While the attention module helps prioritize objects in the
heap and salient regions, the GBC scheme segments out parts and
generates grasp hypotheses for each part. GBC is a very important
component of any scalable and holistic grasping system since it
abstracts point cloud object data with parametric shapes and no apriori
knowledge (such as 3D models) is required. Earlier work in 3D model
building (such as CAD based, simple geometries, bounding boxes,
Superquadrics etc.) have depended on precise shape and pose recognition
as well as exhaustive training to learn or exhaustive searching in grasp
space to generate good grasp hypotheses. These methods are not scalable
for real-time scenarios, complex shapes and unknown environments - key
challenges in robotic grasping. In order to alleviate this concern, we
present a novel parametric algorithm to estimate grasp points and
approach vectors from the 3D parametric shape model, along with
innovative schemes to optimize the computation of the parametric models
as well as to refine the generated grasp hypotheses based on - he scene
information to aid path planning. We present evaluation of our complex
grasping pipeline for cluttered heaps through a series of test sequences
involving removal of objects from a box, along with evaluations for our
attention mechanisms, active segmentation, 3D model fitting
optimizations and quality of our grasp hypotheses.},
keywords={image segmentation;manipulators;path planning;pose
estimation;robot vision;shape recognition;solid modelling;3D model
building;3D model fitting optimizations;3D parametric shape
model;GBC;active segmentation;active segmentation approach;attention
driven grasping;attention driven object modeling approach;attention
driven segmentation;automated object manipulation;cognitive grasp
planning;exhaustive searching;grasp generation;grasp hypothesis
quality;grasp point estimation;grasping by components;object heap
clearance;path planning;point cloud object data;pose
recognition;prioritization scheme;robotic grasping;scene based
attention;shape recognition;Fitting;Grasping;Image edge detection;Object
segmentation;Pipelines;Shape;Solid modeling;Active Segmentation;Approach
Vectors;Attention;Grasp Hypotheses;Superquadrics},
doi={10.1109/IROS.2012.6386005},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{6386359,
author={S. Lee and J. Kim and M. Lee and K. Yoo and L. G. Barajas and R.
Menassa},
booktitle={2012 IEEE International Conference on Automation Science and
Engineering (CASE)},
title={3D visual perception system for bin picking in automotive
sub-assembly automation},
year={2012},
pages={706-713},
abstract={Industries are being swept up by the tide of market
innovations originated from pervasive application of Robotics and
Automation (RA). Given the critical role of RA in industry, it has
become relevant for RA to have human-like decision making capabilities.
Such enablers intrinsically require the use of flexible and robust 3D
perception and control systems. In the process of automating complex
automotive sub-assemblies, 3D vision-based recognition as well as
grasping of complex objects is required not only for detection and
categorization but also for pose estimation and robotic pick-and-place
operations. In this paper, we propose a novel 3D visual perception
system for sub-assembly automation based on a structured light 3D vision
system. We use a novel geometric surface primitive patch segmentation
approach based on Hough transforms to obtain accurate surface normal
estimations from 3D point clouds for the identification of patch
primitives. The most relevant primitives for our application include
planar, cylindrical, conic and spherical surface patches. We extract
primitive surface patches from automotive CAD models in DXF format. The
models are then decomposed to simple entities such as planar polygons,
vertexes and lines. Our resulting models based on the 3D point clouds
are composed only by simple planes and cylinders. Our system takes
advantage of the available CAD data for both object recognition and for
pose estimation. Our experimental results demonstrate that we can
achieve, in only a few seconds, a highly accurate pose and object class
estimation.},
keywords={CAD;Hough transforms;assembling;automobile industry;bin
packing;industrial robots;pose estimation;visual perception;3D point
clouds;3D vision-based recognition;3D visual perception system;DXF
format;Hough transforms;automotive CAD models;automotive subassembly
automation;bin picking;geometric surface primitive patch segmentation
approach;human-like decision making;industrial robotics;industry
automation;market innovations;pose estimation;robotic pick-and-place
operations;Automation;Design automation;Estimation;Object
recognition;Robots;Solid modeling;Vectors},
doi={10.1109/CoASE.2012.6386359},
ISSN={2161-8070},
month={Aug},}
@INPROCEEDINGS{6385470,
author={Y. Li and E. B. Olson},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={IPJC: The Incremental Posterior Joint Compatibility test for fast
feature cloud matching},
year={2012},
pages={3467-3474},
abstract={One of the fundamental challenges in robotics is
data-association: determining which sensor observations correspond to
the same physical object. A common approach is to consider groups of
observations simultaneously: a constellation of observations can be
significantly less ambiguous than the observations considered
individually. The Joint Compatibility Branch and Bound (JCBB) test is
the gold standard method for these data association problems. But its
computational complexity and its sensitivity to non-linearities limit
its practical usefulness. We propose the Incremental Posterior Joint
Compatibility (IPJC) test. While equivalent to JCBB on linear problems,
it is significantly more accurate on non-linear problems. When used for
feature-cloud matching (an important special case), IPJC is also
dramatically faster than JCBB. We demonstrate the advantages of IPJC
over JCBB and other commonly-used methods on both synthetic and
real-world datasets.},
keywords={SLAM (robots);robot vision;sensor
fusion;IPJC;JCBB;commonly-used methods;data association
problems;data-association;fast feature cloud matching;incremental
posterior joint compatibility test;joint compatibility branch and bound
test;robotics;Feature extraction;Joints;Noise;Simultaneous localization
and mapping;Vectors;Data association;SLAM;joint compatibility test},
doi={10.1109/IROS.2012.6385470},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{6386012,
author={F. Rydén and H. J. Chizeck},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Forbidden-region virtual fixtures from streaming point clouds:
Remotely touching and protecting a beating heart},
year={2012},
pages={3308-3313},
abstract={Several established methods for remote touching using
non-contact sensors exist. Applications for these methods are primarily
within the field of robotic teleoperation. In surgical robotics it would
be useful to not only touch, but also be able to maintain a distance
from a certain organ. The latter can be done using non-contact sensors
such as cameras. The novelty in this paper is the idea of combining
forbidden-region virtual fixtures with haptic rendering from streaming
point clouds. This is then used to protect as well as remotely touch a
beating heart without any a priori knowledge of the heart geometry (such
as from CT/MR scans).},
keywords={cardiology;haptic interfaces;medical robotics;rendering
(computer graphics);surgery;telerobotics;beating heart
protection;beating heart remote touching;cameras;forbidden-region
virtual fixtures;haptic rendering;heart geometry;noncontact
sensors;robotic teleoperation;streaming point clouds;surgical
robotics;Force;Haptic interfaces;Heart;Hip;Rendering (computer
graphics);Robots;Vectors},
doi={10.1109/IROS.2012.6386012},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{6343067,
author={M. Draelos and N. Deshpande and E. Grant},
booktitle={2012 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={The Kinect up close: Adaptations for short-range imaging},
year={2012},
pages={251-256},
abstract={With proper calibration of its color and depth cameras, the
Kinect can capture detailed color point clouds at up to 30 frames per
second. This capability positions the Kinect for use in robotics as a
low-cost navigation sensor. Thus, techniques for efficiently calibrating
the Kinect depth camera and altering its optical system to improve
suitability for imaging short-range obstacles are presented. To perform
depth calibration, a calibration rig and software were developed to
automatically map raw depth values to object depths. The calibration rig
consisted of a traditional chessboard calibration target with easily
locatable features in depth at its exterior corners that facilitated
software extraction of corresponding object depths and raw depth values.
To modify the Kinect's optics for improved short-range imaging, Nyko's
Zoom adapter was used due to its simplicity and low cost. Although
effective at reducing the Kinect's minimum range, these optics
introduced pronounced distortion in depth. A method based on capturing
depth images of planar objects at various depths produced an empirical
depth distortion model for correcting such distortion in software.
Together, the modified optics and the empirical depth undistortion
procedure demonstrated the ability to improve the Kinect's resolution
and decrease its minimum range by approximately 30%.},
keywords={calibration;collision avoidance;image colour analysis;image
sensors;mobile robots;robot vision;Kinect;Nyko zoom adapter;calibration
rig;color cameras;color point clouds;depth calibration;depth
cameras;depth distortion model;low-cost navigation
sensor;robotics;short-range imaging;short-range
obstacles;Calibration;Cameras;Lenses;Optical distortion;Robot sensing
systems},
doi={10.1109/MFI.2012.6343067},
month={Sept},}
@INPROCEEDINGS{6343046,
author={I. Dryanovski and W. Morris and R. Kaushik and J. Xiao},
booktitle={2012 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Real-time pose estimation with RGB-D camera},
year={2012},
pages={13-20},
abstract={An RGB-D camera is a sensor which outputs the distances to
objects in a scene in addition to their RGB color. Recent technological
advances in this area have introduced affordable devices in the robotics
community. In this paper, we present a real-time feature extraction and
pose estimation technique using the data from a single RGB-D camera.
First, a set of edge features are computed from the depth and color
images. The down-sampled point clouds consisting of the feature points
are aligned using the Iterative Closest Point algorithm in 3D space. New
features are aligned against a model consisting of previous features
from a limited number of past scans. The system achieves a 10 Hz update
rate running on a desktop CPU, using VGA resolution RGB-D scans.},
keywords={cameras;feature extraction;image colour analysis;image
sensors;pose estimation;robot vision;RGB color;RGB-D camera;VGA
resolution RGB-D scans;color images;depth images;down-sampled point
clouds;edge features;feature points;frequency 10 Hz;iterative closest
point algorithm;real-time feature extraction;real-time pose estimation
technique;robotics
community;sensor;Brightness;Cameras;Estimation;Feature extraction;Image
edge detection;Noise;Vectors},
doi={10.1109/MFI.2012.6343046},
month={Sept},}
@INBOOK{6278711,
author={Rolf Pfeifer and Bruce Blumberg and Jean-Arcady Meyer and
Stewart W. Wilson},
booktitle={From Animals to Animats 5:Proceedings of the Fifth
International Conference on Simulation of Adaptive Behavior},
title={A Society of Agents in Environmental Monitoring},
year={1998},
pages={447-452},
abstract={The evaluation of pollutant levels is a key aspect on the
issue of keeping a clean environment. Conventional techniques include
the utilisation of a fixed setup incorporating pollutant sensors.
However, these approaches are a very long way from an accurate
monitoring. Thus, to improve pollutant monitoring on a power plant
chimney, the use of robotic agent societies (mobile robots) is
suggested. This suggestion is adequate in pollutant monitoring when the
environment is hostile and/or the region to be sampled has large
dimensions. However, the implementation of a system incorporating
robotic agents raises complex technological problems. Before a set of
any kind of real robotic agents is implemented, an accurate evaluation
must be performed. What this paper describes is a simulated application
of small flying robotic agent societies (helicopter models) monitoring a
pollutant cloud. This simulation intends to show that an “intelligent”
search method works better than a systematic or random procedure. In
this kind of environment (dynamic and non-structured) and using mobile
robotics to meet a goal such as this, a behavioural control architecture
seems to meet the performance objectives. The behaviours designed to
control the agents are prepared to implement individual needs (survival
and navigation) and social needs (follow or gather group). The agents as
individuals are capable of performing such a mission, however, global
results are enhanced by social strategies.},
publisher={MIT Press},
isbn={9780262291385},
url={http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6278711},}
@INBOOK{6301077,
author={Hugh Durrant-Whyte and Nicholas Roy and Pieter Abbeel},
booktitle={Robotics:Science and Systems VII},
title={Monte Carlo Pose Estimation with Quaternion Kernels and the
Bingham Distribution},
year={2012},
pages={97-104},
abstract={The success of personal service robotics hinges upon reliable
manipulation of everyday household objects, such as dishes, bottles,
containers, and furniture. In order to accurately manipulate such
objects, robots need to know objects' full 6-DOF pose, which is made
difficult by clutter and occlusions. Many household objects have regular
structure that can be used to effectively guess object pose given an
observation of just a small patch on the object. In this paper, we
present a new method to model the spatial distribution of oriented local
features on an object, which we use to infer object pose given small
sets of observed local features. The orientation distribution for local
features is given by a mixture of Binghams on the hypersphere of unit
quaternions, while the local feature distribution for position given
orientation is given by a locally-weighted (Quaternion kernel)
likelihood. Experiments on 3D point cloud data of cluttered and
uncluttered scenes generated from a structured light stereo image sensor
validate our approach.},
publisher={MIT Press},
isbn={9780262305969},
url={http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6301077},}
@ARTICLE{6220900,
author={M. Bosse and R. Zlot and P. Flick},
journal={IEEE Transactions on Robotics},
title={Zebedee: Design of a Spring-Mounted 3-D Range Sensor with
Application to Mobile Mapping},
year={2012},
volume={28},
number={5},
pages={1104-1119},
abstract={Three-dimensional perception is a key technology for many
robotics applications, including obstacle detection, mapping, and
localization. There exist a number of sensors and techniques for
acquiring 3-D data, many of which have particular utility for various
robotic tasks. We introduce a new design for a 3-D sensor system,
constructed from a 2-D range scanner coupled with a passive linkage
mechanism, such as a spring. By mounting the other end of the passive
linkage mechanism to a moving body, disturbances resulting from
accelerations and vibrations of the body propel the 2-D scanner in an
irregular fashion, thereby extending the device's field of view outside
of its standard scanning plane. The proposed 3-D sensor system is
advantageous due to its mechanical simplicity, mobility, low weight, and
relatively low cost. We analyze a particular implementation of the
proposed device, which we call Zebedee, consisting of a 2-D
time-of-flight laser range scanner rigidly coupled to an inertial
measurement unit and mounted on a spring. The unique configuration of
the sensor system motivates unconventional and specialized algorithms to
be developed for data processing. As an example application, we describe
a novel 3-D simultaneous localization and mapping solution in which
Zebedee is mounted on a moving platform. Using a motion capture system,
we have verified the positional accuracy of the sensor trajectory. The
results demonstrate that the six-degree-of-freedom trajectory of a
passive spring-mounted range sensor can be accurately estimated from
laser range data and industrial-grade inertial measurements in real time
and that a quality 3-D point cloud map can be generated concurrently
using the same data.},
keywords={SLAM (robots);sensors;2D time-of-flight laser range scanner;3D
point cloud map quality;3D simultaneous localization and mapping
solution;SLAM;Zebedee;data processing;industrial-grade inertial
measurements;inertial measurement unit;mechanical simplicity;mobile
mapping;motion capture system;obstacle detection;passive linkage
mechanism;robotic tasks;six-degree-of-freedom trajectory;spring-mounted
3D range sensor design;standard scanning plane;three-dimensional
perception;Lasers;Simultaneous localization and
mapping;Springs;Timing;Trajectory;Mapping;range sensing;simultaneous
localization and mapping (SLAM)},
doi={10.1109/TRO.2012.2200990},
ISSN={1552-3098},
month={Oct},}
@INPROCEEDINGS{6309549,
author={T. Wiemann and A. Nuechter and J. Hertzberg},
booktitle={ROBOTIK 2012; 7th German Conference on Robotics},
title={A Toolkit for Automatic Generation of Polygonal Maps ?? Las Vegas
Reconstruction},
year={2012},
pages={1-6},
abstract={In this paper we present a new open source software package
for automatic generation of polygonal 3D maps from point cloud data for
robotic purposes called ??Las Vegas Reconstruction Toolkit?? [11]. The
implemented algorithms focus on minimizing both the computation costs
and optimization of the number of polygons in the generated maps.
Furthermore, we present two application examples: 6D self localization
and scene interpretation.},
keywords={Estimation;Face;Optimization;Robots;Sensors;Surface
reconstruction;Surface treatment},
month={May},}
@INPROCEEDINGS{6290895,
author={E. Rombokas and P. Brook and J. R. Smith and Y. Matsuoka},
booktitle={2012 4th IEEE RAS EMBS International Conference on Biomedical
Robotics and Biomechatronics (BioRob)},
title={Biologically inspired grasp planning using only orthogonal
approach angles},
year={2012},
pages={1656-1661},
abstract={One approach to robotic grasping is to compute hand
configurations, including contact locations, which would maximize a
variety of measures of grasp quality. There is evidence that
human-guided robotic grasps exhibit orthogonality of the wrist as a key
feature. Orthogonality of the hand to the object frame or surfaces is
often included in state-of-the art grasp synthesis algorithms, but here
we present a systematic study of the efficacy of orthogonality alone.
The orthogonality-alone planner works with a surprisingly good success
rate on a variety of physical objects using only a single exposure from
a depth camera and no object models whatsoever. Principal axes of an
object are identified from the point cloud, the approach angle is
determined from the second principal axis, and the orientation of the
hand is vertical from the first axis. When this technique is applied to
19 novel objects presented in front of a physical robot, utilizing
automatic object segmentation, the grasp success rate is 98.4%.},
keywords={computational geometry;human-robot interaction;image
segmentation;manipulators;planning (artificial intelligence);robot
vision;automatic object segmentation;biologically inspired grasp
planning;contact locations;grasp synthesis algorithms;hand configuration
computation;human-guided robotic grasping;orthogonal approach
angles;point cloud principal axes;Grasping;Humans;Robot kinematics;Robot
sensing systems;Solid modeling;Testing},
doi={10.1109/BioRob.2012.6290895},
ISSN={2155-1774},
month={June},}
@INPROCEEDINGS{6256260,
author={R. Tsuchiya and S. Shimazaki and T. Sakai and S. Terada and K.
Igarashi and D. Hanawa and K. Oguchi},
booktitle={2012 35th International Conference on Telecommunications and
Signal Processing (TSP)},
title={Simulation environment based on smartphones for Cloud computing
robots},
year={2012},
pages={96-100},
abstract={Cloud computing and smartphone technologies are becoming
pervasive in daily life. Cloud robotics, which apply such technologies
to robots, is capturing the spotlight as an attractive implementation
example. However, when implementing a real time robot service on a
wide-range network, it is desirable to grasp the influence of network
delay on the robot's actions and the user's experience in advance.
Therefore, we propose a simulation environment for Cloud computing
robots. This simulation environment replicates the WAN environment
between the robot and the Cloud server, and allows the behavior of the
Cloud computing robot to be observed in real time. We prototype and
evaluate the performance of the proposed simulation environment.},
keywords={cloud computing;control engineering computing;file
servers;service robots;smart phones;wide area networks;WAN
environment;cloud computing robots;cloud server;real time robot
service;simulation environment;smartphone technologies;wide area
network;Databases;Delay;Robot sensing systems;Servers;Smart phones;Time
factors;Android;Cloud Computing;Robot;Sensor Networks;Service-Oriented
Architecture;Smartphone},
doi={10.1109/TSP.2012.6256260},
month={July},}
@INPROCEEDINGS{6237743,
author={L. C. Goron and L. Tamas and G. Lazea},
booktitle={Proceedings of 2012 IEEE International Conference on
Automation, Quality and Testing, Robotics},
title={Classification within indoor environments using 3D perception},
year={2012},
pages={400-405},
abstract={Making sense out of human indoor environments is an essential
feature for robots. In this paper we present a system for the
classification of components inside these environments, starting from
our robotic platform to a simple yet robust labeling process. Our method
starts by acquiring multiple point clouds which are then registered into
one single dataset. An estimation of principle axes is performed and the
planar surfaces are segmented out. Further on, quadrilateral-like shapes
are estimated for each detected plane, by making use of edges. And
finally, since our classification approach relies on physical features,
the method analyses the relationship between the previously mentioned
shapes, as well as their physical sizes. To validate our approach, we
tested the method on different datasets, which were recorded inside our
office environment.},
keywords={optical scanners;robots;3D perception;components
classification;human indoor environments;multiple point clouds;office
environment;planar surfaces;principle axes estimation;quadrilateral-like
shapes;robotic platform;robust labeling process;Indoor
environments;Lasers;Measurement by laser beam;Robot kinematics;Robot
sensing systems;Shape},
doi={10.1109/AQTR.2012.6237743},
month={May},}
@INPROCEEDINGS{6225116,
author={W. Wohlkinger and A. Aldoma and R. B. Rusu and M. Vincze},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={3DNet: Large-scale object class recognition from CAD models},
year={2012},
pages={5384-5391},
abstract={3D object and object class recognition gained momentum with
the arrival of low-cost RGB-D sensors and enables robotics tasks not
feasible years ago. Scaling object class recognition to hundreds of
classes still requires extensive time and many objects for learning. To
overcome the training issue, we introduce a methodology for learning 3D
descriptors from synthetic CAD-models and classification of
never-before-seen objects at the first glance, where classification
rates and speed are suited for robotics tasks. We provide this in 3DNet
(3d-net.org), a free resource for object class recognition and 6DOF pose
estimation from point cloud data. 3DNet provides a large-scale
hierarchical CAD-model databases with increasing numbers of classes and
difficulty with 10, 50, 100 and 200 object classes together with
evaluation datasets that contain thousands of scenes captured with a
RGB-D sensor. 3DNet further provides an open-source framework based on
the Point Cloud Library (PCL) for testing new descriptors and
benchmarking of state-of-the-art descriptors together with pose
estimation procedures to enable robotics tasks such as search and
grasping.},
keywords={CAD;image classification;image colour analysis;image
sensors;intelligent robots;object recognition;pose estimation;public
domain software;3D descriptor learning;3D object recognition;3DNet;6DOF
pose estimation;PCL;classification rates;classification speed;descriptor
benchmarking;descriptor testing;free resource;grasping task;large-scale
hierarchical CAD model databases;large-scale object class
recognition;low-cost RGB-D sensors;object class recognition
scaling;object classification;open source framework;point cloud
data;point cloud library;robotics tasks;scenes;search task;synthetic CAD
models;training issues;Aircraft;Benchmark
testing;Containers;Horses;Irrigation;Robots},
doi={10.1109/ICRA.2012.6225116},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224591,
author={Jaeyong Sung and C. Ponce and B. Selman and A. Saxena},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Unstructured human activity detection from RGBD images},
year={2012},
pages={842-849},
abstract={Being able to detect and recognize human activities is
essential for several applications, including personal assistive
robotics. In this paper, we perform detection and recognition of
unstructured human activity in unstructured environments. We use a RGBD
sensor (Microsoft Kinect) as the input sensor, and compute a set of
features based on human pose and motion, as well as based on image and
point-cloud information. Our algorithm is based on a hierarchical
maximum entropy Markov model (MEMM), which considers a person's activity
as composed of a set of sub-activities. We infer the two-layered graph
structure using a dynamic programming approach. We test our algorithm on
detecting and recognizing twelve different activities performed by four
people in different environments, such as a kitchen, a living room, an
office, etc., and achieve good performance even when the person was not
seen before in the training set^1 .},
keywords={Markov processes;dynamic programming;graph theory;image colour
analysis;image motion analysis;object detection;pose estimation;robot
vision;service robots;Microsoft Kinect;RGBD images;RGBD sensor;dynamic
programming approach;hierarchical maximum entropy Markov model;human
motion;human pose;personal assistive robotics;two-layered graph
structure;unstructured human activity detection;unstructured human
activity recognition;Dynamic programming;Hidden Markov
models;Humans;Joints;Robot sensing systems},
doi={10.1109/ICRA.2012.6224591},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224743,
author={P. Y. Tao and G. Yang and M. Tomizuka},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={A sensor-based approach for error compensation of industrial
robotic workcells},
year={2012},
pages={5240-5245},
abstract={Industrial robotic manipulators have excellent repeatability
while accuracy is significantly poorer. Numerous error sources in the
robotic workcell contributes to the accuracy problem. Modeling and
identification of all the errors to achieve the required levels of
accuracy may be difficult. To resolve the accuracy issues, a sensor
based indirect error compensation approach is proposed in this paper
where the errors are compensated online via measurements of the work
object. The sensor captures a point cloud of the work object and with
the CAD model of the work object, the actual relative pose of the sensor
frame and work object frame can be established via a point cloud
registration. Once this relationship has been established, the robot
will be able to move the tool accurately relative to the work object
frame near the point of compensation. A data pre-processing technique is
proposed to reduce computation time and prevent a local minima solution
during point cloud registration. A simulation study is presented to
illustrate the effectiveness of the proposed solution.},
keywords={CAD;cellular manufacturing;control engineering computing;end
effectors;error compensation;industrial manipulators;motion
control;production engineering computing;sensors;solid modelling;CAD
model;accuracy problem;data preprocessing technique;end effector;error
identification;error modeling;error sources;industrial robotic
manipulators;industrial robotic workcells;online error
compensation;point cloud capture;point cloud
registration;repeatability;sensor based indirect error compensation
approach;sensor frame pose;tool movement;work object frame pose;work
object measurement;work object model;Accuracy;Computational
modeling;Manipulators;Robot sensing systems;Service robots;Surface
treatment},
doi={10.1109/ICRA.2012.6224743},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224581,
author={Y. Jiang and C. Zheng and M. Lim and A. Saxena},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Learning to place new objects},
year={2012},
pages={3088-3095},
abstract={The ability to place objects in an environment is an important
skill for a personal robot. An object should not only be placed stably,
but should also be placed in its preferred location/orientation. For
instance, it is preferred that a plate be inserted vertically into the
slot of a dish-rack as compared to being placed horizontally in it.
Unstructured environments such as homes have a large variety of object
types as well as of placing areas. Therefore our algorithms should be
able to handle placing new object types and new placing areas. These
reasons make placing a challenging manipulation task. In this work, we
propose using a supervised learning approach for finding good placements
given point-clouds of the object and the placing area. Our method
combines the features that capture support, stability and preferred
configurations, and uses a shared sparsity structure in its the
parameters. Even when neither the object nor the placing area is seen
previously in the training set, our learning algorithm predicts good
placements. In robotic experiments, our method enables the robot to
stably place known objects with a 98% success rate and 98% when also
considering semantically preferred orientations. In the case of placing
a new object into a new placing area, the success rate is 82% and 72%.},
keywords={dexterous manipulators;learning (artificial
intelligence);mobile robots;dish rack;personal robot;shared sparsity
structure;stability;supervised learning approach;task
manipulation;unstructured environment;Computational
modeling;Glass;Grasping;Histograms;Robots;Solid modeling;Supervised
learning},
doi={10.1109/ICRA.2012.6224581},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225151,
author={I. Dryanovski and C. Jaramillo and J. Xiao},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Incremental registration of RGB-D images},
year={2012},
pages={1685-1690},
abstract={An RGB-D camera is a sensor which outputs range and color
information about objects. Recent technological advances in this area
have introduced affordable RGB-D devices in the robotics community. In
this paper, we present a real-time technique for 6-DoF camera pose
estimation through the incremental registration of RGB-D images. First,
a set of edge features are computed from the depth and color images. An
initial motion estimation is calculated through aligning the features.
This initial guess is refined by applying the Iterative Closest Point
algorithm on the dense point cloud data. A rigorous error analysis
assesses several sets of RGB-D ground truth data via an error
accumulation metric. We show that the proposed two-stage approach
significantly reduces error in the pose estimation, compared to a
state-of-the-art ICP registration technique.},
keywords={error analysis;image registration;image sensors;iterative
methods;motion estimation;pose estimation;robot vision;6-DoF camera pose
estimation;RGB-D camera;RGB-D images;error analysis;incremental
registration;iterative closest point algorithm;motion
estimation;Cameras;Estimation;Feature extraction;Image edge
detection;Iterative closest point algorithm;Measurement;Robot sensing
systems},
doi={10.1109/ICRA.2012.6225151},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224911,
author={Xiaoxia Huang and I. Walker and S. Birchfield},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Occlusion-aware reconstruction and manipulation of 3D articulated
objects},
year={2012},
pages={1365-1371},
abstract={We present a method to recover complete 3D models of
articulated objects. Structure-from-motion techniques are used to
capture 3D point cloud models of the object in two different
configurations. A novel combination of Procrustes analysis and RANSAC
facilitates a straightforward geometric approach to recovering the joint
axes, as well as classifying them automatically as either revolute or
prismatic. With the resulting articulated model, a robotic system is
able to manipulate the object along its joint axes at a specified grasp
point in order to exercise its degrees of freedom. Because the models
capture all sides of the object, they are occluded-aware, enabling the
robotic system to plan paths to parts of the object that are not visible
in the current view. Our algorithm does not require prior knowledge of
the object, nor does it make any assumptions about the planarity of the
object or scene. Experiments with a PUMA 500 robotic arm demonstrate the
effectiveness of the approach on a variety of objects with both revolute
and prismatic joints.},
keywords={image reconstruction;manipulators;mobile robots;solid
modelling;telerobotics;3D articulated objects manipulation;3D point
cloud models;Procrustes analysis;RANSAC;geometric
approach;occlusion-aware reconstruction;prismatic approach;revolute
approach;robotic system;structure-from-motion
techniques;Cameras;Computational modeling;Image
reconstruction;Joints;Robot kinematics;Solid modeling},
doi={10.1109/ICRA.2012.6224911},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6204763,
author={O. S. Gedik and A. A. Alatan},
booktitle={2012 20th Signal Processing and Communications Applications
Conference (SIU)},
title={3D tracking using visual and range data},
year={2012},
pages={1-4},
abstract={Applications such as robotics and augmented reality (AR)
require 3D tracking of rigid objects. In robotic applications, the
availability of accurate and robust pose estimates increases
reliability, whereas in AR scenarios reliable pose estimates decrease
jitter. Pure vision sensor based 3D trackers require either manual
initializations of pose or off-line training stages. On the other hand,
trackers relying on pure depth sensors are not suitable for AR
applications. Therefore, an automated and flexible 3D tracking
algorithm, which is based on sensor fusion via Extended Kalman Filter
(EKF) and weighting of measurements, is proposed in this paper. 2D, as
well as 3D, tracking performances are increased significantly with the
help of a novel measurement-tracking scheme, based on estimation of
optical flow using both intensity and shape index map (SIM) data of 3D
point cloud. Requiring neither manual initialization of pose nor offline
training, the proposed method enables highly accurate 3D tracking. The
performance of the proposed method is tested against real and artificial
data and yielded superior results against conventional techniques.},
keywords={Kalman filters;augmented reality;image sensors;image
sequences;nonlinear filters;object tracking;pose estimation;robot
vision;3D point cloud;AR applications;AR scenarios;SIM data;artificial
data;automated 3D tracking algorithm;extended Kalman filter;flexible 3D
tracking algorithm;intensity index map data;measurement-tracking
scheme;off-line training stages;optical flow;pure depth sensors;range
data;real data;rigid objects 3D tracking;robotic applications;robust
pose estimation;sensor fusion;shape index map data;tracking
performances;vision sensor-based 3D trackers;visual data;Analytical
models;Optical variables measurement;Robot sensing systems;Shape;Solid
modeling;Three dimensional displays},
doi={10.1109/SIU.2012.6204763},
ISSN={2165-0608},
month={April},}
@INPROCEEDINGS{6181343,
author={S. Röhl and S. Speidel and D. Gonzalez-Aguirre and S. Suwelack
and H. Kenngott and T. Asfour and B. P. Müller-Stich and R. Dillmann},
booktitle={2011 IEEE International Conference on Robotics and Biomimetics},
title={From stereo image sequences to smooth and robust surface models
using temporal information and Bilateral postprocessing},
year={2011},
pages={544-550},
abstract={Reconstruction of surface models from camera images has many
applications in robotics such as surface registration or object
recognition. In this paper, we describe a workflow in which we extract
depth information from stereo image sequences to generate a surface
model. We present our solutions to correspondence analysis, disparity
correction and refinement, as well as 3D reconstruction, point cloud
smoothing and meshing. One important feature of the correspondence
analysis that we evaluate in detail is the use of temporal information.
Another emphasis is on correcting and smoothing the disparity images as
well as the reconstructed point cloud without losing too much detail.
We, hence, introduce our application of the Bilateral filter on
disparity images and our usage of least squares smoothing. The
components of the workflow were evaluated using three image sources:
Endoscopic images from the daVinci® telemanipulator; images from a
stereo camera integrated in the ARMAR III humanoid robot; synthetic
data. Depending on the image resolution and the application, the
workflow reconstructs surface models in real-time. We show that by using
temporal information we obtain more accurate and robust correspondences.
Additionally, the Bilateral filter was especially useful in refining the
correspondences extracted from endoscopic images as well as the
synthetic data sets, whereas the least squares method showed good
results in smoothing the point cloud of ARMAR III images. Overall, the
presented approach achieves good results for different camera settings
and image types, especially with respect to the real-time requirement.},
keywords={cameras;endoscopes;feature extraction;humanoid robots;image
reconstruction;image registration;image resolution;image sequences;least
squares approximations;manipulators;medical robotics;object
recognition;robot vision;smoothing methods;stereo image
processing;telerobotics;3D reconstruction;ARMAR III humanoid robot;ARMAR
III image point cloud smoothing;bilateral filter;bilateral
postprocessing;camera image resolution;daVinci® telemanipulator;depth
information;disparity correction;disparity image correction;disparity
image smoothing;endoscopic image extraction;endoscopic image
source;least square smoothing method;object recognition;point cloud
reconstruction;point cloud smoothing;robust smooth surface model
reconstruction;stereo camera;stereo image sequence;surface
registration;synthetic data;synthetic data sets;temporal
information;Cameras;Image reconstruction;Robot vision systems;Smoothing
methods;Surface reconstruction;Three dimensional displays},
doi={10.1109/ROBIO.2011.6181343},
month={Dec},}
@INPROCEEDINGS{6181275,
author={J. Wu and Z. Huang and Y. Guan and C. Cai and Q. Wang and Z.
Xiao and Z. Zheng and H. Zhang and X. Zhang},
booktitle={2011 IEEE International Conference on Robotics and Biomimetics},
title={An intelligent environmental monitoring system based on
autonomous mobile robot},
year={2011},
pages={138-143},
abstract={Monitoring indoor environmental state of large communication
rooms, warehouses and power stations is an important task. Based on
mobile robotics, we have developed an intelligent environmental
monitoring system. In this system, a mobile robot carrying a number of
sensors autonomously navigates and dynamically sample the environmental
data including the temperature, humidity and airflow velocity. The
system outputs the environmental parameters in appropriate modes such as
clouds. In this paper, we present the development of this monitoring
system, its working principle and application effectiveness. It has been
shown how a mobile robot can be used to as a novel application in
industry environments.},
keywords={environmental monitoring (geophysics);intelligent
robots;mobile robots;autonomous mobile robot;environmental
data;intelligent environmental monitoring system;Clouds;Monitoring;Robot
sensing systems;Temperature measurement;Temperature sensors;Autonomous
navigation;Environmental cloud;Environmental monitoring;Mobile
robot;Visual slice analysis},
doi={10.1109/ROBIO.2011.6181275},
month={Dec},}
@INPROCEEDINGS{6177109,
author={K. Arimoto and S. Kavusi and K. Salisbury},
booktitle={2012 IEEE International Solid-State Circuits Conference},
title={What's next in robots? #x223C;Sensing, processing, networking
toward human brain and body},
year={2012},
pages={514-514},
abstract={Most of us dreamt about robots in our childhood interacting
and assisting us in our daily life. They are way beyond fiction and have
emerged to become unavoidable in minimally-invasive surgery and
industrial automation. There is also an explosion in research areas
around autonomous cars, humanoid/android, medical, mobile remote
manipulation and personal assistance robots. Such advances largely
benefit from to the advances in sensing, signal processing,
analog/digital circuits, and devices in semiconductor technologies that
are driven by consumer and automotive electronics. Increasingly robotic
platforms are also benefiting from the wirelessly connected
infrastructure and the cloud computing. Expansion of such applications
is going to require more human-friendly and humanlike interactive
systems. Improvements in energy efficiency, dependability, security,
intelligent sensor networks are among technologies, which are expected
from the next generation silicon system, that provide the means to the
development of the next generation interactive robots. However, more
closed collaboration of the hardware/software design, integration, and
data fusion are also must. Robot developers' wish list for the
semiconductor industry is endless and their creativ ity is admirable.},
keywords={humanoid robots;semiconductor devices;semiconductor
industry;semiconductor technology;sensor fusion;signal
processing;analog-digital circuits;automotive electronics;autonomous
cars;cloud computing;data fusion;energy efficiency;hardware-software
design;human brain;human-like interactive systems;humanoid-android
robot;industrial automation;intelligent sensor
networks;minimally-invasive surgery;mobile remote manipulation;next
generation interactive robots;next generation silicon system;personal
assistance robots;robotic platforms;semiconductor device
technology;semiconductor industry;signal processing;Tutorials},
doi={10.1109/ISSCC.2012.6177109},
ISSN={0193-6530},
month={Feb},}
@INPROCEEDINGS{6145896,
author={Changwoo Nam and Min-Hyuk Sung and J. H. Lee and J. Kim},
booktitle={2011 8th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI)},
title={Plane-dominant object reconstruction for robotic spatial
augmented reality},
year={2011},
pages={627-630},
abstract={We present a simple reconstruction algorithm of a
plane-dominant 3D environment for the robotic spatial augmented reality
(RSAR). In spatial augmented reality, a projector renders virtual
objects onto 3D objects in the real space. To watch the augmented
virtual objects from a viewpoint without distortions, the final image
should be pre-distorted based on the geometry information of the 3D
objects in the real world. In our RSAR setting, we assume that a robot
is equipped with the devices such as a projector, low-cost depth
cameras, and the sensor of capturing 3D position of the viewpoint. The
robot captures the 3D environment with as a point cloud and reconstructs
the geometry of 3D objects in the real world with a set of planes. In
order that the viewer can see the distortion-free virtual objects, we
compute the pre-warped images of virtual objects for a projector.
Finally, we provide an efficient algorithm using GPU shaders to compute
the pre-warped images for a projector. In experiments, we provide some
results of preliminary simulations for our RSAR scenario.},
keywords={augmented reality;cameras;computational geometry;image
reconstruction;optical projectors;rendering (computer graphics);robot
vision;3D position capturing sensor;GPU shaders;distortion-free virtual
objects;low-cost depth cameras;plane-dominant 3D
environment;plane-dominant object reconstruction;prewarped
images;robotic spatial augmented reality;virtual object
rendering;Augmented reality;Cameras;Image reconstruction;Robot vision
systems;Three dimensional displays;Plane-dominant surface
reconstruction;Projective texture;Robotic spatial augmented reality},
doi={10.1109/URAI.2011.6145896},
month={Nov},}
@INPROCEEDINGS{6144153,
author={W. Wohlkinger and M. Vincze},
booktitle={2011 IEEE International Conference on Signal and Image
Processing Applications (ICSIPA)},
title={Shape distributions on voxel surfaces for 3D object
classification from depth images},
year={2011},
pages={115-120},
abstract={In this work we address the problem of 3D shape based object
class recognition directly from point cloud data obtained from RGB-D
cameras like the Kinect sensor from Microsoft. A novel shape descriptor
is presented, capable of classifying 'never before seen objects' at
their first occurrence in a single view in a fast and robust manner. The
classification task is stated as a matching problem, finding the most
similar 3D model and view from a database of CAD models gathered from
the web to a given depth image. We further show how locally sensitive
hashing can be easily adapted to implement fast matching against a
database of 2500 CAD models with more than 200000 views in 160
categories. This shape descriptor utilizes distributions on voxel
surfaces and can be used in various applications: As a pure 3D
descriptor for 3D model retrieval, as a 2.5D descriptor for finding 3D
models to partial views or as our main indention as a classification
system in the home-robotics domain to enable recognition and
manipulation of everyday objects. Experimental evaluation against the
baseline descriptors on a dataset of real-world objects in table scene
contexts and on a 3D database shows significant improvements.},
keywords={CAD;Internet;cameras;image classification;image coding;image
matching;image retrieval;object recognition;shape recognition;solid
modelling;visual databases;3D model retrieval;3D object
classification;3D shape based object class recognition;3D shape
descriptor;CAD model database;RGB-D cameras;depth image;home robotic
domain;matching problem;object manipulation;object recognition;point
cloud data;sensitive hashing;voxel surface shape distribution;Adaptation
models;Databases;Histograms;Robot sensing systems;Shape;Solid
modeling;Three dimensional displays},
doi={10.1109/ICSIPA.2011.6144153},
month={Nov},}
@INPROCEEDINGS{6141038,
author={A. Ali and S. E. Amin and H. H. Ramadan and M. F. Tolba},
booktitle={The 2011 International Conference on Computer Engineering
Systems},
title={Ozone monitoring instrument aerosol products: Algorithm modeling
and validation with ground based measurements over Europe},
year={2011},
pages={181-186},
abstract={Airborne sun photometer measurements are used to evaluate
retrievals of extinction Aerosol Optical Depth (AOD). These data are
extracted from spatially coincident and temporally near coincident
measurements by the Ozone Monitoring Instrument (OMI) aboard the Aura
satellite during 2005. OMI measured Top Of Atmosphere (TOA) reflectances
are routinely inverted to yield aerosol products such as AOD using two
different retrieval techniques: a near-Ultraviolet (UV) and a
multi-wavelength technique. In this work, we study the application of
the two AOD modeling techniques retrieved by OMI comparing to AOD
captured at several locations containing sites of the Aerosol Robotic
Network (AERONET). The comparison result shows that, just over Europe,
OMI aerosol optical depths are better retrieved in the multi-wavelength
retrieval than in the near UV. Correlations have been improved by
applying a simple criterion to avoid scenes probably contaminated by
thin clouds, and surface scattering.},
keywords={aerosols;air pollution measurement;optical variables
measurement;ozone;photometers;Aura satellite;Europe;aerosol optical
depth retrieval;aerosol product;aerosol robotic network;airborne sun
photometer measurement;ground based measurement;multiwavelength
retrieval technique;near-ultraviolet retrieval technique;ozone
monitoring instrument;top-of-atmosphere reflectance;Aerosols;Atmospheric
measurements;Atmospheric modeling;Land surface;Optical
reflection;Optical variables measurement;Size
measurement;Aerosol;Aerosol optical depth modeling;OMI;sun photometer
measurements},
doi={10.1109/ICCES.2011.6141038},
month={Nov},}
@INPROCEEDINGS{6130388,
author={W. C. Chiu and U. Blanke and M. Fritz},
booktitle={2011 IEEE International Conference on Computer Vision
Workshops (ICCV Workshops)},
title={I spy with my little eye: Learning optimal filters for
cross-modal stereo under projected patterns},
year={2011},
pages={1209-1214},
abstract={With the introduction of the Kinect as a gaming interfaces,
its broad commercial accessibility and high quality depth sensor has
attracted the attention not only from consumers but also from
researchers in the robotics community. The active sensing technique of
the Kinect produces robust depth maps for reliable human pose
estimation. But for a broader range of applications in robotic
perception, its active sensing approach fails under many operating
conditions such like objects with specular and transparent surfaces.
Recently, an initial study has shown that part of the arising problems
can be alleviated by complimenting the active sensing scheme with
passive, cross-modal stereo between the Kinect's rgb and ir camera.
However, the method is troubled by interference from the IR projector
that is required for the active depth sensing method. We investigate
these issues and conduct a more detailed study of the physical
characteristics of the sensors as well as propose a more general method
that learns optimal filters for cross-modal stereo under projected
patterns. Our approach improves results over the baseline in a
point-cloud-based object segmentation task without modifications of the
kinect hardware and despite the interference by the projector.},
keywords={filtering theory;image segmentation;pose estimation;robot
vision;stereo image processing;user interfaces;IR projector;Kinect IR
camera;Kinect RGB camera;active depth sensing method;broad commercial
accessibility;cross-modal stereo;gaming interfaces;high quality depth
sensor;human pose estimation;optimal filters;physical
characteristics;point-cloud-based object segmentation task;projected
patterns;robotic perception;robotics community;robust depth
maps;specular surfaces;transparent surfaces;Image reconstruction;Light
sources;Robot sensing systems;Robustness;Sensitivity;Stereo vision},
doi={10.1109/ICCVW.2011.6130388},
month={Nov},}
@INPROCEEDINGS{6129046,
author={Yan-You Chen and Jhing-Fa Wang and Po-Chuan Lin and Po-Yi Shih
and Hsin-Chun Tsai and Da-Yu Kwan},
booktitle={TENCON 2011 - 2011 IEEE Region 10 Conference},
title={Human-robot interaction based on cloud computing infrastructure
for senior companion},
year={2011},
pages={1431-1434},
abstract={This paper presents a human-robot interactive system for
senior companion based on cloud computing infrastructure. The proposed
senior companion robot system (SCRS) is designed based on cloud
computing network. In the server side, two cloud services are proposed,
1) the web-based user remote management service (WURMS) for remote robot
control; 2) the robotic multimodal interactive computation services
(RMICS) for providing the human-robot operation interfaces including the
speech/sound recognition, speaker identification, face identification,
sound source estimation and text to speech (TTS). In the robot client
side, the behavior model is designed to use WURMS and RMICS services. In
the experiments, two robots called “Robert” and “Davinci” are designed
to evaluate the SCRS's capability. With using only low-cost and
low-power CPUs (Intel Atom N450), both of the two robots can still work
wirelessly for real-time human-robot interaction. Finally, we design
five senior companion scenarios, and the experimental average MOS (Mean
Opinion Score) is 4.16.},
keywords={cloud computing;control engineering computing;face
recognition;human-robot interaction;interactive systems;speaker
recognition;speech synthesis;telerobotics;user interfaces;CPU;Davinci
robot;RMICS services;Robert robot;WURMS;Web-based user remote management
service;cloud computing infrastructure;cloud services;face
identification;human-robot interaction;human-robot interactive
system;human-robot operation interfaces;mean opinion score;remote robot
control;robotic multimodal interactive computation services;senior
companion robot system;sound source estimation;speaker
identification;speech-sound recognition;text to speech;Cloud
computing;Face;Robot sensing systems;Speech;Speech
recognition;Thyristors;human-robot interactive system;robotic multimodal
interactive computation services (RMICS);senior companion robot system
(SCRS);web-based user remote management service (WURMS)},
doi={10.1109/TENCON.2011.6129046},
ISSN={2159-3442},
month={Nov},}
@INPROCEEDINGS{6126417,
author={E. Ringaby and P. E. Forssén},
booktitle={2011 International Conference on Computer Vision},
title={Scan rectification for structured light range sensors with
rolling shutters},
year={2011},
pages={1575-1582},
abstract={Structured light range sensors, such as the Microsoft Kinect,
have recently become popular as perception devices for computer vision
and robotic systems. These sensors use CMOS imaging chips with
electronic rolling shutters (ERS). When using such a sensor on a moving
platform, both the image, and the depth map, will exhibit geometric
distortions. We introduce an algorithm that can suppress such
distortions, by rectifying the 3D point clouds from the range sensor.
This is done by first estimating the time continuous 3D camera
trajectory, and then transforming the 3D points to where they would have
been, if the camera had been stationary. To ensure that image and range
data are synchronous, the camera trajectory is computed from KLT tracks
on the structured-light frames, after suppressing the structured-light
pattern. We evaluate our rectification, by measuring angles between the
visible sides of a cube, before and after rectification. We also measure
how much better the 3D point clouds can be aligned after rectification.
The obtained improvement is also related to the actual rotational
velocity, measured using a MEMS gyroscope.},
keywords={CMOS image sensors;cameras;computational geometry;computer
vision;3D point cloud;CMOS imaging chip;ERS;MEMS gyroscope;Microsoft
Kinect;electronic rolling shutter;geometric distortion;rotational
velocity;scan rectification;structured light range sensor;time
continuous 3D camera trajectory;Cameras;Simultaneous localization and
mapping;Solid modeling;Spline;Three dimensional displays;Trajectory},
doi={10.1109/ICCV.2011.6126417},
ISSN={1550-5499},
month={Nov},}
@INPROCEEDINGS{6106771,
author={J. Poppinga and A. Birk and K. Pathak and N. Vaskevicius},
booktitle={2011 IEEE International Symposium on Safety, Security, and
Rescue Robotics},
title={Fast 6-DOF path planning for Autonomous Underwater Vehicles (AUV)
based on 3D plane mapping},
year={2011},
pages={345-350},
abstract={A method for 6 degree of freedom (6-DOF) path-planning for
Autonomous Underwater Vehicles (AUV) is presented. It is based on an
augmentation of Rapidly-exploring Random Trees (RRT) and the
Probabilistic Roadmap Method (PRM) with a plane-fitting method as part
of 3D underwater mapping. This is of particular interest for Safety,
Security, and Rescue Robotics (SSRR) missions with AUV, which tend to
occur not in plain open waters but in complex environments. Examples
include harbor security, infrastructure inspection after disasters, and
military reconnaissance. The special representation of the 3D map used
here allows for significantly faster generation of paths than with state
of the art point clouds. The method is evaluated with experiments using
real world data from a Tritech Eclipse sonar. We present results of
6-DOF path-planning through a 3D map that is generated in a larger
underwater structure, namely the Lesumer Sperrwerk, a flood gate in the
river Lesum in Bremen, Germany.},
keywords={autonomous underwater vehicles;path planning;probability;trees
(mathematics);3D plane mapping;3D underwater mapping;6-DOF path
planning;Germany;Lesumer Sperrwerk structure;Tritech Eclipse
sonar;autonomous underwater vehicle;degree-of-freedom;plane-fitting
method;point cloud;probabilistic roadmap method;rapidly-exploring random
trees;safety security and rescue robotics mission;Path
planning;Probabilistic logic;Simultaneous localization and
mapping;Sonar;Three dimensional displays;3D map;Autonomous Underwater
Vehicle (AUV);Unmanned Underwater Vehicle (UUV);path planning},
doi={10.1109/SSRR.2011.6106771},
ISSN={2374-3247},
month={Nov},}
@INPROCEEDINGS{6106776,
author={V. Tretyakov and T. Linder},
booktitle={2011 IEEE International Symposium on Safety, Security, and
Rescue Robotics},
title={Range sensors evaluation under smoky conditions for robotics
applications},
year={2011},
pages={215-220},
abstract={In the paper we present performance reviews of some of the
most popular range sensors in robotics under smoky conditions. The
experiments were conducted in the same environment with the same source
of smoke of different densities. The range sensors were setup to produce
3D point clouds. To evaluate the performance of the sensors two metrics
were used: the total number of valid points produced by the 3D sensors
and the number of points which could be extracted as fitting to a planar
surface, both in respect to the smoke density.},
keywords={distance measurement;image sensors;performance
evaluation;service robots;smoke;surface fitting;3D point clouds;3D
sensors;performance evaluation;performance reviews;planar surface
fitting;range sensors evaluation;robotics applications;smoke
density;smoky conditions;Calibration;Cameras;Robot vision systems;Three
dimensional displays;3D Perception;Performance Evaluation;Range
Sensors;Smoky Conditions;USAR},
doi={10.1109/SSRR.2011.6106776},
ISSN={2374-3247},
month={Nov},}
@INPROCEEDINGS{6094490,
author={K. Kwak and D. F. Huber and H. Badino and T. Kanade},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Extrinsic calibration of a single line scanning lidar and a camera},
year={2011},
pages={3283-3289},
abstract={Lidar and visual imagery have been broadly utilized in
computer vision and mobile robotics applications because these sensors
provide complementary information. However, in order to convert data
between the local coordinate systems, we must estimate the rigid body
transformation between the sensors. In this paper, we propose a
robust-weighted extrinsic calibration algorithm that is implemented
easily and has small calibration error. The extrinsic calibration
parameters are estimated by minimizing the distance between
corresponding features projected onto the image plane. The features are
edge and centerline features on a v-shaped calibration target. The
proposed algorithm contributes two ways to improve the calibration
accuracy. First, we use different weights to distance between a point
and a line feature according to the correspondence accuracy of the
features. Second, we apply a penalizing function to exclude the
influence of outliers in the calibration data sets. We conduct several
experiments to evaluate the performance of our extrinsic calibration
algorithm, such as comparison of the RMS distance of the ground truth
and the projected points, the effect of the number of lidar scan and
image, and the effect of pose and range of the calibration target. In
the experiments, we show our extrinsic calibration algorithm has
calibration accuracy over 50% better than an existing state of the art
approach. To evaluate the generality of our algorithm, we also colorize
point clouds with different pairs of lidars and cameras calibrated by
our algorithm.},
keywords={calibration;cameras;computer vision;image sensors;mobile
robots;optical radar;RMS distance;calibration data set;camera;centerline
feature;computer vision;image plane;lidar scan number;local coordinate
system;mobile robotics;penalizing function;point clouds;robust weighted
extrinsic calibration algorithm;single line scanning lidar calibration
error;v-shaped calibration target;visual
imagery;Calibration;Cameras;Feature extraction;Image edge
detection;Laser radar;Sensors;Three dimensional displays},
doi={10.1109/IROS.2011.6094490},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6095027,
author={D. Meger and J. J. Little},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Mobile 3D object detection in clutter},
year={2011},
pages={4885-4892},
abstract={This paper presents a method for multi-view 3D robotic object
recognition targeted for cluttered indoor scenes. We explicitly model
occlusions that cause failures in visual detectors by learning a
generative appearance-occlusion model from a training set containing
annotated 3D objects, images and point clouds. A Bayesian 3D object
likelihood incorporates visual information from many views as well as
geometric priors for object size and position. An iterative,
sampling-based inference technique determines object locations based on
the model. We also contribute a novel robot-collected data set with
images and point clouds from multiple views of 60 scenes, with over 600
manually annotated 3D objects accounting for over ten thousand bounding
boxes. This data has been released to the community. Our results show
that our system is able to robustly recognize objects in realistic
scenes, significantly improving recognition performance in clutter.},
keywords={Computational modeling;Detectors;Geometry;Robots;Solid
modeling;Three dimensional displays;Visualization},
doi={10.1109/IROS.2011.6095027},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6088545,
author={L. F. Gallardo and V. Kyrki},
booktitle={2011 15th International Conference on Advanced Robotics (ICAR)},
title={Detection of parametrized 3-D primitives from stereo for robotic
grasping},
year={2011},
pages={55-60},
abstract={The grasping skill is an indispensable quality for general
service robotics. In a home-like natural environment, manipulated
objects may be unknown in advance, which prevents the use of a
combination of traditional grasp planning and visual pose estimation to
realize grasping. Stereo vision is an inexpensive and relatively general
sensor for 3-D objects. However, the quality of the sensor data from a
stereo camera can be restrictingly low for grasping. This paper proposes
an approach to extract parametrized 3-D primitives, which describe an
object's overall shape, as well as its location, orientation, and size.
These pieces of information are sufficient to grasp the object. Only a
stereo image pair is used to generate a partial three-dimensional point
cloud, which is then approximated by simple primitives, such as a box or
a cylinder. The approach combines initial estimation using RANSAC and
further iterative optimization of the unknown parameters. Experiments
with real world objects show that the approach can be used to grasp a
range of objects using low quality point clouds from single stereo pairs.},
keywords={grippers;pose estimation;robot vision;service robots;stereo
image processing;RANSAC;grasp planning;grasping skill;home-like natural
environment;manipulated objects;parametrized 3D primitives;point
cloud;robotic grasping;service robotics;stereo camera;stereo image
pair;stereo vision;visual pose estimation;Cameras;Cost
function;Grasping;Robots;Shape;Vectors},
doi={10.1109/ICAR.2011.6088545},
month={June},}
@INPROCEEDINGS{6045151,
author={S. Chen and Y. Xu and P. Wang and D. Li and D. Jin and J. Liu
and J. Wang and F. Ren and J. Zhang},
booktitle={2011 IEEE International Conference on Cloud Computing and
Intelligence Systems},
title={Keynote speech: The usage of cloud computing in China Regional
Healthcare},
year={2011},
pages={i-v},
abstract={These keynote speeches discuss the following: cloud computing
in China regional healthcare; service robotics; cloud storage data
services; intelligent transportation; smart city; smart power
consumption; mobile Internet; affective computing; and IOT industry.},
keywords={DP industry;artificial intelligence;cloud computing;health
care;medical administrative data processing;mobile computing;power
engineering computing;service robots;town and country planning;traffic
engineering computing;China regional healthcare;IOT industry;affective
computing;cloud computing;cloud storage data service;intelligent
transportation;mobile Internet;service robotics;smart city;smart power
consumption},
doi={10.1109/CCIS.2011.6045151},
ISSN={2376-5933},
month={Sept},}
@INPROCEEDINGS{5980275,
author={H. Badino and D. Huber and Y. Park and T. Kanade},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Fast and accurate computation of surface normals from range images},
year={2011},
pages={3084-3091},
abstract={The fast and accurate computation of surface normals from a
point cloud is a critical step for many 3D robotics and automotive
problems, including terrain estimation, mapping, navigation, object
segmentation, and object recognition. To obtain the tangent plane to the
surface at a point, the traditional approach applies total least squares
to its small neighborhood. However, least squares becomes
computationally very expensive when applied to the millions of
measurements per second that current range sensors can generate. We
reformulate the traditional least squares solution to allow the fast
computation of surface normals, and propose a new approach that obtains
the normals by calculating the derivatives of the surface from a
spherical range image. Furthermore, we show that the traditional least
squares problem is very sensitive to range noise and must be normalized
to obtain accurate results. Experimental results with synthetic and real
data demonstrate that our proposed method is not only more efficienThe
fast and accurate computation of surface normals from a point cloud is a
critical step for many 3D robotics and automotive problems, including
terrain estimation, mapping, navigation, object segmentation, and object
recognition. To obtain the tangent plane to the surface at a point, the
traditional approach applies total least squares to its small
neighborhood. However, least squares becomes computationally very
expensive when applied to the millions of measurements per second that
current range sensors can generate. We reformulate the traditional least
squares solution to allow the fast computation of surface normals, and
propose a new approach that obtains the normals by calculating the
derivatives of the surface from a spherical range image. Furthermore, we
show that the traditional least squares problem is very sensitive to
range noise and must be normalized to obtain accurate results.
Experimental results with synthetic and real data demonstrate that our
pro- - posed method is not only more efficient by up to two orders of
magnitude, but provides better accuracy than the traditional least
squares for practical neighborhood sizes.t by up to two orders of
magnitude, but provides better accuracy than the traditional least
squares for practical neighborhood sizes.},
keywords={computer vision;distance measurement;least squares
approximations;3D robotics;accurate computation;automotive problems;fast
computation;least squares solution;navigation;object recognition;object
segmentation;point cloud;range images;range noise;range
sensors;spherical range image;surface normals;tangent plane;terrain
estimation;terrain mapping;Accuracy;Equations;Least squares
approximation;Mathematical model;Noise;Sensors;Three dimensional displays},
doi={10.1109/ICRA.2011.5980275},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980567,
author={R. B. Rusu and S. Cousins},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={3D is here: Point Cloud Library (PCL)},
year={2011},
pages={1-4},
abstract={With the advent of new, low-cost 3D sensing hardware such as
the Kinect, and continued efforts in advanced point cloud processing, 3D
perception gains more and more importance in robotics, as well as other
fields. In this paper we present one of our most recent initiatives in
the areas of point cloud perception: PCL (Point Cloud Library -
http://pointclouds.org). PCL presents an advanced and extensive approach
to the subject of 3D perception, and it's meant to provide support for
all the common 3D building blocks that applications need. The library
contains state-of-the art algorithms for: filtering, feature estimation,
surface reconstruction, registration, model fitting and segmentation.
PCL is supported by an international community of robotics and
perception researchers. We provide a brief walkthrough of PCL including
its algorithmic capabilities and implementation strategies.},
keywords={feature extraction;image segmentation;robot vision;surface
reconstruction;3D building blocks;3D perception
gains;Kinect;PCL;advanced point cloud processing;feature
estimation;international robotics community;low-cost 3D sensing
hardware;model fitting;point cloud library;surface
reconstruction;surface registration;surface segmentation},
doi={10.1109/ICRA.2011.5980567},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980106,
author={S. Blumenthal and E. Prassler and J. Fischer and W. Nowak},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Towards identification of best practice algorithms in 3D
perception and modeling},
year={2011},
pages={3554-3561},
abstract={Robots need a representation of their environment to reason
about and to interact with it. Different 3D perception and modeling
approaches exist to create such a representation, but they are not yet
easily comparable. This work tries to identify best practice algorithms
in the domain of 3D perception and modeling with a focus on environment
reconstruction for robotic applications. The goal is to have a
collection of refactored algorithms that are easily measurable and
comparable. The realization follows a methodology consisting of five
steps. After a survey of relevant algorithms and libraries, common
representations for the core data-types Cartesian point, Cartesian point
cloud and triangle mesh are identified for use in harmonized interfaces.
Atomic algorithms are encapsulated into four software components: the
Octree component, the Iterative Closest Point component, the k-Nearest
Neighbors search component and the Delaunay triangulation component. A
sample experiment demonstrates how the component structure can be used
to deduce best practice.},
keywords={identification;iterative methods;mesh generation;mobile
robots;octrees;pattern clustering;search problems;solid modelling;3D
modeling approach;3D perception;Delaunay triangulation component;atomic
algorithm;data-types Cartesian point;environment
representation;harmonized interface;iterative closest point
component;k-nearest neighbors search component;octree
component;refactored algorithm;robotic application;software
component;Benchmark testing;Best practices;Libraries;Robots;Software
algorithms;Solid modeling;Three dimensional displays},
doi={10.1109/ICRA.2011.5980106},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980006,
author={S. Williams and A. M. Howard},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Horizon line estimation in glacial environments using multiple
visual cues},
year={2011},
pages={5887-5892},
abstract={While the arctic possesses significant information of
scientific value, surprisingly little work has focused on developing
robotic systems to collect this data. For arctic robotic data collection
to be a viable solution, a method for navigating in the arctic, and thus
of assessing glacial terrain, must be developed. Segmenting the ground
plane from the rest of the image is one common aspect of a visual hazard
detection system. However, the properties of glacial images, namely low
contrast, overcast sky, and cloud, mountain, and snow sharing common
colors, pose difficulties for most visual algorithms. A horizon line
detection scheme is presented which uses multiple visual cues to rank
candidate horizon segments, then constructs a horizon line consistent
with those cues. Weak cues serve to reinforce a selected path, while
strong cues have the ability to redirect it. Further, the system infers
the horizon location in areas that are visually ambiguous. The
performance of the proposed system has been tested on multiple data sets
collected on two different glaciers in Alaska, and compares favorably,
both in terms of time and classification performance, to representative
segmentation algorithms from several different classes.},
keywords={computational geometry;image colour analysis;image
segmentation;mobile robots;object detection;planetary rovers;robot
vision;arctic robotic data collection;autonomous robotic rovers;glacial
environments;ground plane segmentation;horizon line detection
scheme;horizon line estimation;multiple visual cues;representative
segmentation algorithms;robotic systems;visual hazard detection
system;Histograms;Image color analysis;Image edge detection;Image
segmentation;Meteorology;Robots;Visualization},
doi={10.1109/ICRA.2011.5980006},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980354,
author={J. Bohg and M. Johnson-Roberson and B. León and J. Felip and X.
Gratal and N. Bergström and D. Kragic and A. Morales},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Mind the gap - robotic grasping under incomplete observation},
year={2011},
pages={686-693},
abstract={We consider the problem of grasp and manipulation planning
when the state of the world is only partially observable. Specifically,
we address the task of picking up unknown objects from a table top. The
proposed approach to object shape prediction aims at closing the
knowledge gaps in the robot's understanding of the world. A completed
state estimate of the environment can then be provided to a simulator in
which stable grasps and collision-free movements are planned. The
proposed approach is based on the observation that many objects commonly
in use in a service robotic scenario possess symmetries. We search for
the optimal parameters of these symmetries given visibility constraints.
Once found, the point cloud is completed and a surface mesh
reconstructed. Quantitative experiments show that the predictions are
valid approximations of the real object shape. By demonstrating the
approach on two very different robotic platforms its generality is
emphasized.},
keywords={mesh generation;robots;collision-free movements;gap robotic
grasping;incomplete observation;manipulation planning;mesh
reconstruction;object shape prediction;Approximation
methods;Grasping;Image reconstruction;Planning;Robots;Shape;Surface
reconstruction},
doi={10.1109/ICRA.2011.5980354},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5940405,
author={A. Geiger and J. Ziegler and C. Stiller},
booktitle={2011 IEEE Intelligent Vehicles Symposium (IV)},
title={StereoScan: Dense 3d reconstruction in real-time},
year={2011},
pages={963-968},
abstract={Accurate 3d perception from video sequences is a core subject
in computer vision and robotics, since it forms the basis of subsequent
scene analysis. In practice however, online requirements often severely
limit the utilizable camera resolution and hence also reconstruction
accuracy. Furthermore, real-time systems often rely on heavy parallelism
which can prevent applications in mobile devices or driver assistance
systems, especially in cases where FPGAs cannot be employed. This paper
proposes a novel approach to build 3d maps from high-resolution stereo
sequences in real-time. Inspired by recent progress in stereo matching,
we propose a sparse feature matcher in conjunction with an efficient and
robust visual odometry algorithm. Our reconstruction pipeline combines
both techniques with efficient stereo matching and a multi-view linking
scheme for generating consistent 3d point clouds. In our experiments we
show that the proposed odometry method achieves state-of-the-art
accuracy. Including feature matching, the visual odometry part of our
algorithm runs at 25 frames per second, while - at the same time - we
obtain new depth maps at 3-4 fps, sufficient for online 3d
reconstructions.},
keywords={distance measurement;image matching;image reconstruction;image
sequences;robot vision;stereo image processing;3D maps;3D point
clouds;StereoScan;camera resolution;computer vision;dense 3D
reconstruction;feature matcher;high resolution stereo
sequences;multiview linking scheme;robotics;robust visual odometry
algorithm;scene analysis;stereo matching;video
sequences;Cameras;Estimation;Image reconstruction;Real time
systems;Stereo image processing;Three dimensional displays;Visualization},
doi={10.1109/IVS.2011.5940405},
ISSN={1931-0587},
month={June},}
@INPROCEEDINGS{5741377,
author={Z. Du and W. Yang and Y. Chen and X. Sun and X. Wang and C. Xu},
booktitle={2011 Tenth International Symposium on Autonomous
Decentralized Systems},
title={Design of a Robot Cloud Center},
year={2011},
pages={269-275},
abstract={Service-oriented architecture and cloud computing have become
the prevalent computing paradigm. In this paradigm, computing resources
can be accessed like other utility services available in today's
society. In the meantime, robotics applications are joining the trend.
More and more robot applications are shifting from manufacture to
non-manufacture and service industries. However, for the on-demand
supply of the large-scale heterogeneous robots, It is still a problem
have not yet been studied, including the fundamental management and
efficiency issues in using of these resources. In this paper, we design
a framework of "Robot Cloud Center" (RCC) following the general cloud
computing paradigm to address the current limitations in capacity and
versatility of robotic applications. In this framework, a robot can be
provided as a service just like a public utility service so that
everyone can access the powerful robotic services easily, efficiently,
and cheaply. Based on a given scenario, a robot scheduling algorithm in
RCC is proposed to take advantage of the heterogeneous robot resources
to meet the end user's requirement with the minimum cost.},
keywords={cloud computing;public utilities;robot
programming;scheduling;service robots;service-oriented
architecture;cloud computing;large scale heterogeneous robot;on-demand
supply;public utility service;robot cloud center;robot programming;robot
scheduling algorithm;service oriented architecture;Cloud
computing;Computational modeling;Computer architecture;Hardware;Service
oriented architecture;Service robots;SOA;cloud computing;robot as a
service;robot programming},
doi={10.1109/ISADS.2011.36},
ISSN={1541-0056},
month={March},}
@INPROCEEDINGS{5652160,
author={S. C. Colbert and R. Alqasemi and R. V. Dubey and G. Franz and
K. Wöllhaf},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Development and evaluation of a vision algorithm for 3D
reconstruction of novel objects from three camera views},
year={2010},
pages={5438-5445},
abstract={When planning robotic grasping and manipulation maneuvers,
knowledge of the shape and pose of the object of interest is critical
information. In order for an autonomous or semi-autonomous system to
operate intelligently in an unstructured environment and interact with
novel objects, it must have the ability to recover this information at
run time, even when no a priori information of the object is available.
In this paper, we describe the development and testing of an algorithm
that can reconstruct the full 3D geometry of a novel object from just
three images. A variant of shape from silhouettes, the algorithm first
generates a rough surface approximation in the form of a point cloud.
This approximation is then refined by fitting an eleven parameter
geometric surface to the points in such a manner that the surface
ignores noise and perspective projection shadows. We test the algorithm
in both simulation and on several real world objects. We show that the
algorithm provides accurate reconstructions that can be directly used to
plan grasping maneuvers. Compared to other attempts in the literature,
the proposed algorithm is faster, requires fewer images, is more
accurate, and degrades gracefully in the presence of bad data. A real
world test case is included that shows that the algorithm still yields
usable results when the form of the object is amorphous or otherwise
non-geometric.},
keywords={approximation theory;cameras;grippers;image
reconstruction;pose estimation;robot vision;rough surfaces;solid
modelling;3D object reconstruction;autonomous system;camera view;image
reconstruction;manipulation maneuvering;robotic grasp planning;rough
surface approximation;semiautonomous system;vision algorithm},
doi={10.1109/IROS.2010.5652160},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5648926,
author={A. Roennau and G. Liebel and T. Schamm and T. Kerscher and R.
Dillmann},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Robust 3D scan segmentation for teleoperation tasks in areas
contaminated by radiation},
year={2010},
pages={2419-2424},
abstract={3D data collected by a laser scanner has great potential for
robotic applications. Exact geometrical models of the environment
surrounding the robot can be created from these point clouds. But,
before creating any model, the 3D point cloud has to be segmented and
depending on the size and quality of the point cloud, this can be a very
challenging task. This article describes a robust 3D scan segmentation
technique, which is capable of segmenting a 3D point cloud in a short
amount of time. The results of the segmentation are used to assist a
teleoperator to manoeuvre a robot through an unknown environment. Our
segmentation approach copes with indoor and outdoor environments, using
only a minimum of assumptions, which makes it very robust. A 3D
visualisation illustrates the segmentation results in a clear and
user-friendly way.},
keywords={data visualisation;image segmentation;industrial
robots;optical scanners;radiation effects;telerobotics;3D point cloud;3D
visualisation;contaminated areas;geometrical models;laser
scanner;radiation;robust 3D scan segmentation;teleoperation
tasks;teleoperator;3D data segmentation;3D point cloud;3D
visualisation;rotating laser scanner;teleoperation tasks;traversability
map},
doi={10.1109/IROS.2010.5648926},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5649690,
author={M. Perrollaz and J. D. Yoder and A. Spalanzani and C. Laugier},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Using the disparity space to compute occupancy grids from
stereo-vision},
year={2010},
pages={2721-2726},
abstract={The occupancy grid is a popular tool for probabilistic
robotics, used for a variety of applications. Such grids are typically
based on data from range sensors (e.g. laser, ultrasound), and the
computation process is well known. The use of stereo-vision in this
framework is less common, and typically treats the stereo sensor as a
distance sensor, or fails to account for the uncertainties specific to
vision. In this paper, we propose a novel approach to compute occupancy
grids from stereo-vision, for the purpose of intelligent vehicles.
Occupancy is initially computed directly in the stereoscopic sensor's
disparity space, using the sensor's pixel-wise precision during the
computation process and allowing the handling of occlusions in the
observed area. It is also computationally efficient, since it uses the
u-disparity approach to avoid processing a large point cloud. In a
second stage, this disparity-space occupancy is transformed into a
Cartesian space occupancy grid to be used by subsequent applications. In
this paper, we present the method and show results obtained with real
road data, comparing this approach with others.},
keywords={grid computing;image sensors;probability;robot vision;stereo
image processing;vehicle dynamics;Cartesian space occupancy;disparity
space occupancy;grid computing;intelligent vehicles;point
cloud;probabilistic robotics;stereo sensor;stereo vision;stereoscopic
sensor},
doi={10.1109/IROS.2010.5649690},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5649872,
author={M. Johnson-Roberson and J. Bohg and M. Björkman and D. Kragic},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Attention-based active 3D point cloud segmentation},
year={2010},
pages={1165-1170},
abstract={In this paper we present a framework for the segmentation of
multiple objects from a 3D point cloud. We extend traditional image
segmentation techniques into a full 3D representation. The proposed
technique relies on a state-of-the-art min-cut framework to perform a
fully 3D global multi-class labeling in a principled manner. Thereby, we
extend our previous work in which a single object was actively segmented
from the background. We also examine several seeding methods to
bootstrap the graphical model-based energy minimization and these
methods are compared over challenging scenes. All results are generated
on real-world data gathered with an active vision robotic head. We
present quantitive results over aggregate sets as well as visual results
on specific examples.},
keywords={image representation;image segmentation;robot
vision;statistical analysis;3D representation;3d point cloud;active
vision robotic;bootstrap;graphical model;object segmentation},
doi={10.1109/IROS.2010.5649872},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5625162,
author={M. Perrollaz and J. D. Yoder and C. Laugier},
booktitle={13th International IEEE Conference on Intelligent
Transportation Systems},
title={Using obstacles and road pixels in the disparity-space
computation of stereo-vision based occupancy grids},
year={2010},
pages={1147-1152},
abstract={Occupancy grids have been used for a variety of applications
in the field of robotics. These grids have typically been created based
on data provided by range sensors such as laser or ultrasound. Current
practice is to create the grids based on a probabilistic sensor model
such as [1]. The use of stereovision to create occupancy grids is less
common. This paper will detail a novel approach to compute occupancy
grids, as applied to intelligent vehicles. Occupancy is initially
computed directly in the stereoscopic sensor's disparity space, allowing
the handling of occlusions in the observed area. It is also
computationally efficient, since it uses the u-disparity approach to
avoid processing a large point cloud. The occupancy calculation formally
accounts for the detection of obstacles and the road in disparity space,
as well as partial occlusions in the scene. In a second stage, this
disparity-space occupancy grid is transformed into a Cartesian space
occupancy grid to be used by subsequent applications. This
transformation includes a filtering step to reduce discretization
effects and explicitly account for the relation between range and
uncertainty in stereoscopic data. In this paper, we present the method
and show the results obtained with real road data.},
keywords={collision avoidance;robot vision;stereo image
processing;Cartesian space occupancy grid;disparity space
computation;disparity space occupancy grid;intelligent
vehicles;laser;obstacle detection;occupancy calculation;partial
occlusion;point cloud;probabilistic sensor model;range sensor;road
data;road pixel;robotics;stereo-vision based occupancy
grids;stereoscopic data;stereoscopic sensor disparity space;u-disparity
approach;ultrasound;Cameras;Copper;Estimation;Intelligent
sensors;Pixel;Roads},
doi={10.1109/ITSC.2010.5625162},
ISSN={2153-0009},
month={Sept},}
@INPROCEEDINGS{5604459,
author={D. Klimentjew and N. Hendrich and J. Zhang},
booktitle={2010 IEEE Conference on Multisensor Fusion and Integration},
title={Multi sensor fusion of camera and 3D laser range finder for
object recognition},
year={2010},
pages={236-241},
abstract={This paper proposes multi sensor fusion based on an effective
calibration method for a perception system designed for mobile robots
and intended for later object recognition. The perception system
consists of a camera and a three-dimensional laser range finder. The
three-dimensional laser range finder is based on a two-dimensional laser
scanner and a pan-tilt unit as a moving platform. The calibration
permits the coalescence of the two most important sensors for
three-dimensional environment perception, namely a laser scanner and a
camera. Both sensors permit multi sensor fusion consisting of color and
depth information. The calibration process based upon a specific
calibration pattern is used to define the extrinsic parameters and
calculate the transformation between a laser range finder and a camera.
The found transformation assigns an exact position and the color
information to each point of the surroundings. As a result, the
advantages of both sensors can be combined. The resulting structure
consists of colored unorganized point clouds. The achieved results can
be visualized with OpenGL and used for surface reconstruction. This way,
typical robotic tasks like object recognition, grasp calculation or
handling of objects can be realized. The results of our experiments are
presented in this paper.},
keywords={calibration;cameras;image colour analysis;laser ranging;mobile
robots;object recognition;robot vision;sensor fusion;surface
reconstruction;3D laser range finder;OpenGL;calibration
process;camera;colored unorganized point clouds;effective calibration
method;mobile robots;multisensor fusion;object recognition;pan-tilt
unit;perception system;surface reconstruction;three-dimensional laser
range finder;two-dimensional laser scanner;Calibration;Cameras;Image
color analysis;Laser fusion;Robot sensing systems;Three dimensional
displays},
doi={10.1109/MFI.2010.5604459},
month={Sept},}
@INPROCEEDINGS{5570010,
author={Y. Chen and Z. Du and M. García-Acosta},
booktitle={2010 Fifth IEEE International Symposium on Service Oriented
System Engineering},
title={Robot as a Service in Cloud Computing},
year={2010},
pages={151-158},
abstract={Service-oriented architecture and cloud computing are becoming
a dominant computing paradigm, as all major computing companies are
supporting this paradigm and more and more organizations are adopting
this paradigm. Robotics and service-oriented robotics computing start to
joint this new paradigm in the past five years and are now ready to
participate in large scale. This paper reports our research on
service-oriented robotics computing and our design, implementation, and
evaluation of Robot as a Service (RaaS) unit. To fully qualify the RaaS
as a cloud computing unit, we have kept our design to comply with the
common service standards, development platforms, and execution
infrastructure. We also keep the source code open and allow the
community to configure the RaaS following the Web 2.0 principles of
participation. Developers can add, remove, and modify the RaaS of their
own. For this purpose, we have implemented our RaaS on Windows and Linux
operating systems running on Atom and Core 2 Duo architectures. RaaS
supports programming languages commonly used for service-oriented
computing such as Java and C#. Special efforts have been made to support
Microsoft Visual Programming Language (VPL) for graphic composition. We
are working with high schools to use RaaS and VPL in robotics camps and
robotics competitions.},
keywords={Internet;robot programming;software architecture;visual
programming;Linux operating systems;Microsoft Visual Programming
Language;RaaS;Windows;cloud computing;graphic composition;robot as a
service;service-oriented architecture;service-oriented robotics
computing;Cloud computing;Clouds;Driver circuits;Integrated
circuits;Robot sensing systems;Service oriented
architecture;RaaS;SOA;SOC;Service-oriented robotics computing;and robot
on Intel architecture},
doi={10.1109/SOSE.2010.44},
month={June},}
@INPROCEEDINGS{5556630,
author={R. Cordeiro and J. M. Fonseca},
booktitle={5th Iberian Conference on Information Systems and Technologies},
title={Euronet Lab: Euronet LAB, A cloud V-lab enviroment},
year={2010},
pages={1-5},
abstract={In this paper we present a proposal for the creation of a
European V-labs web space. In its essence it would result in an open
online laboratory, with a primarily practical nature. In this laboratory
students will have the opportunity to develop skills in the
“know-how-to-do” area, enabling them to conduct a series of practical
experiences in “try-out” philosophy that will substantiate and
consolidate all knowledge that the students acquired in lectures. It is
quite possible that these resources aren't available in all universities
and institutions, specifically in the university where the student is.
This environment provides substance to the Directive stated in the
declarations of Bologna and Prague that expresses “the teaching process
is therefore student-centered”, strengthening the final pedagogical aim
of “learning to learn”, as lifelong learning is assumed as an
indispensable stage. What we propose is the creation of a virtual
environment for e-learning where a series of virtual labs in many areas
of electronics, automation and robotics are available, in this
environment it will do possible for any student of any of these
universities to scheduling of experience in any institution that belongs
to this cloud, and thus can perform is work for anytime that will be
available and with technical resources available or not available in its
own university.},
keywords={computer aided instruction;laboratories;virtual
reality;Euronet Lab;cloud V-lab environment;e-learning;know-how-to-do
area;learning-to-learn pedagogy;open online laboratory;teaching
process;try-out philosophy;virtual environment;Collaboration;Educational
institutions;Electronic learning;Internet;Least squares
approximation;Materials;Servers;Cloud-Learning;Cooperative
E-learning;E-learning;Remote Laboratory;Virtual Labs;Virtual Learning},
ISSN={2166-0727},
month={June},}
@INPROCEEDINGS{5552012,
author={T. Li and G. Yuan and Q. Duan},
booktitle={Proceedings of 2010 IEEE/ASME International Conference on
Mechatronic and Embedded Systems and Applications},
title={Navigation technology of autonomous mobile robots in unknown
environments},
year={2010},
pages={533-538},
abstract={To realize the navigation of the autonomous mobile robots in
an unknown environment, the control architectonics with the integration
of deliberative and reactive paradigm should be set up. This paper uses
the cloud model to processing the uncertain sonar sensor data, and
establishes the multi-rules reasoning generator to realize the reaction
avoiding collision. And the finding sub-target method by searching the
tangent point is presented to realize the efficient deliberative
planning. We use the Player / Stage which is developed by USC Robotics
Research Laboratory to simulate the motion of mobile robot. The
simulation result shows the navigation method presented in the paper can
effectively control the robots to reach the target and avoiding
collision.},
keywords={collision avoidance;mobile robots;motion
control;navigation;architectonics control;autonomous mobile robots
navigation technology;cloud model;collision avoidance;deliberative
planning;mobile robot motion;sonar sensor data;unknown
environments;Clouds;Mobile communication;Navigation;Robots},
doi={10.1109/MESA.2010.5552012},
month={July},}
@INPROCEEDINGS{5540159,
author={E. Shechtman and A. Rav-Acha and M. Irani and S. Seitz},
booktitle={2010 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition},
title={Regenerative morphing},
year={2010},
pages={615-622},
abstract={We present a new image morphing approach in which the output
sequence is regenerated from small pieces of the two source (input)
images. The approach does not require manual correspondence, and
generates compelling results even when the images are of very different
objects (e.g., a cloud and a face). We pose the morphing task as an
optimization with the objective of achieving bidirectional similarity of
each frame to its neighbors, and also to the source images. The
advantages of this approach are 1) it can operate fully automatically,
producing effective results for many sequences (but also supports manual
correspondences, when available), 2) ghosting artifacts are minimized,
and 3) different parts of the scene move at different rates, yielding
more interesting (and less robotic) transitions.},
keywords={image morphing;interpolation;minimisation;bidirectional
similarity;ghosting artifact minimization;regenerative image
morphing;temporal interpolation;Clouds;Eyes;Facial
features;Interpolation;Layout;Motion pictures;Mouth;Robotics and
automation;Service robots;Visual effects},
doi={10.1109/CVPR.2010.5540159},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{5518566,
author={R. Walia and R. A. Jarvis},
booktitle={2010 IEEE Conference on Cybernetics and Intelligent Systems},
title={Horizon detection from pseudo spectra images of water scenes},
year={2010},
pages={138-144},
abstract={Horizon detection is a pre-cursor to vision processing in air
and water robotics. This paper makes three contributions to horizon
detection. First, a theoretical framework for generating pseudo spectra
images (PSI), from spectrum analysis of XYZ color-space is presented.
Second, wavelengths in the visible spectrum are identified, at which the
PSI has similar intensities for sky and clouds. Generating PSI at these
wavelengths minimizes artifacts due to clouds in the sky, resulting in
well defined horizon. Third, fitting ellipses are presented as an
alternate to Hough Transform for horizon detection. Ellipses have lower
computational complexity than Hough Transform and can accommodate curved
edges as candidates for horizon.},
keywords={Hough transforms;computer vision;image colour analysis;image
representation;Hough transform;XYZ color-space;air robotics;fitting
ellipses;horizon detection;pseudo spectra images;spectrum
analysis;vision processing;water robotics;water
scenes;Australia;Cameras;Clouds;Color;Computational
complexity;Hardware;Image generation;Layout;Rendering (computer
graphics);Sensor arrays;ellipses;horizon detection;otsu's
threshold;spectrum analysis},
doi={10.1109/ICCIS.2010.5518566},
ISSN={2326-8123},
month={June},}
@INPROCEEDINGS{5509469,
author={R. Arumugam and V. R. Enti and L. Bingbing and W. Xiaojun and K.
Baskaran and F. F. Kong and A. S. Kumar and K. D. Meng and G. W. Kit},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={DAvinCi: A cloud computing framework for service robots},
year={2010},
pages={3084-3089},
abstract={We propose DAvinCi, a software framework that provides the
scalability and parallelism advantages of cloud computing for service
robots in large environments. We have implemented such a system around
the Hadoop cluster with ROS (Robotic Operating system) as the messaging
framework for our robotic ecosystem. We explore the possibilities of
parallelizing some of the robotics algorithms as Map/Reduce tasks in
Hadoop. We implemented the FastSLAM algorithm in Map/Reduce and show how
significant performance gains in execution times to build a map of a
large area can be achieved with even a very small eight-node Hadoop
cluster. The global map can later be shared with other robots introduced
in the environment via a Software as a Service (SaaS) Model. This
reduces the burden of exploration and map building for the new robot and
minimizes it's need for additional sensors. Our primary goal is to
develop a cloud computing environment which provides a compute cluster
built with commodity hardware exposing a suite of robotic algorithms as
a SaaS and share data co-operatively across the robotic ecosystem.},
keywords={Internet;SLAM (robots);parallel architectures;service
robots;software architecture;DAvinCi software framework;FastSLAM
algorithm;cloud computing;hadoop cluster;map-reduce tasks;robotic
ecosystem;robotic operating system;service robots;software as a service
model;Cloud computing;Clustering algorithms;Ecosystems;Operating
systems;Parallel processing;Parallel robots;Performance gain;Robot
sensing systems;Scalability;Service robots},
doi={10.1109/ROBOT.2010.5509469},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509308,
author={C. Sok and M. D. Adams},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Visually aided feature extraction from 3D range data},
year={2010},
pages={2273-2279},
abstract={Robust feature extraction within 3D environments is a crucial
requirement for many autonomous robotic and tracking applications. 3D
Laser range finders and cameras provide extremely rich data about an
environment. However, the algorithms which attempt to compress the vast
data sets produced by these sensors into features, tend to be fragile in
the presence of sensor noise, or computationally expensive. This paper
presents a 3D feature extraction technique which greatly compresses 3D
range data based on principal component analysis (PCA). PCA can provide
a greatly compressed vector set, representing the dominant directions of
data points, thus grouping them into planes or lines. It is shown
however, that the naive application of PCA to full, 3D, point cloud data
sets, results in a poor representation of the dominant data directions.
Therefore, a combination of a panoramic camera and 3D laser range finder
is used to extract robust planes from 3D range data. The panoramic
camera image is first filtered with the Mean Shift algorithm to smooth
segments within it, whilst preserving the integrity of the segment
edges. These segments are then used to guide the PCA, through an
approximate image to range space calibration, to act on the
corresponding individual segments of range data. The application of PCA
to segmented subsets of 3D point cloud data sets, will be shown to be
robust for the detection of planes in both indoor and urban, outdoor
environments.},
keywords={data compression;feature extraction;laser ranging;mobile
robots;principal component analysis;robot vision;smoothing methods;3D
feature extraction technique;3D laser range finders;autonomous robotic
application;cameras;data set compression;mean shift algorithm;panoramic
camera image filtering;principal component analysis;range space
calibration;segment smoothing;tracking application;visually aided
feature extraction;Cameras;Clouds;Feature extraction;Image
segmentation;Laser applications;Laser noise;Principal component
analysis;Robot sensing systems;Robustness;Sensor phenomena and
characterization},
doi={10.1109/ROBOT.2010.5509308},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509679,
author={D. Gingras and É. Dupuis and G. Payre and J. de Lafontaine},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Path planning based on fluid mechanics for mobile robots using
unstructured terrain models},
year={2010},
pages={1978-1984},
abstract={Mobile robots using a 360° field of view LIDAR ranging sensor
can generate enormous 3D point clouds. To reduce the quantity of data in
memory a compression can lead to unstructured environment models such as
irregular meshes. This kind of structure can contain deformed cells and
the path planning can be cumbersome. This paper presents a path planning
method based on fluid mechanics able to deal with unstructured terrain
models. The algorithm uses the finite element method to compute a
velocity potential function free from local minima. Then, several
streamlines are computed as a road map and the optimal path is selected
among the candidate paths. The approach is implemented on the Canadian
Space Agency (CSA) Mars Robotics Testbed (MRT) rover and tested at the
CSA Mars Emulation Terrain (MET). To confirm the feasibility of the
method, the path planner has been tested on 284 LIDAR scans collected in
a realistic outdoor challenging terrain.},
keywords={finite element analysis;fluid mechanics;mobile robots;optical
radar;path planning;3D point cloud;Canadian space agency;LIDAR ranging
sensor;Mars emulation terrain;Mars robotics testbed;finite element
method;fluid mechanics;mobile robot;optimal path;path planning;realistic
outdoor challenging terrain;unstructured environment model;unstructured
terrain model;velocity potential function;Clouds;Emulation;Finite
element methods;Laser radar;Mars;Mobile robots;Orbital robotics;Path
planning;Roads;Testing},
doi={10.1109/ROBOT.2010.5509679},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5479477,
author={K. Hammoudi and F. Dornaika and B. Soheilian and N. Paparoditis},
booktitle={2010 Canadian Conference on Computer and Robot Vision},
title={Extracting outlined planar clusters of street facades from 3D
point clouds},
year={2010},
pages={122-129},
abstract={This paper presents an approach for extracting 3D outlined
planar clusters of street facades. Terrestrial laser data are acquired
using a Mobile Mapping System (MMS). Mapping of street facades is of
great interest in various digital mapping and robotic research topics.
After a filtering step of the 3D point cloud, the dominant hypothetical
facade planes are detected using an adapted Progressive Probabilistic
Hough Transform (PPHT). The corresponding planar clusters are extracted
using a priori geometric knowledge of street. The clusters are
horizontally and vertically delimited using heuristic approaches. The
adapted PPHT allows the automatic extraction of georeferenced planar
clusters of facades with a fine detection of dominant facade lines and a
low computation time. The adopted approach has been tested on a set of
point cloud acquired in the city of Paris under real conditions.
Examples and experimental results show the efficiency and the potential
of the proposed approach.},
keywords={Hough transforms;cartography;mobile computing;robots;3D point
clouds;a priori geometric knowledge;mobile mapping system;outlined
planar clusters;progressive probabilistic Hough transform;robotic
research;street facades;terrestrial laser data;Cities and towns;Computer
vision;Data mining;Image reconstruction;Laser modes;Robot sensing
systems;Robot vision systems;Simultaneous localization and
mapping;Three-dimensional displays;Urban areas;3D city modeling;3D point
cloud;3D street reconstruction;Hough transform;terrestrial laser scanning},
doi={10.1109/CRV.2010.23},
month={May},}
@INPROCEEDINGS{5424164,
author={A. Birk and S. Schwertfeger and K. Pathak and N. Vaskevicius},
booktitle={2009 IEEE International Workshop on Safety, Security Rescue
Robotics (SSRR 2009)},
title={3D data collection at Disaster City at the 2008 NIST Response
Robot Evaluation Exercise (RREE)},
year={2009},
pages={1-6},
abstract={A collection of 3D data sets gathered at the 2008 NIST
Response Robot Evaluation Exercise (RREE) in Disaster City, Texas is
described. The data sets consist of 3D point clouds collected with an
actuated laser range finder in different disaster scenarios. The data
sets can be used for performance evaluation of robotics algorithms,
especially for 3D mapping. An example is discussed where a 3D model is
generated from scans taken in a collapsed car parking.},
keywords={SLAM (robots);service robots;2008 NIST Response Robot
Evaluation Exercise;3D data collection;3D mapping;3D point clouds;Cities
and towns;Clouds;Intelligent robots;Jacobian matrices;Laser
modes;NIST;Ring lasers;Robot sensing systems;Safety;Simultaneous
localization and mapping;3D laser range finder;3D simultaneous
localization and mapping (SLAM);performance evaluation;point cloud},
doi={10.1109/SSRR.2009.5424164},
ISSN={2374-3247},
month={Nov},}
@INPROCEEDINGS{5423231,
author={J. Kim and S. Lee},
booktitle={2009 IEEE International Symposium on Computational
Intelligence in Robotics and Automation - (CIRA)},
title={Fast neighbor cells finding method for multiple octree
representation},
year={2009},
pages={540-545},
abstract={A cell occupancy map has been used widely for efficiently
representing obstacles in robotic navigation. Such a map can often be
formed based on the multi-resolution octree representation (MOR) of 3D
point clouds captured from objects and workspace. This elevated
cell-based approach may offer the capability of understanding the
geometric context of workspace, expanding its applicability to robotic
manipulation in a cluttered workspace. Under this context, the main
issue of MOR becomes how to represent and generate cell addresses in
such a way as to find neighboring cells efficiently. This paper presents
a novel method for efficiently searching for neighboring cells with the
fast generation of all the neighboring cell addresses. The original
contribution of this paper is that not only the direct neighbors defined
by those cells the edges or corners of which are directly connected to
the given cell, but also the indirect neighbors of distance r, defined
by those cells being separated from the given cell by the distance r,
are included. The proposed method have been implemented and applied to
obstacle representation in the 3D workspace modeling.},
keywords={computational geometry;image representation;path
planning;robot vision;trees (mathematics);3D point clouds;3D workspace
modeling;cell occupancy map;fast neighbor cells finding;multiresolution
octree representation;obstacle representation;robotic
manipulation;Clouds;Educational programs;Educational
technology;Encoding;Helium;Intelligent robots;Knowledge
engineering;Navigation;Path planning;Systems engineering education},
doi={10.1109/CIRA.2009.5423231},
month={Dec},}
@INPROCEEDINGS{5420757,
author={D. Klimentjew and M. Arli and J. Zhang},
booktitle={2009 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={3D scene reconstruction based on a moving 2D laser range finder
for service-robots},
year={2009},
pages={1129-1134},
abstract={This paper presents the simulation of a three-dimensional
laser range finder based on a two-dimensional laser scanner and
different moving units. We examine and describe two methods differing in
the way the laser range finder is mounted. In addition, a Pan-Tilt-Unit
and a robot manipulator are used as moving platforms. The qualities of
the laser scanner and the originating point density, which are very
important for the system design, as well as the mathematical grounds for
the reconstruction will be introduced and discussed in detail. In the
next step, the resulting transformation matrixes and error compensation
will be reviewed. Relevant to the moving units, the registration methods
and the possible scan strategy are described and discussed. The
concurrent application of both systems permits the viewing of the scene
from different perspectives. The surroundings can be reconstructed with
the help of mathematical transformations depending on the physical
design, the resulting structure consists of unorganised point clouds.
The achieved results can be visualised with OpenGL or Java3D and used
for surface reconstruction. This way, typical robotic tasks like
collision avoidance, grasp calculation, or handling of objects can be
realised.},
keywords={image reconstruction;laser ranging;manipulators;optical
scanners;service robots;2D laser scanner;3D laser range finder;3D scene
reconstruction;Java3D;OpenGL;error compensation;moving 2D laser range
finder;pan-tilt-unit;robot manipulator;robotic tasks;service
robots;surface reconstruction;transformation matrixes;unorganised point
clouds;Clouds;Collision avoidance;Image reconstruction;Laser modes;Laser
theory;Layout;Navigation;Robot sensing systems;Robot vision
systems;Surface reconstruction;3D model acquisition;3D
reconstruction;laser range finder;robotics;unorganised point clouds},
doi={10.1109/ROBIO.2009.5420757},
month={Dec},}
@INPROCEEDINGS{5415309,
author={J. L. Martinez and A. Mandow and A. Reina and J. Morales},
booktitle={2009 35th Annual Conference of IEEE Industrial Electronics},
title={Outdoor scene registration from 3D laser range data with coarse
binary cubes},
year={2009},
pages={2308-2313},
abstract={Outdoor scene registration of two 3D range scans, as usually
required by field robotics, is very different from object and indoor
registration, since all scan directions and depths may contain relevant
data. This is not considered by 3D matching methods devised for computer
graphics and reverse engineering. In this paper, we propose a
specialized registration method based on the concept of coarse binary
cubes within a cuboid fitted to the point cloud of the first scan. An
integer objective function that does not employ point distances is
defined as the number of coincident cubes. Its value is obtained by fast
intersection of two binary vectors and does not involve any 3D data
structures. This strategy avoids that denser range areas outweigh
farther laser points, which can be useful to achieve good orientation
estimations. The objective function is suitable for any optimization
method. In particular, the Simplex algorithm has been adapted to avoid
local minima. Experimental results in natural and urban environments are
presented, where the method achieves similar accuracy and computation
time as ICP, but increases robustness against initial misalignments.},
keywords={image registration;optimisation;vectors;3D laser range
data;binary vector;coarse binary cube;coincident cubes;integer objective
function;outdoor scene registration;simplex algorithm;Clouds;Computer
graphics;Data structures;Iterative closest point algorithm;Layout;Mobile
robots;Optimization methods;Reverse engineering;Robustness;Service robots},
doi={10.1109/IECON.2009.5415309},
ISSN={1553-572X},
month={Nov},}
@INPROCEEDINGS{5379597,
author={R. B. Rusu and A. Holzbach and R. Diankov and G. Bradski and M.
Beetz},
booktitle={2009 9th IEEE-RAS International Conference on Humanoid Robots},
title={Perception for mobile manipulation and grasping using active
stereo},
year={2009},
pages={632-638},
abstract={In this paper we present a comprehensive perception system
with applications to mobile manipulation and grasping for personal
robotics. Our approach makes use of dense 3D point cloud data acquired
using stereo vision cameras by projecting textured light onto the scene.
To create models suitable for grasping, we extract the supporting planes
and model object clusters with different surface geometric primitives.
The resultant decoupled primitive point clusters are then reconstructed
as smooth triangular mesh surfaces, and their use is validated in
grasping experiments using OpenRAVE . To annotate the point cloud data
with primitive geometric labels we make use of our previously proposed
Fast Point Feature Histograms and probabilistic graphical methods
(Conditional Random Fields), and obtain a classification accuracy of
98.27% for different object geometries. We show the validity of our
approach by analyzing the proposed system for the problem of building
object models usable in grasping applications with the PR2 robot (see
Figure 1).},
keywords={manipulators;mesh generation;pattern clustering;robot
vision;3D point cloud data;OpenRAVE;PR2 robot;active stereo;fast point
feature histograms;mobile manipulation;object clusters;smooth triangular
mesh surfaces;stereo vision cameras;surface geometric
primitives;Cameras;Clouds;Data mining;Histograms;Layout;Mobile
robots;Robot vision systems;Solid modeling;Stereo vision;Surface
reconstruction},
doi={10.1109/ICHR.2009.5379597},
ISSN={2164-0572},
month={Dec},}
@INPROCEEDINGS{5356543,
author={D. Ignakov and G. Okouneva and G. Liu},
booktitle={2009 IEEE Symposium on Computational Intelligence for
Security and Defense Applications},
title={Localization of door handle using a single camera on a door
opening mobile manipulator},
year={2009},
pages={1-7},
abstract={This paper presents a novel approach to localizing a door
handle of unknown geometry to assist in autonomous door opening. The
localization is performed using data from a single CCD camera that is
mounted at the end-effector of a mobile manipulator. The proposed
algorithm extracts a 3D point cloud using optical flow and known camera
motion provided by the manipulator. Segmentation of the point cloud is
then performed, enabling the separation of the door and the handle
points, which is then followed by fitting a boundary box to the door
handle data. The fitted box can then be used to guide robotic grasping.
The proposed algorithm has been validated using a 3D virtual scene, and
the results have demonstrated the effectiveness of the proposed method
to localize a door handle in an unknown environment.},
keywords={CCD image sensors;end effectors;mobile robots;robot vision;3D
point cloud;3D virtual scene;CCD camera;autonomous door opening;door
handle localization;door opening mobile manipulator;end-effector;optical
flow;Cameras;Charge coupled devices;Charge-coupled image
sensors;Clouds;Data mining;Geometrical optics;Image motion
analysis;Layout;Manipulators;Robot vision systems},
doi={10.1109/CISDA.2009.5356543},
ISSN={2329-6267},
month={July},}
@INPROCEEDINGS{5354683,
author={R. B. Rusu and N. Blodow and Z. C. Marton and M. Beetz},
booktitle={2009 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Close-range scene segmentation and reconstruction of 3D point
cloud maps for mobile manipulation in domestic environments},
year={2009},
pages={1-6},
abstract={In this paper we present a framework for 3D geometric shape
segmentation for close-range scenes used in mobile manipulation and
grasping, out of sensed point cloud data. Our proposed approach proposes
a robust geometric mapping pipeline for large input datasets that
extracts relevant objects useful for a personal robotic assistant to
perform manipulation tasks. The objects are segmented out from partial
views and a reconstructed model is computed by fitting geometric
primitive classes such as planes, spheres, cylinders, and cones. The
geometric shape coefficients are then used to reconstruct missing data.
Residual points are resampled and triangulated, to create smooth
decoupled surfaces that can be manipulated. The resulted map is
represented as a hybrid concept and is comprised of 3D shape
coefficients and triangular meshes used for collision avoidance in
manipulation routines.},
keywords={collision avoidance;computational geometry;image
reconstruction;image segmentation;mesh generation;robot vision;3D
geometric shape segmentation;3D point cloud maps;3D shape
coefficients;close-range scene reconstruction;close-range scene
segmentation;collision avoidance;mobile manipulation;personal robotic
assistant;robust geometric mapping pipeline;triangular
meshes;Clouds;Data mining;Layout;Pipelines;Robot sensing
systems;Robustness;Shape;Solid modeling;Surface fitting;Surface
reconstruction},
doi={10.1109/IROS.2009.5354683},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5354759,
author={R. B. Rusu and Z. C. Marton and N. Blodow and A. Holzbach and M.
Beetz},
booktitle={2009 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Model-based and learned semantic object labeling in 3D point
cloud maps of kitchen environments},
year={2009},
pages={3601-3608},
abstract={We report on our experiences regarding the acquisition of
hybrid Semantic 3D Object Maps for indoor household environments, in
particular kitchens, out of sensed 3D point cloud data. Our proposed
approach includes a processing pipeline, including geometric mapping and
learning, for processing large input datasets and for extracting
relevant objects useful for a personal robotic assistant to perform
complex manipulation tasks. The type of objects modeled are objects
which perform utilitarian functions in the environment such as kitchen
appliances, cupboards, tables, and drawers. The resulted model is
accurate enough to use it in physics-based simulations, where doors of
3D containers can be opened based on their hinge position. The resulted
map is represented as a hybrid concept and is comprised of both the
hierarchically classified objects and triangular meshes used for
collision avoidance in manipulation routines.},
keywords={domestic appliances;pipeline
processing;robots;three-dimensional displays;3D object maps;3D point
cloud maps;acquisition hybrid semantic;collision avoidance manipulation
routines;complex manipulation tasks;extracting relevant
objects;geometric learning;geometric mapping;kitchen appliances;kitchen
environments;model based learned semantic object labeling;personal
robotic assistant;physics based simulations;pipeline
processing;processing large input datasets;triangular meshes
used;Containers;Home appliances;Intelligent robots;Labeling;Robot
kinematics;Solid modeling;Three-dimensional displays},
doi={10.1109/IROS.2009.5354759},
ISSN={2153-0858},
month={Oct},}
@ARTICLE{5306926,
author={A. Birk and N. Vaskevicius and K. Pathak and S. Schwertfeger and
J. Poppinga and H. Buelow},
journal={IEEE Robotics Automation Magazine},
title={3-D perception and modeling},
year={2009},
volume={16},
number={4},
pages={53-60},
abstract={In the context of the 2008 Lunar Robotics Challenge (LRC) of
the European Space Agency (ESA), the Jacobs Robotics team investigated
three-dimensional (3-D) perception and modeling as an important basis of
autonomy in unstructured domains. Concretely, the efficient modeling of
the terrain via a 3D laser range finder (LRF) is addressed. The
underlying fast extraction of planar surface patches can be used to
improve situational awareness of an operator or for path planning. 3D
perception and modeling is an important basis for mobile robot
operations in planetary exploration scenarios as it supports good
situation awareness for motion level teleoperation as well as higher
level intelligent autonomous functions. It is hence desirable to get
long-range 3D data with high resolution, large field of view, and very
fast update rates. 3D LRF have a high potential in this respect. In
addition, 3D LRF can operate under conditions where standard vision
based methods fail, e.g., under extreme light conditions. However, it is
nontrivial to transmit the huge amount of data delivered by a 3D LRF to
an operator station or to use this point cloud data as basis for higher
level intelligent functions. Based on our participation in the LRC of
the ESA, it is shown how the huge amount of 3D point cloud data from 3D
LRF can be tremendously reduced. Concretely, large sets of points are
replaced by planar surface patches that are fitted into the data in an
optimal way. The underlying computations are very efficient and hence
suited for online computations onboard of the robot.},
keywords={aerospace robotics;intelligent robots;laser ranging;mobile
robots;path planning;robot vision;telerobotics;3D laser range finder;3D
perception;autonomous robot;intelligent autonomous function;lunar
robotics challenge;mobile robot operation;motion level
teleoperation;path planning;planar surface fast extraction;planetary
exploration;situational awareness improvement;Context
modeling;Intelligent robots;Intelligent sensors;Mobile
robots;Moon;Orbital robotics;Project management;Remotely operated
vehicles;Robot sensing systems;Stereo vision;3-D mapping;Space
robotics;autonomy;plane fitting;planetary exploration;surface
representation;telerobotics},
doi={10.1109/MRA.2009.934822},
ISSN={1070-9932},
month={December},}
@INPROCEEDINGS{5174729,
author={F. Maurelli and D. Droeschel and T. Wisspeintner and S. May and
H. Surmann},
booktitle={2009 International Conference on Advanced Robotics},
title={A 3D laser scanner system for autonomous vehicle navigation},
year={2009},
pages={1-6},
abstract={Road segmentation, obstacle detection, situation awareness
constitute fundamental tasks for autonomous vehicles in urban
environments. This paper describes an end-to-end system capable of
generating high-quality 3D point clouds from one or two of the popular
LMS200 laser on a continuously moving vehicle. Road segmentation and
crossing analysis have been performed on the basis of this newly
developed 3D laser scanner. This system is cost-efficient and provides a
circumferential view that makes it also applicable to mid-sized robotic
platforms, like Volksbot robots. Field experiments from the DARPA Urban
Challenge are presented. Our aim is to provide a level of hardware and
algorithmic detail suitable for replication of our system by interested
parties or to get a cheap 3D laser scanner for research for those who do
not wish to invest in hardware development.},
keywords={collision avoidance;motion control;optical scanners;road
vehicles;robot vision;3D laser scanner system;3D point clouds;LMS200
laser;Volksbot robot;autonomous vehicle navigation;continuously moving
vehicle;crossing analysis;end-to-end system;obstacle detection;road
segmentation;situation awareness;urban
environment;Cameras;Hardware;Humans;Laser radar;Mobile
robots;Navigation;Performance analysis;Remotely operated vehicles;Road
vehicles;Vehicle crash testing},
month={June},}
@ARTICLE{4912410,
author={Y. Wang and A. I. Lyapustin and J. L. Privette and J. T.
Morisette and B. Holben},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Atmospheric Correction at AERONET Locations: A New Science and
Validation Data Set},
year={2009},
volume={47},
number={8},
pages={2450-2466},
abstract={This paper describes an Aerosol Robotic Network
(AERONET)-based Surface Reflectance Validation Network (ASRVN) and its
data set of spectral surface bidirectional reflectance and albedo based
on Moderate Resolution Imaging Spectroradiometer (MODIS) TERRA and AQUA
data. The ASRVN is an operational data collection and processing system.
It receives 50 times 50 km^2 subsets of MODIS level 1B (L1B) data from
MODIS adaptive processing system and AERONET aerosol and water-vapor
information. Then, it performs an atmospheric correction (AC) for about
100 AERONET sites based on accurate radiative-transfer theory with
complex quality control of the input data. The ASRVN processing software
consists of an L1B data gridding algorithm, a new cloud-mask (CM)
algorithm based on a time-series analysis, and an AC algorithm using
ancillary AERONET aerosol and water-vapor data. The AC is achieved by
fitting the MODIS top-of-atmosphere measurements, accumulated for a
16-day interval, with theoretical reflectance parameterized in terms of
the coefficients of the Li Sparse-Ross Thick (LSRT) model of the
bidirectional reflectance factor (BRF). The ASRVN takes several steps to
ensure high quality of results: 1) the filtering of opaque clouds by a
CM algorithm; 2) the development of an aerosol filter to filter residual
semitransparent and subpixel clouds, as well as cases with high
inhomogeneity of aerosols in the processing area; 3) imposing the
requirement of the consistency of the new solution with previously
retrieved BRF and albedo; 4) rapid adjustment of the 16-day retrieval to
the surface changes using the last day of measurements; and 5)
development of a seasonal backup spectral BRF database to increase data
coverage. The ASRVN provides a gapless or near-gapless coverage for the
processing area. The gaps, caused by clouds, are filled most naturally
with the latest solution for a given pixel. The ASRVN products include
three parameters of the LSRT model (k^L , kG, and k^V ), surface albedo,
normalized BRF (computed for a standard viewing geometry, VZA = 0deg,
SZA = 45deg), and instantaneous BRF (or one-angle BRF value derived from
the last day of MODIS measurement for specific viewing geometry) for the
MODIS 500-m bands 1-7. The results are produced daily at a resolution of
1 km in gridded format. We also provide a cloud mask, a quality flag,
and a browse bitmap image. The ASRVN data set, including 6 years of
MODIS TERRA and 1.5 years of MODIS AQUA data, is available now as a
standard MODIS product (MODASRVN) which can be accessed through the
Level 1 and Atmosphere Archive and Distribution System website
((http://ladsweb.nascom.nasa.gov/data/search.html).). It can be used for
a wide range of applications including validation analysis and science
research.},
keywords={aerosols;atmospheric optics;geophysical signal
processing;reflectivity;remote sensing;AERONET aerosol
information;AERONET based Surface Reflectance Validation Network;AERONET
locations;AERONET water vapor information;ASRVN processing
software;Aerosol Robotic Network;L1B data gridding algorithm;LSRT
model;Li Sparse-Ross Thick model;MODIS AQUA data;MODIS TERRA data;MODIS
adaptive processing system;MODIS level 1B data;Moderate Resolution
Imaging Spectroradiometer;aerosol filter;atmospheric correction
algorithm;bidirectional reflectance factor;cloud mask algorithm;data
collection system;data processing system;high aerosol
inhomogeneity;input data quality control;opaque cloud
filtering;radiative transfer theory;residual semitransparent
clouds;seasonal backup spectral BRF database;spectral surface
albedo;spectral surface bidirectional reflectance;subpixel
clouds;Aerosols;remote sensing},
doi={10.1109/TGRS.2009.2016334},
ISSN={0196-2892},
month={Aug},}
@ARTICLE{4840391,
author={A. M. Cretu and P. Payeur and E. M. Petriu},
journal={IEEE Transactions on Instrumentation and Measurement},
title={Selective Range Data Acquisition Driven by Neural-Gas Networks},
year={2009},
volume={58},
number={8},
pages={2634-2642},
abstract={The collection of the rich flow of information provided by the
current generation of fast vision sensing systems brings new challenges
in the selection of only relevant features out of the avalanche of data
generated by those sensors. This paper discusses some aspects of
intelligent sensing for advanced robotic applications, with the main
objective of designing innovative approaches for automatic selection of
regions of observation for fixed and mobile sensors to collect only
relevant measurements without human guidance. The proposed
neural-gas-network solution selects regions of interest for further
sampling from a cloud of sparsely collected 3-D measurements. The
technique automatically determines bounded areas where sensing is
required at a higher resolution to accurately map 3-D surfaces.
Therefore, it provides significant benefits over brute-force strategies
as scanning time is reduced and the size of the data set is kept
manageable. Experimental evaluation of this technology is presented for
3-D surface measurement and modeling.},
keywords={data acquisition;intelligent sensors;neural nets;robot
vision;3D surface measurement;advanced robotic applications;brute-force
strategies;fixed sensors;intelligent sensing;mobile sensors;neural-gas
networks;observation regions;regions automatic selection;selective range
data acquisition;vision sensing systems;3-D vision;Feature
detection;neural gas;neural networks;selective sensing;surface modeling},
doi={10.1109/TIM.2009.2015643},
ISSN={0018-9456},
month={Aug},}
@INPROCEEDINGS{5152533,
author={F. Yuan and A. Swadzba and R. Philippsen and O. Engin and M.
Hanheide and S. Wachsmuth},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Laser-based navigation enhanced with 3D time-of-flight data},
year={2009},
pages={2844-2850},
abstract={Navigation and obstacle avoidance in robotics using planar
laser scans has matured over the last decades. They basically enable
robots to penetrate highly dynamic and populated spaces, such as
people's home, and move around smoothly. However, in an unconstrained
environment the two-dimensional perceptual space of a fixed mounted
laser is not sufficient to ensure safe navigation. In this paper, we
present an approach that pools a fast and reliable motion generation
approach with modern 3D capturing techniques using a time-of-flight
camera. Instead of attempting to implement full 3D motion control, which
is computationally more expensive and simply not needed for the targeted
scenario of a domestic robot, we introduce a ldquovirtual laserrdquo.
For the originally solely laser-based motion generation the technique of
fusing real laser measurements and 3D point clouds into a continuous
data stream is 100% compatible and transparent. The paper covers the
general concept, the necessary extrinsic calibration of two very
different types of sensors, and exemplarily illustrates the benefit
which is to avoid obstacles not being perceivable in the original laser
scan.},
keywords={collision avoidance;image sensors;measurement by laser
beam;motion control;optical sensors;robot vision;3D capturing
techniques;3D time-of-flight data;laser-based navigation;motion
generation approach;obstacle avoidance;planar laser scans;time-of-flight
camera;two-dimensional perceptual space;unconstrained
environment;virtual laser;Calibration;Cameras;Clouds;Laser modes;Motion
control;Motion measurement;Navigation;Orbital robotics;Robot sensing
systems;Robot vision systems},
doi={10.1109/ROBOT.2009.5152533},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152493,
author={K. Klasing and D. Althoff and D. Wollherr and M. Buss},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Comparison of surface normal estimation methods for range sensing
applications},
year={2009},
pages={3206-3211},
abstract={As mobile robotics is gradually moving towards a level of
semantic environment understanding, robust 3D object recognition plays
an increasingly important role. One of the most crucial prerequisites
for object recognition is a set of fast algorithms for geometry
segmentation and extraction, which in turn rely on surface normal
vectors as a fundamental feature. Although there exists a plethora of
different approaches for estimating normal vectors from 3D point clouds,
it is largely unclear which methods are preferable for online processing
on a mobile robot. This paper presents a detailed analysis and
comparison of existing methods for surface normal estimation with a
special emphasis on the trade-off between quality and speed. The study
sheds light on the computational complexity as well as the qualitative
differences between methods and provides guidelines on choosing the
dasiarightpsila algorithm for the robotics practitioner. The robustness
of the methods with respect to noise and neighborhood size is analyzed.
All algorithms are benchmarked with simulated as well as real 3D laser
data obtained from a mobile robot.},
keywords={computational complexity;computational geometry;estimation
theory;feature extraction;image segmentation;laser ranging;mobile
robots;object recognition;robust control;vectors;3D laser range sensing
application;3D point cloud;computational complexity;feature
extraction;geometry segmentation;mobile robotics;robust 3D object
recognition;semantic environment;surface normal vector estimation
method;Clouds;Computational complexity;Computational
geometry;Computational modeling;Guidelines;Laser noise;Mobile
robots;Noise robustness;Object recognition;Robot sensing systems},
doi={10.1109/ROBOT.2009.5152493},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{4809017,
author={S. Kumar},
booktitle={2009 IEEE International Advance Computing Conference},
title={Binocular Stereo Vision Based Obstacle Avoidance Algorithm for
Autonomous Mobile Robots},
year={2009},
pages={254-259},
abstract={Binocular Stereo vision system has been actively used for real
time obstacle avoidance in autonomous mobile robotics for the last
century. The computation of free space is one of the essential tasks in
this field. This paper describes algorithm for obstacle avoidance for
mobile robots which can navigate through obstacle. While most of the
paper based on stereo vision works on the disparity image but we are
proposing a method based on reducing the 3D point cloud obtained from
stereo camera after 3D reconstruction of the environment to build a
stochastic representation of environment navigation map. The algorithm
assigns each cell of the grid with a value (free or obstacle or unknown)
which helps the robot avoid obstacles and navigate in real time. The
algorithm has been successfully tested on "Lakshya" - an UGV^Dagger
platform in both outdoor and indoor condition.},
keywords={collision avoidance;mobile robots;real-time systems;robot
vision;stereo image processing;3D point cloud;3D
reconstruction;autonomous mobile robotics;binocular stereo
vision;environment navigation map;mobile robots;real time obstacle
avoidance algorithm;stereo camera;stochastic
representation;Cameras;Clouds;Image reconstruction;Mobile
robots;Navigation;Orbital robotics;Real time systems;Robot vision
systems;Stereo image processing;Stereo vision},
doi={10.1109/IADCC.2009.4809017},
month={March},}
@INPROCEEDINGS{4669190,
author={A. M. Cretu and E. M. Petriu and P. Payeur},
booktitle={2008 International Workshop on Robotic and Sensors
Environments},
title={Evaluation of growing neural gas networks for selective 3D
scanning},
year={2008},
pages={108-113},
abstract={This paper addresses the issue of intelligent sensing for
advanced robotic applications and is a continuation of our research in
the area of innovative approaches for automatic selection of regions of
observation for fixed and mobile sensors to collect only relevant
measurements without human guidance. The growing neural gas network
solution proposed here for adaptively selecting regions of interest for
further sampling from a cloud of sparsely collected 3D measurements
provides several advantages over the previously proposed neural gas
solution in terms of user intervention, size of resulting scan and
training time. Experimental results and comparative analysis are
presented in the context of selective vision sampling.},
keywords={mobile robots;neural nets;robot vision;3D
measurements;advanced robotic applications;growing neural gas
networks;mobile sensors;selective 3D scanning;selective vision
sampling;user intervention;Anthropometry;Area
measurement;Clouds;Humans;Intelligent robots;Intelligent sensors;Mobile
robots;Robot sensing systems;Robotics and automation;Sampling methods;3D
vision;Selective sensing;feature detection;growing neural gas
network;neural gas network;surface modeling},
doi={10.1109/ROSE.2008.4669190},
month={Oct},}
@INPROCEEDINGS{4650972,
author={R. B. Rusu and Z. C. Marton and N. Blodow and M. E. Dolha and M.
Beetz},
booktitle={2008 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Functional object mapping of kitchen environments},
year={2008},
pages={3525-3532},
abstract={In this paper we investigate the acquisition of 3D functional
object maps for indoor household environments, in particular kitchens,
out of 3D point cloud data. By modeling the static objects in the world
into hierarchical classes in the map, such as cupboards, tables,
drawers, and kitchen appliances, we create a library of objects which a
household robotic assistant can use while performing its tasks. Our
method takes a complete 3D point cloud model as input, and computes an
object model for it. The objects have states (such as open and closed),
and the resultant model is accurate enough to use it in physics-based
simulations, where the doors can be opened based on their hinge
position. The model is built through a series of geometrical reasoning
steps, namely: planar segmentation, cuboid decomposition, fixture
recognition and interpretation (e.g. handles and knobs), and object
classification based on object state information.},
keywords={computational geometry;image classification;image
segmentation;robot vision;3D functional object maps;3D point cloud
data;cuboid decomposition;fixture interpretation;fixture
recognition;geometrical reasoning;household robotic assistant;indoor
household environments;kitchen environments;object classification;object
state information;planar segmentation;Floors;Logic
gates;Manipulators;Meteorology;Robot kinematics;Robots;Three dimensional
displays},
doi={10.1109/IROS.2008.4650972},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{4562122,
author={S. Gill and M. Jenkin},
booktitle={2008 Canadian Conference on Computer and Robot Vision},
title={Polygonal Meshing for 3D Stereo Video Sensor Data},
year={2008},
pages={278-285},
abstract={Many visually-guided robotic systems rely on stereo video data
streams to obtain surface models of environmental structure. However
stereo video-based 3D point clouds are noisier than those produced from
laser-based scanners and are subject to areas of sparse point
information corresponding to textureless or specular surfaces. This
complicates the process of constructing polygonal meshes from these
point-clouds. This paper develops an approach to meshing for stereo
video surface reconstruction that addresses these issues and, by
exploiting the known ego motion of the sensor, obtains surface normal
and texture rendering information. This widens the range of applications
for which stereo video reconstruction can be applied and opens the
possibility of using stereo video to generate models for real-time
rendering applications.},
keywords={image reconstruction;image texture;mesh generation;rendering
(computer graphics);robot vision;stereo image processing;video signal
processing;3D point clouds;3D stereo video sensor data;laser-based
scanners;polygonal meshing;stereo video surface reconstruction;texture
rendering information;visually-guided robotic systems;Algorithm design
and analysis;Laser modes;Robot sensing systems;Robot vision
systems;Smoothing methods;Streaming media;Surface emitting
lasers;Surface reconstruction;Surface
texture;Videoconference;3D;meshing;stereo;vision},
doi={10.1109/CRV.2008.53},
month={May},}
@INPROCEEDINGS{4547083,
author={A. M. Cretu and P. Payeur and E. M. Petriu},
booktitle={2008 IEEE Instrumentation and Measurement Technology
Conference},
title={Selective Vision Sensing with Neural Gas Networks},
year={2008},
pages={478-483},
abstract={Vision sensing systems are experiencing an unprecedented
growth in numerous applications. The collection of such a rich flow of
information has brought a new challenge in the selection of only
relevant features out of the avalanche of data generated by the sensors.
This paper presents some aspects of our research work on intelligent
sensing for advanced robotic applications. The main objective of the
research is to design innovative approaches for automatic selection of
regions of observation for fixed and mobile sensors to collect only
relevant measurements without human guidance. A solution using neural
gas networks has been investigated to adaptively select regions of
interest that require further sampling from a cloud of 3D measurements
sparsely collected. The technique automatically determines bounded areas
where sensing is required at high resolution to accurately map 3D
surfaces. It provides significant benefits over brute force strategies
as scanning time is reduced and datasets size is kept manageable.
Experimental evaluation of this technology is presented for 3D surface
sampling/sensing.},
keywords={feature extraction;image sensors;intelligent sensors;neural
nets;robot vision;3D measurements;3D surface sampling;advanced robotic
applications;automatic region selection;fixed sensors;intelligent
sensing;mobile sensors;neural gas networks;selective vision
sensing;Anthropometry;Clouds;Humans;Intelligent robots;Intelligent
sensors;Machine vision;Robot sensing systems;Robotics and
automation;Sampling methods;Sensor phenomena and characterization;3D
vision;Selective sensing;feature detection;neural gas;neural
networks;surface modeling},
doi={10.1109/IMTC.2008.4547083},
ISSN={1091-5281},
month={May},}
@INPROCEEDINGS{4526460,
author={A. Elfes and G. W. Podnar and J. M. Dolan and S. Stancliff and
E. Lin and J. C. Hosler and T. J. Ames and J. Higinbotham and J. R.
Moisan and T. A. Moisan and E. A. Kulczycki},
booktitle={2008 IEEE Aerospace Conference},
title={The Telesupervised Adaptive Ocean Sensor Fleet Architecture},
year={2008},
pages={1-9},
abstract={Earth science research must bridge the gap between the
atmosphere and the ocean to foster understanding of Earth's climate and
ecology. Ocean sensing is typically done with satellites, buoys, and
crewed research ships. The limitations of these systems include the fact
that satellites are often blocked by cloud cover, and buoys and ships
have spatial coverage limitations. This paper describes a Multilevel
Autonomy Robot Telesupervision Architecture (MARTA) for multi-robot
science exploration, and an embodiment of the MARTA architecture in a
real-world system called the Telesupervised Adaptive Ocean Sensor Fleet
(TAOSF). TAOSF supervises and coordinates a group of robotic boats, the
OASIS platforms, to enable in-situ study of phenomena in the
ocean/atmosphere interface, as well as on the ocean surface and
sub-surface. The OASIS platforms are extended- deployment autonomous
ocean surface vehicles, whose development is funded separately by the
National Oceanic and Atmospheric Administration (NOAA). TAOSF allows a
human operator to effectively supervise and coordinate multiple robotic
assets using the MARTA multi-level autonomy control architecture, where
the operating mode of the vessels ranges from autonomous control to
teleoperated human control. TAOSF increases data-gathering effectiveness
and science return while reducing demands on scientists for robotic
asset tasking, control, and monitoring. The first field application
chosen for TAOSF is the characterization of Harmful Algal Blooms (HABs).
We discuss the overall TAOSF system and the underlying MARTA
architecture, describe field tests conducted under controlled conditions
using rhodamine dye as a HAB simulant, present initial results from
these tests, and outline the next steps in the development of TAOSF.},
keywords={oceanographic techniques;telerobotics;Harmful Algal
Blooms;autonomous control;human operator;in situ study;multilevel
autonomy robot telesupervision architecture;multiple robotic
assets;multirobot science exploration;ocean sensing;rhodamine
dye;robotic boats;teleoperated human control;telesupervised adaptive
ocean sensor fleet architecture;Atmosphere;Geoscience;Humans;Marine
vehicles;Oceans;Robot kinematics;Robot sensing systems;Satellites;Sea
surface;System testing},
doi={10.1109/AERO.2008.4526460},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{4413666,
author={A. Hogue and A. German and M. Jenkin},
booktitle={2007 IEEE International Conference on Systems, Man and
Cybernetics},
title={Underwater environment reconstruction using stereo and inertial
data},
year={2007},
pages={2372-2377},
abstract={The underwater environment presents many challenges for
robotic sensing including highly variable lighting, the presence of
dynamic objects, and the six degree of freedom (6DOF) 3D environment.
Yet in spite of these challenges the aquatic environment presents many
real and practical applications for robotic sensors. A common
requirement of many of these tasks is the need to construct accurate 3D
representations of structures in the environment. In order to address
this requirement we have developed a stereo vision-inertial sensing
device that we have successfully deployed to reconstruct complex 3D
structures in both the aquatic and terrestrial domains. The sensor
temporally combines 3D information, obtained using stereo vision
algorithms with a 3DOF inertial sensor. The resulting point cloud model
is then converted to a volumetric representation and a textured
polygonal mesh is extracted for later processing. Recently obtained
underwater reconstructions of wrecks and coral obtained with the sensor
are presented.},
keywords={feature extraction;image reconstruction;image
representation;robot vision;stereo image processing;3DOF inertial
sensor;point cloud model;robotic sensors;stereo vision-inertial sensing
device;textured polygonal mesh extraction;underwater environment
reconstruction;volumetric
representation;Inspection;Layout;Monitoring;Pollution measurement;Robot
sensing systems;Robot vision systems;Size measurement;Stereo
vision;Temperature sensors;Video recording},
doi={10.1109/ICSMC.2007.4413666},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{4381261,
author={N. Vaskevicius and A. Birk and K. Pathak and J. Poppinga},
booktitle={2007 IEEE International Workshop on Safety, Security and
Rescue Robotics},
title={Fast Detection of Polygons in 3D Point Clouds from Noise-Prone
Range Sensors},
year={2007},
pages={1-6},
abstract={3D sensing and modeling is increasingly important for mobile
robotics in general and safety, security and rescue robotics (SSRR) in
particular. To reduce the data and to allow for efficient processing,
e.g., with computational geometry algorithms, it is necessary to extract
surface data from 3D point clouds delivered by range sensors. A
significant amount of work on this topic exists from the computer
graphics community. But the existing work relies on relatively exact
point cloud data. As also shown by others, sensors suited for mobile
robots are very noise-prone and standard approaches that use local
processing on surface normals are doomed to fail. Hence plane fitting
has been suggested as solution by the robotics community. Here, a novel
approach for this problem is presented. Its main feature is that it is
based on region growing and that the underlying mathematics has been
re-formulated such that an incremental fit can be done, i.e., the best
fit surface does not have to be completely re-computed the moment a new
point is investigated in the region growing process. The worst case
complexity is O(n log(n)), but as shown in experiments it tends to scale
linearly with typical data. Results with real world data from a
Swissranger time-of-flight camera are presented where surface polygons
are always successfully extracted within about 0.3 sec.},
keywords={computational geometry;mobile robots;robot vision;stereo image
processing;surface fitting;3D point clouds;3D sensing;computational
geometry algorithm;data processing;data reduction;incremental fit;mobile
robotics;noise-prone range sensors;plane fitting;polygon
detection;region growing;rescue robotics;safety robotics;security
robotics;stereo vision;surface data extraction;surface normals;surface
polygons;worst case complexity;Clouds;Computational geometry;Computer
graphics;Data mining;Data security;Mathematics;Mobile robots;Robot
sensing systems;Safety;Surface fitting;3D Map;Point Cloud;Range
Sensor;Surface Model},
doi={10.1109/SSRR.2007.4381261},
ISSN={2374-3247},
month={Sept},}
@INPROCEEDINGS{4241301,
author={C. Borel},
booktitle={2006 IEEE International Symposium on Geoscience and Remote
Sensing},
title={Inversion of Atmospheric Parameters from Sky Radiance
Measurements with a Sun Photometer},
year={2006},
pages={585-588},
abstract={NASA has organized a worldwide Aerosol Robotic Network
(AERONET) of Sun photometers. These well characterized instruments
(CIMEL Sun photometer) make aerosol optical depth measurements based on
the transmission of the direct sunlight through the atmosphere in 5 and
8 spectral bands. AERONET aerosol retrievals are limited to clear sky
conditions (no clouds or no cirrus clouds) and retrieve aerosol optical
depth. To retrieve other parameters such as aerosol size distribution,
single scattering albedo, refractive index, etc., retrievals based on
algorithms from Nakajima [2] and Dubovik [1] are used. Since the
algorithms are limited to clear sky conditions and not available to the
public, we decided to write our own retrieval algorithm which uses the
solar principal plane and almucantar scans from the CIMEL instrument. We
studied a variety of radiative transfer methods that are able to
simulate ground measurements by computing the hemispherical sky
radiance. For various reasons we decided to use the Santa Barbara DISORT
Atmospheric Radiative Transfer (SBDART) code [4,5] and combined it with
IDL's Amoeba (down-hill simplex method) [3] to retrieve five parameters:
(1) water cloud optical depth, (2) ice cloud optical depth, (3) cloud
water droplet radius, (4) horizontal visibility (related to aerosol
optical depth), and (5) RH factor which couples relative humidity and
aerosol size distribution, from principal plane and almucantar scans.
The SBDART computed sky radiances are very close to the measured sky
radiances 40 degrees away from the solar aureole since the Discrete
Ordinates Radiative Transfer (DISORT) algorithm [6] is not accurate in
the forward scattering direction. In this paper we will show results of
the inversion of AERONET data using SBDART.},
keywords={aerosols;atmospheric humidity;atmospheric
radiation;clouds;ice;radiative transfer;sunlight;AERONET;Aerosol Robotic
Network;CIMEL instrument;DISORT algorithm;Discrete Ordinates Radiative
Transfer;SBDART;Santa Barbara DISORT Atmospheric Radiative Transfer;Sun
Photometer;aerosol optical depth measurements;aerosol retrievals;aerosol
size distribution;almucantar scans;cloud water droplet radius;ice cloud
optical depth;radiative transfer methods;refractive index;relative
humidity;single scattering albedo;sky radiance measurements;solar
aureole;solar principal plane;sunlight transmission;water cloud optical
depth;Aerosols;Atmospheric measurements;Clouds;Instruments;NASA;Optical
refraction;Optical scattering;Optical variables control;Photometry;Sun},
doi={10.1109/IGARSS.2006.154},
ISSN={2153-6996},
month={July},}
@ARTICLE{4088938,
author={Z. M. Bi and S. Y. T. Lang},
journal={IEEE Transactions on Industrial Informatics},
title={A Framework for CAD- and Sensor-Based Robotic Coating Automation},
year={2007},
volume={3},
number={1},
pages={84-91},
abstract={Coating automation of a complex product family with changes
and uncertainties is investigated. A CAD- and scanner-based robotic
coating system is proposed. Its software system consists of a
data-processing system and automatic programming and simulation system.
The data processing system is capable of generating accurate surface
model from points-cloud and/or as-designed CAD model. The automatic
programming and simulation system is capable of creating robot programs
based on the surface models. In this paper, the system framework and the
methodology for generating robot programs are focused},
keywords={CAD;automatic programming;coating techniques;industrial
robots;production engineering computing;CAD model;CAD-based robotic
coating automation;automatic programming;data-processing
system;points-cloud model;sensor-based robotic coating
automation;simulation system;software system;surface treatment;Automatic
programming;Bismuth;Coatings;Productivity;Robot programming;Robot
sensing systems;Robotics and automation;Software systems;Surface
treatment;Uncertainty;CAD-based system;coating;robotics and
automation;sensor-based systems;surface treatment},
doi={10.1109/TII.2007.891309},
ISSN={1551-3203},
month={Feb},}
@INPROCEEDINGS{4058460,
author={M. Chandran and P. Newman},
booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Motion Estimation from Map Quality with Millimeter Wave Radar},
year={2006},
pages={808-813},
abstract={Simultaneous localization and mapping (SLAM) builds maps of a
priori unknown environments. Whilst this key mobile robotic competency
continues to receive substantial attention, less attention has been paid
to assessing the quality of the resulting maps. This paper proposes a
way to quantify the intrinsic quality of point-cloud maps built from a
stream of range bearing measurements. It does so by considering both the
temporal and spatial distribution of the points within the map. One of
the causes of unsatisfactory maps is the execution of unmodelled or
poorly sensed vehicle manoeuvres. In this paper we show that by
maximizing the quality of the map as a function of a motion
parameterization, the vehicle motion can be recovered while correcting
the map at the same time. In contrast to typical scan matching
techniques, we do not rely on segmentation of the measurement stream
into two separate "scans"; Instead we treat the measurement sequence as
a continuous signal. We illustrate the efficacy of this approach by
processing range data from a 77 GHz millimeter wave radar that completes
2 rotations per second. We show that despite this acquisition speed
being commensurate with vehicle rotation rates, we are able to extract
the underlying vehicle motion and yield crisp, well aligned point clouds},
keywords={SLAM (robots);millimetre wave devices;mobile robots;motion
estimation;path planning;radar applications;77 GHz;map
quality;millimeter wave radar;mobile robot;motion estimation;point-cloud
maps;range bearing measurements;scan matching techniques;simultaneous
localization and mapping;vehicle motion;Intelligent robots;Laser
radar;Marine vehicles;Millimeter wave measurements;Millimeter wave
radar;Millimeter wave technology;Mobile robots;Motion
estimation;Rendering (computer graphics);Simultaneous localization and
mapping},
doi={10.1109/IROS.2006.281673},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{1713755,
author={Zi Ma and Huipu Xu and Ying Hu and Jin Huang and Hu Dong},
booktitle={2006 6th World Congress on Intelligent Control and Automation},
title={Robot Vision System and Artificial Neural Network for Model
Reconstruction in Reverse Engineering},
year={2006},
volume={2},
pages={9073-9078},
abstract={A novel vision system and model reconstruction scheme is
addressed in this article. The main contribution contains two aspects:
first, robot with laser scanning head is used for establishing a vision
system, this system can measure 3D surface data for complex freeform
surface on line at manufacturing site. Then, using artificial neural
network (ANN), a model reconstruction algorithm is developed. For
demonstrating effectiveness of the presented ANN digital surface model,
a group point cloud data with good accuracy, which was measured by the
presented robotics vision system for an existing part, is taken as data
sample to train the ANN so that the surface model of the object is
reconstructed. By comparing the ANN data with the sample value at the
same position point, it is shown that the reconstructed ANN model can
match the surface data very well},
keywords={CAD;image reconstruction;neural nets;reverse engineering;robot
vision;artificial neural network;laser measurement;laser scanning
head;model reconstruction;reverse engineering;robot vision
system;Artificial neural networks;Clouds;Laser modes;Machine
vision;Manufacturing;Reconstruction algorithms;Reverse engineering;Robot
vision systems;Surface emitting lasers;Surface reconstruction;Robot
vision system;artificial neural network;laser measurement;model
reconstruction;reverse engineering},
doi={10.1109/WCICA.2006.1713755},
month={},}
@INPROCEEDINGS{1682897,
author={Zawar Shah and R. A. Malaney},
booktitle={2006 IEEE 63rd Vehicular Technology Conference},
title={Particle Filters and Position Tracking in Wi-Fi Networks},
year={2006},
volume={2},
pages={613-617},
abstract={In this work we quantify the usefulness of particle filters
applied to the problem of mobile device tracking in Wi-Fi networks,
under the assumption of log-normal fading. Our principal aim was to
determine if a real-time deployment of a particle filter was possible
while still providing factor two gains in the prediction performance
relative to a stand-alone optimal Wi-Fi positioning algorithm. We
conclude that the required gains are achieved in our adopted filter
algorithm when the particle number is set to the relatively small number
of 300, meaning that a real-time deployment is possible. In addition, we
quantify the performance gain of the particle filter when intermittent
GPS information is available to the mobile device. We propose the fusion
of the GPS information be implemented as a renormalization of the
particle cloud. Finally, we probe the limits of the filter performance
under biased-error distributions. Our simulations show that tracking of
people, vehicles and robotic devices in an outdoor Wi-Fi network, where
non-linear and non-Gaussian conditions exist, can be significantly
enhanced by the pragmatic real-time particle filter algorithm presented
here},
keywords={Global Positioning System;mobile handsets;particle filtering
(numerical methods);renormalisation;wireless LAN;GPS information;Wi-Fi
networks;biased-error distributions;factor two gains;log-normal
fading;mobile device tracking;nonGaussian condition;nonlinear
condition;particle cloud renormalization;position tracking;pragmatic
real-time particle filter algorithm;real-time deployment;robotic
devices;stand-alone optimal Wi-Fi positioning
algorithm;Australia;Fading;Global Positioning System;Intelligent
networks;Particle filters;Particle tracking;Performance gain;Quality of
service;Vehicles;Wireless networks;Location;particle
filter;tracking;wireless networks},
doi={10.1109/VETECS.2006.1682897},
ISSN={1550-2252},
month={May},}
@INPROCEEDINGS{1642247,
author={C. Roman and H. Singh},
booktitle={Proceedings 2006 IEEE International Conference on Robotics
and Automation, 2006. ICRA 2006.},
title={Consistency based error evaluation for deep sea bathymetric
mapping with robotic vehicles},
year={2006},
pages={3568-3574},
abstract={This paper presents a method to evaluate the mapping error
present in point cloud terrain maps created using robotic vehicles and
range sensors. This work focuses on mapping environments where no a
priori ground truth is available and self consistency is the only
available check against false artifacts and errors. The proposed error
measure is based on a disparity measurement between common sections of
the environment that have been imaged multiple times. This disparity
measure highlights inconsistency in the terrain map by showing regions
where multiple overlapping point clouds do not fit together well. This
error measure provides the map interpreter with a localized error
measurement to help judge the validity of the final point cloud or
gridded surface. It is shown that the proposed method highlights mapping
errors more clearly than a principle component based measure used in the
3D modelling community. Results are presented for bathymetric mapping
over natural terrain in the deep ocean using a remotely operated vehicle
(ROV) outfitted with navigation sensors and a multibeam sonar system},
keywords={bathymetry;error analysis;mobile robots;path planning;remotely
operated vehicles;sonar;telerobotics;underwater vehicles;component based
measure;consistency based error evaluation;deep sea bathymetric
mapping;multibeam sonar system;point cloud terrain maps;range
sensors;remotely operated vehicle;robotic vehicles;Clouds;Oceanographic
techniques;Oceans;Remotely operated vehicles;Robot sensing systems;Sea
measurements;Sea surface;Sonar navigation;Surface fitting;Terrain mapping},
doi={10.1109/ROBOT.2006.1642247},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{1641730,
author={Quan Shi and Ning Xi and Weihua Sheng and Yifan Chen},
booktitle={Proceedings 2006 IEEE International Conference on Robotics
and Automation, 2006. ICRA 2006.},
title={Development of dynamic inspection methods for dimensional
measurement of automotive body parts},
year={2006},
pages={315-320},
abstract={This paper introduces a novel robotic range sensor planning
system, which is developed for 3D dimensional inspection of automotive
body parts. For active, triangulation-based range sensors, shadow and
reflection causes problems when measure a metal part with glossy and
discontinuous surface. To solve these problems, a feedback based dynamic
view planning system is proposed, which not only generate viewpoints
from a CAD model of a part, but also recursively add viewpoints
according to the measured information. The planning process stops only
if the obtained point clouds meet the pre-determined requirements.
General framework of the system is introduced, and the experimental
results are also presented},
keywords={automobile industry;feedback;industrial
robots;inspection;robot vision;3D dimensional inspection;automotive body
parts;dynamic inspection methods;dynamic view planning system;robotic
range sensor planning system;triangulation-based range
sensors;Automotive engineering;Clouds;Feedback;Inspection;Meeting
planning;Process planning;Reflection;Robot sensing systems;Sensor
systems;Vehicle dynamics},
doi={10.1109/ROBOT.2006.1641730},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{1641895,
author={R. Mottaghi and R. Vaughan},
booktitle={Proceedings 2006 IEEE International Conference on Robotics
and Automation, 2006. ICRA 2006.},
title={An integrated particle filter and potential field method for
cooperative robot target tracking},
year={2006},
pages={1342-1347},
abstract={A fundamental challenge for robotic target-tracking systems is
to cope with cases in which the target is not seen for long periods of
time. An additional challenge in multiple-robot systems is to coordinate
robot activity to best track targets with limited visibility. We
describe a novel technique that combines a particle filter target model
with a potential field robot controller. Robots are attracted to points
sampled from the particle cloud that models the probability distribution
over the target's position, subject to environmental constraints. We
show how this method can be used as a coordination strategy whereby a
team of robots cooperatively minimize the uncertainty in the pose of a
tracked target. Simulation results are presented},
keywords={cooperative systems;mobile robots;multi-robot systems;particle
filtering (numerical methods);target tracking;telerobotics;cooperative
robot target tracking;integrated particle filter;multiple-robot
systems;potential field method;Clouds;Mobile robots;Particle
filters;Probability distribution;Robot control;Robot kinematics;Robot
sensing systems;Robotics and automation;Target tracking;Uncertainty},
doi={10.1109/ROBOT.2006.1641895},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{1640416,
author={J. N. Bakambu and P. Allard and E. Dupuis},
booktitle={The 3rd Canadian Conference on Computer and Robot Vision
(CRV'06)},
title={3D Terrain Modeling for Rover Localization and Navigation},
year={2006},
pages={61-61},
abstract={This paper presents the problem of constructing a 3D terrain
model for localization and navigation of planetary rover. We presented
our approach to 3D terrain reconstruction from large sparse range data
sets. In space robotics applications, an accurate and up-to-date model
of the environment is very important for a variety of reasons. In
particular, the model can be used for safe tele-operation, path planning
and mapping points of interest. We propose an on-line terrain modeling
using data provided by an on-board high resolution, accurate, 3D range
sensor. Our approach is based on on-line acquisition of range scans from
different view-points with overlapping regions, merge them together into
a single point cloud, and then fit an irregular triangular mesh on the
merged data. The outdoor experimental results demonstrate the
effectiveness of the reconstructed terrain model for rover localization,
path planning and motion execution scenario.},
keywords={Clouds;Mars;Navigation;Orbital robotics;Path planning;Space
technology;Spline;Surface fitting;Surface reconstruction;Terrain mapping},
doi={10.1109/CRV.2006.2},
month={June},}
@INPROCEEDINGS{1565436,
author={P. Biber and S. Fleck and T. Duckett},
booktitle={2005 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR'05) - Workshops},
title={3D Modeling of Indoor Environments for a Robotic Security Guard},
year={2005},
pages={124-124},
abstract={Autonomous mobile robots will play a major role in future
security and surveillance tasks for large scale environments such as
shopping malls, airports, hospitals and museums. Robotic security guards
will autonomously survey such environments, unless a remote human
operator takes over control. In this context a 3D model can convey much
more useful information than the typical 2D maps used in many robotic
applications today, both for visualization of information and as human
machine interface for remote control. This paper addresses the challenge
of building such a model of a large environment (50× 60m2) using data
from the robot’s own sensors: a 2D laser scanner and a panoramic camera.
The data are processed in a pipeline that comprises automatic,
semiautomatic and manual stages. The user can interact with the
reconstruction process where necessary to ensure robustness and
completeness of the model. A hybrid representation, tailored to the
application, has been chosen: floors and walls are represented
efficiently by textured planes. Non-planar structures like stairs and
tables, which are represented by point clouds, can be added if desired.
Our methods to extract these structures include: simultaneous
localization and mapping in 2D and wall extraction based on laser
scanner range data, building textures from multiple omnidirectional
images using multiresolution blending, and calculation of 3D geometry by
a graph cut stereo technique. Various renderings illustrate the
usability of the model for visualizing the security guard’s position and
environment.},
keywords={Data mining;Data visualization;Humans;Indoor
environments;Information security;Large-scale systems;Laser modes;Mobile
robots;Robot sensing systems;Surveillance},
doi={10.1109/CVPR.2005.381},
ISSN={2160-7508},
month={June},}
@INPROCEEDINGS{1545526,
author={Han-Young Jang and H. Moradi and S. Lee and JungHyun Han},
booktitle={2005 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={A visibility-based accessibility analysis of the grasp points for
real-time manipulation},
year={2005},
pages={3111-3116},
abstract={This paper presents a novel approach to accessibility analysis
for manipulative robotic tasks. The workspace is captured using a stereo
camera, and heterogeneously modeled with the recognized plane features,
recognized objects with complete solid models, and unrecognized 3D point
clouds organized with a multi-resolution octree. When the service robot
is requested to manipulate a recognized object, the local accessibility
information for the object is retrieved from the object database. Then,
accessibility analysis is done to verify the object accessibility and
determine the global accessibility. The verification process utilizes
the visibility query, which is accelerated by graphics hardware. The
experimental results show the feasibility of real-time and
behavior-oriented 3D modeling of workspace for robotic manipulative
tasks, and also the performance gain of the hardware-accelerated
accessibility analysis obtained using the commodity graphics card.},
keywords={computer graphic equipment;manipulators;service robots;solid
modelling;stereo image processing;behavior-oriented 3D modeling;graphics
card;graphics hardware;grasp points;hardware-accelerated accessibility
analysis;manipulative robotic task;multiresolution octree;object
database;real-time 3D modeling;real-time manipulation;service
robot;stereo camera;visibility query;visibility-based accessibility
analysis;Acceleration;Cameras;Clouds;Data analysis;Graphics;Information
retrieval;Robot vision systems;Service robots;Solid modeling;Spatial
databases;3D workspace modeling;accessibility analysis;robotic
manipulation;visibility},
doi={10.1109/IROS.2005.1545526},
ISSN={2153-0858},
month={Aug},}
@INPROCEEDINGS{1545105,
author={Sukhan Lee and Daesik Jang and Eunyoung Kim and Suyeon Hong and
JungHyun Han},
booktitle={2005 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={A real-time 3D workspace modeling with stereo camera},
year={2005},
pages={2140-2147},
abstract={This paper presents a novel approach to real-time 3D modeling
of workspace for manipulative robotic tasks. First, we establish the
three fundamental principles that human uses for modeling and
interacting with environment. These principles have led to the
development of an integrated approach to real-time 3D modeling, as
follows: 1) It starts with a rapid but approximate characterization of
the geometric configuration of workspace by identifying global plane
features. 2) It quickly recognizes known objects in workspace and
replaces them by their models in database based on in-situ registration.
3) It models the geometric details on the fly adaptively to the need of
the given task based on a multi-resolution octree representation. SIFT
features with their 3D position data, referred to here as stereo-sis
SIFT, are used extensively, together with point clouds, for fast
extraction of global plane features, for fast recognition of objects,
for fast registration of scenes, as well as for overcoming incomplete
and noisy nature of point clouds. The experimental results show the
feasibility of real-time and behavior-oriented 3D modeling of workspace
for robotic manipulative tasks.},
keywords={feature extraction;image registration;manipulators;object
recognition;octrees;real-time systems;robot vision;stereo image
processing;behavior-oriented 3D modeling;feature identification;image
registration;multiresolution octree representation;object
recognition;real-time 3D workspace modeling;robot manipulation;robot
stereo vision;scene registration;stereo camera;stereo-sis
scale-invariant feature transform;workspace geometric
configuration;Cameras;Clouds;Humans;Intelligent sensors;Iterative
closest point algorithm;Layout;Mobile robots;Object recognition;Solid
modeling;Working environment noise;3D object recognition;3D workspace
modeling;SIFT;planar feature;stereo vision},
doi={10.1109/IROS.2005.1545105},
ISSN={2153-0858},
month={Aug},}
@INPROCEEDINGS{1545577,
author={Quan Shi and Ning Xi and Heping Chen and Yifan Chen},
booktitle={2005 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Calibration of robotic area sensing system for dimensional
measurement of automotive part surfaces},
year={2005},
pages={1526-1531},
abstract={This paper presents new calibration methods for a robotic area
sensing system developed for industrial manufacturing inspection. A
pixel-to-pixel calibration scheme is introduced to obtain standoff and
baseline distance of the developed area sensor. In the three-dimensional
(3D) space, the area sensor also has an offset angle in a plane parallel
to the reference plane. Calibration of this offset angle improves the
measurement accuracy. This pixel-to-pixel scheme is particularly useful
for calibrating a 3D optical area sensor with off-the-shelf lenses. An
exploding vector method was also developed to transform the measured
depth map to a 3D point cloud. A robotic area sensing system was
developed by integrating an area sensor prototype and a robot sensor
planning system. Calibration of the integrated system includes robot
calibration, part calibration, and robot hand-eye calibration.
Calibration error of the robot system is related to the area sensor
positioning accuracy in the robotic system. Experimental results show a
successful practice of the developed robotic area sensing system.
Measurement performance of the developed system can be improved if
advanced lenses are adopted.},
keywords={area measurement;automobile industry;automotive
materials;calibration;industrial robots;inspection;robot vision;3D
optical area sensor;automated dimensional measurement system;automotive
part surfaces;exploding vector method;industrial manufacturing
inspection;pixel-to-pixel calibration;robot hand-eye calibration;robotic
area sensing system;sensor planning;Area measurement;Automotive
engineering;Calibration;Lenses;Manufacturing industries;Optical
sensors;Orbital robotics;Robot sensing systems;Sensor systems;Service
robots;Sensor planning;area sensor;automated dimensional measurement
system;calibration},
doi={10.1109/IROS.2005.1545577},
ISSN={2153-0858},
month={Aug},}
@ARTICLE{1499014,
author={B. G. Henderson and P. Chylek},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={The effect of spatial resolution on satellite aerosol optical
depth retrieval},
year={2005},
volume={43},
number={9},
pages={1984-1990},
abstract={We use data from the Multispectral Thermal Imager (MTI) to
evaluate the effects of spatial resolution on the accuracy of aerosol
optical depth (AOD) retrieval. Our results show that increasing the
pixel size by itself has little effect on AOD retrieval accuracy at our
chosen study site. However, increased pixel size does increase the error
in AOD retrieval as a result of clouds. High-resolution sensors like MTI
are able to avoid most clouds, but as the pixel size increases, subpixel
clouds avoid the cloud mask, creep into selected pixels, and add a
positive bias to the retrieved value of AOD. In accompanying work, we
show that increasing pixel size has a small but noticeable impact on the
normalized difference vegetation index (NDVI) and the 2.2-/spl mu/m
reflectance, both used in the retrieval algorithm. We also examine the
uniformity of the aerosol layer and show that the AOD varies by less
than 0.02 in optical depth units over a 2.3/spl times/3.8 km/sup 2/
area. An analysis of the temporal variability of Aerosol Robotic
Network-retrieved AOD shows a standard deviation of 0.02 on partly
cloudy days and 0.004 on clear days.},
keywords={aerosols;atmospheric optics;atmospheric techniques;infrared
imaging;remote sensing;2.2 micron;Aerosol Robotic Network;Multispectral
Thermal Imager;aerosol layer;aerosol optical depth retrieval;cloud
mask;normalized difference vegetation index;pixel
size;reflectance;spatial resolution;subpixel clouds;temporal
variability;Aerosols;Clouds;Creep;Image retrieval;Information
retrieval;Optical network units;Optical sensors;Satellites;Spatial
resolution;Vegetation mapping;Aerosols;Multispectral Thermal Imager
(MTI);multispectral;resolution;satellite},
doi={10.1109/TGRS.2005.852078},
ISSN={0196-2892},
month={Sept},}
@ARTICLE{1463329,
author={G. J. Bootsma and G. W. Brodland},
journal={IEEE Transactions on Biomedical Engineering},
title={Automated 3-D reconstruction of the surface of live early-stage
amphibian embryos},
year={2005},
volume={52},
number={8},
pages={1407-1414},
abstract={Although three-dimensional (3-D) reconstructions of the
surfaces of live embryos are vital to understanding embryo development,
morphogenetic tissue movements and other factors have prevented the
automation of this task. Here, we report an integrated set of software
algorithms that overcome these challenges, making it possible to
completely automate the reconstruction of embryo surfaces and other
textured surfaces from multiview images. The process involves: 1)
building accurate point correspondences using a robust deformable
template block matching algorithm; 2) removing outliers using
fundamental matrix calculations in conjunction with a RANSAC algorithm;
3) generating 3-D point clouds using a bundle adjustment algorithm that
includes camera position and distortion corrections; 4) meshing the
point clouds into triangulated surfaces using a Tight Cocone algorithm
that produces water tight models; 5) refining surfaces using midpoint
insertion and Laplacian smoothing algorithms; and 6) repeating these
steps until a measure of convergence G, the rms difference between
successive reconstructions, is below a specified threshold.
Reconstructions were made of 2.2-mm diameter, neurulation-stage axolotl
(amphibian) embryos using 44 multiview images collected with a robotic
microscope. A typical final model (sixth iteration) contained 3787
points and 7562 triangles and had an error measure of G=5.9 μm.},
keywords={Laplace equations;image matching;image reconstruction;image
texture;medical image processing;mesh
generation;obstetrics;physiological models;smoothing methods;2.2 mm;3-D
point clouds;Laplacian smoothing;RANSAC algorithm;Tight Cocone
algorithm;automated 3D surface reconstruction;camera position
correction;distortion correction;embryo development;fundamental matrix
calculations;live early-stage amphibian embryos;mesh;midpoint
insertion;morphogenetic tissue movements;neurulation-stage axolotl;point
correspondences;robotic microscope;robust deformable template block
matching algorithm;surface refining;water tight
models;Automation;Cameras;Clouds;Embryo;Image
reconstruction;Robustness;Software algorithms;Surface
reconstruction;Surface texture;Three dimensional displays;Bundle
adjustment;deformable template matching;mesh refinement;multiview
images;robotic microscope;Algorithms;Ambystoma;Animals;Artificial
Intelligence;Embryo, Nonmammalian;Image Enhancement;Image
Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information
Storage and Retrieval;Microscopy;Pattern Recognition, Automated},
doi={10.1109/TBME.2005.851500},
ISSN={0018-9294},
month={Aug},}
@INPROCEEDINGS{1036852,
author={J. L. Hall and V. V. Kerzhanovich and J. A. Jones and J. A.
Cutts and A. A. Yavrouian and A. Colozza and R. D. Lorenz},
booktitle={Proceedings, IEEE Aerospace Conference},
title={Titan Airship Explorer},
year={2002},
volume={1},
pages={1-336 vol.1},
abstract={Saturn's moon Titan is considered to be one of the prime
locations for understanding the origins of life due to its rich
environment of organic chemicals. A unique combination of dense
atmosphere (more than four times that of the Earth), low gravity
(one-sixth of that on the Earth) and small temperature variations makes
Titan well-suited for studies with buoyant robotic vehicles (aerobots).
Moreover, since the methane clouds obscure the entire surface, aerial
platforms flying below the clouds are the only means of getting
high-resolution global mapping of the Titan surface in the visible and
infrared wavelengths. Major technical challenges for Titan aerobots
include an extremely cold atmosphere (∼-90K), remoteness from the Earth
(∼10 AU), which limit data transmission and prevents meaningful
real-time control, and the consequent need for a high degree of
autonomous operation. Remoteness from the Sun and obscuring cloud cover
make nuclear energy the only practical source of power. Radioisotope
thermal generators (RTGs) can provide sufficient power to give an
aerobot limited propulsion capability. Propeller-driven aerobots are
essentially airships that can move to specific targets on Titan rather
than being constrained to travel with the prevailing winds. Such
vehicles also can be used for in situ studies of the surface either
through combined airship/rover concepts ("aerover") or through winching
down an instrumented surface platform from a station-keeping airship. An
airship baseline design is described including the key technical aspects
of airship configuration, propulsion system, navigation and control
concepts, data acquisition and communications. The resulting baseline
design appears to be very attractive and suggests that airships are a
good candidate for the post-Cassini exploration of Titan.},
keywords={Saturn;balloons;planetary atmospheres;planetary
satellites;space vehicles;Saturn satellite;Titan Airship Explorer;aerial
platforms;aerover;airship
configuration;airship/rover;atmosphere;autonomous operation;buoyant
robotic vehicles;control;data acquisition;data communications;data
transmission;design;gravity;instrumented surface platform;methane
clouds;nuclear power source;propeller-driven aerobots;propulsion
system;radioisotope thermal generators;station-keeping
airship;temperature
variations;Atmosphere;Clouds;Earth;Gravity;Moon;Organic
chemicals;Propulsion;Saturn;Surface waves;Vehicles},
doi={10.1109/AERO.2002.1036852},
month={},}
@INPROCEEDINGS{526147,
author={A. Baader and G. Hirzinger},
booktitle={Proceedings 1995 IEEE/RSJ International Conference on
Intelligent Robots and Systems. Human Robot Interaction and Cooperative
Robots},
title={World modeling for a sensor-in-hand robot arm},
year={1995},
volume={2},
pages={110-115 vol.2},
abstract={In many robotics applications, e.g. telerobotics under long
time delay, building a geometric world model from multisensory data is a
crucial requirement. Using a sensor-in-hand configuration to generate a
model referenced to the robot base, imposes specific problems. Moving
the sensor frame on an arbitrary path over and around an unknown object
generates a completely unordered `data cloud'. A surface reconstruction
algorithm, which is based on Kohonen's self-organizing feature maps is
used to process this data cloud and to generate a useful surface
description. This surface can be used for object recognition and pose
estimation using algorithms which were developed in the field of range
image understanding},
keywords={CAD;manipulators;object recognition;robot
vision;self-organising feature maps;sensor fusion;surface
reconstruction;telerobotics;Kohonen's self-organizing feature
maps;multisensory data;object recognition;pose estimation;robot
base;sensor-in-hand robot arm;surface description;surface reconstruction
algorithm;unordered data cloud;world modeling;Aerodynamics;Clouds;Delay
effects;Electronic mail;Grippers;Layout;Object recognition;Orbital
robotics;Reconstruction algorithms;Robot sensing systems;Solid
modeling;Surface reconstruction;Telerobotics},
doi={10.1109/IROS.1995.526147},
month={Aug},}
@INPROCEEDINGS{978157,
author={I. F. Grant and H. H. Kieffer and T. C. Stone and J. M. Anderson},
booktitle={IGARSS 2001. Scanning the Present and Resolving the Future.
Proceedings. IEEE 2001 International Geoscience and Remote Sensing
Symposium (Cat. No.01CH37217)},
title={Lunar calibration of the GMS-5 visible band},
year={2001},
volume={6},
pages={2769-2771 vol.6},
abstract={Calibration of the visible band of the VISSR radiometer on the
Geostationary Meteorological Satellite GMS-5 is necessary for
quantitative analysis of the images it produces. Applications include
the retrieval of quantities such as aerosol amount, surface albedo and
cloud properties. The lunar photometry program conducted by the US
Geological Survey with its Robotic Lunar Observatory (ROLO) is
establishing the Moon as a radiometric calibration target for
space-based Earth observation sensors. The occasional appearance of the
Moon in the operational GMS-5 images near the times of ROLO observations
has allowed the acquisition of several pairs of images with almost
identical view and illumination geometry at ROLO and GMS-5. The lunar
disk presents a range of radiances and has a diameter of a few hundred
pixels in both the ROLO and GMS-5 images. This allows, from a single
image pair, calibration over most of the GMS-5 dynamic range used by
clear terrestrial land scenes. An absolute calibration of the GMS-5
sensor to within several percent has been derived from one of the
ROLO-GMS image pairs. The advantages of lunar calibration over other
calibration methods are discussed},
keywords={Moon;aerospace instrumentation;calibration;geophysical
equipment;radiometers;GMS-5 visible band;Moon;VISSR
radiometer;aerosol;cloud properties;illumination geometry;lunar
calibration;lunar photometry program;space-based Earth observation
sensors;surface albedo;Aerosols;Calibration;Clouds;Geology;Image
analysis;Meteorology;Moon;Photometry;Radiometry;Satellite broadcasting},
doi={10.1109/IGARSS.2001.978157},
month={},}
@ARTICLE{7489968,
author={J. Li and Y. Wang and J. Lai and H. Tan},
journal={China Communications},
title={Ambient assisted living},
year={2016},
volume={13},
number={5},
pages={vi-vii},
abstract={The cost of formal health care and the caregiver burden
introduced by the increasingly aging population pose a drasticchallenge
to our society. The efforts to find solutions to address this issue have
fosteredAmbient Assisted Living (AAL)as a novel technology discipline,
the aim of which is to exploit the potentials provided bythe
emerginginformation and communication technologies tohelp the aged and
recovering people keep an active, independent and safe lifestyle in
their daily routine. In recent years, the progress in health sensing,
wireless communication, smart home, cloud computing, andassistive
robotics has essentially helped the vision of AAL to become a reality.
Many research efforts in AAL have been carried out and made significant
advances in the development of AALsystems, services, and applicationsin
the areas such as the daily task facilitation, mobility assistance, and
the rehabilitation. The goal of this Feature Topic is to explore the
recent research activities related to AAL and attract the attention of
the academic and industrial communities for developing advanced and
innovative methodologies and techniques of AAL.},
doi={10.1109/CC.2016.7489968},
ISSN={1673-5447},
month={May},}
@ARTICLE{7254290,
author={V. Isler and B. Sadler and L. Preuchil and S. Nishio},
journal={IEEE Robotics Automation Magazine},
title={Networked Robots [TC Spotlight]},
year={2015},
volume={22},
number={3},
pages={25-29},
abstract={Many robotics applications require robots to communicate with
each other, with a human operator, or with a remote server. The IEEE
Robotics and Automation Society???s Technical Committee (TC) on
Networked Robots focuses on research issues related to robots connected
to a communication network. The TC was founded in 2001. The name
???Networked Robots??? was adopted in 2004. The original focus of the TC
was on Internet-based teleoperated robots. At the moment, most of the
research activity in our TC revolves around robotic networks and cloud
robotics.},
keywords={Communication networks;Compuer applications;Robots},
doi={10.1109/MRA.2015.2452172},
ISSN={1070-9932},
month={Sept},}
@ARTICLE{7057690,
author={K. Goldberg},
journal={IEEE Transactions on Automation Science and Engineering},
title={Editorial Multiplicity Has More Potential Than Singularity},
year={2015},
volume={12},
number={2},
pages={395-395},
abstract={Presents an editorial on the developing concept in artificial
intelligence referred to as "muliplicity." The term “Singularity” is
used to describe a hypothetical punctuation point sometime in the future
where Artificial Intelligence (AI) will surpass human intelligence. The
concept has been popularized by science fiction author Vernor Vinge and
Ray Kurzweil. “Superintelligence,” a 2014 book by Nick Bostrom, explores
a similar theme. Recently, Stephen Hawking, Elon Musk, and Bill Gates
publicly expressed fears about the dangers of post-Singularity AI and
robotics. Here, it is suggested that "Singularity" is distracting
attention from a far more realistic and important development that we
might call “Multiplicity.” Multiplicity characterizes an emerging
category of systems where diverse groups of humans work together with
diverse groups of machines to solve difficult problems. Multiplicity
combines the wisdom of crowds with the power of cloud computing.},
keywords={Artificial intelligence},
doi={10.1109/TASE.2015.2408458},
ISSN={1545-5955},
month={April},}
@ARTICLE{6898725,
journal={Computer},
title={News Briefs},
year={2014},
volume={47},
number={9},
pages={17-21},
abstract={Topics include Russian hackers stealing more than 1 billion
usernames and passwords, researchers developing a new type of
brain-inspired chip, a US judge rejecting a Silicon Valley
corporate-collusion settlement, proponents saying that their new
networking approach could replace TCP/IP, researchers setting a speed
record for data transmission over copper wiring, a "digital tattoo" that
unlocks smartphones, a group proposing a new smart-home wireless
technology, cloud computing making life difficult for forensics
investigators, smart robotic furniture that can assemble and reassemble
itself, and a sensor system that will track players' performance during
this year's US professional football season.},
keywords={ARM;Aalborg University;Adobe Systems;Alcatel-Lucent;AllSeen
Alliance;Apple;Bell Labs;Bluetooth;Cloud Computing Forensic Science
Working Group;Code On;Cornell University;DARPA;Freescale
Semiconductor;G.fast;Google;Google's Nest;Hold
Security;IBM;Intel;Internet communications;Judge Lucy Koh;MIT;Moto
X;Motorola;NFC;NIST;National Football League;Next Gen
Stats;RFID;RLNC;Random Linear Network
Coding;Roombot;Samsung;Steinwurf;Swiss Federal Institute of Technology
Lausanne;SyNapse;Systems of Neuromorphic Adaptive Plastic Scalable
Electronics;TCP/IP;Thread Group;TrueNorth;US District court;US National
Institute of Standards and Technology;US
football;VivaLnk;Wi-Fi;XG-FAST;Yale Security;Zebra
Technologies;chip;cloud computing;collusion;copper wiring;digital
tattoo;eSkin;energy efficient;forensics investigators;lawsuit;near-field
communications;neurons;processor;robotics;security;sensors;smart
furniture;smart home;the California Institute of Technology;wage
fixing;wireless},
doi={10.1109/MC.2014.258},
ISSN={0018-9162},
month={Sept},}
@ARTICLE{6104197,
author={K. Goldberg},
journal={IEEE Transactions on Automation Science and Engineering},
title={What Is Automation?},
year={2012},
volume={9},
number={1},
pages={1-2},
abstract={It is argued that automation has expanded beyond its roots in
Manufacturing to include applications in Healthcare, Security,
Transportation, Agriculture, Construction, Energy, and many other areas.
Both Robotics and Automation explore the frontiers of automated and
semi-automated machines. Both fields are increasingly concerned with the
role of humans and human interfaces, and with the potential of the
Internet and Cloud Computing. So what is the difference between Robotics
and Automation? There are many possible distinctions. Here is the
summary from our Society's Field of Interest Statement: "...Robotics
focuses on systems incorporating sensors and actuators that operate
autonomously or semi-autonomously in cooperation with humans. Robotics
research emphasizes intelligence and adaptability to cope with
unstructured environments. Automation research emphasizes efficiency,
productivity, quality, and reliability, focusing on systems that operate
autonomously, often in structured environments over extended periods,
and on the explicit structuring of such environments." This statement
emphasizes how Automation emphasizes structured versus unstructured
environments, reliability versus adaptability, and efficiency versus
exploratory operations. These are valuable distinctions and the author
would like to propose another one. In his view, research in Robotics
emphasizes Feasibility. Feasibility focuses on proof-of-concept,
demonstrating how a new functionality can be achieved. On the other
hand, he feels, research in Automation emphasizes Quality. The author
wishes to dispel the myth of the excluded middle: Robotics and
Automation are not disjoint. Feasibility and Quality are closely related.},
doi={10.1109/TASE.2011.2178910},
ISSN={1545-5955},
month={Jan},}
@ARTICLE{5676363,
author={P. E. Ross},
journal={IEEE Spectrum},
title={The jacked-in decade [Spectral Lines]},
year={2011},
volume={48},
number={1},
pages={8-8},
abstract={Connectedness is now a given. Smartphones offer us advice, the
Internet carries our voices and video, the cloud archives our data, and
e-readers pull it to Earth again. We share all this stuff and more via
our social networks. Behind the scenes, brainy grids juggle power, and
vast military networks stretch out to robotic planes equipped with
devastating firepower above distant battlefields. Nothing seems to
happen anymore without global resources coming into play.},
keywords={Technological innovation},
doi={10.1109/MSPEC.2011.5676363},
ISSN={0018-9235},
month={Jan},}
@INPROCEEDINGS{7886869,
booktitle={2016 4th International Conference on Robotics and
Mechatronics (ICROM)},
title={[Front cover]},
year={2016},
pages={1-1},
abstract={The following topics are dealt with: flexible joint
underactuated manipulator; multifunctional elastic actuator; spacecraft
rendezvous; robotic arms; quadruped robot; robot manipulator visual
servoing control; endoscopic sinus surgery simulation system; uncertain
teleoperation system; skull based surgery virtual system; parallel cable
robot; steerable screw in-pipe inspection robot; foot-controlled
endoscope positioner; UAV; SLAM; 2RT parallel robot; eye surgery; input
constrained differential drive robots; heart robotic-assisted surgery;
2-DOF spherical parallel robot; 3-DOF decoupled parallel robot;
pneumatic actuator; articulated eight-wheeled mobile robot; n-DOF
nonlinear manipulators; overconstrained 3-DOF decoupled parallel
mechanism; energy harvesting CMOS digital pixel sensor; series elastic
actuator; dynamically tuned gyroscope; five axes satellite simulator;
4-DOF parallel mechanism; knee rehabilitation robot; digital hydraulic
system; quadrotor obstacle avoidance; servo-pneumatic actuator position
control; electro-hydraulic servo systems; uncertain switched system
adaptive control; human emotion state recognition; human-robot
interaction; linear system; Euler-Bernoulli beam vibration
stabilization; piezoelectric actuators; Timoshenko beam vibration
suppression; force-reflecting dual user teleoperation system; lower-limb
walking assistant robot; quadcopter nonlinear tracking control; plate
parallel manipulator; wheeled mobile- robot motion planning; Sharif
lower limb exoskeleton; three-mass inverted pendulum model; biped
robots; revolute-prismatic-spherical parallel robot; SURENA III humanoid
robot; RoboParrot social robot; 3DOF ankle prosthesis; 2-DOF ankle joint
actuation mechanism; Trackerbot robotic surveillance system; gripper;
redundant electro-hydraulic servo system; RoboCab; wire-driven parallel
manipulator; 3-CRR spatial parallel robot; NAO humanoid robot; bilateral
teleoperation; Novint Falcon haptic device; social humanoid robot;
elderly people assistance; tendon-based manipulator; pediatric
hospitals; point-cloud-based haptic rendering; moving object detection;
SCARA robots; omniwheeled mobile robot; educational mobile robot;
real-time video stabilization; and leader-labor multirobot localization.},
keywords={CMOS image sensors;SLAM (robots);adaptive control;artificial
satellites;autonomous aerial vehicles;cardiology;collision
avoidance;control engineering education;electrohydraulic control
equipment;emotion recognition;endoscopes;energy harvesting;flexible
manipulators;grippers;gyroscopes;handicapped aids;haptic
interfaces;helicopters;human-robot interaction;humanoid robots;hydraulic
systems;legged locomotion;linear systems;medical robotics;multi-robot
systems;nonlinear control systems;object detection;paediatrics;patient
rehabilitation;pendulums;piezoelectric actuators;pneumatic
actuators;prosthetics;rendering (computer graphics);robot
vision;servomechanisms;surgery;switching systems
(control);telerobotics;uncertain systems;vibration control;video
surveillance;visual servoing;2-DOF ankle joint actuation mechanism;2-DOF
spherical parallel robot;2RT parallel robot;3-CRR spatial parallel
robot;3-DOF decoupled parallel robot;3DOF ankle prosthesis;4-DOF
parallel mechanism;Euler-Bernoulli beam vibration stabilization;NAO
humanoid robot;Novint Falcon haptic device;RoboCab;RoboParrot social
robot;SCARA robots;SLAM;SURENA III humanoid robot;Sharif lower limb
exoskeleton;Timoshenko beam vibration suppression;Trackerbot robotic
surveillance system;UAV;articulated eight-wheeled mobile robot;bilateral
teleoperation;biped robots;digital hydraulic system;dynamically tuned
gyroscope;educational mobile robot;elderly people
assistance;electro-hydraulic servo systems;endoscopic sinus surgery
simulation system;energy harvesting CMOS digital pixel sensor;eye
surgery;five axes satellite simulator;flexible joint underactuated
manipulator;foot-controlled endoscope positioner;force-reflecting dual
user teleoperation system;gripper;heart robotic-assisted surgery;human
emotion state recognition;human-robot interaction;input constrained
differential drive robots;knee rehabilitation robot;leader-labor
multirobot localization;linear system;lower-limb walking assistant
robot;moving object detection;multifunctional elastic actuator;n-DOF
nonlinear manipulators;omniwheeled mobile robot;overconstrained 3-DOF
decoupled parallel mechanism;parallel cable robot;pediatric
hospitals;piezoelectric actuators;plate parallel
manipulator;point-cloud-based haptic rendering;quadcopter nonlinear
tracking control;quadrotor obstacle avoidance;quadruped robot;real-time
video stabilization;redundant electro-hydraulic servo
system;revolute-prismatic-spherical parallel robot;robot manipulator
visual servoing control;robotic arms;series elastic
actuator;servo-pneumatic actuator position control;skull based surgery
virtual system;social humanoid robot;spacecraft rendezvous;steerable
screw in-pipe inspection robot;tendon-based manipulator;three-mass
inverted pendulum model;uncertain switched system adaptive
control;uncertain teleoperation system;wheeled mobile robot motion
planning;wire-driven parallel manipulator},
doi={10.1109/ICRoM.2016.7886869},
month={Oct},}
@INPROCEEDINGS{7880273,
booktitle={2017 IEEE 15th International Symposium on Applied Machine
Intelligence and Informatics (SAMI)},
title={[Front cover]},
year={2017},
pages={c1-c1},
abstract={The following topics are dealt with: cognitive computing; data
mining; social robotics; cloud computing; neural networks; reinforcement
learning; image processing; mobile computing; health care; handicapped
aids.},
keywords={cognitive systems;data mining;handicapped aids;health
care;image processing;learning (artificial intelligence);mobile
computing;neural nets;robots;cloud computing;cognitive computing;data
mining;handicapped aids;health care;image processing;mobile
computing;neural networks;reinforcement learning;social robotics},
doi={10.1109/SAMI.2017.7880273},
month={Jan},}
@INPROCEEDINGS{7878182,
booktitle={2016 2nd International Conference on Communication Control
and Intelligent Systems (CCIS)},
title={[Front cover]},
year={2016},
pages={1-1},
abstract={The following topics are dealt with: wireless ad hoc network,
sensor network and VANET; image, signal, speech processing; robotics;
fuzzy logic; artificial neural network; VLSI design, SoC and system
optimisation; data mining, high performance computing, cloud computing,
cryptography, security and authentication algorithm; and energy
management system, thermal power plants, transport, logistics, and
management techniques.},
keywords={VLSI;cloud computing;cryptography;data mining;energy
management systems;fuzzy logic;image processing;integrated circuit
design;logistics;neural nets;optimisation;robots;speech
processing;system-on-chip;thermal power
stations;transportation;vehicular ad hoc networks;wireless sensor
networks;SoC;VANET;VLSI design;artificial neural network;authentication
algorithm;cloud computing;cryptography;data mining;energy management
system;fuzzy logic;high performance computing;image
processing;logistics;management techniques;robotics;security
algorithm;signal processing;speech processing;system
optimisation;thermal power plants;transport;wireless ad hoc
network;wireless sensor network},
doi={10.1109/CCIntelS.2016.7878182},
month={Nov},}
@INPROCEEDINGS{7866710,
booktitle={2016 International Conference on Frontiers of Information
Technology (FIT)},
title={[Front cover]},
year={2016},
pages={c1-c1},
abstract={The following topics are dealt with: information technology;
robotics; computer networks; communication networks; cloud computing;
software engineering; data mining; computer vision; pattern recognition;
image processing; signal processing; smart grid; and electronic
computing.},
keywords={Internet;data mining;electronic engineering computing;pattern
recognition;robots;signal processing;smart power grids;software
engineering;telecommunication networks;cloud computing;communication
networks;computer networks;computer vision;data mining;electronic
computing;image processing;information technology;pattern
recognition;robotics;signal processing;smart grid;software engineering},
doi={10.1109/FIT.2016.001},
month={Dec},}
@INPROCEEDINGS{7791813,
booktitle={2016 10th International Conference on Complex, Intelligent,
and Software Intensive Systems (CISIS)},
title={[Front cover]},
year={2016},
pages={c1-c1},
abstract={The following topics are dealt with: cloud computing and power
management; fuzzy systems and soft computing; multimedia and e-Learning
systems; complex and scalable system; wireless communication;
bioinformatics; wireless sensor networks; autonomic distributed systems;
multi-core systems; robotics; motion capture and classification;
agent-based computing; and virtual reality systems.},
keywords={bioinformatics;cloud computing;computer aided
instruction;distributed processing;image classification;image motion
analysis;multimedia systems;multiprocessing systems;power aware
computing;radiocommunication;robots;virtual reality;wireless sensor
networks;agent-based computing;autonomic distributed
systems;bioinformatics;cloud computing;complex systems;e-learning
systems;electronic learning;fuzzy systems;motion capture;motion
classification;multicore systems;multimedia systems;power
management;robotics;scalable systems;soft computing;virtual reality
systems;wireless communication;wireless sensor networks},
doi={10.1109/CISIS.2016.149},
month={July},}
@INPROCEEDINGS{7777933,
booktitle={2016 IEEE 7th Annual Ubiquitous Computing, Electronics Mobile
Communication Conference (UEMCON)},
title={Contents},
year={2016},
pages={1-5},
abstract={The following topics are dealt with: ad hoc networks;
artificial intelligence; machine learning; big data; data management;
data analytics; cloud computing; cognitive radio networks; control
theory and applications; distributed systems; electronics and
information technology; electronic instrumentation; electronic power
converters; inverters; evolutionary computation and algorithms; FPGA
design; green wireless network; image processing; multimedia technology;
information security; information encoding technology; mobile computing;
wireless computing; network architectures; network security; robotics;
autonomous system; software engineering; telecommunication systems;
VLSI; microelectronic circuits; and embedded systems.},
keywords={Big Data;VLSI;artificial intelligence;cognitive radio;control
theory;data analysis;database management systems;distributed
processing;embedded systems;evolutionary computation;field programmable
gate arrays;green computing;image processing;invertors;logic
design;multimedia systems;power convertors;radio
networks;robots;security of data;software engineering;FPGA
design;VLSI;ad hoc networks;artificial intelligence;autonomous
system;big data;cloud computing;cognitive radio networks;control
theory;data analytics;data management;distributed systems;electronic
instrumentation;electronic power converters;electronics
technology;embedded systems;evolutionary computation;green wireless
network;image processing;information encoding technology;information
security;information technology;inverters;machine
learning;microelectronic circuits;mobile computing;multimedia
technology;network architectures;network security;robotics;software
engineering;telecommunication systems;wireless computing},
doi={10.1109/UEMCON.2016.7777933},
month={Oct},}
@INPROCEEDINGS{7695005,
booktitle={2016 International Conference on Intelligent Networking and
Collaborative Systems (INCoS)},
title={[Title page i]},
year={2016},
pages={i-i},
abstract={The following topics are dealt with: intelligent networking;
cloud computing; management systems; collaborative robotics;
collaborative sensing; social networking; semantics; cognitive
networking systems; wireless systems; mobile systems; hidden complex
networks; nature inspired computing; optimization; machine learning;
data mining; scheduling; resource management; image processing; secure
intelligent collaborative systems; learner profiling; adaptive learning;
emotional approach; interactive learning systems; social networks;
business productivity; network management; network performance;
information network design; communication networks; data privacy; and
intelligent secure collaborative e-business systems.},
keywords={cloud computing;computer aided instruction;computer network
performance evaluation;data mining;data privacy;electronic
commerce;groupware;image processing;interactive systems;learning
(artificial intelligence);mobile
computing;optimisation;radiocommunication;resource
allocation;robots;scheduling;security of data;social networking
(online);telecommunication network management;Scheduling;adaptive
learning;business productivity;cloud computing;cognitive networking
systems;collaborative robotics;collaborative sensing;communication
networks;data mining;data privacy;emotional approach;hidden complex
networks;image processing;information network design;intelligent
networking;intelligent secure collaborative e-business
systems;interactive learning systems;learner profiling;machine
learning;management systems;mobile systems;nature inspired
computing;network management;network performance;optimization;resource
management;secure intelligent collaborative systems;semantics;social
networking;social networks;wireless systems},
doi={10.1109/INCoS.2016.1},
month={Sept},}
@INPROCEEDINGS{7560551,
author={Y. Chen and Q. Xin and S. Guan},
booktitle={2016 IEEE Information Technology, Networking, Electronic and
Automation Control Conference},
title={Keynote speech 1: Creating IoT applications through visual
programming and service orchestration},
year={2016},
pages={1-7},
abstract={Service-oriented computing and service-level orchestration
through visual or graphical programming have been widely used in
software and particularly in Web application development. Their further
developments and fusing into Internet of Things, Robot as a Service
(RaaS), cloud computing, and big data analysis have created the
infrastructure for the information technologies to penetrate into all
sectors of industry, service, and society. Figure 1 shows the stack of
the related concepts, technologies, and applications. IoT service
orchestration and visual programming separates the high-level
composition from the low-level device programming, which allows the
application builders to focus on the business logic, while the service
providers to focus on efficient service development. This presentation
will discuss the current service orchestration and visual programming
environments for IoT application development. The presentation
centralizes the topics around IoT. It starts with the upper layer of
IoT, including service-oriented computing, Web software development, and
service orchestration for system integration, which forms the foundation
of cloud computing and a major data source of big data analysis. On the
other hand, IoT standards and protocols are presented, which connect the
service orchestration infrastructure tohardware and devices. As IoT
applications and implementations, ADS (Autonomous Decentralized
Systems), DPWS (Device Profile for Web Services), and Robot as a Service
will be discussed. Different orchestration environments that have been
widely used in industry and academic world will be presented, including
BPEL in Oracle SOA Suite, Microsoft Workflow Foundation, Intel IoT
Service Orchestration Layer, Microsoft Robotics Development Studio, MIT
App Inventor, CMU Alice visual game programming environment, and ASU
VIPLE (Visual IoT/Robotics Programming Environment). As a case study,
VIPLE will be presented in more detail. VIPLE is a free software with
ope- source IoT middleware developed at Arizona State University. VIPLE
orchestration environment has an open interface to IoT devices. Any IoT
device that can be programmed to interpret JSON objects and wrap sensory
data into JSON objects can interface with VIPLE and be programmed using
VIPLE orchestration environment. A demonstration will be given to show
how an IoT application can be developed in minutes to control Intel
Edison IoT device. VIPLE software, including orchestration environment,
IoT middleware, full document, and lecture slides can be downloaded at:
http://venus.eas.asu.edu/WSRepository/VIPLE.},
keywords={Internet of Things;graphical programming;robotics;service
orchestration;service-oriented architecture},
doi={10.1109/ITNEC.2016.7560551},
month={May},}
@INPROCEEDINGS{7509362,
booktitle={2016 IEEE Students' Conference on Electrical, Electronics and
Computer Science (SCEECS)},
title={Table of contents},
year={2016},
pages={1-7},
abstract={The following topics are dealt with: MOSFET; Cognitive Radio
Reconfigurable Antenna; Blind Video Watermarking; Data Hiding; power
factor correction; Vehicular Ad-hoc Networks; FPGA; BPSK modulation;
Mobile Ad hoc Networks; DC-DC Converter; Multilevel Inverter; Soft
Computing; Robotic System; Motor Drive; Power System; SRAM Cell; Energy
Management System; QPSK Modulator; FFT Analysis; Recommender System;
Cloud Computing; Wind Turbine; Edge Detection.},
keywords={DC-DC power convertors;MOSFET;SRAM chips;antennas;cloud
computing;cognitive radio;computer science;data encapsulation;edge
detection;energy management systems;fast Fourier transforms;field
programmable gate arrays;invertors;mobile ad hoc
networks;modulation;motor drives;power factor;power systems;recommender
systems;robots;video watermarking;wind turbines;BPSK modulation;DC-DC
converter;FFT analysis;FPGA;MOSFET;QPSK modulator;SCEECS;SRAM cell;blind
video watermarking;cloud computing;cognitive radio reconfigurable
antenna;computer science;data hiding;edge detection;electronics;energy
management system;mobile ad hoc networks;motor drive;multilevel
inverter;power factor correction;power system;recommender system;robotic
system;soft computing;vehicular ad-hoc networks;wind turbine},
doi={10.1109/SCEECS.2016.7509362},
month={March},}
@INPROCEEDINGS{7492635,
booktitle={2015 International Conference on Trends in Automation,
Communications and Computing Technology (I-TACT-15)},
title={[Front cover]},
year={2015},
pages={i-i},
abstract={The following topics are dealt with: Software Engineering,
Data Mining And Big Data; Cyber Forensics and Information Security;
Sensor/Mobile Ad-hoc and Cloud Computing; Communications and Embedded
Systems; VLSI; Signal Processing and Applications; Control System and
Robotics; Electrical Power System Studies.},
keywords={Big Data;VLSI;automation;cloud computing;control systems;data
mining;electric power generation;embedded systems;security of
data;signal processing;software engineering;Big
Data;VLSI;automation;cloud computing;communications;computing
technology;control system;cyber forensics;data mining;electrical power
system;embedded systems;information security;robotics;sensor/mobile
ad-hoc;signal processing;software engineering},
doi={10.1109/ITACT.2015.7492635},
month={Dec},}
@INPROCEEDINGS{7468880,
booktitle={2015 8th International Symposium on Computational
Intelligence and Design (ISCID)},
title={[Copyright notice - vol 1]},
year={2015},
volume={1},
pages={iv-iv},
abstract={The following topics are dealt with: computational
intelligence; image processing; evolutionary computation; ontologies;
neural networks; cloud computing; learning; signal processing; Big Data;
wireless sensor networks; data security; pattern classification;
robotics; data mining; and watermarking.},
keywords={Big Data;cloud computing;data mining;evolutionary
computation;learning (artificial intelligence);neural nets;ontologies
(artificial intelligence);pattern classification;robots;security of
data;signal processing;wireless sensor networks;Big Data;cloud
computing;computational intelligence;data mining;data
security;evolutionary computation;image processing;learning;neural
networks;ontologies;pattern classification;robotics;signal
processing;watermarking;wireless sensor networks},
doi={10.1109/ISCID.2015.4},
month={Dec},}
@INPROCEEDINGS{7449653,
booktitle={2015 5th Nirma University International Conference on
Engineering (NUiCONE)},
title={[Front cover]},
year={2015},
pages={1-1},
abstract={The following topics are dealt with: cloud computing; ANN;
SMIB power system; robotic control; support vector machine;
telemedicine; mobile social network; engineering education; high speed
brushed DC motor; data privacy; face recognition; RFID-based library;
WDM optical system; low power CMOS low noise amplifier; encryption; SAR
image data; and copy move forgery detection.},
keywords={CMOS integrated circuits;DC motors;cloud
computing;cryptography;data privacy;engineering education;face
recognition;low noise amplifiers;mobile computing;neural nets;power
engineering computing;radiofrequency identification;robots;social
networking (online);support vector machines;wavelength division
multiplexing;ANN;RFID-based library;SAR image data;SMIB power system;WDM
optical system;cloud computing;copy move forgery detection;data
privacy;encryption;engineering education;face recognition;high speed
brushed DC motor;low power CMOS low noise amplifier;mobile social
network;robotic control;support vector machine;telemedicine},
doi={10.1109/NUICONE.2015.7449653},
month={Nov},}
@INPROCEEDINGS{7444021,
booktitle={2015 IEEE International WIE Conference on Electrical and
Computer Engineering (WIECON-ECE)},
title={Table of content},
year={2015},
pages={1-6},
abstract={The following topics are dealt with: antenna, microwave & RF
engineering; artificial intelligence, computer networks security;
biomedical engineering & bioinformatics; cloud, big data & ICT; computer
architecture & systems; computer vision; graphics & HCI; multimedia
engineering; devices, materials & processing; electrical machines &
drives; nanotechnology; semiconductor technology; photonic technologies;
power electronics; power system; renewable energy; robotics, control &
automation; software & database systems; signal processing; image
processing; video processing; VLSI circuits; wireless communication; and
optical communication.},
keywords={Big Data;VLSI;artificial
intelligence;bioinformatics;biomedical engineering;cloud
computing;computer architecture;computer graphics;computer network
security;computer vision;database management systems;drives;electric
machines;information technology;microwave antennas;multimedia
communication;nanotechnology;optical communication;power
electronics;renewable energy sources;robots;semiconductor
technology;video signal processing;wireless channels;HCI;ICT;RF
engineering;VLSI circuits;antenna;artificial intelligence;automation;big
data;bioinformatics;biomedical engineering;cloud;computer
architecture;computer networks security;computer vision;database
systems;device processing;drives;electrical machines;graphics;image
processing;material processing;microwave engineering;multimedia
engineering;nanotechnology;optical communication;photonic
technologies;power electronics;power system;renewable
energy;robotics;semiconductor technology;signal processing;software
systems;video processing;wireless communication},
doi={10.1109/WIECON-ECE.2015.7444021},
month={Dec},}
@INPROCEEDINGS{7424230,
booktitle={2015 10th International Conference on Broadband and Wireless
Computing, Communication and Applications (BWCCA)},
title={[Title page i]},
year={2015},
pages={i-i},
abstract={The following topics are dealt with: wireless ad-hoc networks;
wireless sensor networks; privacy; vehicular networks; intelligent
knowledge management; wireless mesh networks; mobile networks;
multimedia applications; Web applications; energy-aware computing;
network protocols; resource management; e-commerce security; robotics
systems; support systems; cloud computing; network virtualization and
natural language processing.},
keywords={Internet;cloud computing;computer network management;computer
network security;data privacy;electronic commerce;knowledge based
systems;knowledge management;mobile radio;multimedia
communication;natural language processing;protocols;vehicular ad hoc
networks;virtualisation;wireless mesh networks;wireless sensor
networks;Web application;cloud computing;e-commerce
security;energy-aware computing;intelligent knowledge management;mobile
network;multimedia application;natural language processing;network
protocol;network virtualization;privacy;resource management;robotics
system;support system;vehicular network;wireless ad-hoc network;wireless
mesh network;wireless sensor network},
doi={10.1109/BWCCA.2015.1},
month={Nov},}
@INPROCEEDINGS{7339250,
booktitle={2015 6th IEEE International Conference on Software
Engineering and Service Science (ICSESS)},
title={Table of contents},
year={2015},
pages={1-18},
abstract={The following topics are dealt with: software engineering;
service science; MAS; medical computing; mobile computing; image
processing; robotics; signal processing; cloud computing; multiagent
system; information retrieval; user interface; data security; social
networking; agricultural computing; computer games; knowledge
engineering; data handling; business data processing; and computer
networks.},
keywords={agricultural engineering;artificial intelligence;business data
processing;cloud computing;computer games;computer networks;information
retrieval;medical computing;mobile computing;robots;security of
data;signal processing;social networking (online);software
engineering;user interfaces;MAS;agricultural computing;business data
processing;cloud computing;computer games;computer networks;data
handling;data security;image processing;information retrieval;knowledge
engineering;medical computing;mobile computing;multiagent
system;robotics;service science;signal processing;social
networking;software engineering;user interface},
doi={10.1109/ICSESS.2015.7339250},
ISSN={2327-0586},
month={Sept},}
@INPROCEEDINGS{7298289,
booktitle={2015 IEEE Symposium on Computer Applications Industrial
Electronics (ISCAIE)},
title={[Front cover]},
year={2015},
pages={1-1},
abstract={The following topics are dealt with: signal & image
processing, medical electronics; computational intelligence,
bioinformatics & computational biology; trusted computing & secure
systems, cloud computing, software engineering; medical robotics and
informatics; and industrial electronics-automation and mechatronics.},
keywords={bioinformatics;biomedical electronics;cloud computing;image
processing;industrial control;mechatronics;medical information
systems;medical robotics;software engineering;trusted
computing;bioinformatics;cloud computing;computational
biology;computational intelligence;image processing;industrial
automation;industrial electronics;mechatronics;medical
electronics;medical informatics;medical robotics;secure systems;signal
processing;software engineering;trusted computing},
doi={10.1109/ISCAIE.2015.7298289},
month={April},}
@INPROCEEDINGS{7294029,
booktitle={2015 IEEE International Conference on Automation Science and
Engineering (CASE)},
title={Content list},
year={2015},
pages={31-55},
abstract={The following topics are dealt with: simulation optimization;
building automation; production automation; smart logistics; discrete
event system; mechatronics; industrial wireless networking;
manufacturing automation; supply chain management; networked control
system; welding automation; multiagent system; semiconductor
manufacturing; health care management; vision system; process control;
additive manufacturing; robotics; assembly automation; scheduling;
machine learning; Internet of Things; industrial robotics; scheduling;
machine learning; Internet of Things; cloud computing; and machine tool.},
keywords={Internet of Things;assembling;building management
systems;computer vision;discrete event simulation;health care;learning
(artificial intelligence);logistics;machine tools;manufacturing
systems;mechatronics;multi-agent systems;networked control
systems;optimisation;production engineering
computing;scheduling;semiconductor industry;supply chain
management;welding;Internet of Things;additive manufacturing;assembly
automation;building automation;discrete event system;health care
management;industrial robotics;industrial wireless networking;machine
learning;machine tool;manufacturing automation;mechatronics;multiagent
system;networked control system;process control;production
automation;scheduling;semiconductor manufacturing;simulation
optimization;smart logistics;supply chain management;vision
system;welding automation},
doi={10.1109/CoASE.2015.7294029},
ISSN={2161-8070},
month={Aug},}
@INPROCEEDINGS{7208269,
booktitle={2015 IEEE 10th Jubilee International Symposium on Applied
Computational Intelligence and Informatics},
title={[Copyright notice]},
year={2015},
pages={1-1},
abstract={The following topics are dealt with: Big Data; deep learning;
space robotics; computer-aided patient evaluation; human-robot
interaction; cryptography; inverted pendulum control; bidirectional
flyback inverter; Web-based real-time collaboration; mobile robots;
wireless sensor network; content-based image retrieval; PID controller;
robot arms; PD control; digital signal processing; fuzzy automata;
adaptive control; multiview computer vision; university education; cloud
computing; mobile network; business process similarity; social network;
distribution network; PageRank-based recommender system; intrusion
detection; surgical robots; and robot car model.},
keywords={Big Data;PD control;adaptive control;aerospace
robotics;automata theory;business data processing;cloud
computing;computer vision;content-based
retrieval;cryptography;distribution networks;engineering
education;human-robot interaction;image retrieval;invertors;learning
(artificial intelligence);manipulators;medical computing;medical
robotics;mobile radio;mobile robots;nonlinear control
systems;recommender systems;security of data;signal processing;social
networking (online);three-term control;wireless sensor networks;Big
Data;PD control;PID controller;PageRank-based recommender
system;Web-based real-time collaboration;adaptive control;bidirectional
flyback inverter;business process similarity;cloud
computing;computer-aided patient evaluation;content-based image
retrieval;cryptography;deep learning;digital signal
processing;distribution network;fuzzy automata;human-robot
interaction;intrusion detection;inverted pendulum control;mobile
network;mobile robots;multiview computer vision;robot arms;robot car
model;social network;space robotics;surgical robots;university
education;wireless sensor network},
doi={10.1109/SACI.2015.7208269},
month={May},}
@INPROCEEDINGS{7155783,
booktitle={2015 International Conference on Computing Communication
Control and Automation},
title={[Title page i]},
year={2015},
pages={i-i},
abstract={The following topics are dealt with: robotics; cloud
computing; cryptography; databases; data mining; high performance
computing; signal processing; software engineering; project management;
soft computing; VLSI; and SoC.},
keywords={VLSI;cloud computing;cryptography;data mining;database
management systems;fuzzy logic;neural nets;parallel processing;project
management;robots;signal processing;software
engineering;system-on-chip;uncertainty handling;SoC;VLSI;cloud
computing;cryptography;data mining;databases;high performance
computing;project management;robotics;signal processing;soft
computing;software engineering},
doi={10.1109/ICCUBEA.2015.1},
month={Feb},}
@INPROCEEDINGS{7133562,
booktitle={2015 IEEE Sensors Applications Symposium (SAS)},
title={Table of contents},
year={2015},
pages={ii-xvi},
abstract={The following topics are dealt with: multiparametric sensors
in biomedicine; magnetic sensing and measurements; smart transducers;
sensor clouds; power management and low power design; sensors and
algorithms for resource-constrained agricultural wireless sensor
networks; integrated system health management; sensors for smart
building and home automation; nondestructive evaluation and remote
sensing; agriculture, environment and health monitoring; sensor arrays
and multisensor data fusion; marine robotics; and microsensors.},
keywords={agriculture;biomedical equipment;building management
systems;environmental monitoring (geophysics);health care;home
automation;intelligent sensors;low-power electronics;magnetic
sensors;marine control;microsensors;nondestructive testing;remote
sensing;robots;sensor arrays;sensor fusion;transducers;wireless sensor
networks;agriculture;biomedicine;environment monitoring;health
monitoring;home automation;integrated system health management;low power
design;magnetic measurements;magnetic sensing;marine
robotics;microsensors;multiparametric sensors;multisensor data
fusion;nondestructive evaluation;power management;remote sensing;sensor
arrays;sensor clouds;smart building;smart transducers;wireless sensor
networks},
doi={10.1109/SAS.2015.7133562},
month={April},}
@INPROCEEDINGS{7133056,
booktitle={SoutheastCon 2015},
title={[Front cover]},
year={2015},
pages={c1-c1},
abstract={The following topics are dealt with: cloud computing; sensor
networks; military system; software engineering; power system;
sustainable energy; robotics; computer vision; health care; health
informatics; data mining; machine learning; geosciences; remote sensing;
transportation; logistics; bioengineering; bioinformatics;
nanotechnology; social networking; technical education; image
processing; computer engineering; embedded system; electromagnetics;
microwaves; game development.},
keywords={bioinformatics;cloud computing;computer aided instruction;data
mining;geophysics computing;health care;image processing;learning
(artificial intelligence);military computing;power engineering
computing;robots;social networking (online);software
engineering;bioengineering;bioinformatics;cloud computing;computer
engineering;computer vision;data mining;electromagnetics;embedded
system;game development;geosciences;health care;health informatics;image
processing;logistics;machine learning;microwaves;military
system;nanotechnology;power system;remote sensing;robotics;sensor
networks;social networking;software engineering;sustainable
energy;technical education;transportation},
doi={10.1109/SECON.2015.7133056},
ISSN={1091-0050},
month={April},}
@INPROCEEDINGS{7086812,
booktitle={2015 International Conference on Electronics, Communications
and Computers (CONIELECOMP)},
title={[Front cover]},
year={2015},
pages={c1-c1},
abstract={The following topics are dealt with: image encoding; image
filtering; cloud computing system; cognitive radio; mobile networks;
robotic hand; mobile robots.},
keywords={cloud computing;cognitive radio;image coding;image
filtering;manipulators;mobile radio;mobile robots;cloud computing
system;cognitive radio;image encoding;image filtering;mobile
networks;mobile robots;robotic hand},
doi={10.1109/CONIELECOMP.2015.7086812},
month={Feb},}
@INPROCEEDINGS{7079257,
booktitle={2014 8th Asia Modelling Symposium},
title={Table of Contents},
year={2014},
pages={v-ix},
abstract={The following topics are dealt with: neural networks;
bioinformatics; bioengineering; computational finance; economics;
intelligent systems; hybrid computing; soft computing; computational
intelligence; control intelligence; robotics; cybernetics; discrete
event systems; real time systems; image processing; speech processing;
signal processing; natural language processing; industrial factor;
business factor; management factor; human factors; social issues; energy
management; power management; transportation; logistics; harbours;
shipping; marine simulation; Internet modelling; semantic Web; cloud
computing; data security; ontologies; mobile-ad hoc wireless networks;
Mobicast; sensor placement; target tracking; performance engineering;
and computer-and-communication systems.},
doi={10.1109/AMS.2014.4},
ISSN={2376-1164},
month={Sept},}
@INPROCEEDINGS{7062780,
booktitle={2014 International Conference on Communication and Network
Technologies},
title={Table of contents},
year={2014},
pages={1-5},
abstract={The following topics are dealt with: artificial intelligence;
robotics; antenna system; cloud computing; computer networks; cognitive
radio networks; image processing; mobile ad hoc networking; mobile
sensor networking; network security; optical communication; soft
computing techniques; VLSI signal processing; video processing and
wireless networks.},
keywords={VLSI;antennas;cloud computing;cognitive radio;computer
networks;fuzzy logic;image processing;mobile ad hoc networks;neural
nets;optical communication;robots;telecommunication security;uncertainty
handling;wireless sensor networks;VLSI signal processing;antenna
system;artificial intelligence;cloud computing;cognitive radio
network;computer network;image processing;mobile ad hoc
networking;mobile sensor networking;network security;optical
communication;robotics;soft computing technique;video
processing;wireless network},
doi={10.1109/CNT.2014.7062780},
month={Dec},}
@INPROCEEDINGS{7045288,
booktitle={2014 International Conference on High Performance Computing
and Applications (ICHPCA)},
title={Table of contents},
year={2014},
pages={iii-x},
abstract={The following topics are dealt with: greedy load balancing
algorithms; heterogeneous distributed computing system; exploiting fault
tolerance; cache memory structures; reconfigurable cache architecture;
multiphase applications scheduling; identical parallel multiprocessor;
MATLAB distributed computing server; AIM; resource provisioning
strategy; dynamic cloud environment; cloud computing memory allocation;
timestamped signature scheme; message recovery; CMMSPEED; reliable
real-time protocol; industrial mesh network; MANET; TCP variants
protocols; wireless network; selective cooperation method; dynamic
traffic pattern; NS2; data hiding; halftone images; mathematical
morphology; conjugate ordered dithering; secure group key agreement
protocol design; elliptic curve cryptography; fingerprint based
symmetric cryptography; BER performance comparison; MIMO System; STBC;
MRC; different fading channels; fault node detection algorithm; wireless
sensor networks; secured packet inspection; hierarchical pattern
matching; incremental clustering algorithm; GPU; STBC-OFDM WiMAX system
performance evaluation; graphics processing unit; string sorting;
many-threaded architectures; multithreaded architectures; differential
evolution; cost reduction cellular network; game theory; mobile wireless
sensor network; GNU radio; error rate performance enhancement; hybrid
equalization technique; MIMO-OFDM systems; BER; Brewster's angle;
polarization diversity technique; indoor visible light communication
system; route stability; AODV; parallel Hadamard transform; multimesh
network; dense matrix multiplication; 2D mesh; scalable parallel
clustering approach; parallel K means; firefly algorithms; apriori
algorithm; HDFS; parameter optimization; nonlinear fitting; epidemic
disease propagation detection algorithm; MapReduce; realistic social
contact networks; leather defects identification; auto adaptive edge
detection image processing algorithm; list scheduling algorithms
analysis; parallel sys- em; evolutionary Jordan Pi-sigma neural network;
gradient descent learning; HDFS performance evaluation; big data
management; securing financial network system; multilevel security;
cyber physical system; data mining concepts; Web crawling; convex
points; convex hull; image searching algorithm development; social
network; graph mining techniques; clustering algorithm; binary data
sets; human facial expression recognition; image color analysis; FPGA
realization; particle swarm optimisation algorithm; floating point
arithmetic; gradient local auto correlation; handwritten devanagari
character recognition; feature extraction; feature matching; speech
recognition; intelligent steganography detection; fuzzy logic; wind
energy conversion system; solide oxide fuel; biobjective portfolio
optimisation; muliobjective simulated annealing; test case
prioritization technique; free-text user authentication technique;
keystroke dynamics; weighted bag hybrid multiple classifier machine;
boosting prediction accuracy; automated document indexing; intelligent
hierarchical clustering; training feedforward neural network; novel
gravitational search optimization; real time robotic arm control; hand
gestures; hybrid differential evolution-flower pollination algorithm;
function minimization; bijective mapping; arbitrary finite state
machine; high performance computing application; lattice physics code
VISWAM; stencil computation; CPU-GPU; threshold voltage roll-off; triple
gate FinFET analysis; Ultra-Thin Si directly; cardiac pulse measurement;
multicore architecture; optimum gene selection; gene expression dataset;
cancer dataset; PKI; timestamped secure signing tool; e-documents;
trigger action reaction model; artificial energy drinks; stability
enhancement; differential evolution algorithm; molecular dynamics
application optimization; Intel Xeon Phi Coprocessor; pair programming
team;UTB-SG; DG MOSFET; security robot-SECBOT;AODV routing protocol
modification; VANET; lifting biorthogona},
keywords={Hadamard transforms;MIMO communication;OFDM modulation;cache
storage;cellular automata;cloud computing;data mining;distributed
processing;edge detection;feature extraction;floating point
arithmetic;graphics processing units;greedy algorithms;image
processing;image texture;mathematical morphology;median filters;mobile
communication;multi-threading;multiprocessing systems;neural
nets;optimisation;particle swarm optimisation;pattern matching;public
key cryptography;reconfigurable architectures;resource
allocation;routing protocols;simulated annealing;speech
recognition;steganography;transport protocols;wavelet
transforms;wireless sensor networks;AIM;AODV;AODV routing protocol
modification;BER;BER performance comparison;Brewster'
angle;CMMSPEED;CPU-GPU;GNU radio;HDFS;HDFS performance
evaluation;MANET;MATLAB distributed computing server;MIMO
System;MIMO-OFDM systems;MRC;MapReduce;ROM design;SDR;STBC;STBC-OFDM
WiMAX system;TCP variants protocols;Web crawling;adaptive background
subtraction;apriori algorithm;arbitrary finite state machine;auto
adaptive edge detection image processing algorithm;automated document
indexing;big data management;bijective mapping;binary data
sets;biobjective portfolio optimisation;biorthogonal wavelet
design;boosting prediction accuracy;cache memory structures;cancer
dataset;cardiac pulse measurement;cellular network;cloud computing
memory allocation;clustering algorithm;conjugate ordered
dithering;convex hull;convex points;cyber physical system;data
hiding;data mining concepts;decoder segment optimization;dense matrix
multiplication;differential evolution;dynamic cloud environment;dynamic
traffic pattern;e-documents;elliptic curve cryptography;epidemic disease
propagation detection algorithm;error rate performance
enhancement;evolutionary Jordan Pi-sigma neural network;exploiting fault
tolerance;fading channels;fault node detection algorithm;feature
extraction;feature matching;financial network system
security;fingerprint based symmetric cryptography;firefly
algorithms;floating point arithmetic;free-text user authentication
technique;function minimization;fuzzy based median filter;fuzzy
logic;game theory;gene expression dataset;generation reliability
evaluation;gradient descent learning;gradient local auto
correlation;graph mining techniques;graphics processing unit;greedy load
balancing algorithms;halftone images;hand gestures;handwritten
devanagari character recognition;heterogeneous distributed computing
system;hierarchical pattern matching;human facial expression
recognition;hybrid differential evolution-flower pollination
algorithm;hybrid equalization technique;identical parallel
multiprocessor;image color analysis;image searching algorithm
development;impulse noise detection;incremental clustering
algorithm;indoor visible light communication system;industrial mesh
network;intelligent hierarchical clustering;intelligent steganography
detection;intrusion detection systems;keystroke dynamics;lattice physics
code VISWAM;leather defects identification;list scheduling algorithms
analysis;many-threaded architectures;mathematical morphology;medical
images;message recovery;mobile communications;mobile wireless sensor
network;muliobjective simulated annealing;multicore
architecture;multilevel security;multimesh network;multiphase
applications scheduling;multithreaded architectures;neighborhood
switching median filter;nonlinear fitting;novel gravitational search
optimization;optimum gene selection;parallel Hadamard transform;parallel
K means;parallel system;parameter optimization;particle swarm
optimisation algorithm;quantum dot cellular automata;real time robotic
arm control;reconfigurable cache architecture;reconfigurable viterbi
decoder;reliable real-time protocol;resource provisioning strategy;route
stability;scalable parallel clustering approach;secure group key
agreement protocol design;secured packet inspection;selective
cooperation method;social contact networks;social network;solide oxide
fuel;speech recognition;stencil computation;string sorting;texture
analysis;threshold voltage roll-off;timestamped secure signing
tool;timestamped signature scheme;training feedforward neural
network;trigger action reaction model;triple gate FinFET
analysis;weighted bag hybrid multiple classifier machine;wind energy
conversion system;wind energy penetrated power system;wireless
network;wireless sensor networks},
doi={10.1109/ICHPCA.2014.7045288},
month={Dec},}
@INPROCEEDINGS{7044997,
booktitle={2014 International Conference on Computational Science and
Technology (ICCST)},
title={Table of contents},
year={2014},
pages={1-3},
abstract={The following topics are dealt with: cloud computing; image
compression; swarm robotic system; mobile ad hoc networks; software
evolution analysis; mobile robot; software requirements; multi-agent
system.},
keywords={cloud computing;data compression;image coding;mobile ad hoc
networks;mobile robots;multi-agent systems;multi-robot systems;software
engineering;cloud computing;image compression;mobile ad hoc
networks;mobile robot;multi-agent system;software evolution
analysis;software requirements;swarm robotic system},
doi={10.1109/ICCST.2014.7044997},
month={Aug},}
@INPROCEEDINGS{7036461,
booktitle={2014 9th International Conference on Industrial and
Information Systems (ICIIS)},
title={Table of contents},
year={2014},
pages={1-4},
abstract={The following topics are dealt with: cloud computing; image
processing; data security; power engineering computing; medical image
processing; pattern classification; wireless sensor networks; load
balancing; face recognition; wireless communication; and robotics.},
keywords={cloud computing;face recognition;medical image
processing;pattern classification;power engineering
computing;radiocommunication;resource allocation;robots;security of
data;wireless sensor networks;cloud computing;data security;face
recognition;image processing;load balancing;medical image
processing;pattern classification;power engineering
computing;robotics;wireless communication;wireless sensor networks},
doi={10.1109/ICIINFS.2014.7036461},
ISSN={2164-7011},
month={Dec},}
@INPROCEEDINGS{7021006,
booktitle={2014 International Conference on Emerging Technologies (ICET)},
title={Table of contents},
year={2014},
pages={ix-xii},
abstract={The following topics are dealt with: software engineering;
requirements engineering; image processing; computer networks; cloud
computing; cryptography; information retrieval; machine learning;
cognitive radio networks; robotics; MEMS; power systems; and
telecommunications.},
keywords={cloud computing;cognitive radio;computer
networks;cryptography;image processing;information retrieval;learning
(artificial intelligence);micromechanical devices;power
systems;robots;software engineering;MEMS;cloud computing;cognitive radio
networks;computer networks;cryptography;image processing;information
retrieval;machine learning;microelectromechanical systems;power
systems;requirements engineering;robotics;software
engineering;telecommunications},
doi={10.1109/ICET.2014.7021006},
month={Dec},}
@INPROCEEDINGS{7014652,
booktitle={Proceedings of 3rd International Conference on Reliability,
Infocom Technologies and Optimization},
title={Contents},
year={2014},
pages={1-7},
abstract={The following topics are dealt with: software engineering;
data security; data privacy; pattern classification; learning; neural
networks; cloud computing; program testing; wireless sensor networks;
computer games; robotics; signal processing; image processing;
ontologies; and support vector machines.},
keywords={cloud computing;computer games;data privacy;learning
(artificial intelligence);neural nets;ontologies (artificial
intelligence);pattern classification;program testing;security of
data;signal processing;software engineering;support vector
machines;wireless sensor networks;cloud computing;computer games;data
privacy;data security;image processing;learning;neural
networks;ontologies;pattern classification;program testing;signal
processing;software engineering;support vector machines;wireless sensor
networks},
doi={10.1109/ICRITO.2014.7014652},
month={Oct},}
@INPROCEEDINGS{7016003,
booktitle={2014 Ninth International Conference on Broadband and Wireless
Computing, Communication and Applications},
title={Table of contents},
year={2014},
pages={v-xvi},
abstract={The following topics are dealt with: wireless ad-hoc networks;
wireless sensor networks; watermarking; wireless body networks; wireless
mesh networks; mobile computing; wireless computing; cluster computing;
cloud computing; wireless communication systems; trust computing;
authentication; vehicular networks; sustainability management;
ubiquitous e-business; sharing network data; security auditing;
protocols; numerical analysis; P2P systems; multimedia support
networking systems; cognitive radio and robotics systems.},
keywords={body area networks;body sensor networks;cloud
computing;cognitive radio;computer network security;cryptographic
protocols;mobile computing;multimedia communication;numerical
analysis;peer-to-peer computing;robots;sustainable development;vehicular
ad hoc networks;watermarking;wireless mesh networks;workstation
clusters;P2P system;authentication;cloud computing;cluster
computing;cognitive radio;mobile computing;multimedia support networking
system;numerical analysis;protocol;robotics system;security
auditing;sharing network data;sustainability management;trust
computing;ubiquitous e-business;vehicular network;watermarking;wireless
ad-hoc network;wireless body network;wireless communication
system;wireless computing;wireless mesh network;wireless sensor network},
doi={10.1109/BWCCA.2014.4},
month={Nov},}
@INPROCEEDINGS{7010193,
booktitle={2014 IEEE Symposium on Computer Applications and Industrial
Electronics (ISCAIE)},
title={Table of contents},
year={2014},
pages={ii-ii},
abstract={The following topics are dealt with: bioinformatics;
computational biology; cloud computing; medical electronics; network
technologies; communication technologies; computer applications;
industrial electronics; industrial applications; signal processing;
image processing; mechatronics; automation; robotics; and software
engineering.},
keywords={biology computing;biomedical electronics;cloud
computing;mechatronics;robots;signal processing;software
engineering;automation;bioinformatics;cloud computing;communication
technologies;computational biology;computer applications;image
processing;industrial applications;industrial
electronics;mechatronics;medical electronics;network
technologies;robotics;signal processing;software engineering},
doi={10.1109/ISCAIE.2014.7010193},
month={April},}
@INPROCEEDINGS{6998248,
booktitle={2014 Tenth International Conference on Intelligent
Information Hiding and Multimedia Signal Processing},
title={Table of contents},
year={2014},
pages={v-xxiv},
abstract={The following topics are dealt with: intelligent information
hiding; multimedia signal processing; ergonomic information; IPv6;
multimedia services; intelligent computing; multimedia computing;
privacy; computer forensics; network technology; audio signals; speech
signals; intelligent video processing; system-on-chip; video information
processing; pattern recognition; multimedia retrievals; intelligent
image processing; print-scan process; industrial control system
security; music information retrieval; future Internet; speech
communication; spatial audio technologies; wireless sensor network;
cloud computing and robotics.},
keywords={Internet;control systems;data encapsulation;information
retrieval;multimedia systems;pattern recognition;robots;security;signal
processing;wireless sensor networks;IPv6;audio signals;cloud
computing;computer forensics;ergonomic information;future
Internet;industrial control system security;intelligent
computing;intelligent image processing;intelligent information
hiding;intelligent video processing;multimedia computing;multimedia
retrievals;multimedia services;multimedia signal processing;music
information retrieval;network technology;pattern recognition;print-scan
process;privacy;robotics;spatial audio technologies;speech
communication;speech signals;system-on-chip;video information
processing;wireless sensor network},
doi={10.1109/IIH-MSP.2014.4},
month={Aug},}
@INPROCEEDINGS{6926414,
booktitle={2014 9th International Conference on Computer Science
Education},
title={Program at a glance of ICCSE 2014},
year={2014},
pages={16-18},
abstract={The following topics are dealt with: computer science;
education; artificial intelligence; robotics; big data; cloud computing;
communication technology; softtware engineering; Web application;
Internet of Things; and computer graphics.},
keywords={Big Data;Internet of Things;artificial intelligence;cloud
computing;computer graphics;computer science;education;robots;software
engineering;Internet of Things;Web application;artificial
intelligence;big data;cloud computing;communication technology;computer
graphics;computer science;education;robotics;softtware engineering},
doi={10.1109/ICCSE.2014.6926414},
month={Aug},}
@INPROCEEDINGS{6918779,
booktitle={2013 International Conference on Machine Intelligence and
Research Advancement},
title={Table of contents},
year={2013},
pages={v-xiv},
abstract={The following topics are dealt with: mobile communication;
sensor networks; vehicular networks; ad hoc networks; soft computing;
fuzzy logic; ANN; microwave IC; antenna; filter; wave propagation; data
mining; web mining; security algorithms; ontology; robotics; control
system; design issues; power system; digital design; VLSI; SOC; hardware
implementation; embedded systems; image processing; speech processing;
video processing; computer vision; algorithm coding; mathematical
modelling; mathematical simulation; software engineering; software
testing; grid computing; cloud computing; high-performance computing; IT
management; and bio-informatics.},
keywords={DP management;VLSI;antennas;bioinformatics;computer
vision;control systems;data mining;design;embedded
systems;encoding;filters;fuzzy logic;image processing;microwave
integrated circuits;mobile communication;neural nets;ontologies
(artificial intelligence);parallel processing;power systems;program
testing;robots;security of data;software engineering;speech
processing;system-on-chip;vehicular ad hoc networks;video signal
processing;wave propagation;ANN;IT management;SOC;VLSI;Web mining;ad hoc
networks;algorithm coding;antenna;bioinformatics;cloud
computing;computer vision;control system;data mining;design
issues;digital design;embedded systems;filter;fuzzy logic;grid
computing;hardware implementation;high-performance computing;image
processing;mathematical modelling;mathematical simulation;microwave
IC;mobile communication;ontology;power system;robotics;security
algorithms;sensor networks;soft computing;software engineering;software
testing;speech processing;vehicular networks;video processing;wave
propagation},
doi={10.1109/ICMIRA.2013.4},
month={Dec},}
@ARTICLE{6905505,
author={S. Cass},
journal={IEEE Spectrum},
title={What are venture capitalists betting on? [Dataflow]},
year={2014},
volume={51},
number={10},
pages={76-76},
abstract={According to a recent international survey of hundreds of
venture capitalists conducted by Deloitte Consulting for the U.S.
National Venture Capital Association, overall investor confidence is
growing across almost every tech sector. The greatest growth globally is
in the clean technology and robotics sectors, but looking at just U.S.
venture investors, robotics and consumer businesses led growth. Cloud
computing remained the sector inspiring the most confidence, for the
second year in a row.},
doi={10.1109/MSPEC.2014.6905505},
ISSN={0018-9235},
month={October},}
@INPROCEEDINGS{6900664,
booktitle={2014 IEEE Congress on Evolutionary Computation (CEC)},
title={Table of contents},
year={2014},
pages={I-XXIV},
abstract={The following topics are dealt with: computational
intelligence; computational games; memetic computing; evolutionary
computer vision; bio-inspired computation; evolutionary multiobjective
optimization; decision making; differential evolution; combinatorial
optimization; artificial bee colony algorithms; swarm intelligence;
real-world engineering optimization; complex networks; machine learning;
statistical learning; nature-inspired constrained optimization;
bioinformatics; data mining; evolutionary games; multiagent systems;
hybrid evolutionary computational methods; large scale global
optimization; cloud computing; learning classifier systems;
opposition-based learning; genetic programming; hyperheuristics;
predictive maintenance; cultural algorithms; knowledge extraction;
single objective numerical optimization; process mining; estimation of
distribution algorithms; biomedical applications; robotics applications;
engineering applications; constraint handling; preference handling; and
firework algorithms.},
keywords={artificial intelligence;biocomputing;bioinformatics;cloud
computing;combinatorial mathematics;computer vision;constraint
handling;data mining;decision making;game theory;multi-agent
systems;optimisation;pattern classification;robots;artificial bee colony
algorithms;bio-inspired computation;bioinformatics;biomedical
applications;cloud computing;combinatorial optimization;complex
networks;computational games;computational intelligence;constraint
handling;cultural algorithms;data mining;decision making;differential
evolution;engineering applications;estimation of distribution
algorithms;evolutionary computer vision;evolutionary games;evolutionary
multiobjective optimization;firework algorithms;genetic
programming;hybrid evolutionary computational
methods;hyperheuristics;knowledge extraction;large scale global
optimization;learning classifier systems;machine learning;memetic
computing;multiagent systems;nature-inspired constrained
optimization;opposition-based learning;predictive maintenance;preference
handling;process mining;real-world engineering optimization;robotics
applications;single objective numerical optimization;statistical
learning;swarm intelligence},
doi={10.1109/CEC.2014.6900664},
ISSN={1089-778X},
month={July},}
@INPROCEEDINGS{6893957,
booktitle={2014 2nd 2014 2nd International Conference on Emission
Electronics (ICEE)},
title={Table of contents},
year={2014},
pages={1-2},
abstract={The following topics are dealt with: field emission cathodes;
bioceramics; cloud computing middleware; robotics; hydroacoustic
monitoring and radio monitoring; particle beam focusing; economics
modelling; laser radiation control; digital device testing;
computational complexity; chaos generation; virtual laboratories for
education; thermoelectric conversion; object tracking; biological
mobility; and tokamaks.},
keywords={biology;cathodes;chaos generators;cloud
computing;computational complexity;digital
circuits;economics;educational computing;electron field
emission;laboratories;lasers;middleware;object tracking;optical
control;particle beam focusing;plasma toroidal
confinement;robots;testing;thermoelectric conversion;underwater
sound;vacuum microelectronics;bioceramics;biological mobility;chaos
generation;cloud computing;computational complexity;digital device
testing;economics modelling;field emission cathodes;hydroacoustic
monitoring;laser radiation control;middleware;object tracking;particle
beam focusing;radio monitoring;robotics;thermoelectric
conversion;tokamak;virtual laboratories},
doi={10.1109/Emission.2014.6893957},
month={June},}
@INPROCEEDINGS{6808551,
booktitle={2014 International Conference on Electronics, Communications
and Computers (CONIELECOMP)},
title={Table of contents},
year={2014},
pages={1-8},
abstract={The following topics are dealt with: soft computing; fuzzy
cognitive maps; biomedical engineering; information security; optical
communications; cloud computing; parallel computing; power electronics;
power simulation; computer vision; robotics; human-computer interaction;
signal processing; video processing; neural networks and image
processing.},
keywords={biomedical engineering;cloud computing;computer vision;fuzzy
logic;neural nets;optical communication;power
electronics;robots;security of data;simulation;biomedical
engineering;cloud computing;computer vision;fuzzy cognitive
maps;human-computer interaction;image processing;information
security;neural networks;optical communications;parallel computing;power
electronics;power simulation;robotics;signal processing;soft
computing;video processing},
doi={10.1109/CONIELECOMP.2014.6808551},
month={Feb},}
@INPROCEEDINGS{6757875,
booktitle={2013 Africon},
title={Sustainable engineering, energy and environment [table of
contents]},
year={2013},
pages={1-21},
abstract={The following topics are dealt with: sustainable engineering,
energy and environment; communication theory and wireless
communications; communication networks and security; multimedia and
signal processing; cloud computing, GRIDs and virtualisation; mobile
computing and geoprocessing; scientific computing and computational
engineering; E-learning and educational technology; instrumentation and
control; nanotechnology; power electronics and power systems; business
informatics, software engineering and Web services; artificial
intelligence and robotics.},
keywords={Web services;artificial intelligence;business data
processing;cloud computing;computer aided instruction;control
systems;data visualisation;engineering computing;grid
computing;instrumentation;mobile computing;multimedia
communication;nanoelectronics;physics computing;power electronics;power
systems;radiocommunication;robots;signal processing;software
engineering;sustainable development;telecommunication security;GRID;Web
services;artificial intelligence;business informatics;cloud
computing;communication network security;e-learning;educational
technology;geoprocessing system;mobile computing;multimedia
communication;nanoelectronics;power electronics;power
system;robotics;signal processing;software engineering;sustainable
energy system;sustainable engineering;wireless communication},
doi={10.1109/AFRCON.2013.6757875},
ISSN={2153-0025},
month={Sept},}
@INPROCEEDINGS{6707231,
booktitle={2013 E-Health and Bioengineering Conference (EHB)},
title={Table of contents},
year={2013},
pages={1-16},
abstract={The following topics are dealt with: medical signal
processing; wearable devices; patient monitoring; medical information
systems; EEG activity: ultrasonic imaging; biomedical materials;
radiation protection; radiography; image compression; PACS; pattern
recognition techniques; home care; telemedicine; geriatrics; patient
treatment; oximetry; patient rehabilitation; epidemics; biomechanics;
cancer; intelligent materials; hyperthermia; image segmentation; ECG
devices; blood vessels; medical image processing; cloud computing for
healthcare; thermographic analysis; polymer based-immunosensing;
wireless sensor networks; herbal medicines; cellular biophysics; digital
breast tomosynthesis; magnetic nanoparticles; sleep apnea; biolectric
potentials; neuronal computing; electromagnetic radiations and medical
robotics.},
keywords={PACS;bioelectric potentials;biological effects of
microwaves;biomechanics;biomedical materials;biomedical optical
imaging;biomedical ultrasonics;biosensors;blood vessels;body sensor
networks;cancer;cellular biophysics;cloud computing;computerised
tomography;data compression;diagnostic
radiography;electrocardiography;electroencephalography;epidemics;geriatrics;health
care;hyperthermia;image coding;image segmentation;infrared
imaging;intelligent materials;magnetic particles;medical image
processing;medical robotics;nanomedicine;nanoparticles;oximetry;patient
monitoring;patient rehabilitation;pattern recognition;polymers;radiation
protection;sleep;telemedicine;ECG devices;EEG activity;PACS;biolectric
potentials;biomechanics;biomedical materials;blood
vessels;cancer;cellular biophysics;cloud computing;digital breast
tomosynthesis;electromagnetic
radiations;epidemics;geriatrics;healthcare;herbal medicines;home
care;hyperthermia;image compression;image segmentation;intelligent
materials;magnetic nanoparticles;medical image processing;medical
information systems;medical robotics;medical signal processing;neuronal
computing;oximetry;patient monitoring;patient rehabilitation;patient
treatment;pattern recognition techniques;polymer
based-immunosensing;radiation protection;radiography;sleep
apnea;telemedicine;thermographic analysis;ultrasonic imaging;wearable
devices;wireless sensor networks},
doi={10.1109/EHB.2013.6707231},
month={Nov},}
@INPROCEEDINGS{6707153,
booktitle={2013 8th International Conference on Computer Engineering
Systems (ICCES)},
title={Table of contents},
year={2013},
pages={xii-xvi},
abstract={The following topics are dealt with: artificial intelligence;
cloud computing; computer security; robotics; evolutionary computing;
hardware architectures; image processing; software engineering and
databases.},
keywords={artificial intelligence;cloud computing;database management
systems;evolutionary computation;image processing;robots;security of
data;software engineering;artificial intelligence;cloud
computing;computer security;databases;evolutionary computing;hardware
architectures;image processing;robotics;software engineering},
doi={10.1109/ICCES.2013.6707153},
month={Nov},}
@INPROCEEDINGS{6664651,
booktitle={2013 7th Asia Modelling Symposium},
title={Table of contents},
year={2013},
pages={v-x},
abstract={The following topics are dealt with: fuzzy systems;
evolutionary computation; bioinformatics; bioengineering; intelligent
systems; soft computing; robotics; cybernetics; manufacturing systems;
operations research; image processing; speech processing; signal
processing; natural language processing; business management; human
factors; social aspects; parallel architecture; distributed
architecture; software architecture; Internet; cloud computing; semantic
Web; ontologies; mobile ad hoc networks; performance evaluation; target
tracking; and sensors.},
keywords={Internet;ad hoc networks;bioinformatics;distributed
processing;evolutionary computation;fuzzy set theory;human
factors;management;manufacturing systems;natural language
processing;ontologies (artificial intelligence);operations
research;performance evaluation;robots;sensors;signal processing;social
aspects of automation;software
architecture;Internet;bioengineering;bioinformatics;business
management;cloud computing;cybernetics;distributed
architecture;evolutionary computation;fuzzy systems;human factors;image
processing;intelligent systems;manufacturing systems;mobile ad hoc
networks;natural language processing;ontologies;operations
research;parallel architecture;performance evaluation;robotics;semantic
Web;sensors;signal processing;social aspects;soft computing;software
architecture;speech processing;target tracking},
doi={10.1109/AMS.2013.4},
ISSN={2376-1164},
month={July},}
@INPROCEEDINGS{6653264,
booktitle={2013 IEEE 4th Control and System Graduate Research Colloquium},
title={Table of contents},
year={2013},
pages={vii-ix},
abstract={The following topics are dealt with: fault detection and
monitoring; principal component analysis; PCA; grid-connected
photovoltaic system; water quality estimation; independent component
analysis; cloud computing; GIS; landslide; reactive power compensation;
agriculture; distillation; linear and nonlinear ARX approach; video
streaming, mobile WiMAX; UMTS; bacterial foraging optimization;
microstrip antenna; flood modelling; artificial neural network; PI
control; MOSFET/ISFET structure; photogrammetry; position control;
robotic finger; and security.},
keywords={3G mobile communication;MOSFET;PI
control;WiMax;agriculture;autoregressive processes;cloud
computing;distillation;fault diagnosis;floods;geomorphology;independent
component analysis;ion sensitive field effect transistors;microstrip
antennas;mobile radio;neural nets;photogrammetry;photovoltaic power
systems;position control;power grids;reactive power
control;robots;security of data;video streaming;water quality;GIS;ISFET
structure;MOSFET structure;PCA;PI control;UMTS;agriculture;artificial
neural network;bacterial foraging optimization;cloud
computing;distillation;fault detection;fault monitoring;flood
modelling;grid-connected photovoltaic system;independent component
analysis;landslide;microstrip antenna;mobile WiMAX;nonlinear ARX
approach;photogrammetry;position control;principal component
analysis;reactive power compensation;robotic finger;security;video
streaming;water quality estimation},
doi={10.1109/ICSGRC.2013.6653264},
month={Aug},}
@INPROCEEDINGS{6648898,
booktitle={2013 International Conference on Control, Computing,
Communication and Materials (ICCCCM)},
title={Table of contents},
year={2013},
pages={1-15},
abstract={The following topics are dealt with: information retrieval
system; cloud computing; pattern clustering; data mining; image
processing; power system control; grid computing; MOSFET; signal
processing; speech processing; information retrieval; robotics;
optimisation; medical image processing; logic circuits; mobile ad hoc
networks; face recognition; cryptography; medical signal processing;
wireless sensor networks; and big data analytics.},
keywords={MOSFET;cloud computing;cryptography;data analysis;data
mining;grid computing;information retrieval;logic circuits;medical
computing;mobile ad hoc networks;optimisation;pattern clustering;power
system control;robots;signal processing;wireless sensor
networks;MOSFET;big data analytics;cloud computing;cryptography;data
mining;face recognition;grid computing;image processing;information
retrieval system;logic circuits;medical image processing;medical signal
processing;mobile ad hoc networks;optimisation;pattern clustering;power
system control;robotics;signal processing;speech processing;wireless
sensor networks;Blogs;Encoding;Frequency
control;MOSFET;Reliability;Time-frequency analysis},
doi={10.1109/ICCCCM.2013.6648898},
month={Aug},}
@INPROCEEDINGS{6642431,
booktitle={2013 IEEE 14th International Conference on Information Reuse
Integration (IRI)},
title={Table of contents},
year={2013},
pages={i-xi},
abstract={The following topics are dealt with: information security;
information privacy; recommender systems; empirical method for
recognizing inference in text; data mining; databases; semistructured
data; structured data; robotics; intelligent machines; education;
component-based design; software reuse; information reuse and
integration in health informatics; artificial intelligene; optimization;
process engineering; systems engineering; biomedical systems; healthcare
systems; cloud computing; large scale data integration; large scale
system integration; automation; distributed systems; software
engineering; machine learning; formal method integration; multimedia
systems; energy harvesting; aerospace industry; and government.},
keywords={artificial intelligence;data integration;data mining;data
privacy;database management systems;distributed processing;educational
technology;energy harvesting;government;health care;medical
computing;multimedia systems;object-oriented
programming;optimisation;recommender systems;security of data;software
engineering;systems engineering;text analysis;aerospace
industry;artificial intelligence;automation;biomedical systems;cloud
computing;component-based design;data mining;databases;distributed
systems;education;empirical method for recognizing inference in
text;energy harvesting;formal method integration;government;healthcare
systems;information privacy;information reuse and integration in health
informatics;information security;intelligent machines;large scale data
integration;large scale system integration;machine learning;multimedia
systems;optimization;process engineering;recommender
systems;robotics;semistructured data;software engineering;software
reuse;structured data;systems engineering},
doi={10.1109/IRI.2013.6642431},
month={Aug},}
@INPROCEEDINGS{6575231,
booktitle={2013 8th International Conference on System of Systems
Engineering},
title={Table of content},
year={2013},
pages={i-v},
abstract={The following topics are dealt with: systems engineering;
product design; system-of-systems operation; aerospace engineering;
robotics; cloud computing; Big Data; power engineering; sensors;
manufacturing systems; wireless sensor networks; genetic engineering;
and military engineering.},
keywords={aerospace engineering;cloud computing;data handling;genetic
engineering;manufacturing systems;military systems;power
engineering;product design;robots;sensors;systems engineering;wireless
sensor networks;Big Data;aerospace engineering;cloud computing;genetic
engineering;manufacturing systems;military engineering;power
engineering;product design;robotics;sensors;system-of-systems
operation;systems engineering;wireless sensor networks},
doi={10.1109/SYSoSE.2013.6575231},
month={June},}
@INPROCEEDINGS{6554165,
booktitle={2013 8th International Conference on Computer Science
Education},
title={[Front cover]},
year={2013},
pages={1-1},
abstract={The following topics are dealt with: computer science and
education; algorithms; artificial intelligence; communication
technology; robotics and control; cloud computing; Web applications;
computer graphics; databases and data mining; e-health; and software
engineering.},
keywords={Internet;artificial intelligence;computer graphics;control
engineering;data mining;database management systems;engineering
education;health care;robots;software engineering;telecommunication;Web
applications;algorithms;artificial intelligence;cloud
computing;communication technology;computer graphics;computer
science;control;data mining;databases;e-health;robotics;software
engineering},
doi={10.1109/ICCSE.2013.6554165},
month={April},}
@INPROCEEDINGS{6295472,
booktitle={2012 7th International Conference on Computer Science
Education (ICCSE)},
title={[Copyright notice]},
year={2012},
pages={1-1},
abstract={The following topics are dealt with: artificial intelligence;
cloud computing; computer graphics; Internet of things; recommendation
system; robotics technology; software engineering; courseware design;
engineering education; engineering training; education reform.},
keywords={Internet;artificial intelligence;cloud computing;computer
graphics;courseware;engineering education;recommender
systems;robots;software engineering;Internet of things;artificial
intelligence;cloud computing;computer graphics;courseware
design;education reform;engineering education;engineering
training;recommendation system;robotics technology;software engineering},
doi={10.1109/ICCSE.2012.6295472},
month={July},}
@INPROCEEDINGS{6223663,
booktitle={2012 International Conference on Systems and Informatics
(ICSAI2012)},
title={Table of contents},
year={2012},
pages={IX-LIX},
abstract={The following topics are dealt with: control system theory;
control system applications; human-machine interface; computer vision;
robotics; computer control; power systems; energy systems; fuel cells;
electrical vehicles; power electronics; wind energy; solar energy;
nuclear energy; smart grids; power management; intelligent systems;
pattern recognition; autonomous systems; knowledge engineering;
computational intelligence; artificial intelligence; computer systems;
distributed systems; grid computing; cloud computing; services
computing; parallel computing; embedded systems; VLSI; nanocomputing;
nanomaterials; aerospace engineering; biomedical engineering;
biotechnology; computational sciences; ad hoc networking; sensor
networking; wireless networking; Internet; system security; QoS; optical
networks; communication theory; signal processing; video processing;
image processing; remote sensing; computer graphics; animation; virtual
reality; multimedia; data mining; bioinformatics; medical informatics;
data engineering; and software engineering.},
keywords={VLSI;ad hoc networks;aerospace engineering;artificial
intelligence;bioinformatics;cloud computing;computer animation;computer
vision;control engineering computing;control theory;data mining;electric
vehicles;embedded systems;fuel cells;grid computing;man-machine
systems;medical computing;multimedia computing;nanotechnology;nuclear
power;optical communication;parallel processing;power electronics;power
system management;radiocommunication;remote sensing;robots;security of
data;smart power grids;software engineering;solar power;video signal
processing;virtual reality;wind power;Internet;QoS;VLSI;ad hoc
networking;aerospace engineering;animation;artificial
intelligence;autonomous systems;bioinformatics;biomedical
engineering;biotechnology;cloud computing;communication
theory;computational intelligence;computational sciences;computer
control;computer graphics;computer systems;computer vision;control
system applications;control system theory;data engineering;data
mining;distributed systems;electrical vehicles;embedded systems;energy
systems;fuel cells;grid computing;human-machine interface;image
processing;intelligent systems;knowledge engineering;medical
informatics;multimedia;nanocomputing;nanomaterials;nuclear
energy;optical networks;parallel computing;pattern recognition;power
electronics;power management;power systems;remote
sensing;robotics;sensor networking;services computing;signal
processing;smart grids;software engineering;solar energy;system
security;video processing;virtual reality;wind energy;wireless networking},
doi={10.1109/ICSAI.2012.6223663},
month={May},}
@INPROCEEDINGS{6141704,
booktitle={2011 First International Conference on Informatics and
Computational Intelligence},
title={[Title page i]},
year={2011},
pages={i-i},
abstract={The following topics are dealt with: computational
intelligence; fuzzy system; evolutionary computation; adaptive dynamic
programming; reinforcement learning; bioinformatics; bioengineering;
data mining; intelligent system intelligence; soft computing; robotics;
operation research; discrete event system; real time system; image
processing; speech processing; signal processing; marine simulation;
parallel computing; distributed computing; cloud computing; Internet
modelling; semantic Web; ontologies; mobile-ad hoc wireless network;
sensor placement; target tracking; computer system; and communication
system.},
keywords={bioinformatics;cloud computing;data mining;discrete event
systems;fuzzy logic;image processing;learning (artificial
intelligence);marine engineering;mobile ad hoc networks;neural
nets;ontologies (artificial intelligence);operations research;parallel
processing;real-time systems;robots;semantic Web;sensor placement;speech
processing;target tracking;uncertainty handling;wireless sensor
networks;Internet modelling;adaptive dynamic
programming;bioengineering;bioinformatics;cloud computing;communication
system;computational intelligence;computer system;data mining;discrete
event system;distributed computing;evolutionary computation;fuzzy
system;image processing;intelligent system intelligence;marine
simulation;mobile-ad hoc wireless network;ontologies;operation
research;parallel computing;real time system;reinforcement
learning;robotics;semantic Web;sensor placement;signal processing;soft
computing;speech processing;target tracking},
doi={10.1109/ICI.2011.1},
month={Dec},}
@INPROCEEDINGS{6090033,
booktitle={2011 IEEE Conference on Sustainable Utilization and
Development in Engineering and Technology (STUDENT)},
title={[Front cover]},
year={2011},
pages={c1-c1},
abstract={The following topics are dealt with: single phase induction
motor; distributed generation system; cascaded multilevel inverter based
STATCOM; rotary reluctance motors; Coanda effect test bench; seed
growing fusion approach; monocular vision system; mobile robot; highly
optimized OpenCV based cellphone; heart rate monitoring; cloud enabling
spam filtering service; speech classification; laser glass cutting;
genetic algorithms; infinite Mario Bross Al; interactive evolution
programming; support vector machine; MIMO orthogonal MC CDMA system;
intelligent traffic management sytem; acoustic energy conversion; buck
DC-DC converter; semi automated floor tilling robotic system; hybrid
energy harvesting system; CdTe grid-connected photovoltaic system;
ZCS-SR inverter fed high voltage DC-DC converter and heat recovery steam
generator.},
keywords={DC-DC power convertors;boilers;code division multiple
access;cutting;genetic algorithms;induction motors;invertors;machine
control;mobile robots;photovoltaic power systems;reluctance
motors;speech processing;traffic engineering computing;zero current
switching;CdTe grid-connected photovoltaic system;Coanda effect test
bench;MIMO orthogonal MC CDMA system;ZCS-SR inverter fed high voltage
DC-DC converter;acoustic energy conversion;automated floor tilling
robotic system;buck DC-DC converter;cascaded multilevel inverter based
STATCOM;cloud enabling spam filtering service;distributed generation
system;genetic algorithms;heart rate monitoring;heat recovery steam
generator;highly optimized OpenCV based cellphone;hybrid energy
harvesting system;intelligent traffic management sytem;interactive
evolution programming;laser glass cutting;mobile robot;monocular vision
system;rotary reluctance motors;seed growing fusion approach;single
phase induction motor;speech classification;support vector machine},
doi={10.1109/STUDENT.2011.6090033},
month={Oct},}
@INPROCEEDINGS{6083632,
booktitle={2011 IEEE International Conference on Systems, Man, and
Cybernetics},
title={[Copyright notice]},
year={2011},
pages={1-1},
abstract={The following topics are dealt with: brain-machine interface;
machine learning technology; service systems; homeland security systems;
virtual reality; agent-based modeling; human centered transportation
systems; awareness science and engineering; soft computing; enterprise
information systems; social signal processing; infrastructure system;
manufacturing systems; pattern recognition; medical mechatronics;
minimally invasive surgery; medical robotics; medical technology;
intelligent power systems; discrete event systems; Petri nets;
biometrics; bioinformatics; computational intelligence; supply chain
management; shared control; fault diagnosis; systems engineering;
Internet; support vector machines; knowledge acquisition; cloud
computing; grey systems; humanoid robots; redundant manipulators; formal
methods; granular computing; wireless sensor networks; nonlinear control
systems; gesture-based interaction; software engineering; multi-agent
systems; cognitive computing; social robotics; natural language
processing; conflict resolution; intelligent transportation systems;
human-robot interaction; image processing; medical informatics; decision
support systems; assistive technology; human-centered design; data
mining; and anti-terrorism applications.},
keywords={Internet;Petri nets;biology computing;decision support
systems;discrete event systems;fault diagnosis;grey systems;handicapped
aids;information systems;learning (artificial
intelligence);manufacturing systems;medical computing;medical control
systems;military computing;natural language processing;pattern
recognition;power engineering computing;robots;security of data;signal
processing;software agents;software engineering;supply chain
management;traffic engineering computing;user interfaces;virtual
reality;wireless sensor networks;Internet;Petri nets;agent-based
modeling;anti-terrorism applications;assistive technology;awareness
engineering;awareness science;bioinformatics;biometrics;brain-machine
interface;cloud computing;cognitive computing;computational
intelligence;conflict resolution;data mining;decision support
systems;discrete event systems;enterprise information systems;fault
diagnosis;formal methods;gesture-based interaction;granular
computing;grey systems;homeland security systems;human centered
transportation systems;human-centered design;human-robot
interaction;humanoid robots;image processing;infrastructure
system;intelligent power systems;intelligent transportation
systems;knowledge acquisition;machine learning technology;manufacturing
systems;medical informatics;medical mechatronics;medical
robotics;medical technology;minimally invasive surgery;multi-agent
systems;natural language processing;nonlinear control systems;pattern
recognition;redundant manipulators;service systems;shared control;social
robotics;social signal processing;soft computing;software
engineering;supply chain management;support vector machines;systems
engineering;virtual reality;wireless sensor networks},
doi={10.1109/ICSMC.2011.6083632},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{5966623,
booktitle={2011 6th International Conference on System of Systems
Engineering},
title={Table of content},
year={2011},
pages={i-v},
abstract={The following topics are dealt with: formal verification;
cloud computing; data security; system-of-systems; software
architecture; image processing; military computing; aerospace computing;
and robotics.},
keywords={aerospace computing;cloud computing;formal verification;image
processing;military computing;robots;security of data;software
architecture;aerospace computing;cloud computing;data security;formal
verification;image processing;military computing;robotics;software
architecture;system-of-systems},
doi={10.1109/SYSOSE.2011.5966623},
month={June},}
@INPROCEEDINGS{5743366,
booktitle={2010 International Conference on Intelligent System Design
and Engineering Application},
title={[Front cover - Vol 1]},
year={2010},
volume={1},
pages={C1-C1},
abstract={The following topics are dealt with: intelligent system
design; neural networks, AI and expert systems; evolutionary
computation; genetic algorithms; natural language processing & machine
translation; artificial life and artificial immune systems; rough and
fuzzy rough set; gray system and cloud computing; cognitive radio and
computer vision; pattern recognition and machine learning; fuzzy system
and fuzzy control; intelligent control; nonlinear system and control;
process automation; electric automation; system theory and control
theory; system modeling and simulation; intelligent mechatronics and
robotics; ITS and traffic control; computer integrated manufacturing
systems; human-computer interaction; intelligent sensor and sensor
network; information processing and information security; information
retrieval; database engineering and system; intelligent traditional
Chinese medicine systems; knowledge management and knowledge
engineering; management information systems; data mining & knowledge
discovery; e-service intelligence; video & image processing; wireless
communication systems; wireless networks; systems biology and
neurobiology.},
keywords={artificial intelligence;cloud computing;cognitive
radio;computer integrated manufacturing;data mining;database management
systems;fuzzy set theory;fuzzy systems;genetic algorithms;grey
systems;image processing;information retrieval;intelligent
control;knowledge management;language translation;management information
systems;mechatronics;medicine;natural language processing;neural
nets;neurophysiology;nonlinear control systems;radio
networks;robots;rough set theory;security of data;system theory;traffic
control;AI;ITS;artificial immune systems;artificial life;cloud
computing;cognitive radio;computer integrated manufacturing
systems;computer vision;control theory;data mining;database
engineering;database system;e-service intelligence;electric
automation;evolutionary computation;expert systems;fuzzy control;fuzzy
rough set;fuzzy system;genetic algorithms;gray system;human-computer
interaction;image processing;information processing;information
retrieval;information security;intelligent control;intelligent
mechatronics;intelligent sensor;intelligent system design;intelligent
traditional Chinese medicine systems;knowledge discovery;knowledge
engineering;knowledge management;machine learning;machine
translation;management information systems;natural language
processing;neural networks;neurobiology;nonlinear control system;pattern
recognition;process automation;robotics;sensor network;system
modeling;system simulation;system theory;systems biology;traffic
control;video processing;wireless communication systems;wireless networks},
doi={10.1109/ISDEA.2010.460},
month={Oct},}
@INPROCEEDINGS{5633719,
booktitle={2010 International Conference on Broadband, Wireless
Computing, Communication and Applications},
title={[Title page i]},
year={2010},
pages={i-i},
abstract={The following topics are dealt with: broadband, wireless
computing, communication and applications; ad hoc networks; sensor
networks; parallel and multi-core systems; mesh networks; practical
security and privacy application; optical networks; networks algorithms
and protocols; distributed schemes and protocols; wireless networks;
disaster information systems; network security, privacy and trust;
distributed algorithms and systems; multimedia and Web applications;
wireless communication; digital signature; computer forensics;
authentication and privacy; cloud and wireless security; image secret,
sensor network and computer security; fault detection and diagnosis;
intelligent networks; human behavior understanding; image and video
analysis; scattering and diffraction; wireless modelling; wireless
technologies; wireless traffic control and resource allocation;
intelligent algorithms; network performance analysis; network services;
teleoperation and robot control; robotics; agent technology and
multiagent systems; symbolic computing; and wireless sensor networks
protocols.},
keywords={Internet;computer networks;multi-agent systems;radio
networks;robots;security of data;transport protocols;video signal
processing;Web applications;ad hoc networks;agent
technology;authentication;broadband communication;computer
forensics;computer security;data privacy;digital signature;disaster
information systems;distributed algorithms;distributed schemes;fault
detection;fault diagnosis;human behavior understanding;image
analysis;image secret;intelligent algorithms;intelligent
networks;multiagent systems;multicore systems;multimedia;network
performance analysis;network security;network services;networks
algorithms;optical networks;parallel systems;resource allocation;robot
control;robotics;symbolic computing;teleoperation;video
analysis;wireless computing;wireless mesh networks;wireless
modelling;wireless security;wireless sensor networks protocols;wireless
technology;wireless traffic control},
doi={10.1109/BWCCA.2010.1},
month={Nov},}
@INPROCEEDINGS{5631785,
booktitle={2010 Fourth IEEE International Conference on Self-Adaptive
and Self-Organizing Systems},
title={[Title page i]},
year={2010},
pages={i-i},
abstract={The following topics are dealt with: cloud computing; P2P;
robotics; social computing; distributed algorithm; software adaptation
and agents.},
keywords={Internet;distributed algorithms;peer-to-peer
computing;robots;social sciences computing;software
agents;P2P;agents;cloud computing;distributed algorithms;robotics;social
computing;software adaptation},
doi={10.1109/SASO.2010.1},
ISSN={1949-3673},
month={Sept},}
@INPROCEEDINGS{5491203,
booktitle={2010 International Joint Conference on Computational
Cybernetics and Technical Informatics},
title={[Title page]},
year={2010},
pages={1-1},
abstract={The following topics are dealt with: nonlinear systems;
learning systems, neural networks; fuzzy system; fuzzy control; sliding
mode control; control system; adaptive control; data mining; software
architecture; wireless sensor networks; e-learning; protocols; image
processing; video processing; MIMO systems; robotics; wind generators;
mobile devices; mobile robots; XML; power supply; energy consumption;
cloud computing; task scheduling; Web services; sensors and local area
network.},
keywords={Internet;artificial intelligence;computer applications;control
engineering;control theory;fuzzy systems;nonlinear systems;signal
processing;software engineering;MIMO systems;Web services;adaptive
control;cloud computing;control system;data mining;energy
consumption;fuzzy control;fuzzy system;image processing;learning
systems;local area network;mobile devices;mobile robots;neural
networks;nonlinear systems;power supply;robotics;sensors;sliding mode
control;software architecture;task scheduling;video processing;wind
generators;wireless sensor networks},
doi={10.1109/ICCCYB.2010.5491203},
month={May},}
@INPROCEEDINGS{7899989,
author={D. Kim and J. Choi and J. T. Leksut and G. Medioni},
booktitle={2016 23rd International Conference on Pattern Recognition
(ICPR)},
title={Expression invariant 3D face modeling from an RGB-D video},
year={2016},
pages={2362-2367},
abstract={We aim to reconstruct an accurate neutral 3D face model from
an RGB-D video in the presence of extreme expression changes. Since each
depth frame, taken by a low-cost sensor, is noisy, point clouds from
multiple frames can be registered and aggregated to build an accurate 3D
model. However, direct aggregation of multiple data produces erroneous
results in natural interaction (e.g., talking and showing expressions).
We propose to analyze facial expression from an RGB frame and neutralize
the corresponding 3D point cloud if needed. We first estimate the
person's expression by fitting blendshape coefficients using 2D facial
landmarks for each frame and calculate an expression deformity
(expression score). With the estimated expression score, we determine
whether an input face is neutral or non-neutral. If the face is
non-neutral, we proceed to neutralize the expression of the 3D point
cloud in that frame. To neutralize the 3D point cloud of a face, we
deform our generic 3D face model by applying the estimated blendshape
coefficients, find displacement vectors from the deformed generic face
to a neutral generic face, and apply the displacement vectors to the
input 3D point cloud. After preprocessing frames in a video, we rank
frames based on the expression scores and register the ranked frames
into a single 3D model. Our system produces a neutral 3D face model in
the presence of extreme expression changes even when neutral faces do
not exist in the video.},
keywords={Computational modeling;Deformable models;Face;Iterative
closest point algorithm;Solid modeling;Three-dimensional displays;Two
dimensional displays},
doi={10.1109/ICPR.2016.7899989},
month={Dec},}
@INPROCEEDINGS{7899913,
author={R. Ferreira and N. Gonçalves},
booktitle={2016 23rd International Conference on Pattern Recognition
(ICPR)},
title={Accurate and fast micro lenses depth maps from a 3D point cloud
in light field cameras},
year={2016},
pages={1893-1898},
abstract={Light field cameras capture a scene's multi-directional light
field with one image, allowing the estimation of depth. In this paper,
we introduce a fully automatic method for depth estimation from a single
plenoptic image running a RANSAC-like algorithm for feature matching.
The novelty about our method is the global method to back project
correspondences found using photometric similarity to obtain a 3D
virtual point cloud and different methods to build a depth map from the
3D point cloud generated. We use lenses with different focal-lengths in
a multiple depth map refining phase, generating a dense depth map. Tests
with simulations and real images are presented and compared with the
state of the art, showing comparable accuracy for substantial less
computational time.},
keywords={Cameras;Computational modeling;Estimation;Image
sensors;Lenses;Robots;Three-dimensional displays},
doi={10.1109/ICPR.2016.7899913},
month={Dec},}
@INPROCEEDINGS{7890301,
author={D. Chung and S. Hong and J. Kim},
booktitle={2017 IEEE Underwater Technology (UT)},
title={Underwater pose estimation relative to planar hull surface using
stereo vision},
year={2017},
pages={1-4},
abstract={For creating a precise visual map by autonomous ship-hull
inspection using an unmanned underwater vehicle, it is a crucial
capability for the vehicle (or camera) to maintain a pose relative to
the hull surface. In this study, a relative pose estimation algorithm is
introduced using a stereo vision system. The proposed approach utilizes
3D point cloud data that can be generated by a sparse feature matching
technique between a pair of stereo images. The relative pose information
can be obtained by applying a surface normal estimation algorithm for
the 3D points. Experimental results using underwater images is shown to
verify the practical feasibility of the proposed approach.},
keywords={image matching;pose estimation;stereo image
processing;underwater optics;3D point cloud data;autonomous ship-hull
inspection;feature matching technique;planar hull surface;stereo
images;stereo vision system;surface normal estimation
algorithm;underwater pose estimation;unmanned underwater
vehicle;Cameras;Feature extraction;Inspection;Pose estimation;Stereo
vision;Three-dimensional displays;Visualization;Unmanned underwater
vehicle;autonomous hull inspection;stereo vision},
doi={10.1109/UT.2017.7890301},
month={Feb},}
@INPROCEEDINGS{7889276,
author={G. Piumatti and A. Sanna and M. Gaspardone and F. Lamberti},
booktitle={2017 IEEE International Conference on Consumer Electronics
(ICCE)},
title={Spatial Augmented Reality meets robots: Human-machine interaction
in cloud-based projected gaming environments},
year={2017},
pages={176-179},
abstract={Augmented Reality (AR) is expected to change the way we play,
by transforming the world around us in an incredibly rich gaming
environment. In this work, connected robots and natural interaction
means are combined with projected AR to create a gaming experience more
physical and engaging.},
keywords={augmented reality;cloud computing;computer games;mobile
robots;AR;cloud-based projected gaming environments;connected
robots;human-machine interaction;natural interaction;spatial augmented
reality;Artificial intelligence;Avatars;Games;Robot kinematics;Robot
sensing systems;Tracking},
doi={10.1109/ICCE.2017.7889276},
month={Jan},}
@INPROCEEDINGS{7886853,
author={O. Esrafilian and H. D. Taghirad},
booktitle={2016 4th International Conference on Robotics and
Mechatronics (ICROM)},
title={Autonomous flight and obstacle avoidance of a quadrotor by
monocular SLAM},
year={2016},
pages={240-245},
abstract={In this paper, a monocular vision based autonomous flight and
obstacle avoidance system for a commercial quadrotor is presented. The
video stream of the front camera and the navigation data measured by the
drone is sent to the ground station laptop via wireless connection.
Received data processed by the vision based ORB-SLAM to compute the 3D
position of the robot and the environment 3D sparse map in the form of
point cloud. An algorithm is proposed for enrichment of the
reconstructed map, and furthermore, a Kalman Filter is used for sensor
fusion. The scaling factor of the monocular slam is calculated by the
linear fitting. Moreover, a PID controller is designed for 3D position
control. Finally, by means of the potential field method and Rapidly
exploring Random Tree (RRT) path planning algorithm, a collision-free
road map is generated. Moreover, experimental verifications of the
proposed algorithms are reported.},
keywords={Kalman filters;SLAM (robots);aircraft control;autonomous
aerial vehicles;cameras;collision avoidance;control system
synthesis;helicopters;image reconstruction;mobile robots;robot
vision;sensor fusion;three-term control;trees (mathematics);3D sparse
map;Kalman filter;PID controller design;RRT path planning
algorithm;autonomous flight;collision-free road map;map
reconstruction;monocular camera;navigation data;obstacle
avoidance;potential field method;quadrotor;rapidly exploring random
tree;robot 3D position control;scaling factor;sensor fusion;video
stream;vision based ORB-SLAM;Cameras;Collision
avoidance;Navigation;Robot kinematics;Simultaneous localization and
mapping;Three-dimensional displays;Autonomous navigation;autonomous
quadrotor;monocular SLAM;obstacle avoidance},
doi={10.1109/ICRoM.2016.7886853},
month={Oct},}
@INPROCEEDINGS{7886807,
author={E. Iravani and H. A. Talebi and M. Zareinejad and M. R. Dehghan},
booktitle={2016 4th International Conference on Robotics and
Mechatronics (ICROM)},
title={Real-time point-cloud-based haptic rendering with fast contact
detection},
year={2016},
pages={578-583},
abstract={This paper presents a haptic system which has appropriate
frequency to contribute much more real feeling to the users. In this
system, haptic rendering is accomplished using streaming point cloud
data of Kinect. A preprocessing phase including downsampling and noise
reduction is applied to the raw point clouds to prevent huge computation
load. In preprocessing phase, it should be concerned that the density of
points firmly threats the proxy situation to be in popthrough.
Furthermore, the contact is detected by a proxy based algorithm and
using a kd-tree search to find contact point among lots of points
derived from Kinect. Force can be computed by using a virtual spring
coupling between HIP and proxy. The noise is reduced in first phase to
reduce the next errors in force calculation. This feature helps us to
have a stable system in contact with a dynamic environment including
movement and new objects.},
keywords={computational geometry;haptic interfaces;noise;rendering
(computer graphics);signal processing;tree searching;downsampling;fast
contact detection;haptic rendering;haptic system;kd-tree search;noise
reduction;point cloud data streaming;preprocessing phase;Haptic
interfaces;Hip;Noise reduction;Real-time systems;Rendering (computer
graphics);Robot sensing systems;Three-dimensional
displays;downsampling;dynamic environment;fast contact
detection;haptic;point clouds},
doi={10.1109/ICRoM.2016.7886807},
month={Oct},}
@ARTICLE{7862751,
author={H. Kim and B. Liu and C. Y. Goh and S. Lee and H. Myung},
journal={IEEE Robotics and Automation Letters},
title={Robust Vehicle Localization Using Entropy-Weighted Particle
Filter-based Data Fusion of Vertical and Road Intensity Information for
a Large Scale Urban Area},
year={2017},
volume={2},
number={3},
pages={1518-1524},
abstract={This letter proposes a robust vehicle localization method
based on a prior point cloud in urban area. The high resolution point
cloud collected six months ago is provided from Singapore Land Authority
around One-north area in Singapore, because the data are outdated there
are many changed aspects of the environment such as redrawn road
markings, construction areas, and changing tree shapes. In response,
this paper proposes a novel fusion algorithm based on a particle filter
using vertical and road intensity information for robust localization.
Whereas the state-of-the-art fusion algorithm focus on optimization of
the vehicle pose based on multiple measurements, the proposed method
estimates a robust vehicle pose by considering the reliability of each
feature from the prior map. We also propose an efficient management
strategy of a grid map that includes multilayer vertical and road
intensity information for real-time operation. A sensor system equipped
on a vehicle consists of 32 channels of 3D Light Detection And Ranging,
IMU, wheel odometry, which are used for the proposed algorithm. RTK-GPS,
wheel odometry, and iterative closest point algorithm are utilized by
using a graph-structure optimization method in off-line for estimating
ground truth. The total data set for the demonstration is collected in a
19.9 km way in an urban area. The proposed approach has successfully
performed an autonomous vehicle driving in an urban area.},
keywords={graph theory;iterative methods;optimisation;particle filtering
(numerical methods);remotely operated vehicles;sensor fusion;3D light
detection and ranging;IMU;RTK-GPS;autonomous vehicle
driving;entropy-weighted particle filter-based data
fusion;graph-structure optimization method;grid map;iterative closest
point algorithm;large scale urban area;management strategy;road
intensity information;robust vehicle localization;sensor system;wheel
odometry;Autonomous vehicles;Feature extraction;Real-time
systems;Roads;Robustness;Three-dimensional displays;Urban
areas;Autonomous vehicle navigation;field robots;localization;sensor
fusion},
doi={10.1109/LRA.2017.2673868},
ISSN={2377-3766},
month={July},}
@INPROCEEDINGS{7881389,
author={J. W. Wallace and S. J. Kambouris},
booktitle={2016 International Conference on Computational Science and
Computational Intelligence (CSCI)},
title={A Language, Framework, and SDK for Robotic Communications,
Integration, and Interoperability},
year={2016},
pages={472-477},
abstract={A framework to integrate different artificial intelligence and
machine learning algorithms is combined with an execution framework to
create a powerful cloud computing system development platform. By
providing an execution framework and control software that is native to
cloud architectures and supports interactivity and time synchronization,
the true utility of cloud computing and "big data systems" can be
increased. Many "big data" software systems are not interactive,
automated, or can run in real-time.},
keywords={Big Data;cloud computing;control engineering
computing;learning (artificial intelligence);open systems;robots;Big
Data system;artificial intelligence;cloud architectures;cloud computing
system;machine learning;robotic communications;Computer
architecture;Hardware;Programming;Semantics;Software;Standards;Synchronization;communication;integration;interoperability;language;robotics},

doi={10.1109/CSCI.2016.0096},
month={Dec},}
@INPROCEEDINGS{7877972,
author={J. Bao and M. Ye and Y. Dou},
booktitle={2016 IEEE 13th International Conference on Signal Processing
(ICSP)},
title={Mobile phone-based internet of things human action recognition
for E-health},
year={2016},
pages={957-962},
abstract={Human action recognition plays an important role in E-health,
such as risk assessment, disease treatment, rehabilitation and so on. We
proposes a mobile phone-based internet of things method for human action
recognition. In the work, data are collected from a smart phone worn on
the waist and transmitted to the application server on the internet. The
application server program cuts these data into segments of 128 samples
with 50% overlap. And each segment is embedded into a 6-dimensional
pseudo phase space, then a geometric template matching algorithm is
applied to classify them into different actions. Last, Bayesian
principle and voting rule are combined to confuse the results of the
k-nearest neighbor classifiers. Experimental results on UCI HAR datasets
show that this method can obtain a significant improvement in accuracy
compared with the traditional SVM methods.},
keywords={Bayes methods;Internet of Things;medical computing;smart
phones;6-dimensional pseudo phase space;Bayesian principle;Internet of
things method;SVM methods;UCI HAR datasets;application server
program;disease treatment;e-health;geometric template matching
algorithm;human action recognition;k-nearest neighbor classifiers;mobile
phone;rehabilitation;risk assessment;smart phone;voting rule;Cloud
computing;Delay effects;Mobile communication;Monitoring;Sensors;Smart
phones;action recognition;e-health;geometric template matching;internet
of things;mobile Phone;time delay embedding},
doi={10.1109/ICSP.2016.7877972},
ISSN={2164-5221},
month={Nov},}
@ARTICLE{7819508,
author={L. Li and M. Yang and L. Guo and C. Wang and B. Wang},
journal={IEEE Transactions on Intelligent Vehicles},
title={Hierarchical Neighborhood Based Precise Localization for
Intelligent Vehicles in Urban Environments},
year={2016},
volume={1},
number={3},
pages={220-229},
abstract={High-precision localization has drawn more and more attention
in recent research of intelligent vehicle systems and autonomous robot
navigation technology. In most methods, the approaches are only
effective in some specific situations. In other words, these methods can
only perform well with obvious features, like tall building walls, road
curbs, etc. In this paper, a novel framework for precise localization of
autonomous vehicle applying to different scenes especially some typical
urban environments is proposed. The main procedures of this method
include mapping and localization. During mapping process, inertial
measurement unit, odometry, and high-precision GPS are fused together
with the data sensed by LIDAR, a high-precision map that could provide
global position and pose is generated using rolling window. When
localizing, live laser data align with the prior-map. A particle filter
based point cloud matching method is utilized here. Based on this, a
hierarchical localizing method is proposed, which is more accurate and
faster than the original matching method. With this method, the sampling
guided by proposal density is propagated upward every hierarchy. Besides
that, some accelerating algorithms are utilized to make this approach
real time. Finally, decimeter-level localization is achieved in
different environments, which is proven by some experiments.},
keywords={Global Positioning System;intelligent transportation
systems;mobile robots;optical radar;sampling methods;LIDAR;autonomous
robot navigation technology;decimeter-level localization;hierarchical
neighborhood;high-precision GPS;high-precision localization;inertial
measurement unit;intelligent vehicles;odometry;sampling method;urban
environments;Feature extraction;Global Positioning System;Laser
radar;Roads;Three-dimensional displays;Two dimensional
displays;Vehicles;Localization;hierarchical structure;mapping;point
cloud;point set registration},
doi={10.1109/TIV.2017.2654065},
ISSN={2379-8858},
month={Sept},}
@ARTICLE{7850997,
author={S. M. Prakhya and J. Lin and V. Chandrasekhar and W. Lin and B.
Liu},
journal={IEEE Robotics and Automation Letters},
title={3DHoPD: A Fast Low-Dimensional 3-D Descriptor},
year={2017},
volume={2},
number={3},
pages={1472-1479},
abstract={Three-dimensional feature descriptors are heavily employed in
various 3-D perception applications to find keypoint correspondences
between two point clouds. The availability of mobile devices equipped
with depth sensors compels the developed applications to be both memory
and computationally efficient. Toward this, in this letter, we present
3DHoPD, a new low-dimensional 3-D feature descriptor that is extremely
fast to compute. The novelty lies in compactly encoding the “3-D”
keypoint position by transforming it to a new 3-D space, where the
keypoints arising from similar 3-D surface patches lie close to each
other. Then, we propose histograms of point distributions (HoPD) to
capture the neighborhood structure, thus forming 3DHoPD (3D+HoPD). We
propose a tailored feature descriptor matching technique, wherein the
“3-D” keypoint position in the new 3-D space is used to remove false
positive matches, effectively reducing the search space by 90%, and
then, the exact match is found using the “HoPD” descriptor. Experimental
evaluation on multiple publicly available datasets shows that 3DHoPD is
robust to noise and offers stable and competitive keypoint matching
performance to the existing state-of-the-art 3-D descriptors with
similar dimensionality across datasets, while requiring dramatically
low-computational time (10× faster). The source code and additional
experimental results are available at
https://sites.google.com/site/3dhopd/.},
keywords={computational geometry;computer graphics;image colour
analysis;image matching;object detection;3D feature descriptor;3D
histograms of point distributions;3D keypoint position encoding;3D
object detection;3D perception;HoPD;RGB-D perception;depth
sensors;feature descriptor matching;mobile devices;point clouds;Feature
extraction;Histograms;Mobile handsets;Sensors;Solid
modeling;Three-dimensional displays;Transforms;3D feature descriptors;3D
object detection;Descriptor matching;RGB-D perception;SLAM},
doi={10.1109/LRA.2017.2667721},
ISSN={2377-3766},
month={July},}
@INPROCEEDINGS{7866543,
author={Q. Wu and K. Sun and W. Zhang and C. Huang and X. Wu},
booktitle={2016 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Visual and LiDAR-based for the mobile 3D mapping},
year={2016},
pages={1522-1527},
abstract={Multi-sensors fusion to construct an accurate 3D map about
unknown indoor scene and outdoor in the simultaneous localization and
mapping (SLAM) area is becoming increasingly popular. In this paper our
methods pay an attention to how to optimize the 2D map from the indoor
or outdoor unknown large-scale environment and how to make the 3D map
intuitive and accurate. Firstly, this paper accomplished the calibration
between 2D Light Detection And Ranging sensor (LiDAR) and panoramic
camera, using horizontal 2D LiDAR to finish 2D SLAM. Then the new method
that closed-loop detection was based on visual was used to assist the 2D
SLAM to get a better optimization result in sub-maps efficiently.
Finally, vertical LiDAR data was added to get the 3D point cloud, using
the images from a panoramic camera to accomplish the registration with
point cloud. The result of final 3D map with RGB information
demonstrated that our methods are feasible.},
keywords={SLAM (robots);closed loop systems;image sensors;mobile
robots;optical radar;robot vision;sensor fusion;2D LiDAR;2D SLAM;2D
light detection and ranging sensor;3D point cloud;mobile 3D
mapping;multisensors fusion;panoramic camera;simultaneous localization
and mapping;Calibration;Cameras;Laser radar;Simultaneous localization
and mapping;Three-dimensional displays;Two dimensional displays},
doi={10.1109/ROBIO.2016.7866543},
month={Dec},}
@INPROCEEDINGS{7866366,
author={H. Zhang and C. Ye},
booktitle={2016 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={An indoor navigation aid for the visually impaired},
year={2016},
pages={467-472},
abstract={This paper presents a 6-DOF pose estimation (PE) method and an
indoor wayfinding system based on the method for the visually impaired
(VI). The PE method involves two graph SLAM processes to reduce the
accumulative pose error of device. In the first step, the floor plane is
extracted from the 3D camera's point cloud and added as a landmark node
into the graph for 6-DOF SLAM to reduce roll, pitch and Z errors. In the
second step, the wall lines are extracted and incorporated into the
graph for 3-DOF SLAM to reduce X, Y and yaw errors. The method reduces
the 6-DOF pose error and results in more accurate pose with less
computational time than the existing state-of-the-art planar SLAM
methods. Based on the PE method, a wayfinding system is developed for
navigating a VI person in an indoor environment. The system uses the
estimated pose and floorplan to locate the device user in a building and
guide the user by announcing the points of interest and navigational
commands through a speech interface. Experimental results validate the
effectiveness of the PE method and demonstrate that the system may
substantially ease an indoor navigation task.},
keywords={SLAM (robots);feature extraction;graph theory;handicapped
aids;indoor navigation;pose estimation;speech-based user
interfaces;3-DOF SLAM;3D camera point cloud;6-DOF SLAM;6-DOF pose error
reduction;6-DOF pose estimation;X error reduction;Y error reduction;Z
error reduction;accumulative pose error reduction;floor plane
extraction;graph SLAM processes;indoor navigation aid;indoor wayfinding
system;pitch error reduction;roll error reduction;speech
interface;visually impaired;wall line extraction;yaw error
reduction;Cameras;Feature extraction;Floors;Navigation;RNA;Simultaneous
localization and mapping;Three-dimensional displays},
doi={10.1109/ROBIO.2016.7866366},
month={Dec},}
@INPROCEEDINGS{7866420,
author={J. Zheng and E. Li and Z. Liang},
booktitle={2016 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={The grasping point for planar workpiece based on fuzzy
connectedness prior knowledge},
year={2016},
pages={794-799},
abstract={This paper describes the problem that a manipulator grasps the
planar workpiece with a negative pressure suction cup. The grasping
point must be determined to meet the command of planar grasping
criterion to lay the foundation of the workpiece placement. The first
contribution of this paper is the segmentation of the workpiece under
industry condition with illumination and occlusion in the background.
The method of iterative graph cuts and relative fuzzy connectedness are
combined to segment the workpiece. The second contribution is the
computation of the optimal grasping point using point cloud of the
workpiece. The point cloud coming from the feature points, which are
extracted from the structured light stripes according to the range of
the image segmentation and transformed to the coordinates of the base,
is used to segment the largest plane of the workpiece which is feasible
for grasping. The normal vector of the planar workpiece are also
determined. The experiment results show that the algorithm is effective
and feasible in grasping the planar workpiece.},
keywords={fuzzy control;graph theory;image segmentation;iterative
methods;manipulators;robot vision;vectors;grasping point;image
segmentation;iterative graph cuts;manipulator;negative pressure suction
cup;planar grasping criterion;planar workpiece normal vector;relative
fuzzy connectedness;workpiece point cloud;Grasping;Image
segmentation;Industries;Lighting;Manipulators;Three-dimensional displays},
doi={10.1109/ROBIO.2016.7866420},
month={Dec},}
@INPROCEEDINGS{7866321,
author={X. Meng and C. Zhou and Z. Cao and L. Zhang and X. Liu and S.
Wang},
booktitle={2016 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={A slope location and orientation estimation method based on 3D
LiDAR suitable for quadruped robots},
year={2016},
pages={197-201},
abstract={3D LiDAR is widely used for mobile robots environment
perception nowadays, which has high precision of distance measurement.
In this paper, a calibration method which is suitable for Velodyne
VLP-16 is firstly introduced. And then we utilize statistical filtering
for isolated points removal. After statistical filtering, a slope
location and orientation estimation method is presented based on PROSAC
algorithm, which can achieve plane extraction from the 3D point cloud
data. Experiments are conducted to testify the performance of the
presented method.},
keywords={calibration;distance measurement;estimation theory;mobile
robots;optical radar;robot vision;3D LiDAR;3D point cloud data;PROSAC
algorithm;Velodyne VLP-16;calibration;distance measurement;environment
perception;isolated points removal;mobile robots;orientation
estimation;quadruped robots;slope location;statistical
filtering;Calibration;Estimation;Laser radar;Measurement by laser
beam;Robots;Sensors;Three-dimensional displays},
doi={10.1109/ROBIO.2016.7866321},
month={Dec},}
@INPROCEEDINGS{7866470,
author={A. Khan and L. Sun and G. Aragon-Camarasa and J. P. Siebert},
booktitle={2016 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Interactive perception based on Gaussian Process classification
for house-hold objects recognition sorting},
year={2016},
pages={1087-1092},
abstract={We present an interactive perception model for object sorting
based on Gaussian Process (GP) classification that is capable of
recognizing objects categories from point cloud data. In our approach,
FPFH features are extracted from point clouds to describe the local 3D
shape of objects and a Bag-of-Words coding method is used to obtain an
object-level vocabulary representation. Multi-class Gaussian Process
classification is employed to provide and probable estimation of the
identity of the object and serves a key role in the interactive
perception cycle - modelling perception confidence. We show results from
simulated input data on both SVM and GP based multi-class classifiers to
validate the recognition accuracy of our proposed perception model. Our
results demonstrate that by using a GP-based classifier, we obtain true
positive classification rates of up to 80%. Our semi-autonomous object
sorting experiments show that the proposed GP based interactive sorting
approach outperforms random sorting by up to 30% when applied to scenes
comprising configurations of household objects.},
keywords={Gaussian processes;cloud computing;computer vision;feature
extraction;image classification;object recognition;support vector
machines;FPFH feature extraction;GP-based classifier;SVM;house-hold
object recognition;house-hold object sorting;interactive
perception;multiclass Gaussian process classification;object 3D
shape;object-level vocabulary representation;semi-autonomous object
sorting;Color;Feature extraction;Gaussian
processes;Robots;Sorting;Three-dimensional displays;Visualization},
doi={10.1109/ROBIO.2016.7866470},
month={Dec},}
@INPROCEEDINGS{7866451,
author={Y. Tian and L. Xu and T. Jia and A. Wang and L. Li},
booktitle={2016 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Research on a fast measurement equipment for robot repeatability},
year={2016},
pages={975-980},
abstract={According to the demand of manipulator's repeatability in
industrial production line, we developed a portable measurement
equipment and especially designed the structure of reference block.
Meanwhile, we proposed an algorithm to reduce difficulty of measurement.
Then we operated a simulation experiment on manipulator through
Pro/Engineer, which is abbreviated to Proe in the following text, and
got two point clouds. One is from ideal point which is basically can't
be got in actual measurement and another one is from equivalent point
calculated by the algorithm. After that, we could analyze the
repeatability to verify accuracy of the method we proposed and finally
provide a reference for such manipulator's repeatability measurement.},
keywords={industrial manipulators;instruments;Pro-Engineer;Proe;fast
measurement equipment;industrial production line;manipulator
repeatability;portable measurement equipment;reference block;robot
repeatability;Coordinate measuring machines;Manipulators;Position
measurement;Sensors;Service robots;Standards},
doi={10.1109/ROBIO.2016.7866451},
month={Dec},}
@INPROCEEDINGS{7866388,
author={P. Wang and D. Li and Y. Wang and R. Xiong},
booktitle={2016 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Work day and night: A learning based illumination irrelevant
grasp planning method},
year={2016},
pages={602-607},
abstract={This paper presents a new grasp planning method to solve grasp
synthesis and grasp selection problems. A depth-projection algorithm is
proposed to solve synthesis problem. Using only depth information of
working scene, the algorithm establishes the relationship, which is
expressed as depth-projections, between point cloud and grasp poses. And
a learning network is constructed and trained to accomplish the grasp
selection, i.e. selecting the best one from depth-projections in this
paper. Besides parallel acceleration methods on both CPU and GPU are
applied to further improve the efficiency of grasp planning. Experiments
suggest that (1) There is no need to model gripper or target object,
which is inaccurate or even impossible for some deformable objects; (2)
Our method is illumination irrelevant with 83.3% test accuracy,
therefore, robots can work no matter day and night, and no matter in
household or outside environments; (3) Grasp planning for one object
costs only a few seconds with the CPU and GPU acceleration, meaning that
our method is applicable to pratical environment online.},
keywords={grippers;learning (artificial intelligence);path
planning;CPU;GPU;depth information;depth-projection algorithm;grasp
selection;grasp selection problems;grasp synthesis;gripper;learning
based illumination irrelevant grasp planning method;parallel
acceleration methods;point cloud;pose grasping;synthesis problem;target
object;working scene;Acceleration;Graphics processing
units;Grippers;Learning systems;Lighting;Planning;Three-dimensional
displays},
doi={10.1109/ROBIO.2016.7866388},
month={Dec},}
@INPROCEEDINGS{7866471,
author={Y. Zou and T. Zhang and X. Wang and Y. He and J. Song},
booktitle={2016 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={BRoPH: A compact and efficient binary 3D feature descriptor},
year={2016},
pages={1093-1098},
abstract={Growing availability and use of 3D data acquisition devices
have spurred interest in 3D vision field. Feature descriptor is
essential to feature matching. Current 3D feature descriptors are mostly
represented in float vector. In this paper, we propose a binary 3D
feature descriptor, called BRoPH, by turning the binary description of
the 3D point cloud into a series of binarization of projected 2D image
patches. We demonstrate through experiments that BRoPH achieves a
comparable descriptiveness and robustness with respect to the
state-of-the-art 3D feature descriptors while outperforms the others in
terms of compactness and efficiency.},
keywords={computer vision;data acquisition;feature extraction;image
matching;2D image patches;3D data acquisition devices;3D point cloud;3D
vision field;BRoPH;binarization series;binary 3D feature
descriptor;binary description;feature matching;float vector;Image
resolution;Open area test sites;Robot sensing
systems;Robustness;Three-dimensional displays;Two dimensional displays},
doi={10.1109/ROBIO.2016.7866471},
month={Dec},}
@ARTICLE{7822998,
author={T. M. Bonanni and B. D. Corte and G. Grisetti},
journal={IEEE Robotics and Automation Letters},
title={3-D Map Merging on Pose Graphs},
year={2017},
volume={2},
number={2},
pages={1031-1038},
abstract={In this letter, we propose an approach for merging
three-dimensional maps represented as pose graphs of point clouds. Our
method can effectively deal with typical distortions affecting
simultaneous localization and mapping-generated maps. Traditional map
merging techniques that use a single rigid body transformation to relate
the reference frames of different maps. Instead, our approach achieves
more accurate results by eliminating the inconsistencies resulting from
distortions affecting the inputs, and can succeed in those situations
where traditional approaches fail for substantial deformations. The core
idea behind our solution is to localize the robot in a reference map by
using the data from another map as observations. We validated our
approach on publicly available datasets, and provide quantitative
results that confirm its effectiveness on challenging instances of the
merging problem.},
keywords={SLAM (robots);graph theory;mobile robots;3D map
merging;SLAM;point clouds;pose graphs;rigid body transformation;robot
localization;simultaneous localization and mapping-generated
maps;three-dimensional map
merging;Distortion;Merging;Optimization;Simultaneous localization and
mapping;Three-dimensional displays;Two dimensional
displays;Localization;mapping;slam},
doi={10.1109/LRA.2017.2655139},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{7853105,
author={A. N. Reganti and S. Ananthapalli and B. C. V. Mohan and R.
Mounica and V. K. Mittal},
booktitle={2016 IEEE 1st International Conference on Power Electronics,
Intelligent Control and Energy Systems (ICPEICES)},
title={Mobile Quad-Controlled wireless Robotic Arm},
year={2016},
pages={1-5},
abstract={In this paper, we develop a remotely controlled robotic arm
with 4 degree of freedom (D.O.F) that is wirelessly controlled using
four control mechanisms, i.e, Voice Control, Smart Phone-Tilt Control,
Remote control and Hand Gesture Control. Wireless technologies such as
Bluetooth and Wi-Fi are used to access the Quad-Controlled Robotic Arm
(QCRA). A prototype QCRA is developed. The QCRA can be used to pick and
place objects from one place to another on receiving the commands from
distance, thereby reducing the human effort. An Android application is
developed for convenience of the user in operating this QCRA, using
different control mechanisms. Performance evaluation results are
encouraging. Potential applications of the QCRA in homes, industries and
for physically challenged/aged people are also discussed.},
keywords={attitude control;control engineering
computing;manipulators;mobile computing;smart
phones;telerobotics;Android application;QCRA;hand gesture control;mobile
quad-controlled wireless robotic arm;remote control;smart phone-tilt
control;voice control;wireless control;IEEE 802.11 Standard;Process
control;Programming;Prototypes;Service robots;Smoothing
methods;Bluetooth;Cloud-Server;Hand-gesture Recognition;Quad-controlled
Robotic Arm;Remote Control;Smart Phone-tilt Control;Voice-control;Wi-Fi},
doi={10.1109/ICPEICES.2016.7853105},
month={July},}
@INPROCEEDINGS{7849530,
author={J. Lambert and L. Clement and M. Giamou and J. Kelly},
booktitle={2016 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Entropy-based sim(3) calibration of 2D lidars to egomotion sensors},
year={2016},
pages={455-461},
abstract={This paper explores the use of an entropy-based technique for
point cloud reconstruction with the goal of calibrating a lidar to a
sensor capable of providing egomotion information. We extend recent work
in this area to the problem of recovering the Sim(3) transformation
between a 2D lidar and a rigidly attached monocular camera, where the
scale of the camera trajectory is not known a priori. We demonstrate the
robustness of our approach on realistic simulations in multiple
environments, as well as on data collected from a hand-held sensor rig.
Given a non-degenerate trajectory and a sufficient number of lidar
measurements, our calibration procedure achieves millimetre-scale and
sub-degree accuracy. Moreover, our method relaxes the need for specific
scene geometry, fiducial markers, or overlapping sensor fields of view,
which had previously limited similar techniques.},
keywords={calibration;cameras;computer graphics;entropy;image motion
analysis;image reconstruction;image sensors;2D lidar;Sim(3)
transformation;camera trajectory;egomotion sensors;entropy-based Sim(3)
calibration;lidar calibration;lidar measurements;monocular
camera;nondegenerate trajectory;point cloud
reconstruction;Calibration;Cameras;Entropy;Laser
radar;Sensors;Three-dimensional displays;Trajectory},
doi={10.1109/MFI.2016.7849530},
month={Sept},}
@INPROCEEDINGS{7849532,
author={R. C. Luo and V. W. S. Ee and C. K. Hsieh},
booktitle={2016 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={3D point cloud based indoor mobile robot in 6-DoF pose
localization using Fast Scene Recognition and Alignment approach},
year={2016},
pages={470-475},
abstract={This paper describes an algorithm for localization of a robot
which can efficiently estimate robot in 6 degrees-offreedom (DoF) pose
which consist of position and orientation with large scale point cloud
data without giving the initial pose. We introduce the Fast Scene
Recognition and Alignment algorithm to reduce the computation time
needed for the point cloud alignment by matching robot's scene only with
the retrieved Sub-Map in database. Our developed algorithm is to extract
Sub-Maps descriptor by cascading several features, and learn a
Distance-Metric to increase the precision of place recognition due to
the environmental changes. We then align the robot's scene with Sub-Map
to estimate robot pose. Our technique has been implemented and tested
extensively in different buildings. The experimental results show that
our Fast Scene Recognition and Alignment system can localize mobile
robot in a variety of large scale 3D point cloud dataset efficiently.},
keywords={mobile robots;object recognition;pose estimation;position
control;robot vision;3D point cloud based indoor mobile robot;6-DoF pose
localization;alignment approach;distance-metric;environmental
changes;fast scene recognition;large scale 3D point cloud dataset;large
scale point cloud data;place recognition;point cloud alignment;robot
pose;sub-maps descriptor;Cameras;Databases;Feature extraction;Robot
kinematics;Robot vision systems;Three-dimensional displays},
doi={10.1109/MFI.2016.7849532},
month={Sept},}
@INPROCEEDINGS{7849501,
author={J. Leaman and H. M. La and L. Nguyen},
booktitle={2016 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Development of a smart wheelchair for people with disabilities},
year={2016},
pages={279-284},
abstract={The intelligent power wheelchair (iChair) is designed to
assist people with mobility, sensory, and cognitive impairment lead a
higher quality, more independent lifestyle. The iChair includes a power
wheelchair (PW), laptop computer, laptop mount, multi-modal input
platform, and a custom plastic enclosure for the environmental sensors,
made with a 3D printer. We have developed the configuration of sensors
to facilitate scientific observation, while maintaining the flexibility
to mount the system on almost any power wheelchair, and remain easily
removed for maintenance or travel. The first scientific observations
have been used to compile ACCESS Reports that quantify a location or
event's level of accessibility. If barriers exist we collect a 3D point
cloud to be used as evidence and to make recommendations on how to
remedy the problem. The iChair will serve a wide variety of disability
types by incorporating several input methods, voice, touch, proximity
switch, and head tracking camera. The HD camera and 3D scanner have been
mounted in such a way as to provide reliable data with the precision
necessary to detect obstacles, build 3D maps, follow guides, anticipate
events, and provide navigational assistance. We evaluate the human
factors in the current prototype to ensure that the technology will be
accepted by those it is designed to serve, and propose a wheelchair
skills test for future trial participants.},
keywords={handicapped aids;human factors;knowledge based systems;sensor
fusion;wheelchairs;3D maps;3D point cloud;3D scanner;HD camera;disabled
people;head tracking camera;human factors;iChair;intelligent power
wheelchair;navigational assistance;obstacle detection;smart
wheelchair;wheelchair skills test;Cameras;High definition
video;Photonics;Prototypes;Sensors;Three-dimensional
displays;Wheelchairs;3D mapping;Human factors;Smart wheelchair;Structure
scanner},
doi={10.1109/MFI.2016.7849501},
month={Sept},}
@INPROCEEDINGS{7844293,
author={L. Zhang and X. Yin and Junnan Shen and Haitao Yu},
booktitle={2016 IEEE International Conference on Systems, Man, and
Cybernetics (SMC)},
title={Cloud-aided moving horizon state estimation of a full-car
semi-active suspension system},
year={2016},
pages={000527-000532},
abstract={In this work, we investigate a state estimation problem for a
full-car semi-active suspension system. To account for the complex
calculation and optimization problems, a vehicle-to-cloud-to-vehicle
(V2C2V) scheme is utilized. Moving horizon estimation is introduced for
the state estimation system design. All the optimization problems are
solved in a remotely-embedded agent with high computational ability.
Measurements and state estimates are transmitted between the vehicle and
the remote agent via networked communication channels. The effectiveness
of the proposed method is illustrated via a set of simulations.},
keywords={automobiles;cloud computing;computational complexity;embedded
systems;multi-agent systems;optimisation;traffic engineering
computing;V2C2V scheme;cloud-aided moving horizon state
estimation;complex calculation;computational ability;full-car
semi-active suspension system;networked communication
channels;optimization problems;remotely-embedded agent;state estimation
system
design;vehicle-to-cloud-to-vehicle;Automobiles;Optimization;Roads;State
estimation;Suspensions;Wheels},
doi={10.1109/SMC.2016.7844293},
month={Oct},}
@ARTICLE{7778241,
author={S. F. Atashzar and I. G. Polushin and R. V. Patel},
journal={IEEE Transactions on Robotics},
title={A Small-Gain Approach for Nonpassive Bilateral Telerobotic
Rehabilitation: Stability Analysis and Controller Synthesis},
year={2017},
volume={33},
number={1},
pages={49-66},
abstract={In this paper, the design of a novel bilateral telerobotic
architecture for rehabilitation purposes is proposed and the related
feasibility, stability, and control challenges are studied. The
objective is to incorporate the supervision of a local/remote human
physiotherapist into haptics-enabled rehabilitation systems and allow
the therapist to provide nonpassive nonlinear assistive/resistive forces
in response to the patient's movements. This can address a challenge of
conventional software-based rehabilitation systems, i.e., limited
capability in adjusting the therapy. To guarantee human-robot
interaction safety, a new design framework and a stabilizing controller
are developed based on the small-gain approach. System stability and
transparency are analyzed in the presence of the nonpassive, nonlinear,
and nonautonomous behavior of the terminals (the therapist and the
patient) and time-varying delays for the case of remote and cloud-based
therapy. Several practical considerations have been taken into account
to match the clinical needs and minimize the implementation cost.
Simulation studies, practical implementation, and experimental
evaluations are presented.},
keywords={control system synthesis;delays;haptic interfaces;human-robot
interaction;medical robotics;nonlinear control systems;patient
rehabilitation;stability;telerobotics;time-varying systems;bilateral
telerobotic architecture;cloud-based therapy;haptics-enabled
rehabilitation systems;human-robot interaction safety;implementation
cost minimization;local human physiotherapist;nonpassive bilateral
telerobotic rehabilitation;nonpassive nonlinear assis- tive/resistive
forces;nonpassive nonlinear nonautonomous behavior;patient
movements;remote human physiotherapist;remote therapy;small-gain
approach;stabilizing controller;system stability;system
transparency;time-varying delays;Delays;Medical
treatment;Safety;Stability criteria;Telerobotics;Haptics;physical
human–robot interaction;rehabilitation robotics;stability;telerobotics},
doi={10.1109/TRO.2016.2623336},
ISSN={1552-3098},
month={Feb},}
@INPROCEEDINGS{7832002,
author={W. Chen and S. Gu and Y. Guan and H. Zhang and G. Liu and H. Tang},
booktitle={2016 IEEE International Conference on Information and
Automation (ICIA)},
title={A multi-layered path planning algorithm for truss climbing with a
biped robot},
year={2016},
pages={1200-1205},
abstract={Autonomous climbing is an essential function for a climbing
robot to be applied in practical high-rise work. And for autonomous
climbing, perception of climbing environment and planning of climbing
path are two basic issues. Truss-style environment modeling and global
path planning are still open for a biped climbing robot. In this paper,
RGBD-SLAM is used to model truss-style climbing environment, in terms of
pointcloud. Based on the model in pointcloud format, a multi-layered
algorithm is presented to plan a global path along the poles in a truss.
The algorithm can solve the problem of local optima and the discrete
caused by pointcloud. The effectiveness of the modeling method and the
planning algorithm is verified by experiments, where the time taken in
the planning is less than 0.25s in a pointcloud model with 454411 nodes.},
keywords={SLAM (robots);computer graphics;image colour analysis;legged
locomotion;path planning;robot vision;RGB image;RGBD-SLAM;autonomous
climbing;biped climbing robot;climbing path planning;global path
planning;high-rise work;multilayered path planning algorithm;point-cloud
model;truss-style climbing environment modeling;Algorithm design and
analysis;Climbing robots;Heuristic algorithms;Path planning;Robot
sensing systems;Search problems;Climbing Robot;Environment
Perception;Path Planning;Point-cloud Model},
doi={10.1109/ICInfA.2016.7832002},
month={Aug},}
@INPROCEEDINGS{7838769,
author={E. Mihankhah and D. Wang},
booktitle={2016 14th International Conference on Control, Automation,
Robotics and Vision (ICARCV)},
title={Environment characterization using Laplace eigenvalues},
year={2016},
pages={1-6},
abstract={This paper introduces a new methodology for environment
characterization. This methodology is based on analysis of the
eigenvalues of Laplace-Beltrami operator over 3 dimensional point
clouds. Recognizing revisited places can be facilitated by
characterizing the environment through a descriptor. The idea of
analyzing point clouds using the eigenvalues of Laplace-Beltrami
operator for characterization of an environment can be used for place
detection which is a critical functionality of autonomous mobile robots.
Place detection is a requirement for transition detection in multi
environment missions, common frame identification in multi robot
mapping, and detection of previously visited location in SLAM for loop
closure phase.},
keywords={SLAM (robots);mobile robots;multi-robot systems;3 dimensional
point clouds;Laplace eigenvalues;Laplace-Beltrami
operator;SLAM;autonomous mobile robots;environment characterization;loop
closure phase;multirobot detection;multirobot mapping;place
detection;point cloud analysis;transition detection;Eigenvalues and
eigenfunctions;Global Positioning System;Robot kinematics;Simultaneous
localization and mapping;Three-dimensional displays;3D point cloud
analysis;Lalpace Beltrami spectra;eigenvalue analysis;environment
characterization;place detection;transition detection},
doi={10.1109/ICARCV.2016.7838769},
month={Nov},}
@INPROCEEDINGS{7838613,
author={T. Kopinski and A. Gepperth and U. Handmann},
booktitle={2016 14th International Conference on Control, Automation,
Robotics and Vision (ICARCV)},
title={A time-of-flight-based hand posture database for human-machine
interaction},
year={2016},
pages={1-6},
abstract={We present a publicly available benchmark database for the
problem of hand posture recognition from noisy depth data and fused
RGB-D data obtained from low-cost time-of-flight (ToF) sensors. The
database is the most extensive database of this kind containing over a
million data samples (point clouds) recorded from 35 different
individuals for ten different static hand postures. This captures a
great amount of variance, due to person-related factors, but also
scaling, translation and rotation are explicitly represented. Benchmark
results achieved with a standard classification algorithm are computed
by cross-validation both over samples and persons, the latter implying
training on all persons but one and testing on the remaining one. An
important result using this database is that cross-validation
performance over samples (which is the standard procedure in machine
learning) is systematically higher than cross-validation performance
over persons, which is to our mind the true application-relevant measure
of generalization performance.},
keywords={image classification;image colour analysis;image
fusion;learning (artificial intelligence);man-machine systems;visual
databases;RGB-D data fusion;ToF sensors;cross-validation
performance;generalization performance;hand posture
recognition;human-machine interaction;low-cost time-of-flight
sensors;noisy depth data;person-related factors;standard classification
algorithm;time-of-flight-based hand posture database;Benchmark
testing;Cameras;Databases;Noise
measurement;Sensors;Standards;Three-dimensional displays},
doi={10.1109/ICARCV.2016.7838613},
month={Nov},}
@ARTICLE{7814251,
author={R. Ambruş and S. Claici and A. Wendt},
journal={IEEE Robotics and Automation Letters},
title={Automatic Room Segmentation From Unstructured 3-D Data of Indoor
Environments},
year={2017},
volume={2},
number={2},
pages={749-756},
abstract={We present an automatic approach for the task of
reconstructing a 2-D floor plan from unstructured point clouds of
building interiors. Our approach emphasizes accurate and robust
detection of building structural elements and, unlike previous
approaches, does not require prior knowledge of scanning device poses.
The reconstruction task is formulated as a multiclass labeling problem
that we approach using energy minimization. We use intuitive priors to
define the costs for the energy minimization problem and rely on
accurate wall and opening detection algorithms to ensure robustness. We
provide detailed experimental evaluation results, both qualitative and
quantitative, against state-of-the-art methods and labeled ground-truth
data.},
keywords={image colour analysis;image reconstruction;indoor
environment;minimisation;2D floor plan reconstruction;automatic room
segmentation;building interiors;building structural elements;energy
minimization;indoor environments;labeled ground-truth data;multiclass
labeling problem;reconstruction task;scanning device poses;unstructured
3D data;unstructured point clouds;Clutter;Image
segmentation;Labeling;Minimization;Semantics;Three-dimensional
displays;Two dimensional displays;Mapping;RGB-D perception;semantic
scene understanding},
doi={10.1109/LRA.2017.2651939},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{7838803,
author={C. Berger and P. Rudol and M. Wzorek and A. Kleiner},
booktitle={2016 14th International Conference on Control, Automation,
Robotics and Vision (ICARCV)},
title={Evaluation of reactive obstacle avoidance algorithms for a
quadcopter},
year={2016},
pages={1-6},
abstract={In this work we are investigating reactive avoidance
techniques which can be used on board of a small quadcopter and which do
not require absolute localization. We propose a local map representation
which can be updated with proprioceptive sensors. The local map is
centred around the robot and uses spherical coordinates to represent a
point cloud. The local map is updated using a depth sensor, the Inertial
Measurement Unit and a registration algorithm. We propose an extension
of the Dynamic Window Approach to compute a velocity vector based on the
current local map. We propose to use an OctoMap structure to compute a
2-pass A* which provide a path which is converted to a velocity vector.
Both approaches are reactive as they only make use of local information.
The algorithms were evaluated in a simulator which offers a realistic
environment, both in terms of control and sensors. The results obtained
were also validated by running the algorithms on a real platform.},
keywords={autonomous aerial vehicles;collision avoidance;dynamic
programming;image registration;image representation;mobile robots;robot
vision;vectors;OctoMap structure;UAV;depth sensor;dynamic window
approach;inertial measurement unit;map
representation;quadcopter;reactive obstacle avoidance
algorithm;registration algorithm;unmanned aerial vehicle;velocity
vector;Collision avoidance;Path planning;Robot kinematics;Robot sensing
systems;Three-dimensional displays},
doi={10.1109/ICARCV.2016.7838803},
month={Nov},}
@INPROCEEDINGS{7838683,
author={M. D. Phung and C. H. Quach and D. T. Chu and N. Q. Nguyen and
T. H. Dinh and Q. P. Ha},
booktitle={2016 14th International Conference on Control, Automation,
Robotics and Vision (ICARCV)},
title={Automatic interpretation of unordered point cloud data for UAV
navigation in construction},
year={2016},
pages={1-6},
abstract={The objective of this work is to develop a data processing
system that can automatically generate waypoints for navigation of an
unmanned aerial vehicle (UAV) to inspect surfaces of structures like
buildings and bridges. The input includes data recorded by two 2D laser
scanners, orthogonally mounted on the UAV, and an inertial measurement
unit (IMU). To achieve the goal, algorithms are developed to process the
data collected. They are separated into three major groups: (i) the data
registration and filtering to generate a 3D model of the structure and
control the density of point clouds for data completeness enhancement;
(ii) the surface and obstacle detection to assist the UAV in monitoring
tasks; and (iii) the waypoint generation to set the flight path.
Experiments on different data sets show that the developed system is
able to reconstruct a 3D point cloud of the structure, extract its
surfaces and objects, and generate waypoints for the UAV to accomplish
inspection tasks.},
keywords={autonomous aerial vehicles;collision avoidance;inertial
systems;inspection;optical scanners;2D laser scanners;3D point
cloud;IMU;UAV navigation;bridges;buildings;data completeness
enhancement;data processing system;data registration;flight
path;inertial measurement unit;inspection tasks;obstacle
detection;unmanned aerial vehicle;unordered point cloud data;waypoint
generation;Clustering algorithms;Iterative closest point
algorithm;Lasers;Monitoring;Surface treatment;Three-dimensional
displays;Unmanned aerial vehicles},
doi={10.1109/ICARCV.2016.7838683},
month={Nov},}
@INPROCEEDINGS{7838630,
author={G. Paul and L. Liu and D. Liu},
booktitle={2016 14th International Conference on Control, Automation,
Robotics and Vision (ICARCV)},
title={A novel approach to steel rivet detection in poorly illuminated
steel structural environments},
year={2016},
pages={1-7},
abstract={It is becoming increasingly achievable for steel bridge
structures, which are normally both inaccessible and hazardous for
humans, to be inspected and maintained by autonomous robots. Steel
bridges have been traditionally constructed by securing plate members
together with rivets. However, rivets present a challenge for robots
both in terms of cleaning and surface traversal. This paper presents a
novel approach to RGB-D image and point cloud analysis that enables
rivets to be rapidly and robustly located using low cost, non-contact
sensing devices that can be easily affixed to a robot. The approach
performs classification based on: (a) high-intensity blobs in color
images, (b) the non-linear perturbations in depth images, and (c)
surface normal clusters in 3D point clouds. The predicted rivet
locations from the three classifiers are combined using a probabilistic
occupancy mapping technique. Experiments are conducted in several
different lab and real-world steel bridge environments, where there is
no external lighting infrastructure, and the sensors are attached to a
mobile platform, i.e. a climbing inspection robot. The location of
rivets within 2m of the robot can be robustly located within 10mm of
their correct location. The state of voxels can be predicted with above
95% accuracy, in approximately 1 second per frame.},
keywords={bridges (structures);control engineering computing;image
colour analysis;inspection;maintenance engineering;mobile
robots;riveting;robot vision;service robots;steel;RGB-D image;autonomous
robots;climbing inspection robot;color images;high-intensity
blobs;illuminated steel structural environments;maintenance;point cloud
analysis;steel bridge structures;steel rivet
detection;Cameras;Inspection;Robot sensing
systems;Steel;Three-dimensional displays},
doi={10.1109/ICARCV.2016.7838630},
month={Nov},}
@INPROCEEDINGS{7838685,
author={M. L. Tazir and P. Checchin and L. Trassoudaine},
booktitle={2016 14th International Conference on Control, Automation,
Robotics and Vision (ICARCV)},
title={Color-based 3D point cloud reduction},
year={2016},
pages={1-7},
abstract={Significant advances have recently been obtained in 3D sensor
domain, such as 3D lasers and the Microsoft Kinect, yielding
synchronized depth and color information. Nowadays, these sensors are
able to collect very dense point clouds. However the interest of such an
amount of information is not always justified. The point cloud sampling
procedure is an important processing step. It affects the relevance and
the accuracy of the remaining steps of this processing chain. In this
paper, we present a novel RGB-D down-sampling method. The proposed
approach is based on the use of both color information and geometry of
the scene. First, a voxelization is performed to maintain the
topological details of the scene, then for each voxel, a color based
classification of its points is done. Thereafter, only one point of each
color class is maintained and all the remaining points are removed. We
evaluate the method on the RGB-D data taken from 3D laser scanner and
compare it to the method implemented in PCL. The results show that the
new method gives better results than the state-of-the-art method. In
addition, it opens interesting perspectives to merge color and geometric
information.},
keywords={computational geometry;image classification;image colour
analysis;image sampling;optical scanners;3D laser scanner;3D sensor
domain;PCL;RGB-D data;RGB-D down-sampling method;color based
classification;color information;color-based 3D point cloud
reduction;geometric information;geometry;point cloud
sampling;synchronized depth;voxelization;Geometry;Image color
analysis;Lasers;Robot kinematics;Robot sensing systems;Three-dimensional
displays},
doi={10.1109/ICARCV.2016.7838685},
month={Nov},}
@INPROCEEDINGS{7838767,
author={F. Ning and F. Baochuan and W. Hongjie and W. Xiuhua},
booktitle={2016 14th International Conference on Control, Automation,
Robotics and Vision (ICARCV)},
title={Modeling of fuzzy comprehensive evaluation based on cloud model},
year={2016},
pages={1-5},
abstract={A fuzzy comprehensive evaluation model based on cloud model
which can be applied to evaluate air quality was established in this
paper. Firstly, the basic content of the cloud model was introduced. The
evaluation set, the weight set and the membership degree matrix based on
the cloud model were discussed, and the evaluation model was
established, which overcame the shortage of traditional fuzzy
mathematics that describes these factors with precise numbers. Secondly,
the result of the comprehensive evaluation was expressed by the cloud
model, in which fuzziness and randomness were considered. Thus, it can
avoid the large deviation resulting from maximum membership degree
principle used in traditional fuzzy comprehensive evaluation, leading to
more precious evaluation results, which correspond to human
understanding. Finally, a case study of the comprehensive evaluation
model was conducted, in which the air quality can be evaluated
effectively.},
keywords={air quality;fuzzy set theory;matrix algebra;random
processes;air quality;cloud model;evaluation set;fuzzy comprehensive
evaluation;maximum membership degree principle;membership degree
matrix;randomness;weight set;Air quality;Atmospheric modeling;Cloud
computing;Clouds;Entropy;Mathematical model;Numerical models;air
quality;cloud model;fuzzy comprehensive evaluation},
doi={10.1109/ICARCV.2016.7838767},
month={Nov},}
@ARTICLE{7805273,
author={P. P. Ray},
journal={IEEE Access},
title={Internet of Robotic Things: Concept, Technologies, and Challenges},
year={2016},
volume={4},
pages={9489-9500},
abstract={Internet of Things allow massive number of uniquely
addressable “things” to communicate with each other and transfer data
over existing internet or compatible network protocols. This paper
proposes a new concept which tackles the issues for supporting control
and monitoring activities at deployment sites and industrial
automations, where intelligent things can monitor peripheral events,
induce sensor data acquired from a variety of sources, use ad hoc,
local, and distributed “machine intelligence” to determine appropriate
course of actions, and then act to control or disseminate static or
dynamic position aware robotic things in the physical world through a
seamless manner by providing a means for utilizing them as Internet of
robotic things (IoRT). Although progressive advancements can be seen in
multi-robotic systems, robots are constantly getting enriched by easier
developmental functionalities, such vertical robotic service centric
silos are not enough for continuously and seamlessly supporting for
which they are meant. In this paper, a novel concept-IoRT is presented
that highlights architectural principles, vital characteristics, as well
as research challenges. The aim of this paper is to provide a better
understanding of the architectural assimilation of IoRT and identify
important research directions on this term.},
keywords={Internet of Things;control engineering computing;data
acquisition;multi-robot systems;position control;Internet of robotic
things;IoRT;data acquisition;data transfer;machine
intelligence;multi-robotic system;Automation;Intelligent
systems;Internet of things;Monitoring;Robot sensing systems;Internet of
things;IoRT;cloud;robotics},
doi={10.1109/ACCESS.2017.2647747},
ISSN={2169-3536},
month={},}
@ARTICLE{7829258,
author={Q. Ma and Y. Li and J. Liu and J. M. Chen},
journal={IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing},
title={Long Temporal Analysis of 3-km MODIS Aerosol Product Over East
China},
year={2017},
volume={PP},
number={99},
pages={1-13},
abstract={The 3-km resolution MODerate resolution Imaging
Spectroradiometer (MODIS) aerosol product has advantages for local-scale
aerosol monitoring over land. This study assessed the accuracy and
feasibility of the product over East China and investigated the
potential for aerosol climatology studies. The long-temporal aerosol
optical depth (AOD) of the product from 2002 to 2015 was collected and
analyzed. Validation results show good overall accuracy. The correlation
coefficient between MODIS AOD and ground measurements of the Aerosol
Robotic Network (AERONET) is 0.79, and 63.1% data points are within the
expected error range. However, in some areas, the MODIS AODs are highly
overestimated because of bias and noise. Seasonal average AOD maps
indicate the spatio-temporal distributions of aerosol. In general,
seasonal AOD values follow the sequence summer > spring > fall > winter.
Higher AODs (>1.0) usually occur over urban areas and cropland whereas
lower values coincide with forest, shrub, and grassland. A simple moving
average technique was applied to remove noise. Trend slopes were
calculated and the significances were tested. Most areas show remarkable
increases in AOD values prior to 2010, followed by significant downward
trends. Differences in MODIS AOD were calculated between 2002 and 2009
and 2010 and 2015. Despite significant downward trends after 2010, the
AODs are still higher than before 2010. The study demonstrates potential
application of the 3 km product in aerosol climatology but confirms that
it is crucial to first remove noise.},
keywords={Adaptive optics;Aerosols;Clouds;MODIS;Market research;Remote
sensing;Aerosols;atmospheric measurements;remote sensing;time series},
doi={10.1109/JSTARS.2017.2650144},
ISSN={1939-1404},
month={},}
@INPROCEEDINGS{7819179,
author={F. G. Quintanilla and O. Cardin and A. L'Anton and P. Castagna},
booktitle={2016 IEEE 14th International Conference on Industrial
Informatics (INDIN)},
title={Implementation framework for cloud-based holonic control of
cyber-physical production systems},
year={2016},
pages={316-321},
abstract={This work intends to introduce a new implementation framework,
based on the classical holonic paradigm, for partly distributing the
intelligence surrounding cyber-physical production systems in the cloud.
In order to achieve this transfer, an innovative framework is proposed
and the interface between local and cloud platform is extensively
justified and described. A performance evaluation of this framework is
achieved through its implementation on a fully automated
industrial-sized assembly line equipped with 6 axis robots. The chosen
performance indicator is the volume of data exchanged during production
in order to validate the positioning of the interface in the framework.
The results show a low volume of messages exchanged through the
interface and a distribution of the volume along time making it valuable
for further developments.},
keywords={assembling;cloud computing;industrial manipulators;position
control;production engineering computing;user interfaces;6-axis robotic
arm;CPS;cloud-based holonic control;cyber-physical production
system;implementation framework;industrial-sized assembly line;interface
positioning},
doi={10.1109/INDIN.2016.7819179},
month={July},}
@ARTICLE{7574370,
author={P. Wu and Y. Liu and M. Ye and J. Li and S. Du},
journal={IEEE Transactions on Multimedia},
title={Fast and Adaptive 3D Reconstruction With Extensively High
Completeness},
year={2017},
volume={19},
number={2},
pages={266-278},
abstract={The seed-and-expand scheme is appropriate for multiple view
stereo, since it can build dense point clouds adaptively by avoiding
unnecessary computation. However, due to the irregularity of the
algorithm, it is not suitable for parallel computing on general public
utilities (GPU). This paper is the first attempt to implement the
irregular seed-and-expand method on GPU for multiple view stereo
problems. Meanwhile, a hierarchical parallel computing architecture is
also proposed to maximize the usage of both CPU and GPU. The adaptivity
of the seed-and-expand scheme is pushed further by processing a pixel
several rounds while, in order to maintain regularity for GPU
implementation, every seed has exactly the same behavior in a single
round of optimization. The high adaptivity also improves the robustness
of the proposed method, thus aggressive matching score and a view
selection method can be used to improve the reconstruction completeness
extensively, without smearing out local details and lowering the
accuracy. Compared with the state of the art, the proposed method
achieves higher accuracy and completeness on standard datasets. The
proposed method is also very fast. It is maximally five times faster
than other methods running on a CPU and is on par with the regular depth
map-based methods on GPU, which are naturally suitable for GPU
acceleration.},
keywords={graphics processing units;image reconstruction;GPU;adaptive 3D
reconstruction;general public utilities;hierarchical parallel computing
architecture;irregular seed-and-expand method;multiple view stereo
problems;Computer architecture;Graphics processing units;Image
reconstruction;Optimization;Parallel processing;Surface
reconstruction;Three-dimensional displays;3D reconstruction;general
purpose graphics processing units (GPGPU);hierarchical parallel
computing;multiple view stereo},
doi={10.1109/TMM.2016.2612761},
ISSN={1520-9210},
month={Feb},}
@INPROCEEDINGS{7816903,
author={D. D. Nguyen and M. Abouzahir and A. Elouardi and S. Bouaziz and
B. Larnaudie},
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence
Computing, Advanced and Trusted Computing, Scalable Computing and
Communications, Cloud and Big Data Computing, Internet of People, and
Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)},
title={GPU Accelerated Robust-Laser Based Fast Simultaneous Localization
and Mapping},
year={2016},
pages={643-648},
abstract={This paper presents an OpenCL implementation of a simultaneous
localization, mapping (SLAM) algorithm. The general purpose is to
accelerate the computing on a modern GPU. We have studied the
FastSLAM2.0 algorithm based on a laser SICK as an exteroceptive sensor
for an environment perception. Implementation, evaluation results
demonstrate that a GPGPU implementation based OpenCL is an efficient way
to speed up SLAM algorithms.},
keywords={SLAM (robots);application program interfaces;graphics
processing units;FastSLAM2.0 algorithm;GPGPU implementation;GPU
accelerated robust-laser based fast simultaneous localization and
mapping;OpenCL implementation;SICK laser;SLAM algorithm;environment
perception;exteroceptive sensor;robotics;Atmospheric
measurements;Covariance matrices;Feature extraction;Graphics processing
units;Particle measurements;Simultaneous localization and
mapping;FastSLAM2.0;GPGPU;Localization;Mapping;OpenCL;Performance
Evaluation},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0107},
month={July},}
@INPROCEEDINGS{7816968,
author={A. A. Khaliq and F. Pecora and A. Saffiotti},
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence
Computing, Advanced and Trusted Computing, Scalable Computing and
Communications, Cloud and Big Data Computing, Internet of People, and
Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)},
title={Children Playing with Robots Using Stigmergy on a Smart Floor},
year={2016},
pages={1098-1122},
abstract={Reliable, safe interaction is essential when humans, robots
move in close proximity. In this paper, we present a stigmergic approach
where humans interact with robots via a smart floor. Stigmergy has been
widely studied in robotic systems, however, HRI has thus far not availed
itself of stigmergic solutions. We realize a stigmergic medium via RFID
tags embedded in the floor,, use these to enable robot navigation, human
tracking, as well as the interaction between robots, humans. The
proposed method allows to employ robots with minimal sensing,
computation capabilities. The approach relies only on the RFID sensors,
the information stored in the tags,, no internal map is required for
navigation. We design, implement a prototype game which involves a
robot, a child moving together in a shared space. The prototype
demonstrates that the approach is reliable, adheres to given safety
constraints when human, robot are moving within close proximity of each
other.},
keywords={human-robot interaction;indoor navigation;radiofrequency
identification;sensors;target tracking;HRI;RFID sensors;RFID
tags;children;human interaction;human tracking;human-robot
interaction;playing;prototype game;robot navigation;robotic
systems;smart floor;stigmergy;Cameras;Navigation;RFID tags;Robot sensing
systems;Child Robot Interaction;Human Robot Interaction;Human
following;Mobile robot navigation;RFID technology;Safe;Smart
floor;Stigmergy in robotics;reliable robot navigation},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0172},
month={July},}
@INPROCEEDINGS{7813127,
author={J. Joshi and D. Kurian and S. Bhasin and S. Mukherjee and P.
Awasthi and S. Sharma and S. Mittal},
booktitle={2016 International Conference on Cybernetics, Robotics and
Control (CRC)},
title={Health Monitoring Using Wearable Sensor and Cloud Computing},
year={2016},
pages={104-108},
abstract={Obesity and overweight has been a great cause of death in 21st
Century. In era of innovation in technology, people are becoming less
active to complete their daily work and more use of Automation
techniques are used in different scenarios. With the help of technology
we try to cope up with obesity problem and make people healthier. We use
WBAN and cloud computing to address this problem. It uses small
electronic devices that are incorporated with human to supervise health
related problems such blood pressure, sugar level, obesity etc. Wearable
computing help in health monitoring of patient. WBANs promise
unobtrusive ambulatory health monitoring for a long period of time and
provide real-time updates of the patient's status to the physician. In
this paper we integrate Wireless Sensor Network with the Internet of
Things. We have implemented a CoAP protocol which is used in order to
provide web service based WBAN prototype.},
keywords={Web services;body sensor networks;cloud computing;medical
computing;patient monitoring;protocols;wearable computers;CoAP
protocol;WBAN;Web service;cloud computing;small electronic
devices;unobtrusive ambulatory health monitoring;wearable
computing;wearable sensor;Biomedical monitoring;Body area
networks;Monitoring;Obesity;Wireless communication;Wireless sensor
networks;Body Sensor Network;Iris;Smart Garments;WSN;Wearable
Computing;Wearable Sensor Network;e-health},
doi={10.1109/CRC.2016.031},
month={Aug},}
@INPROCEEDINGS{7810021,
author={Chengliang Liang and Ning Ye and R. Malekian and Ruchuan Wang},
booktitle={2016 2nd International Symposium on Agent, Multi-Agent
Systems and Robotics (ISAMSR)},
title={The hybrid encryption algorithm of lightweight data in cloud
storage},
year={2016},
pages={160-166},
abstract={In this paper, we proposed a hybrid encryption algorithm in
order to solve the security problems on lightweight data on the cloud
storage service. This paper first improves the RSA algorithm in the
cloud environment by increasing the length of the RSA key to an extent
that it can quickly generate big primes. Then, we merge AES and RSA
algorithms on the basis of the improved RSA algorithm. This is called a
hybrid encryption algorithm which is suitable for security of the
lightweight data in cloud storage environment in order to further
enhance the confidentiality of data in the cloud. The experimental
result shows that the hybrid encryption algorithm has advantages of fast
encryption and decryption, more secure, and it is also able to settle
the security problems of users' lightweight data in a certain extent.},
keywords={Access control;Algorithm design and analysis;Cloud
computing;Encryption;Public key;AES and RSA algorithms;Cloud
storage;hybrid encryption algorithm;lightweight},
doi={10.1109/ISAMSR.2016.7810021},
month={Aug},}
@INPROCEEDINGS{7806217,
author={N. Joy and K. Chandrasekaran and A. Binu},
booktitle={2016 IEEE Distributed Computing, VLSI, Electrical Circuits
and Robotics (DISCOVER)},
title={Energy aware SLA and green cloud federations},
year={2016},
pages={7-11},
abstract={Cloud computing has been an emerging research topic since
2007. The online services are delivered as pay-as-you-use in Cloud
computing. The customers need not be in a long term contract with Cloud
Service Providers (CSPs). Service level agreements (SLAs) are agreements
signed between a cloud service provider and customers. Energy aware SLAs
extend the existing SLA agreements in order to include energy and carbon
aware parameters. The efficiency and availability of the system are not
disturbed when certain jobs are relaxed in order to obtain high energy
consumption. In order to increase the energy consumption further we
incorporated the concept of cloud federations in which a group of CSPs,
mutually agree to make use of their resources to execute the VMs of
other CSPs. An algorithm based on the amount of energy consumed by
different CSPs is being readily implemented. This algorithm grant a
group of CSPs to jointly set the federations in a manner to distribute
their work equally so as to reduce the energy consumed. The results from
the above approach show that it can be considered as a hopeful solution
to the problem of reducing the energy cost even though there are a few
challenges during implementation.},
keywords={cloud computing;contracts;energy conservation;power aware
computing;virtual machines;CSP;VM;cloud computing;cloud service
providers;energy aware SLA;energy consumption reduction;energy cost
reduction;green cloud federations;service level agreements;Carbon
dioxide;Cloud computing;Computational modeling;Electronic mail;Energy
consumption;Green products;Organizations;Cloud Computing;Cloud
Federation;Energy Aware SLA;Service Level Agreements},
doi={10.1109/DISCOVER.2016.7806217},
month={Aug},}
@INPROCEEDINGS{7806238,
author={P. K. Rahulkrishna and R. Eshwari and N. J. S. Harsha and R.
Hegde},
booktitle={2016 IEEE Distributed Computing, VLSI, Electrical Circuits
and Robotics (DISCOVER)},
title={Design and development of remote load monitoring suitable for
non-residential loads through wireless data transmission},
year={2016},
pages={35-40},
abstract={In this paper, communication strategy of dynamic data
transmission from a sensor to the cloud storage is discussed. Simulation
will be done in arduino platform and net connectivity will be done
through GSM module interfacing. The work was inspired by the idea of the
approach on Internet of things and to develop a solution to minimize the
excessive power usage and thereby minimizing the power consumption cost.
Solution is been proposed for monitoring the power utilization. The
solution can be collaborated in transmission of any sensor data.
Currently, in this implementation, dynamic data is being received by
Arduino from a delphino DSP, and GSM module is used as the gateway to
transmit the data into cloud storage. This data in cloud storage is been
analysed using user interfaces and thus excessive power consumption can
be monitored. Brief overview of the proposed solution at the stage of
communication is also discussed.},
keywords={Internet of Things;cellular radio;cloud computing;load
management;Arduino platform;GSM module interfacing;Internet of
Things;cloud storage;excessive power consumption;remote load
monitoring;user interfaces;wireless data transmission;Cloud
computing;GSM;Google;Logic gates;Monitoring;Power demand;Uniform
resource locators;Arduino;Delphino;GSM module;cloud storage;dynamic
data;power-monitoring},
doi={10.1109/DISCOVER.2016.7806238},
month={Aug},}
@INPROCEEDINGS{7806111,
author={A. Rubinstein and T. Erez},
booktitle={2016 IEEE International Conference on the Science of
Electrical Engineering (ICSEE)},
title={Autonomous robot for tunnel mapping},
year={2016},
pages={1-4},
abstract={We present a novel turnkey system for autonomous tunnel
mapping called LiTANK - LiDAR TANK. The system depends entirely on LiDAR
sensor readings, without the use of other sensors such as GPS or
odometry. Using point cloud registration to perform real-time
simultaneous localization and mapping (SLAM) the pose and the
surrounding's mapping are estimated. Based on the pose and mapping an
autonomous navigation algorithm is applied. Our system was tested inside
a hallway with similar characteristics to a tunnel. It navigated
autonomously to a selected finishing position with great accuracy and
while mapping large unknown area. The final results prove our system
concept.},
keywords={SLAM (robots);image registration;mobile robots;optical
radar;optical sensors;tunnels;LiDAR sensor readings;LiDAR
tank;LiTANK;autonomous navigation;autonomous robot;finishing
position;pose mapping;simultaneous localization and mapping;surrounding
mapping;tunnel mapping;Finishing;Laser radar;Navigation;Simultaneous
localization and mapping;Three-dimensional displays;Autonomous
navigation;BFS;D∗;ICP;Kinect fusion;LiDAR;Point Cloud;Point Cloud
Registration;Robotic System;SLAM;Tunnel mapping},
doi={10.1109/ICSEE.2016.7806111},
month={Nov},}
@INPROCEEDINGS{7806245,
author={S. Raghavendra and K. Nithyashree and C. M. Geeta and R. Buyya
and K. R. Venugopal and S. S. Iyengar and L. M. Patnaik},
booktitle={2016 IEEE Distributed Computing, VLSI, Electrical Circuits
and Robotics (DISCOVER)},
title={FRORSS: Fast result object retrieval using similarity search on
cloud},
year={2016},
pages={107-112},
abstract={This paper involves a cloud computing environment in which the
data owner out sources the similarity search service to a third party
service provider. The user provides an example query to the server to
retrieve similar data. Privacy of the outsourced data is important
because they may be sensitive, valuable or confidential data. The data
should be made available to the authorized client/client groups, but not
to be revealed to the service provider in which the data is stored.
Given this scenario, the paper presents a technique called FRORSS which
has build phase, data transformation and search phase. The build phase
is about uploading the data; the data transformation phase transforms
the data before submitting it to the service provider for similarity
queries on the transformed data; search phase involves searching similar
object with respect to query object. Experiments have been carried out
on real data sets which exhibits that the proposed work is capable of
providing privacy and achieving accuracy at a lower value of result
measure in comparision with FDH [1].},
keywords={cloud computing;data privacy;query processing;FRORSS;build
phase;cloud computing;confidential data;data transformation
phase;example query;fast result object retrieval;outsourced data
privacy;search phase;similar data retrieval;similarity
queries;similarity search;third party service provider;Cloud
computing;Cryptography;Data privacy;Extraterrestrial measurements;Search
problems;Servers;Cloud computing;Data transformation;FRORSS;Result
Measure;Similarity search},
doi={10.1109/DISCOVER.2016.7806245},
month={Aug},}
@INPROCEEDINGS{7806260,
author={S. K. Routray and M. K. Jha and A. Javali and L. Sharma and S.
Sarkar and T. Ninikrishna},
booktitle={2016 IEEE Distributed Computing, VLSI, Electrical Circuits
and Robotics (DISCOVER)},
title={Software defined networking for optical networks},
year={2016},
pages={133-137},
abstract={Optical networks are evolving with the emerging advanced
technologies. Their sizes and functionalities too grow with every
passing year. All these complexities cannot be handled through the
traditional framework for network control and management. Software
defined networking (SDN) has been proposed for the control and
management of networks. SDN provides several advantages for the control,
operation and management of large networks. It provides flexibility and
agility at every level of the network. In this paper, we present the
utilities of SDN for optical networks. We also discuss the issues
related to the implementation and benefits of SDN in optical networks.},
keywords={optical fibre networks;software defined networking;network
control;network management;network operation;optical fibers;optical
networks;software defined networking;Bandwidth;Cloud
computing;High-speed optical techniques;Optical fiber networks;Optical
fibers;Optical switches;SDN for optical networks;Software defined
networking;benefits of SDN;network function virtualization;optical
networks},
doi={10.1109/DISCOVER.2016.7806260},
month={Aug},}
@INPROCEEDINGS{7803397,
author={B. Ridge and A. Ude},
booktitle={2016 IEEE-RAS 16th International Conference on Humanoid
Robots (Humanoids)},
title={Action-grounded surface geometry and volumetric shape feature
representations for object affordance prediction},
year={2016},
pages={1022-1028},
abstract={Many 3D feature descriptors have been developed over the years
to solve problems that require the representation of object shape, e. g.
object recognition or pose estimation, but comparatively few have been
developed specifically to tackle the problem of object affordance
learning, a domain where the interaction between action parameters and
sensory features play a crucial role. In previous work, we introduced a
feature descriptor that divided an object point cloud into
coarse-grained cells, derived simple features from each of the cells,
and grounded those features with respect to a reference frame defined by
a pushing action. We also compared this action-grounded descriptor to an
equivalent non-action-grounded descriptor coupled with action features
in a push affordance classification task and established that the
action-grounded encoding can provide improved performance. In this
paper, we investigate modifying more well-established 3D shape
descriptors based on surface geometry, in particular the Viewpoint
Feature Histogram (VFH), such that they are action-grounded in a similar
manner, compare them to volumetric octree-based representations, and
conclude that having multi-scaled representations in which parts at each
scale can be referenced with respect to each other may be a crucial
component in action-grounded affordance learning.},
keywords={computational geometry;image classification;learning
(artificial intelligence);octrees;robot vision;3D feature
descriptors;VFH;action-grounded affordance learning;action-grounded
encoding;action-grounded surface geometry;coarse-grained
cells;multiscaled representations;nonaction-grounded descriptor;object
affordance learning;object affordance prediction;object point
cloud;object shape representation;surface geometry;viewpoint feature
histogram;volumetric feature shape representations;volumetric
octree-based representations;Geometry;Histograms;Octrees;Robot
kinematics;Shape;Three-dimensional displays},
doi={10.1109/HUMANOIDS.2016.7803397},
month={Nov},}
@INPROCEEDINGS{7803419,
author={A. Yamaguchi and C. G. Atkeson},
booktitle={2016 IEEE-RAS 16th International Conference on Humanoid
Robots (Humanoids)},
title={Stereo vision of liquid and particle flow for robot pouring},
year={2016},
pages={1173-1180},
abstract={We explore stereo vision for recognizing liquid and particle
flow as 3D points (a point cloud). In our pouring research [1], we
noticed that we could detect liquid flow using optical flow detection,
especially with the Lucas-Kanade method [2]. In this paper we extend
this idea so that we can reconstruct 3D liquid flow from a stereo camera
in order to learn dynamical models of flow. Such dynamical models would
be useful to reason about pouring behaviors. We demonstrate our method
in pouring various materials: water, coke, jelly, dish liquid, and
creamer powder. The results show that our method could detect the 3D
flow as a point cloud, and they captured the actual flow phenomenon. We
also show that our method works in a robot pouring scenario.
Accompanying video: https://youtu.be/2oFjVJwXhKs.},
keywords={cameras;image sequences;manipulators;stereo image
processing;3D liquid flow;3D points;Lucas-Kanade method;coke;creamer
powder;dish liquid;dynamical models;jelly;liquid flow detection;liquid
flow recognition;optical flow detection;particle flow recognition;robot
pouring behaviors;stereo camera;stereo
vision;water;Cameras;Containers;Liquids;Optical imaging;Robot
kinematics;Three-dimensional displays},
doi={10.1109/HUMANOIDS.2016.7803419},
month={Nov},}
@INPROCEEDINGS{7796270,
author={D. McLoughlin and E. O'Connell and W. Elgenaidi and J. Coleman
and T. Newe},
booktitle={2016 10th International Conference on Sensing Technology
(ICST)},
title={Review and evaluation of WSN simulation tools in a cloud based
environment},
year={2016},
pages={1-6},
abstract={A wireless sensor network (WSN) is a collection of sensor
nodes which can monitor the physical activities and attributes of a
whole spectrum of objects and environments. The purpose of collecting
this data and processing it in a meaningful manner requires that it adds
significant value to the end user. WSN are deployed ever more frequently
and applications vary from providing an early alert system on the
activity of a volcano, to controlling the movements of livestock so that
farming output can be maximized using virtual fencing. There are many
challenges facing wireless sensor networks, with one being found in the
uniqueness of each application, and challenges arise when WSN developers
try to adequately test such applications when they are deployed with
varying numbers of nodes, in all ranges of environmental conditions. To
implement such systems in hardware involves a huge financial investment
and from a development perspective prevents the developer from changing
hardware platforms or communications protocols throughout the testing
phase without incurring further financial hardship. Due to this fact,
tools which can accurately simulate the behavior and performance of real
world WSN applications are in high demand and a crucial aspect in the
continued growth of WSN in the Internet of Things (IoT). This paper
presents a cloud based system which makes various simulation platforms
available to multiple users who wish to test WSN routing protocols or
the hardware constraints of the various WSN platforms. This simulation
testbed is hosted on a Dell PowerEdge R720 server in the communications
laboratory in the University of Limerick and it is made available to
users through the use of a virtual machine. This offers users access to
many of the popular WSN simulation environments with vast computing
resources and storage but at far reduced costs for protocol simulation.},
keywords={Internet;Internet of Things;cloud computing;routing
protocols;wireless sensor networks;Dell PowerEdge R720 server;Internet
of Things;IoT;University of Limerick;WSN;WSN simulation tool;cloud based
environment;communications protocol;cost reduction;data
collection;farming;financial investment;livestock;routing
protocol;sensor node collection;virtual fencing;virtual machine;wireless
sensor network;Cloud computing;Computational
modeling;Hardware;Protocols;Servers;Wireless sensor
networks;WSN;cloud;simulation;testbed;virtual machine},
doi={10.1109/ICSensT.2016.7796270},
month={Nov},}
@INPROCEEDINGS{7795953,
author={C. Premebida and L. Garrote and A. Asvadi and A. P. Ribeiro and
U. Nunes},
booktitle={2016 IEEE 19th International Conference on Intelligent
Transportation Systems (ITSC)},
title={High-resolution LIDAR-based depth mapping using bilateral filter},
year={2016},
pages={2469-2474},
abstract={High resolution depth-maps, obtained by upsampling sparse
range data from a 3D-LIDAR, find applications in many fields ranging
from sensory perception to semantic segmentation and object detection.
Upsampling is often based on combining data from a monocular camera to
compensate the low-resolution of a LIDAR. This paper, on the other hand,
introduces a novel framework to obtain dense depth-map solely from a
single LIDAR point cloud; which is a research direction that has been
barely explored. The formulation behind the proposed depth-mapping
process relies on local spatial interpolation, using sliding-window
(mask) technique, and on the Bilateral Filter (BF) where the variable of
interest, the distance from the sensor, is considered in the
interpolation problem. In particular, the BF is conveniently modified to
perform depth-map upsampling such that the edges (foreground-background
discontinuities) are better preserved by means of a proposed method
which influences the range-based weighting term. Other methods for
spatial upsampling are discussed, evaluated and compared in terms of
different error measures. This paper also researches the role of the
mask's size in the performance of the implemented methods. Quantitative
and qualitative results from experiments on the KITTI Database, using
LIDAR point clouds only, show very satisfactory performance of the
approach introduced in this work.},
keywords={Cameras;Estimation;Interpolation;Laser radar;Spatial
resolution;Three-dimensional displays;Vehicles},
doi={10.1109/ITSC.2016.7795953},
month={Nov},}
@INPROCEEDINGS{7795718,
author={A. Asvadi and P. Girão and P. Peixoto and U. Nunes},
booktitle={2016 IEEE 19th International Conference on Intelligent
Transportation Systems (ITSC)},
title={3D object tracking using RGB and LIDAR data},
year={2016},
pages={1255-1260},
abstract={Object tracking is one of the key components of the perception
system of autonomous cars and ADASs. With tracking, an ego-vehicle can
make a prediction about the location of surrounding objects in the next
time epoch and plan for next actions. Object tracking algorithms
typically rely on sensory data (from RGB cameras or LIDAR). In fact, the
integration of 2D-RGB camera images and 3D-LIDAR data can provide some
distinct benefits. This paper proposes a 3D object tracking algorithm
using a 3D-LIDAR, an RGB camera and INS (GPS/IMU) sensors data by
analyzing sequential 2D-RGB, 3D point-cloud, and the ego-vehicle's
localization data and outputs the trajectory of the tracked object, an
estimation of its current velocity, and its predicted location in the 3D
world coordinate system in the next time-step. Tracking starts with a
known initial 3D bounding box for the object. Two parallel mean-shift
algorithms are applied for object detection and localization in the 2D
image and 3D point-cloud, followed by a robust 2D/3D Kalman filter based
fusion and tracking. Reported results, from both quantitative and
qualitative experiments using the KITTI database demonstrate the
applicability and efficiency of the proposed approach in driving
environments.},
keywords={Global Positioning System;Kalman filters;cameras;image
filtering;image fusion;image sensors;object tracking;optical
radar;2D-RGB camera imaging;3D object tracking algorithm;3D point-cloud
analysis;3D world coordinate system;3D-LIDAR data;ADAS perception
system;GPS-IMU;INS;KITTI database;autonomous car perception
system;current velocity estimation;ego-vehicle localization data
analysis;parallel mean-shift algorithm;robust 2D-3D Kalman filter;sensor
data;Cameras;Image color analysis;Laser radar;Object
tracking;Three-dimensional displays;Two dimensional displays},
doi={10.1109/ITSC.2016.7795718},
month={Nov},}
@INPROCEEDINGS{7795979,
author={Z. Chen and W. Yuan and M. Yang and C. Wang and B. Wang},
booktitle={2016 IEEE 19th International Conference on Intelligent
Transportation Systems (ITSC)},
title={SVM based people counting method in the corridor scene using a
single-layer laser scanner},
year={2016},
pages={2632-2637},
abstract={People counting plays an important role in public safety,
building automation control and other data analysis like consumer
behaviors, passenger management and so on. Among all the public places,
the corridor is one of the most common scenes. Therefore, this paper
proposes a people counting system which is applied in the corridor. The
proposed system counts the amount of the passing pedestrians as well as
detects the direction of each pedestrian. A mirror reflection device is
designed to achieve a double-plane scanning by a single lidar, which
doesn't only double the scanning data and make use of the wasted
scanning angle, but also realize the direction detection by judging
which scanning plane the pedestrian has passed first. A novel shape
descriptor is used in the sliding window algorithm, and a support vector
machine is trained as a classifier. In the proposed algorithm,
head-shoulder feature in the pedestrian point cloud is detected to get
the point cloud of every pedestrian. In the experiment, actual
pedestrian data are collected to confirm the proposed system. The
proposed method shows a better performance than the comparison methods.},
keywords={computational geometry;image classification;object
detection;optical radar;optical scanners;pedestrians;shape
recognition;support vector machines;SVM based people counting
method;classifier;corridor scene;double-plane scanning;head-shoulder
feature;mirror reflection device;pedestrian passing;pedestrian point
cloud;shape descriptor;single lidar;single-layer laser scanner;sliding
window algorithm;Feature extraction;Laser
radar;Lasers;Mirrors;Reflection;Support vector
machines;Three-dimensional displays},
doi={10.1109/ITSC.2016.7795979},
month={Nov},}
@INPROCEEDINGS{7795532,
author={Lindong Guo and Ming Yang and Bing Wang and Chunxiang Wang},
booktitle={2016 IEEE 19th International Conference on Intelligent
Transportation Systems (ITSC)},
title={Occupancy grid based urban localization using weighted point cloud},
year={2016},
pages={60-65},
abstract={Localization is considered as a key capability for autonomous
vehicles act in urban environments. Though have been proved to be able
to perform convictive results, localization methods using neither laser
scanners nor vision sensors could achieve the goal about balancing
between accuracy and cost. In this paper, an occupancy grid based
localization framework is presented in order to obtain a precise
positioning result with relatively low-cost sensor configuration in
large scale urban environment. The proposed approach takes a prebuilt
global grid map as prior knowledge for localization. Model based feature
extraction method is introduced to provide laser points classification,
with each extracted point allocated a specified weight to describe local
characteristic. The prior grid map is generated from weighted point
cloud to be able to describe the local metric features such as curbs and
building facades. Localization function is then carried out with a
weight point based maximum likelihood matching method to determine the
correspondence between local point cloud and the reference grid map.
There are also position initialization and reference map management
modules to make the framework more practical and reliable. In the end,
the proposed approach has been validated by promising experimental
results with long distance tests in large urban environments.},
keywords={Global Positioning System;distance measurement;mobile
robots;optical scanners;road vehicles;sensor fusion;autonomous
vehicle;global grid map;occupancy grid;position initialization;precise
position;reference map management;urban localization;weighted point
cloud;Feature extraction;Laser modes;Roads;Sensors;Three-dimensional
displays;Vehicles},
doi={10.1109/ITSC.2016.7795532},
month={Nov},}
@INPROCEEDINGS{7785986,
author={S. Cobialca},
booktitle={2016 IEEE Congreso Argentino de Ciencias de la Inform
#225;tica y Desarrollos de Investigaci #243;n (CACIDI)},
title={Robotics as a teaching aid in university courses},
year={2016},
pages={1-4},
abstract={We will be discussing how using robotics in two university
courses helped students relate the topics learned in class to
applications in real life of those topics. The changes introduced in the
course structure created a different atmosphere in class and the way the
students faced the newly learned topics.},
keywords={control engineering education;educational
courses;robots;teaching;robotics;teaching aid;university courses;Data
models;Databases;Education;Hardware;Queueing analysis;Robots;Tag
clouds;learning experience;robotics;teaching;university},
doi={10.1109/CACIDI.2016.7785986},
month={Nov},}
@INPROCEEDINGS{7784101,
author={L. Xu and H. Luo},
booktitle={2016 IEEE International Conference on Real-time Computing and
Robotics (RCAR)},
title={Towards autonomous tracking and landing on moving target},
year={2016},
pages={620-628},
abstract={The battery capacity of Unmanned Aerial Vehicle (UAV) is the
main limitation, but with the rapid growth of UAV deployment in both
military and civilian application, there is an urgent need to
development the reliable and automated landing procedure. This paper is
aim to propose a basic framework for autonomous landing on moving target
for the Vertical Take-off and Landing (VTOL) UAVs. The VTOL vehicle is
assumed to equip with a Global Navigation Satellite System (GNSS) system
and a stereo vision system which could generate the point cloud within
20 meters. We applied a particle filter based Visual Servo in the UAV
vision system to detect and track the moving the target at real time. We
also combine the inertial measurement unit (IMU) data with the stereo
vision based visual odometry to make the relative accurate pose
estimation. The relative position, orientation and velocity to the
landing area on the moving carrier is obtained by a modified optical
flow method. The control method used in this framework combined tracking
and approaching base on the range distance. We has applied our proposed
framework on both simulation and landing task on a moving vehicle, and
the result shows the efficiency and extended ability of our framework.},
keywords={autonomous aerial vehicles;distance measurement;image motion
analysis;image sequences;pose estimation;robot vision;satellite
navigation;stereo image processing;GNSS;IMU;UAV vision system;VTOL
vehicle;automated landing procedure;autonomous landing;autonomous
tracking;civilian application;global navigation satellite
system;inertial measurement unit data;landing area;military
application;modified optical flow method;moving target;particle filter
based visual servo;stereo vision based visual odometry;stereo vision
system;unmanned aerial vehicle battery capacity;vertical
take-off-and-landing UAV;Computer vision;Image motion analysis;Optical
filters;Particle filters;Target tracking;Unmanned aerial
vehicles;Visualization},
doi={10.1109/RCAR.2016.7784101},
month={June},}
@INPROCEEDINGS{7784290,
author={F. Gao and S. Shen},
booktitle={2016 IEEE International Symposium on Safety, Security, and
Rescue Robotics (SSRR)},
title={Online quadrotor trajectory generation and autonomous navigation
on point clouds},
year={2016},
pages={139-146},
abstract={In this paper, we present a framework for online generation of
safe trajectories directly on point clouds for autonomous quadrotor
flight. Considering a quadrotor operateing in unknown environments, we
use a 3-D laser range finder for state estimation and simultaneously
build a point cloud map of the environment. Based on the incrementally
built point cloud map, we utilize the property of the fast nearest
neighbor search in KD-tree and adopt the sampling-based path finding
method to generate a flight corridor with safety guarantee in 3-D space.
A trajectory generation method formulated in quadratically constrained
quadratic programming (QCQP) is then used to generate trajectories that
constrained entirely within the corridor. Our method runs onboard within
100 milliseconds, making it suitable for online re-planning. We
integrate the proposed planning method with laser-based state estimation
and mapping modules, and demonstrate the autonomous quadrotor flight in
unknown indoor and outdoor environments.},
keywords={aerospace control;helicopters;laser ranging;path
planning;quadratic programming;trajectory control;3D laser range
finder;KD-tree;QCQP;autonomous navigation;autonomous quadrotor
flight;cloud map;flight corridor;nearest neighbor search;online
quadrotor trajectory generation;online replanning;outdoor
environments;point cloud map;point clouds;quadratically constrained
quadratic programming;safe trajectories;Data
structures;Navigation;Safety;Sonar measurements;State
estimation;Three-dimensional displays;Trajectory},
doi={10.1109/SSRR.2016.7784290},
month={Oct},}
@INPROCEEDINGS{7784091,
author={H. Zhou and D. Zhou and K. Peng and R. Guo and Y. Liu},
booktitle={2016 IEEE International Conference on Real-time Computing and
Robotics (RCAR)},
title={Seamless stitching of large area UAV images using modified camera
matrix},
year={2016},
pages={561-566},
abstract={In this study, we present a novel, robust, and accurate method
to register multiple images captured by an unmanned aerial vehicle (UAV)
with modified camera matrixes, and then stitch UAV images to a seamless
one. Firstly, the camera parameters, camera poses and sparse 3D points
are solved using feature correspondence and SFM. Secondly, all images
are registered using sparse 3D point cloud and modified camera matrixes.
Thirdly, the stitching plane is determined by performing a view
selection. At last, all images are stitched using multi-band blending
based on graph-cut. Experiments and comparisons demonstrate the accuracy
and feasibility of our approach.},
keywords={autonomous aerial vehicles;cameras;cloud computing;image
capture;matrix algebra;pose estimation;robot vision;3D point
cloud;SFM;camera parameters;camera poses;large area UAV images;modified
camera matrix;multi-band blending;seamless stitching;sparse 3D
points;unmanned aerial vehicle;Conferences;Decision support
systems;Handheld computers;Real-time systems;Robots;Support vector
machines},
doi={10.1109/RCAR.2016.7784091},
month={June},}
@INPROCEEDINGS{7781953,
author={H. Sobreira and L. Rocha and C. Costa and J. Lima and P. Costa
and A. P. Moreira},
booktitle={2016 International Conference on Autonomous Robot Systems and
Competitions (ICARSC)},
title={2D Cloud Template Matching - A Comparison between Iterative
Closest Point and Perfect Match},
year={2016},
pages={53-59},
abstract={Self-localization of mobile robots in the environment is one
of the most fundamental problems in the robotics field. It is a complex
and challenging problem due to the high requirements of autonomous
mobile vehicles, particularly with regard to algorithms accuracy,
robustness and computational efficiency. In this paper we present the
comparison of two of the most used map-matching algorithm, which are the
Iterative Closest Point and the Perfect Match. This category of
algorithms are normally applied in localization based on natural
landmarks. They were compared using an extensive collection of metrics,
such as accuracy, computational efficiency, convergence speed, maximum
admissible initialization error and robustness to outliers in the robots
sensors data. The test results were performed in both simulated and real
world environments.},
keywords={mobile robots;robust control;vehicles;2D cloud template
matching;autonomous mobile vehicles;iterative closest point;map-matching
algorithm;mobile robots;natural landmarks;robustness;Iterative closest
point algorithm;Robot sensing systems;Robustness;Service robots;Two
dimensional displays;Autonomous Robots;Iterative Closest
Point;Map-Matching;Perfect Match;Self-Localization},
doi={10.1109/ICARSC.2016.13},
month={May},}
@INPROCEEDINGS{7784014,
author={Y. Lin and S. Song and M. Q. H. Meng},
booktitle={2016 IEEE International Conference on Real-time Computing and
Robotics (RCAR)},
title={The implementation of augmented reality in a robotic
teleoperation system},
year={2016},
pages={134-139},
abstract={The Wheelchair Mounted Robotic Manipulators (WMRM) are
prevalently used to help the elderly or people with spine injury to
conduct Activities of Daily Living (ADL). In this project, we extended
its working range by making such system teleoperatable. In addition, we
enhanced its efficiency by providing more natural and intuitive method
for manipulation. With Augmented Reality (AR) technology, our system
could present the reconstructed 3D scene of remote area at local station
in an interactive way. Besides adjusting perspective and scale of the
display, the gesture operating on virtual object would also be converted
into control commands of robotic arm in real-time. Series of experiments
have been carried out to demonstrate the function of our system.},
keywords={augmented reality;geriatrics;handicapped aids;image
reconstruction;manipulators;medical computing;3D scene
reconstruction;ADL;AR;WMRM;activities-of-daily living;augmented reality
technology;gesture operation;robotic arm control commands;robotic
teleoperation system;virtual object;wheelchair mounted robotic
manipulators;Cameras;Grippers;Manipulators;Mobile robots;Robot sensing
systems;Three-dimensional displays;3D Reconstruction;Augmented
Reality;Point Cloud;Teleoperation;Wheelchair Mounted Robotic Manipulator},
doi={10.1109/RCAR.2016.7784014},
month={June},}
@INPROCEEDINGS{7784298,
author={E. Mendes and P. Koch and S. Lacroix},
booktitle={2016 IEEE International Symposium on Safety, Security, and
Rescue Robotics (SSRR)},
title={ICP-based pose-graph SLAM},
year={2016},
pages={195-200},
abstract={Odometry-like localization solutions can be built upon Light
Detection And Ranging (LIDAR) sensors, by sequentially registering the
point clouds acquired along a robot trajectory. Yet such solutions
inherently drift over time: we propose an approach that adds a graphical
model layer on top of such LIDAR odometry layer, using the Iterative
Closest Points (ICP) algorithm for registration. Reference frames called
keyframes are defined along the robot trajectory, and ICP results are
used to build a pose graph, that in turn is used to solve an
optimization problem that provides updates for the keyframes upon loop
closing, enabling the correction of the path of the robot and of the map
of the environment. We present in details the configuration used to
register data from the Velodyne High Definition LIDAR (HDL), and a
strategy to build local maps upon which current frames are registered,
either when discovering new areas or revisiting previously mapped areas.
Experiments show that it is possible to build the graph using data from
ICP and that the loop closings in the graph level reduces the overall
drift of the system.},
keywords={SLAM (robots);graph theory;iterative methods;mobile
robots;optical radar;optimisation;ICP-based pose-graph SLAM;LIDAR
odometry layer;Velodyne high definition LIDAR;graphical model
layer;iterative closest points algorithm;light detection and ranging
sensors;odometry-like localization solutions;optimization problem;robot
trajectory;Iterative closest point algorithm;Laser
radar;Optimization;Robot sensing systems;Three-dimensional displays},
doi={10.1109/SSRR.2016.7784298},
month={Oct},}
@INPROCEEDINGS{7784325,
author={C. Berger and M. Wzorek and J. Kvarnström and G. Conte and P.
Doherty and A. Eriksson},
booktitle={2016 IEEE International Symposium on Safety, Security, and
Rescue Robotics (SSRR)},
title={Area coverage with heterogeneous UAVs using scan patterns},
year={2016},
pages={342-349},
abstract={In this paper we consider a problem of scanning an outdoor
area with a team of heterogeneous Unmanned Air Vehicles (UAVs) equipped
with different sensors (e.g. LIDARs). Depending on the availability of
the UAV platforms and the mission requirements there is a need to either
minimise the total mission time or to maximise certain properties of the
scan output, such as the point cloud density. The key challenge is to
divide the scanning task among UAVs while taking into account the
differences in capabilities between platforms and sensors. Additionally,
the system should be able to ensure that constraints such as limit on
the flight time are not violated. We present an approach that uses an
optimisation technique to find a solution by dividing the area between
platforms, generating efficient scan trajectories and selecting flight
and scanning parameters, such as velocity and flight altitude. This
method has been extensively tested on a large set of randomly generated
scanning missions covering a wide range of realistic scenarios as well
as in real flights.},
keywords={aerospace control;autonomous aerial vehicles;mobile
robots;optical radar;optimisation;sensors;telerobotics;trajectory
control;UAV;area coverage;heterogeneous unmanned air
vehicles;optimisation technique;point cloud density;scan output;scan
patterns;scan trajectories;scanning parameters;selecting flight;Laser
radar;Optimization;Robots;Sensors;Three-dimensional
displays;Trajectory;Unmanned aerial vehicles},
doi={10.1109/SSRR.2016.7784325},
month={Oct},}
@INPROCEEDINGS{7784057,
author={G. Yang and F. Chen and W. Chen and M. Fang and Y. H. Liu and L.
Li},
booktitle={2016 IEEE International Conference on Real-time Computing and
Robotics (RCAR)},
title={A new algorithm for obstacle segmentation in dynamic environments
using a RGB-D sensor},
year={2016},
pages={374-378},
abstract={Obstacle detection is one of the main components of the
control system and navigation of autonomous vehicles. This paper
presents a novel approach to obstacle detection and segmentation with a
RGB-D sensor. Different from the traditional approaches which only
detect whether there exist obstacles, our approach not only can detect
the obstacles but also can distinguish between dynamic obstacles and
static obstacles. Base on the information received by the kinect sensor,
the mobile robot can choose different avoidance strategy when facing
different kinds of obstacles intelligently. First, we get a 3D point
cloud from the depth image and compute the height of each point from
ground plane which is estimated during a calibration step. In this step,
we can discriminate which point belongs to obstacles. Then we use a
batch of depth images to get the dynamic objects of each image. Finally,
the obstacle map is an orthographic projection of these obstacle points
along the normal to the ground plane. Experimental results carried out
with a mobile platform in an indoor environment have shown that the
system can detect and segment stationary and moving obstacles.},
keywords={calibration;collision avoidance;image colour analysis;image
segmentation;image sensors;mobile robots;object detection;robot
vision;RGB-D sensor;autonomous vehicle control system;autonomous vehicle
navigation;calibration;dynamic environments;dynamic obstacle
avoidance;indoor environment;kinect sensor;mobile robot;obstacle
detection;obstacle segmentation;orthographic projection;static obstacle
avoidance;Cameras;Heuristic algorithms;Image edge detection;Image
segmentation;Robot sensing systems;Three-dimensional displays},
doi={10.1109/RCAR.2016.7784057},
month={June},}
@INPROCEEDINGS{7759489,
author={G. Best and J. Faigl and R. Fitch},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Multi-robot path planning for budgeted active perception with
self-organising maps},
year={2016},
pages={3164-3171},
abstract={We propose a self-organising map (SOM) algorithm as a solution
to a new multi-goal path planning problem for active perception and data
collection tasks. We optimise paths for a multi-robot team that aims to
maximally observe a set of nodes in the environment. The selected nodes
are observed by visiting associated viewpoint regions defined by a
sensor model. The key problem characteristics are that the viewpoint
regions are overlapping polygonal continuous regions, each node has an
observation reward, and the robots are constrained by travel budgets.
The SOM algorithm jointly selects and allocates nodes to the robots and
finds favourable sequences of sensing locations. The algorithm has
polynomial-bounded runtime independent of the number of robots. We
demonstrate feasibility for the active perception task of observing a
set of 3D objects. The viewpoint regions consider sensing ranges and
self-occlusions, and the rewards are measured as discriminability in the
ensemble of shape functions feature space. Simulations were performed
using a 3D point cloud dataset from a real robot in a large outdoor
environment. Our results show the proposed methods enable multi-robot
planning for budgeted active perception tasks with continuous sets of
candidate viewpoints and long planning horizons.},
keywords={mobile robots;multi-robot systems;neurocontrollers;path
planning;polynomials;self-organising feature maps;3D point cloud
dataset;SOM algorithm;associated viewpoint regions;budgeted active
perception;data collection tasks;multigoal path planning
problem;multirobot path planning;overlapping polygonal continuous
regions;polynomial-bounded runtime;self-organising maps;sensing location
sequences;shape functions feature space;Data collection;Path
planning;Planning;Robot sensing systems;Three-dimensional displays},
doi={10.1109/IROS.2016.7759489},
month={Oct},}
@INPROCEEDINGS{7759485,
author={S. Luo and W. Mou and K. Althoefer and H. Liu},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Iterative Closest Labeled Point for tactile object shape
recognition},
year={2016},
pages={3137-3142},
abstract={Tactile data and kinesthetic cues are two important sensing
sources in robot object recognition and are complementary to each other.
In this paper, we propose a novel algorithm named Iterative Closest
Labeled Point (iCLAP) to recognize objects using both tactile and
kinesthetic information. The iCLAP first assigns different local tactile
features with distinct label numbers. The label numbers of the tactile
features together with their associated 3D positions form a 4D point
cloud of the object. In this manner, the two sensing modalities are
merged to form a synthesized perception of the touched object. To
recognize an object, the partial 4D point cloud obtained from a number
of touches iteratively matches with all the reference cloud models to
identify the best fit. An extensive evaluation study with 20 real
objects shows that our proposed iCLAP approach outperforms those using
either of the separate sensing modalities, with a substantial
recognition rate improvement of up to 18%.},
keywords={haptic interfaces;object recognition;robot vision;shape
recognition;iCLAP;iterative closest labeled point;kinesthetic
cues;kinesthetic information;label numbers;local tactile
features;partial 4D point cloud;robot object recognition;substantial
recognition rate improvement;tactile data;tactile object shape
recognition;Feature extraction;Measurement;Object recognition;Robot
sensing systems;Shape;Three-dimensional displays},
doi={10.1109/IROS.2016.7759485},
month={Oct},}
@INPROCEEDINGS{7759113,
author={S. Shin and I. Shim and J. Jung and Y. Bok and J. H. Oh and I.
S. Kweon},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Object proposal using 3D point cloud for DRC-HUBO+},
year={2016},
pages={590-597},
abstract={We present an object proposal method which utilizes the 3D
data obtained from a depth sensor as well as the color information of
images. Our object proposal method is designed to improve the
performance of the object detection for a mobile robot equipped with a
camera and a laser scanner. Compared to traditional object proposal
methods using only 2D images, the proposed method provides much less
number of candidate windows for object detection. We show less than 100
object proposal windows per image using the proposed method result in
high recall tested on the public dataset. Our method presents object
proposals in 3D space as well as in 2D image thus it can further be
applied to following tasks for mobile robots such as 3D location and
pose estimation of the target object after successful object detection.
We validate our method using the real-world object detection dataset for
outdoor mobile robots captured during the DRC Finals 2015 and the public
dataset for comparison with the previous methods.},
keywords={cameras;mobile robots;object detection;pose estimation;robot
vision;2D image;3D location;3D point cloud data;3D space;DRC-HUBO+;color
information;depth sensor;laser scanner;object detection dataset;object
proposal;outdoor mobile robots;pose estimation;Cameras;Image color
analysis;Object detection;Proposals;Robot sensing
systems;Three-dimensional displays},
doi={10.1109/IROS.2016.7759113},
month={Oct},}
@INPROCEEDINGS{7759618,
author={T. T. Pham and M. Eich and I. Reid and G. Wyeth},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Geometrically consistent plane extraction for dense indoor 3D
maps segmentation},
year={2016},
pages={4199-4204},
abstract={Modern SLAM systems with a depth sensor are able to reliably
reconstruct dense 3D geometric maps of indoor scenes. Representing these
maps in terms of meaningful entities is a step towards building semantic
maps for autonomous robots. One approach is to segment the 3D maps into
semantic objects using Conditional Random Fields (CRF), which requires
large 3D ground truth datasets to train the classification model.
Additionally, the CRF inference is often computationally expensive. In
this paper, we present an unsupervised geometric-based approach for the
segmentation of 3D point clouds into objects and meaningful scene
structures. We approximate an input point cloud by an adjacency graph
over surface patches, whose edges are then classified as being either on
or off. We devise an effective classifier which utilises both global
planar surfaces and local surface convexities for edge classification.
More importantly, we propose a novel global plane extraction algorithm
for robustly discovering the underlying planes in the scene. Our
algorithm is able to enforce the extracted planes to be mutually
orthogonal or parallel which conforms usually with human-made indoor
environments. We reconstruct 654 3D indoor scenes from NYUv2 sequences
to validate the efficiency and effectiveness of our segmentation method.},
keywords={SLAM (robots);feature extraction;graph theory;image
classification;image segmentation;image sequences;robot vision;3D ground
truth datasets;CRF;NYUv2 sequences;SLAM systems;adjacency
graph;autonomous robots;classification model;conditional random
fields;dense indoor 3D maps segmentation;edge
classification;geometrically consistent plane extraction;global plane
extraction algorithm;simultaneous localization and mapping;unsupervised
geometric-based approach;Clustering algorithms;Image
reconstruction;Image segmentation;Robots;Semantics;Surface
reconstruction;Three-dimensional displays},
doi={10.1109/IROS.2016.7759618},
month={Oct},}
@INPROCEEDINGS{7759597,
author={R. Bichsel and P. V. K. Borges},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Low-obstacle detection using stereo vision},
year={2016},
pages={4054-4061},
abstract={Real-time obstacle detection is a key component of autonomous
vehicles. In this context, low obstacles are particularly challenging,
as they are often discarded by traditional algorithms. Curb detection
methods that can potentially be suitable for the problem usually target
roads with clearly defined curbs and sidewalks. We propose a real-time
algorithm for the detection of low obstacles (including, but not
restricted to curbs), merging 2-D and 3-D information from stereo
imaging. A set of candidate object lines is extracted based on their
combined 2-D and 3-D features, tracked over time and clustered according
to a novel similarity metric. Finally, a 3^rd order polynomial spline is
fitted to each cluster to represent the obstacle. The proposed system
can deal with noisy and incomplete point clouds and keeps the model
assumptions to a minimum. To evaluate the algorithm, a new stereo
dataset is provided and made available online. We present experiments in
different scenarios and lighting conditions, illustrating the
applicability of the method.},
keywords={feature extraction;mobile robots;object detection;road
vehicles;robot vision;splines (mathematics);stereo image processing;2D
features;3D features;autonomous vehicles;curb detection;low-obstacle
detection;real-time obstacle detection;stereo imaging;stereo
vision;third order polynomial spline;Cameras;Clustering
algorithms;Roads;Sensors;Three-dimensional
displays;Vehicles;Visualization},
doi={10.1109/IROS.2016.7759597},
month={Oct},}
@INPROCEEDINGS{7759731,
author={V. Grabe and S. T. Nuske},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Long distance visual ground-based signaling for unmanned aerial
vehicles},
year={2016},
pages={4976-4983},
abstract={We present a long-range visual signal detection system that is
suitable for an unmanned aerial vehicle to find an optical signal
released at a desired landing site for the purposes of cargo delivery or
rescue situations where radio signals or other communication systems are
not available or the wind conditions at the landing site need to be
signaled. The challenge here is to have a signal and detection system
that works from long range (> 1000m) amongst ground clutter during
various seasonal conditions on passive imagery. We use a smoke-grenade
as a ground signal, which has the advantageous properties of being easy
to carry by ground crews because of its light weight and small size, but
when released has a long visual signaling range. We employ a camera
system on the UAV with a visual texture feature extraction approach in a
machine learning framework to classify image patches as `signal' or
`background'. We study conventional approaches and develop a visual
feature descriptor that can better differentiate the appearance of the
visual signal under varying conditions and, when used to train a
random-forest classifier, outperforms commonly used feature descriptors.
The system was rigorously and quantitatively evaluated on data collected
from a camera mounted on a helicopter and flown towards a plume of
signal smoke over a variety of seasons, ground conditions, weather
conditions, and environments. Our system was capable of detecting the
smoke cloud with both precision and recall rates greater than 0.95 from
ranges between 1000m and 1500m. Further, we develop a method to estimate
wind orientation and approximate wind strength by assessing the shape of
the smoke signal. We present a preliminary evaluation of the wind
estimation in conditions with different wind intensities and
orientations relative to the approach direction.},
keywords={autonomous aerial vehicles;cameras;control engineering
computing;feature extraction;image classification;image texture;learning
(artificial intelligence);robot vision;camera system;cargo
delivery;ground clutter;ground conditions;image patches
classification;landing site;long distance visual ground-based
signaling;long-range visual signal detection system;machine
learning;optical signal;passive imagery;random-forest classifier;rescue
situations;smoke cloud detection;smoke-grenade;unmanned aerial
vehicles;visual feature descriptor;visual signaling range;visual texture
feature extraction;weather conditions;Cameras;Feature
extraction;Helicopters;Histograms;Image color analysis;Image
segmentation;Visualization},
doi={10.1109/IROS.2016.7759731},
month={Oct},}
@INPROCEEDINGS{7759184,
author={Y. Kim and H. Lim and S. C. Ahn and A. Kim},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)},
title={Simultaneous segmentation, estimation and analysis of articulated
motion from dense point cloud sequence},
year={2016},
pages={1085-1092},
abstract={In this paper, we present a unified approach for Expectation
Maximization (EM) based motion segmentation, estimation and analysis
from dense point cloud data. When identifying an underlying motion,
literature mainly focuses on three related topics: motion segmentation,
estimation and analysis. These topics are, however, mostly considered
separately while integrated approaches are rare. Our approach
specifically focuses on analyzing articulated motion from dense point
cloud data by simultaneously solving for three topics using an
integrated approach. No prior knowledge, such as background regions,
number of segments and correspondence, is required since two iterations
in this algorithm allow us to seamlessly accomplish integration of the
three tasks. The first iteration of the algorithm is performed between
segmentation and estimation, followed by the second iteration between
motion estimation and analysis. For the first iteration, we propose EM
based subspace clustering algorithm. For the second iteration, we simply
fuse the motion analysis method from [1] into an iterative motion
estimation algorithm. As a result, we can extract label, correspondence
and motion of moving objects simultaneously from dense point cloud
sequence. In experiment, we validate the performance of the proposed
method on both synthetic and real world data.},
keywords={expectation-maximisation algorithm;image segmentation;image
sequences;iterative methods;motion estimation;EM based subspace
clustering algorithm;articulated motion analysis;articulated motion
estimation;articulated motion segmentation;dense point cloud
sequence;expectation maximization;iterative motion estimation
algorithm;moving object motion;Algorithm design and analysis;Clustering
algorithms;Computer vision;Motion estimation;Motion
segmentation;Three-dimensional displays;Trajectory},
doi={10.1109/IROS.2016.7759184},
month={Oct},}
@INPROCEEDINGS{7745646,
author={Xinqiao Wu and Zhengliang Yan and Ping Wang and Xiao Chen and
Xiaojie Wu and Junbo Huang},
booktitle={2016 4th International Conference on Applied Robotics for the
Power Industry (CARPI)},
title={A processing method for LiDAR data of power line patrol},
year={2016},
pages={1-4},
abstract={The Paper propose a processing method for LiDAR data in power
line inspection tour. The classification of the point cloud with the
data feature in the patrol is the first step. On this basis, using the
geometric information of different types of point cloud in feature
extraction and statistical analysis to obtain the cross-cutting and
security risks information which is required by the power line patrol.
Tests show that the method can effectively evaluate the security status
of the transmission line.},
keywords={automatic optical inspection;data visualisation;feature
extraction;optical radar;pattern classification;power transmission
lines;risk management;statistical analysis;LiDAR data processing
method;cross-cutting risk information;data feature;feature
extraction;geometric information;point cloud classification;power line
inspection;power line patrol;power transmission line;security risk
information;statistical analysis;Feature extraction;Laser radar;Poles
and towers;Roads;Three-dimensional displays;Vegetation;Vegetation
mapping;LiDAR;classification;security risks;statistical analysis},
doi={10.1109/CARPI.2016.7745646},
month={Oct},}
@INPROCEEDINGS{7743415,
author={V. F. Filaretov and D. A. Yukhimets and A. V. Zuev and A. S.
Gubankov and E. S. Mursalimov},
booktitle={2016 IEEE International Conference on Automation Science and
Engineering (CASE)},
title={Method of combination of three-dimensional models of details with
their CAD-models at the presence of deformations},
year={2016},
pages={257-261},
abstract={In this work the combination method of three-dimensional
models of processed details with their reference CAD-models when these
details have unknown in advance deformation is proposed. This method
assumes preliminary transformation of models to clouds of points,
special density increasing of these clouds and their iterative
combination by means of ICP (Iterative Closest Points) algorithm. Model
example demonstrates the efficiency and effectiveness of proposed
approach.},
keywords={CAD;deformation;iterative methods;solid modelling;CAD
models;ICP algorithm;computer aided design;deformation;iterative closest
points algorithm;three-dimensional model combination method;Algorithm
design and analysis;Automation;Computational modeling;Deformable
models;Iterative closest point algorithm;Solid
modeling;Three-dimensional displays},
doi={10.1109/COASE.2016.7743415},
month={Aug},}
@INPROCEEDINGS{7738261,
author={N. Rofalis and A. S. Olesen and M. L. Jakobsen and V. Krüger},
booktitle={2016 IEEE International Conference on Imaging Systems and
Techniques (IST)},
title={Fast visual part inspection for bin picking},
year={2016},
pages={412-417},
abstract={In this paper we present a novel 3D sensing approach for
industrial bin-picking applications that is low-cost, fast and precise.
The system uses a simple laser-line emitter. While a robot arm moves the
object through the laser light, a synchronized camera captures the laser
line image on the object. A full point cloud as well as an edge point
cloud suitable for subsequent pose estimation is generated by the
developed system. The aim of this work is to deliver an accurate point
cloud based on which an object pose can be generated to support a
manufacturing robot to deliver an object with high precision. The
experimental evaluation of our system shows robust and accurate scanning
results.},
keywords={automatic optical inspection;control engineering
computing;image capture;industrial manipulators;pose
estimation;production engineering computing;robot vision;3D sensing
approach;edge point cloud;full point cloud;industrial bin-picking
applications;laser line image capture;laser-line emitter;manufacturing
robot;robot arm;subsequent pose estimation;synchronized camera;visual
part inspection;Cameras;Grippers;Inspection;Robot kinematics;Robot
vision systems;Three-dimensional displays},
doi={10.1109/IST.2016.7738261},
month={Oct},}
@INPROCEEDINGS{7736292,
author={H. Zhang and Y. Cao and X. Zhu and M. G. Fujie and Q. Cao},
booktitle={2016 IEEE Workshop on Advanced Robotics and its Social
Impacts (ARSO)},
title={An improved approach for model-based detection and pose
estimation of texture-less objects},
year={2016},
pages={261-266},
abstract={Detection and pose estimation of texture-less objects still
faces several challenges such as foreground occlusions, background
clutter, multi-instance objects, large scale and pose changes to name
but a few. In this paper, we present an improved approach for model
based detection and pose estimation of texture-less objects, LINEMOD
[4], in order to improve the robustness of pose estimation with partial
foreground occlusions. For template creation, we modify Gradient
Response Maps and propose Gradient Orientation Maps, where Non-Maximum
Suppression and Dual Threshold Algorithm are applied. And we adopt image
pyramid searching method for fast template matching. Next, the
approximate object pose associated with each detected template is used
as a starting point for fine pose estimation with Iterative Closest
Point algorithm. Thirdly, we improve the accuracy of fine pose
estimation by using point cloud filter. Experimental results show that
our approach is more robust to estimate the pose of texture-less objects
with partial foreground occlusions.},
keywords={gradient methods;image filtering;image matching;image
retrieval;image texture;pose estimation;LINEMOD;background clutter;dual
threshold algorithm;fast template matching;foreground
occlusions;gradient orientation maps;gradient response maps;image
pyramid searching method;iterative closest point algorithm;model-based
detection;multiinstance objects;nonmaximum suppression;point cloud
filter;pose changes;pose estimation;texture-less objects;Biomedical
imaging;Image edge detection;Iterative closest point algorithm;Pose
estimation;Robots;Robustness;Three-dimensional displays},
doi={10.1109/ARSO.2016.7736292},
month={July},}
@INPROCEEDINGS{7730062,
author={X. Fan and G. Nie and Y. Deng and J. An and P. Song and H. Li
and Y. Gu},
booktitle={2016 IEEE International Geoscience and Remote Sensing
Symposium (IGARSS)},
title={Influence of earthquake on the atmospheric aerosols study using
aeronet retrieved aerosol optical depth},
year={2016},
pages={4080-4083},
abstract={The atmospheric aerosols are one of the uncertainties that
significantly influence the study of energy balance at regional or
global scale. However, few studies focus on the aerosols caused by
earthquake induced damage of hazard-affected bodies. To study the
influences of earthquakes on the loadings of atmospheric aerosols, the
AErosol RObotic NETwork (AERONET) sites retrieved aerosol optical depth
(AOD) are used in this study. It showed that the AOD became larger after
earthquakes for six cases in this study. After approximately one day,
the AOD of the six cases tend to close to the normal range. In addition,
the other four cases show that the there is no significant increasement
of AOD or the AOD tend to become smaller after earthquake.},
keywords={aerosols;earthquakes;hazards;AERONET retrieved aerosol optical
depth;Aerosol Robotic Network sites;atmospheric
aerosols;earthquake;energy balance;global scale;hazard- affected
bodies;normal
range;Aerosols;Buildings;Clouds;Earthquakes;Geology;Loading;Robots;AERONET;AOD;atmospheric
aerosols;earthquake},
doi={10.1109/IGARSS.2016.7730062},
month={July},}
@INPROCEEDINGS{7606975,
author={M. Li and F. Huang},
booktitle={2016 International Conference on Advanced Robotics and
Mechatronics (ICARM)},
title={Formal describing the organizations in the pervasive healthcare
information system: Multi-agent system perspective},
year={2016},
pages={524-529},
abstract={With the development of computing technology, the architecture
of healthcare information systems (HIS) may evolve into an application
of pervasive computing. HIS requires a more complex high-level design
because of its involvement of many different organizations with
individual interests and goals. Therefore, an agent-oriented
architecture is preferable over other traditional models, such as
object-oriented and service-oriented architectures. Through expanding
the temporal logic with deontic logic, a novel formal model for agent
organization in a multi-agent system is addressed. This new formal
description method is more flexible and robust for describing the
complex behavior of agent organization, and this, it can be applied to
the pervasive HIS and improve its capability for high-level design and
description. Recently, remote and pervasive healthcare information
systems have become more popular with the development of cloud
computing, the internet, and multi-agent systems. HIS is much more
complex than many information systems. Therefore, the formal description
of a pervasive HIS based on a multi-agent system appears very difficult.
A novel description of an agent organization cannot solve all of the
problems of designing a pervasive HIS, but it can improve its capability
of high-level design over traditional agent models.},
keywords={health care;medical information systems;multi-agent
systems;object-oriented programming;software architecture;temporal
logic;ubiquitous computing;HIS;agent organization;agent-oriented
architecture;computing technology;deontic logic;formal description
method;formal model;healthcare information systems
architecture;high-level design;multiagent system;pervasive
computing;pervasive healthcare information system;remote healthcare
information systems;temporal logic;Computational modeling;Computer
architecture;Information systems;Medical services;Multi-agent
systems;Organizations;Problem-solving;formal description;healthcare
information system;multi-agent system;pervasive computing},
doi={10.1109/ICARM.2016.7606975},
month={Aug},}
@INPROCEEDINGS{7606921,
author={B. Gao and F. Chen and F. Trapani and M. Selvaggio and D.
Caldwell},
booktitle={2016 International Conference on Advanced Robotics and
Mechatronics (ICARM)},
title={Robust object localization based on error patterns learning for
dexterous mobile manipulation},
year={2016},
pages={213-218},
abstract={In this article we describe an approach for object detection
and pose estimation from stereo RGB frames for robot manipulation in
manufacturing scenarios. This solution was developed in the framework of
the second challenge of the EuRoC project, and meets the need of a
registration method invariant to the view perspective and robust to the
structural symmetries and ambiguities of the target objects. Our
contribution consists of automatic correction of sub-optimal results of
registration algorithms. As most registration algorithms only converge
on local optima, a tool for recognizing and correcting wrong alignments
is highly desirable. Our insight is that, for a given target point
cloud, it is important to study the alignment space offline and identify
sub-optimal solutions before the registration. The convergence of the
algorithm leads to the error pattern knowledge that can be used to
discard the wrong solutions, and recover the correct alignment.
Experiments on synthesized and real data show that exploiting the known
information about the spatial properties of the objects, together with
appropriate pre-processing and refining of the data, we can have a
substantial improvement in discarding wrong hypothesis for geometrically
ambiguous items.},
keywords={dexterous manipulators;image colour analysis;image
registration;industrial manipulators;learning systems;mobile
robots;object detection;pose estimation;robot vision;robust
control;stereo image processing;EuRoC project;alignment space
offline;dexterous mobile manipulation;error patterns learning;local
optima;manufacturing scenarios;object detection;object spatial
properties;pose estimation;registration algorithms;robot
manipulation;robust object localization;stereo RGB frames;structural
symmetries;target objects;target point cloud;view
perspective;Cameras;Iterative closest point algorithm;Measurement;Mobile
communication;Plastics;Robots;Three-dimensional displays},
doi={10.1109/ICARM.2016.7606921},
month={Aug},}
@INPROCEEDINGS{7592741,
author={T. M. Yousif and A. K. Alharam and W. Elmedany and A. A.
AlKhalaf and Z. Fardan},
booktitle={2016 IEEE 4th International Conference on Future Internet of
Things and Cloud Workshops (FiCloudW)},
title={GPRS-Based Robotic Tracking System with Real Time Video Streaming},
year={2016},
pages={299-303},
abstract={This paper presents a remote monitoring system using
web-based/mobile application for detecting explosive gases. ROBODEM
(Robot-Detection-Explosive-Materials) system aims to develop a remotely
controlled explosive gas detection system handled by a
LEGO-Mindstrom-NXT robot. The main controller has been designed using
Ardunio-Uno microcontroller with IP camera for live video streaming, GPS
for live tracking and gas detection system using MQ6 and MQ5 sensors,
and. This robot can be controlled either indoor using Bluetooth or
outdoor using the Internet. ROBODEM provides sensors results, daily
reports, an alarm and notification emails/SMS. The prototype has been
tested experimentally and the results are analyzed and discussed.},
keywords={microcontrollers;object tracking;robot vision;sensors;service
robots;video streaming;Ardunio-Uno microcontroller;Bluetooth;GPRS-based
robotic tracking system;Global Packet Radio Service;IP
camera;Internet;LEGO-Mindstrom-NXT robot;MQ5 sensor;MQ6 sensor;ROBODEM
system;Web-based application;explosive gases detection;mobile
application;realtime video streaming;remote monitoring
system;Bluetooth;Explosives;Gases;Mobile applications;Robot sensing
systems;Ardunio;GPS;LEGO;Robedem;Tracking},
doi={10.1109/W-FiCloud.2016.67},
month={Aug},}
@INPROCEEDINGS{7587150,
author={G. Zhang and J. Wang and F. Cao and Y. Li and X. Chen},
booktitle={2016 12th IEEE/ASME International Conference on Mechatronic
and Embedded Systems and Applications (MESA)},
title={3D curvature grinding path planning based on point cloud data},
year={2016},
pages={1-6},
abstract={This paper proposes a novel method which uses online
measurement data to create robot path for 3D grinding of workpieces with
complex surfaces. The surface is measured as point cloud by a 3D
scanner. The point cloud data is processed by filtering and smoothing,
and then used for path planning. The grinding paths are generated by
creating a series of planes to intersect with the target surface. A
point cloud data slicing algorithm based on octree is developed to
calculate the intersecting lines between point cloud data and planes.
Subsequently the equal step method developed discretizes the grinding
path for smooth curvatures, and utilizes local plane fitting to estimate
the normal at contact points.},
keywords={grinding;industrial robots;octrees;path planning;3D curvature
grinding path planning;3D scanner;complex surfaces;equal step
method;local plane fitting;octree;online measurement data;point cloud
data slicing algorithm;robot path;workpieces;Measurement by laser
beam;Octrees;Path planning;Robots;Smoothing methods;Surface
treatment;Three-dimensional displays;octree;online measurement;path
planning;point cloud;robotic grinding},
doi={10.1109/MESA.2016.7587150},
month={Aug},}
@INPROCEEDINGS{7582999,
author={B. A. Erol and S. Vaishnav and J. D. Labrado and P. Benavidez
and M. Jamshidi},
booktitle={2016 World Automation Congress (WAC)},
title={Cloud-based control and vSLAM through cooperative Mapping and
Localization},
year={2016},
pages={1-6},
abstract={Simultaneous Localization and Mapping (SLAM) is one of the
most widely popular and applied methods designed for more accurate
localization and navigation operations. Our experiments showed that
vision based mapping helps agents navigate in unknown environments using
feature based mapping and localization. Instead of using a classical
monocular camera as a vision source, we have decided to use RGB-D (Red,
Green, Blue, Depth) cameras for better feature detection, 3D mapping,
and localization. This is due to the fact that the RGB-D camera returns
depth data as well as the normal RGB data. Moreover, we have applied
this method on multiple robots using the concept of cooperative SLAM.
This paper illustrates our current research findings and proposes a new
architecture based on gathered data from RGB-D cameras, which are the
Microsoft Kinect and the ASUS Xtion Pro for 3D mapping and localization.},
keywords={SLAM (robots);cameras;cloud computing;control engineering
computing;feature extraction;multi-robot systems;path planning;robot
vision;3D mapping;ASUS Xtion Pro;Microsoft Kinect;RGB-D
cameras;agents;cloud-based control;cooperative SLAM;cooperative mapping
and localization;depth data;feature based mapping;feature
detection;multiple robots;normal RGB data;red, green, blue, depth
cameras;simultaneous localization and mapping;vSLAM;vision based
mapping;Cameras;Cloud computing;Feature
extraction;Navigation;Simultaneous localization and
mapping;Three-dimensional displays;Cloud Computing;Cooperative
SLAM;RGB-D;Robotics;vSLAM},
doi={10.1109/WAC.2016.7582999},
month={July},}
@INPROCEEDINGS{7580747,
author={E. Di Stefano and E. Ruffaldi and C. A. Avizzano},
booktitle={2016 IEEE International Smart Cities Conference (ISC2)},
title={Automatic 2D-3D vision based assessment of the attitude of a
train pantograph},
year={2016},
pages={1-5},
abstract={In this paper we propose an automatic visual based technique,
integrated in a wayside monitoring system for train inspection, that
allows to assess the attitude of the metal bow of a pantograph by
combining a colour image captured by an RGB digital camera and a point
cloud built from a range sensor scan. An efficient and fast
template-matching procedure allows to detect the pantograph in the scene
and associate a matching attitude, searching for the most similar model
present in a database. The record of templates belonging to the database
exploits a virtual rendering environment that allows to optimize the
training stage in terms of computational load and time. During actual
inspection the RGB image and point cloud of the pantograph are
opportunely processed and aligned to the same reference frame. After the
preliminary template-matching step, the point cloud is augmented with
the virtual model of the matched template and the attitude angular
values are refined by applying the iterative closest point (ICP)
algorithm between the real object and the virtual one, with the aim of
reducing eventual residual errors.},
keywords={cameras;image colour analysis;image
matching;inspection;pantographs;railways;rendering (computer
graphics);virtual reality;ICP algorithm;RGB digital camera;automatic
2D-3D vision based assessment;colour image;eventual residual error
reduction;iterative closest point algorithm;matching attitude;metal bow
attitude assessment;point cloud;reference frame;sensor scan;train
inspection;train pantograph attitude;virtual rendering
environment;wayside monitoring system;Cameras;Computational
modeling;Feature extraction;Iterative closest point
algorithm;Monitoring;Solid modeling;Visualization},
doi={10.1109/ISC2.2016.7580747},
month={Sept},}
@ARTICLE{7553466,
author={T. Oliveira and A. P. Aguiar and P. Encarnação},
journal={IEEE Transactions on Robotics},
title={Moving Path Following for Unmanned Aerial Vehicles With
Applications to Single and Multiple Target Tracking Problems},
year={2016},
volume={32},
number={5},
pages={1062-1078},
abstract={This paper introduces the moving path following (MPF) problem,
in which a vehicle is required to converge to and follow a desired
geometric moving path, without a specific temporal specification, thus
generalizing the classical path following that only applies to
stationary paths. Possible tasks that can be formulated as an MPF
problem include tracking terrain/air vehicles and gas clouds monitoring,
where the velocity of the target vehicle or cloud specifies the motion
of the desired path. We derive an error space for MPF for the general
case of time-varying paths in a two-dimensional space and subsequently
an application is described for the problem of tracking single and
multiple targets on the ground using an unmanned aerial vehicle (UAV)
flying at constant altitude. To this end, a Lyapunov-based MPF control
law and a path-generation algorithm are proposed together with
convergence and performance metric results. Real-world flight tests
results that took place in Ota Air Base, Portugal, with the ANTEX-X02
UAV demonstrate the effectiveness of the proposed method.},
keywords={Lyapunov methods;autonomous aerial vehicles;path
planning;target tracking;time-varying systems;ANTEX- X02
UAV;Lyapunov-based MPF control law;MPF problem;Ota Air Base;Portugal;air
vehicles;gas cloud monitoring;geometric moving path;moving path
following;multiple target tracking problems;path-generation
algorithm;real-world flight tests;single target tracking
problems;terrain vehicles;time-varying paths;two-dimensional
space;unmanned aerial vehicles;Aerospace
electronics;Clouds;Robots;Target tracking;Unmanned aerial
vehicles;Aerial robotics;path following;target tracking},
doi={10.1109/TRO.2016.2593044},
ISSN={1552-3098},
month={Oct},}
@INPROCEEDINGS{7576920,
author={H. Gu and Y. Zhang and S. Fan and M. Jin and H. Zong and H. Liu},
booktitle={2016 IEEE International Conference on Advanced Intelligent
Mechatronics (AIM)},
title={Model recovery of unknown objects from discrete tactile points},
year={2016},
pages={1121-1126},
abstract={This paper presents a novel method for robot to reconstruct
unknown object models from discrete tactile point clouds. In model
recovery process, 6D tactile data which contain point positions and
corresponding normal vectors are firstly clustered. Then geometric
feature descriptors are used to extract geometric feature vectors from
tactile data. The object feature vectors are used in object shape
classification and unknown object models are recovered. Simulation and
experiment show that the present approach can be used to recognize
unknown object shapes from sparse and noisy tactile point clouds and
reconstruct object model accurately.},
keywords={feature extraction;image classification;image
reconstruction;object recognition;robot vision;vectors;6D tactile
data;discrete tactile point clouds;discrete tactile points;geometric
feature descriptors;geometric feature vector extraction;model recovery
process;noisy tactile point cloud;normal vectors;object feature
vectors;object shape classification;robot;sparse tactile point
cloud;unknown object model reconstruction;unknown object models;unknown
object shape recognition;Data collection;Feature extraction;Image
reconstruction;Shape;Tactile sensors;Three-dimensional displays},
doi={10.1109/AIM.2016.7576920},
month={July},}
@INPROCEEDINGS{7575205,
author={A. Vick and J. Guhl and J. Krüger},
booktitle={2016 21st International Conference on Methods and Models in
Automation and Robotics (MMAR)},
title={Model predictive control as a service #x2014; Concept and
architecture for use in cloud-based robot control},
year={2016},
pages={607-612},
abstract={This paper presents the concept and architecture of a model
predictive feedback control system to be used for compensating
communication delays in networked industrial robot control. This
approach follows the ideas given by the paradigms of Industrie 4.0 that
demand for highly networked production devices and functions on
different machine layers and IT hierarchy levels. We push the concept of
fully outsourced control systems to a point, where even real-time
critical feedback processes are driven from cloud-based services over
uncertain public networks.},
keywords={control engineering computing;feedback;industrial
robots;predictive control;IT hierarchy levels;Industrie 4.0;cloud-based
robot control;communication delays;machine layers;model predictive
feedback control system;networked industrial robot control;networked
production devices;outsourced control systems;uncertain public
networks;Cloud computing;Computer architecture;Feedback control;Robot
control;Servers;Service robots},
doi={10.1109/MMAR.2016.7575205},
month={Aug},}
@INPROCEEDINGS{7575289,
author={T. Kornuta and M. Stefańczyk},
booktitle={2016 21st International Conference on Methods and Models in
Automation and Robotics (MMAR)},
title={Utilization of textured stereovision for registration of 3D
models of objects},
year={2016},
pages={1088-1093},
abstract={RGB-D sensors triggered a rapid progress in the field of robot
visual perception. A typical visual perception subsystem relies on
finding the correspondences between features extracted from RGB-D images
retrieved from robot sensors and models of objects. In this paper we
introduce a multi-camera setup supplemented with an additional pattern
projector used for registration of high-resolution images of objects.
The objects are placed on a fiducial board with two dot patterns
enabling robust extraction of masks of the placed objects and estimation
of their initial poses. The acquired dense point clouds constituting
subsequent object views undergo pairwise registration and at the end are
optimized with a graph-based technique derived from SLAM. The
combination of all those elements resulted in a system for generation of
consistent 3D models of objects. We present details of the developed
system and conclude the paper with discussion of the achieved results.},
keywords={SLAM (robots);feature extraction;graph theory;image colour
analysis;image registration;image resolution;image sensors;image
texture;robot vision;stereo image processing;3D model registration;RGB-D
sensors;SLAM;dense point clouds;dot patterns;extracted features;fiducial
board;graph-based technique;high-resolution images;multicamera
setup;pattern projector;robot visual perception;textured
stereovision;Cameras;Cloud computing;Estimation;Feature
extraction;Sensors;Solid modeling;Three-dimensional displays},
doi={10.1109/MMAR.2016.7575289},
month={Aug},}
@INPROCEEDINGS{7575275,
author={M. Krzysztoń and E. Niewiadomska-Szynkiewicz},
booktitle={2016 21st International Conference on Methods and Models in
Automation and Robotics (MMAR)},
title={Mobile ad hoc network for a heavy gas cloud boundary estimation
and tracking},
year={2016},
pages={1004-1009},
abstract={Mobile wireless ad hoc network (MANET) can significantly
enhance the capability to monitor contaminated areas, detect pollution
and gas clouds, tackle emergency incidents and support rescue teams that
are working in the emergency areas. In this paper, we investigate the
problem of estimation of boundary of discovered heavy gas cloud and
tracking this clouds. The MANET comprised of mobile sensing devices is
used to solve this task. We describe a three-phase strategy for
construction a sensing system, in which mobile sensors explore the
region of interest to detect the gas cloud, create preliminarily network
topology and finally, adapt this topology to detect the cloud boundary
and track the moving cloud maintaining the permanent communication with
the central operator of the system. We evaluate the performance of the
proposed strategy based on the results of simulations.},
keywords={mobile ad hoc networks;wireless sensor networks;heavy gas
cloud boundary estimation;heavy gas cloud boundary tracking;mobile
sensing devices;mobile wireless ad hoc network;three-phase strategy;Base
stations;Estimation;Mobile ad hoc networks;Mobile
communication;Monitoring;Sensors;Trajectory},
doi={10.1109/MMAR.2016.7575275},
month={Aug},}
@INPROCEEDINGS{7575264,
author={W. Dudek and K. Banachowicz and W. Szynkiewicz and T. Winiarski},
booktitle={2016 21st International Conference on Methods and Models in
Automation and Robotics (MMAR)},
title={Distributed NAO robot navigation system in the hazard detection
application},
year={2016},
pages={942-947},
abstract={Advanced robot control algorithms, that are based on sensor
data fusion and a lot of processing, require high computational power.
Hence, most of the modern robots have their own highly efficient
computers. It makes the service and assistant robots costly and still
unavailable for the most of the home users. In this paper the navigation
system of the humanoid NAO robot is presented that is distributed
through the robot and the cloud. It reduces the cost of the particular
robot and makes it affordable for customers. Furthermore, the
multi-agent task specification and implementation method is proposed and
illustrated on an example of hazard detection task - the assistant robot
application to help the elderly people to dwell at home.},
keywords={collision avoidance;geriatrics;handicapped aids;home
automation;humanoid robots;mobile robots;multi-robot systems;object
detection;sensor fusion;service robots;advanced robot control
algorithms;assistant robot application;assistant robots;distributed Nao
robot navigation system;elderly people;hazard detection
application;hazard detection task;humanoid Nao robot;multiagent task
specification;robot computers;sensor data fusion;service
robots;Cameras;Collision avoidance;Navigation;Robot kinematics;Robot
vision systems},
doi={10.1109/MMAR.2016.7575264},
month={Aug},}
@INPROCEEDINGS{7575839,
author={N. Le Sommer and L. Touseau and Y. Mahéo and M. Auzias and F.
Raimbault},
booktitle={2016 IEEE 4th International Conference on Future Internet of
Things and Cloud (FiCloud)},
title={A Disruption-Tolerant RESTful Support for the Web of Things},
year={2016},
pages={17-24},
abstract={The Web of Things (WoT) extends the Internet of Things (IoT)
considering that each physical object can be accessed and controlled
using Web-based languages and protocols. However, due to the mobility of
physical objects and to the short radio range of the wireless interfaces
they are equipped with, frequent and unpredictable connectivity
disruptions may occur between the physical objects and the Web clients
used to control and access these objects. This paper presents a
disruption-tolerant RESTful support for the WoT, in which resources
offered by physical objects are identified by URIs and accessed through
stateless services. Service requests and responses are forwarded using
the store-carry-and-forward principle, and can be cached by intermediate
nodes. A complete service invocation model is provided, allowing to
perform unicast, anycast, multicast and broadcast service invocations
either using HTTP or CoAP, which makes it particularly suited for the
WoT. This disruption-tolerant support is illustrated by a scenario in
the context of agricultural robotics.},
keywords={Internet of Things;Web services;broadcast communication;mobile
radio;transport protocols;CoAP;HTTP;Internet of Things;IoT;URI;Web
clients;Web of Things;Web-based languages;Web-based
protocols;WoT;anycast service invocations;broadcast service
invocations;disruption-tolerant RESTful support;multicast service
invocations;physical objects;service invocation model;service
requests;service responses;stateless services;store-carry-and-forward
principle;unicast service invocations;wireless interfaces;Avatars;Cloud
computing;Internet of things;Protocols;Robots;Sensors;Delay-tolerant
Networking;REST Services;Web of Things},
doi={10.1109/FiCloud.2016.11},
month={Aug},}
@INPROCEEDINGS{7561416,
author={P. Prempraneerach and M. Janthong and K. Phothongkum and C.
Choosui and S. Timpitak},
booktitle={2016 13th International Conference on Electrical
Engineering/Electronics, Computer, Telecommunications and Information
Technology (ECTI-CON)},
title={Hydrographical survey using point cloud data from laser scanner
and echo sounder},
year={2016},
pages={1-6},
abstract={This research proposes a hydrographical survey technique using
combined point cloud data from rotating echo sounder and two fixed laser
scanners and correcting instantaneous position and orientation from GPS
and IMU data to reconstructed a three-dimensional environment image. As
a result, river/canal depth and bank slope/height can be measured. All
survey sensors can be installed on a 1.6-m long remote-control catamaran
to effectively measure data from river/canal with width less than 160 m.
Then, the hydrographic information, including cross section, section
width and bank height, could be extracted from the reconstructed image.
Furthermore, measurement accuracies of reconstructed images from laser-
and echo sounder-cloud points are validated against fixed size objects,
located at various locations, and specified water channel, respectively.
Laser and echo sounder maximum measurement error are (15%, 6%) and (8%,
6%) along horizontal and vertical directions, respectively.},
keywords={geophysical image processing;hydrological techniques;image
reconstruction;remote sensing by laser beam;rivers;surveying;3D
environment image;GPS data;IMU data;bank height;bank slope;canal
depth;cross section;echo sounder;fixed laser scanner;hydrographic
information;hydrographical survey;image reconstruction;point cloud
data;remote-control catamaran;river depth;section width;specified water
channel;survey sensor;Boats;Global Positioning System;Image
reconstruction;Measurement by laser beam;Rivers;Surface
reconstruction;Three-dimensional displays;catamaran;cross section;echo
sounder;hydrographical survey;laser scanner;point cloud;reconstructed
image},
doi={10.1109/ECTICon.2016.7561416},
month={June},}
@INPROCEEDINGS{7558810,
author={F. Chenf and B. Gao and M. Selvaggio and Z. Li and D. Caldwell
and K. Kershaw and A. Masi and M. Di Castro and R. Losito},
booktitle={2016 IEEE International Conference on Mechatronics and
Automation},
title={A framework of teleoperated and stereo vision guided mobile
manipulation for industrial automation},
year={2016},
pages={1641-1648},
abstract={Smart and flexible manufacturing requests the adoption of
industrial mobile manipulators in factory. The goal of autonomous mobile
manipulation is the execution of complex manipulation tasks in
unstructured and dynamic environments. It is significant that a mobile
manipulator is able to detect and grasp the object in a fast and
accurate manner. In this research, we developed a stereo vision system
providing qualified point cloud data of the object. A modified and
improved iterative closest point algorithm is applied to recognize the
targeted object greatly avoiding the local minimum in template matching.
Moreover, a stereo vision guided teleoperation control algorithm using
virtual fixtures technology is adopted to enhance robot teaching
ability. Combining these two functions, the mobile manipulator is able
to learn semi-autonomously and work autonomously. The key components and
the system performance are then tested and proved in both simulation and
experiments.},
keywords={image matching;industrial robots;mobile robots;robot
vision;telerobotics;complex manipulation;dynamic environments;flexible
manufacturing;industrial automation;industrial mobile
manipulators;mobile manipulation;mobile manipulator;point cloud
data;smart manufacturing;stereo vision;teleoperated
framework;teleoperation control algorithm;template matching;virtual
fixtures technology;Manipulators;Mobile communication;Navigation;Service
robots;Stereo vision;Three-dimensional displays;mobile
manipulation;stereo vision;teleoperation},
doi={10.1109/ICMA.2016.7558810},
month={Aug},}
@INPROCEEDINGS{7560364,
author={J. Sun and Y. Jiang and J. Jiang and X. Bai},
booktitle={2016 IEEE Information Technology, Networking, Electronic and
Automation Control Conference},
title={Triangular mesh construction based on point cloud matrix and edge
feature extraction},
year={2016},
pages={275-279},
abstract={This paper builds a 3D laser scanner by a one-dimensional
pitching rotation pan-tilt and a 2D laser range finder to get the 3D
laser point cloud data from the motion environment. Depends on this
data, a triangular mesh construction algorithm based on the point cloud
matrix is proposed to construct the triangular mesh of the motion
environment. Then a triangular plane normal vector clustering algorithm
is used to extract the edge feature from the triangular mesh and the
mean square deviation is employed as further process to make the edge
feature more accurately. The experiment results show that the triangular
mesh of motion environment can be constructed effectively and edge
feature can be exacted accurately by the algorithms applied above. It
lays the foundation of mobile robot autonomous movement in unknown
complex environments.},
keywords={edge detection;feature extraction;mesh generation;optical
scanners;pattern clustering;robot vision;2D laser range finder;3D laser
point cloud data;3D laser scanner;edge feature extraction;mean square
deviation;motion environment;one-dimensional pitching rotation
pan-tilt;point cloud matrix;robot autonomous movement;triangular mesh
construction;triangular plane normal vector clustering algorithm;unknown
complex environments;Computers;Feature extraction;Lasers;Mobile
robots;Surface reconstruction;Three-dimensional displays;Two dimensional
displays;2D laser range finder;edge feature extraction;point cloud
data;point cloud matrix;triangular mesh},
doi={10.1109/ITNEC.2016.7560364},
month={May},}
@INPROCEEDINGS{7558972,
author={H. Cao and J. Gao and F. Zhao and J. Zhao and C. Liu and Y. Liu
and X. Shi},
booktitle={2016 IEEE International Conference on Mechatronics and
Automation},
title={The 3D map building of the mobile robot},
year={2016},
pages={2576-2581},
abstract={The map building is the prerequisite in the mobile robots'
autonomous operation in an unknown environment. Most current mobile
robots mainly use the 2D map, and the others which use the 3D map have
some shortcomings such as the complex data processing algorithm, large
computation and time-consuming and so on. The paper presents a 3D data
scanning system that combined with the 2D laser radar and the precise
rotation head to capture the information of the unknown environment, and
building a coordinate system model for calculating the transform
matrixes of each coordinate system. Then the pose of the robot got by
the odometer and electronic compass, the improved ICP algorithm and the
transform matrixes are combined to process the environment information
for getting the 3D point cloud, finally the 3D point cloud is displayed
on the OpenGL. At last, the experiment about the 3D map building is gone
on in the laboratory corridor and the indoor environment. The
experimental results show that this 3D data scanning system has higher
efficiency and accuracy, and can better reflect the real situation of
the environment.},
keywords={matrix algebra;mobile robots;optical radar;robot vision;2D
laser radar;3D data scanning system;3D map building;3D point
cloud;OpenGL;coordinate system model;mobile robot;rotation
head;transform matrix;Iterative closest point algorithm;Laser
radar;Robot kinematics;Three-dimensional displays;Transforms;3D map
building;ICP;Laser radar;mobile robot},
doi={10.1109/ICMA.2016.7558972},
month={Aug},}
@INPROCEEDINGS{7559147,
author={L. K. Lee and S. Y. Oh},
booktitle={Proceedings of ISR 2016: 47st International Symposium on
Robotics},
title={Fast and efficient traversable region extraction using quantized
elevation map and 2D laser rangefinder},
year={2016},
pages={1-5},
abstract={In this paper, we propose a fast and efficient traversable
region extraction using quantized digital elevation map (Q-DEM) from the
data obtained by the 2D laser rangefinder at indoor-outdoor
environments. Generally, the structure of tilting a 2D laser rangefinder
is a widespread strategy to acquire precise 3D point clouds. But in this
research, using low-cost microcontroller-based modularization and
improved tilting mechanism which controls the tilting motion for each of
the scanning areas, we can not only obtain reliable and dense 3D point
clouds with relative uniform distribution from single laser scans, but
also enhance the accuracy of sensor measurements. Furthermore for fast
computation and efficient management of the raw 3D data acquisition, we
firstly adopt the modified voxel grid filtering with adaptive sampling
of scalar distance fields, and then generate a grayscale reconstruction
based quantized elevation map by applying a non-linear measurement model
for extracted data sets. Especially through the proposed quantized
elevation map representation, we have a relatively simple and fast data
processing operation by leveraging the advantages of existing image
processing techniques. Finally, for the implementation of the stable
traversable region extraction, we mainly divided into two main
categories, "traversable" and "non-traversable regions" with histogram
and edge/texture information of Q-DEM, and then more detailed
distinction for terrain classification (flat region, slope, stair, and
obstacle) is performed according to the characteristics of each terrain.
The experimental results show that our proposed method has a stable
terrain classification performance(Avg. 83%) based on the fast map
generation(Avg. 10ms) with an effective mapping capability, regardless
of the variety of environmental characteristics. As a result, our
proposed method was able to make a more stable path generation and
utilization for mapping and navigation in given environment.},
month={June},}
@INPROCEEDINGS{7554333,
author={J. Li and F. Jing and E. Li},
booktitle={2016 35th Chinese Control Conference (CCC)},
title={A new teaching system for arc welding robots with auxiliary path
point generation module},
year={2016},
pages={6217-6221},
abstract={Arc welding robots are widely used in factories. Most robots
require highly-skilled workers doing time-consuming and tedious
programming work. The proposed new teaching system in this paper is
aiming to simplify the programming process by adding a path point
generation module, which depends on a RGB-D sensor to obtain the point
cloud and generate path points for space curve seam. The advantage of
this system is simplifying the teaching process, which is a step forward
of realizing task-level programming. The system is used on an ordinary
arc welding robot while not changing it, working as a plug-and-play type
solution for easy programming. In this paper, the structure of this
teaching system is described. Software architecture of important modules
in the system is presented in details. An example is showed that how
this system is working.},
keywords={arc welding;control engineering computing;image
sensors;production engineering computing;robot programming;robotic
welding;software architecture;RGB-D sensor;arc welding robot;auxiliary
path point generation module;plug-and-play type solution;robot
programming process;software architecture;task-level
programming;teaching system;Computers;Education;Robot kinematics;Robot
sensing systems;Three-dimensional displays;Welding;Robot Operation
System;Teaching system;path generation;point cloud},
doi={10.1109/ChiCC.2016.7554333},
month={July},}
@INPROCEEDINGS{7554861,
author={L. Lv and L. Sun},
booktitle={2016 35th Chinese Control Conference (CCC)},
title={An improved registration algorithm based on geometric feature for
the calibration of workpiece coordinate},
year={2016},
pages={9475-9480},
abstract={Since target points could be generated quickly and accurately
by off-line programming in the industrial robot machining, it is used
widely in industrial robots. Due to installation error and mismachining
tolerance which causes the offset of processing paths in the actual
machining process, the registration algorithms for path calibration
would be involved. While the traditional approach would converge to
local optimal solution, an improved registration algorithm based on
geometric properties of point clouds is proposed in the paper. Firstly,
feature point sets are acquired by the curvature of point clouds and
matched by traditional PCA algorithm, an initial value for precise
registration is provided. Secondly, the improved ICP algorithm by
curvature is utilized to modify the former result and reduce the
execution time of precise registration, the path calibration can be
achieved. The experimental result of the approach is validated in this
paper which is significative for calibration of processing paths.},
keywords={calibration;industrial robots;machining;principal component
analysis;ICP algorithm;PCA algorithm;curvature;feature point
sets;geometric feature;geometric properties;industrial robot machining
process;industrial robots;installation error;mismachining
tolerance;offline programming;path calibration;point clouds;precise
registration;registration algorithms;workpiece coordinate;Approximation
algorithms;Calibration;Iterative closest point algorithm;Principal
component analysis;Robot kinematics;Three-dimensional
displays;Curvature;Feature Point Sets;Path Calibration;Registration},
doi={10.1109/ChiCC.2016.7554861},
month={July},}
@INPROCEEDINGS{7550873,
author={S. Tokunaga and H. Horiuchi and K. Tamamizu and S. Saiki and M.
Nakamura and K. Yasuda},
booktitle={2016 IEEE/ACIS 15th International Conference on Computer and
Information Science (ICIS)},
title={Deploying service integration agent for personalized smart
elderly care},
year={2016},
pages={1-6},
abstract={In recently years, many care robots have received a lot of
attention to help elderly people. However existing care robots have
difficult to adopt personalization. For instance, some programmers have
to customize robot program to meet needs of each elderly. Even if a care
robot which has a feature of machine learning, it takes a long time to
learn a preference for each elderly. In this paper, our goal is to
deploy a smart care service integration agent that provides a
personalization and integration for each elderly people. Our proposed
service consists of three essential components, Virtual Care Giver
(VCG), Virtual Care Personalizer (VCP) and Care Template. VCG is a robot
agent, where executes care tasks in each home. The VCG is offered care
tasks based on care template which Virtual Care Personalizer (VCP)
generates. Virtual Care Personalizer (VCP) manages and generates
personalization of care tasks the on cloud. Moreover, we deploy Care
Template on the cloud which enables to provide the basic care tasks. To
demonstrate the feasibility, we consider three kinds of usecase
scenarios for two persona people.},
keywords={cloud computing;geriatrics;health care;intelligent
robots;learning (artificial intelligence);medical computing;medical
robotics;mobile agents;robot programming;VCG;VCP;care robots;care
template;elderly people;machine learning;robot agent;robot program
customization;service integration agent deployment;smart care service
integration agent;smart elderly care;virtual care giver;virtual care
personalizer;Cloud computing;Computer architecture;Medical
services;Robot sensing systems;Senior citizens;Service robots},
doi={10.1109/ICIS.2016.7550873},
month={June},}
@INPROCEEDINGS{7551610,
author={F. H. Lee and S. C. Hsiao and P. Huan-Ning and H. Samani and C.
Y. Yang},
booktitle={2016 International Conference on System Science and
Engineering (ICSSE)},
title={Design and development of a personal robot doctor for healthcare},
year={2016},
pages={1-4},
abstract={The aim of this paper is to explain the design and development
of a futuristic robot which can act as a personal doctor. This robot
aims to act with the user interactively while it is connected to the
cloud as well as human doctors. The robot can be used for real-time
health monitoring in addition to provide basic health instructions to
the user. The ultimate goal is to make the robot in a form of
personal/social creature where everyone can benefit from it in home or
office. This paper explains the early steps of development of doctor
robot and in future we aim to improve this report in design,
architecture, interaction and functionality aspects.},
keywords={cloud computing;health care;human-robot interaction;medical
diagnostic computing;medical robotics;patient
monitoring;cloud;futuristic robot development;health
instructions;healthcare;home;human doctors;office;personal robot doctor
design;real-time health monitoring;user interaction;Biomedical
monitoring;Medical diagnostic imaging;Medical robotics;Medical
services;Robot sensing systems},
doi={10.1109/ICSSE.2016.7551610},
month={July},}
@INPROCEEDINGS{7533194,
author={C. Liang and Y. Song and Y. Zhang},
booktitle={2016 IEEE International Conference on Image Processing (ICIP)},
title={Hand gesture recognition using view projection from point cloud},
year={2016},
pages={4413-4417},
abstract={In this paper we propose a multi-view method to recognize hand
gestures using point cloud. The main idea of this paper is to project
point cloud into view images and hand gestures are described by
extracting and fusing features in view images. The conversion of feature
space increases the inner-class similarity and meanwhile reduces the
inter-class similarity. The features of view images are extracted in
parallel so the scale of each feature extractor can be reduced to
converge easily. In our method we perform a refined hand segmentation to
segment hand form background firstly. Then the segmented hand point
cloud is projected into different view planes to form view images. Next
we use convolutional neural networks as feature extractors to extract
features of view images. The extracted view image features are fused to
form the features of hand gestures. Finally a SVM is trained for hand
gesture recognition. The experimental results show that our multiview
method achieves higher recognition rate and more robust to the
challenging rotation changes especially out-plane rotations.},
keywords={computational geometry;feature extraction;gesture
recognition;image segmentation;palmprint recognition;convolutional
neural networks;feature extraction;hand gesture recognition;hand
segmentation;point cloud;rotation changes;view images;view
projection;Feature extraction;Gesture recognition;Image
segmentation;Real-time systems;Robustness;Support vector
machines;Three-dimensional displays;Gesture Recognition;Point
Cloud;Rotation Changes;View Image},
doi={10.1109/ICIP.2016.7533194},
month={Sept},}
@ARTICLE{7482853,
author={W. l. Li and H. Xie and G. Zhang and S. j. Yan and Z. p. Yin},
journal={IEEE/ASME Transactions on Mechatronics},
title={3-D Shape Matching of a Blade Surface in Robotic Grinding
Applications},
year={2016},
volume={21},
number={5},
pages={2294-2306},
abstract={Robotic grinding is a promising technique to generate the
final shape of blades. It can relieve human from participating in dirty
and noisy environments, improve product quality, and lower production
costs. One important task in robotic grinding is 3-D shape matching.
However, existing matching methods do not consider the requirements
associated with different grinding allowances, which can potentially
lead to an unstable grinding force. This paper proposes a novel shape
matching method for robotic grinding. The goal is to define a new
objective function considering different allowance weights for stable
grinding, and address incorrect shape matching from the missing points
or uneven density points. The main contribution of this paper is the
application of variance minimization to construct an objective function,
from which the required shape matching parameters are iteratively
calculated. This method balances the contributions of all the measured
points, weighs the allowances for the pressure and suction surfaces of a
blade, and avoids incorrect matching tendencies for high-density points.
It is advantageous to maintaining a relatively stable grinding force.
The effectiveness of this method is verified through simulations and
scanning/grinding experiments of different blades.},
keywords={blades;force control;grinding;image matching;industrial
robots;product quality;robot vision;shape recognition;3D shape
matching;blade pressure surface;blade shape;blade suction surface;blade
surface;grinding force stability;product quality;production cost;robotic
grinding application;variance minimization;Blades;Force;Iterative
closest point algorithm;Minimization;Robots;Shape;Three-dimensional
displays;Blade;laser scanning;point cloud;robot grinding;shape matching},
doi={10.1109/TMECH.2016.2574813},
ISSN={1083-4435},
month={Oct},}
@INPROCEEDINGS{7542934,
author={J. D. Labrado and B. A. Erol and J. Ortiz and P. Benavidez and
M. Jamshidi and B. Champion},
booktitle={2016 11th System of Systems Engineering Conference (SoSE)},
title={Proposed testbed for the modeling and control of a system of
autonomous vehicles},
year={2016},
pages={1-6},
abstract={Large scale multi-agent systems are very important in todays
world because of their varying uses. The Center for Testing, Evaluation
and Control of Heterogeneous Large scale Autonomous Vehicles (TECHLAV)
has been tasked to conduct research on Large Scale Autonomous Systems of
Vehicles (LSASV). This paper focuses on the proposed testbed system that
will help model the large scale system out in the field for Modeling,
Analysis and Control tasks for LSASV (MACLAV). The system will use a
team of UGVs, UAVs and AUVs to navigate, interact and complete missions
through an unknown area as a cohesive unit. A small private cloud
provides a computational backbone to the system.},
keywords={autonomous aerial vehicles;autonomous underwater
vehicles;multi-agent systems;multi-robot
systems;AUV;LSASV;MACLAV;TECHLAV;UAV;UGV;autonomous vehicles;large scale
autonomous system-of-vehicles;large scale multiagent systems;Analytical
models;Cloud computing;Computational modeling;Mathematical
model;Navigation;Robots;Servers;Cloud Computing;Cooperative
SLAM;Modeling;RGB-D;Robotics;Testbed;vS-LAM},
doi={10.1109/SYSOSE.2016.7542934},
month={June},}
@INPROCEEDINGS{7536437,
author={M. Burgin and R. Mikkilineni},
booktitle={2016 IEEE 25th International Conference on Enabling
Technologies: Infrastructure for Collaborative Enterprises (WETICE)},
title={Agent Technology, Superrecursive Algorithms, and DNa as a Tool
for Distributed Clouds and Grids},
year={2016},
pages={89-94},
abstract={Agents and agent systems are becoming more and more important
in the development of a variety of fields such as ubiquitous computing,
ambient intelligence, autonomous computing, intelligent systems and
intelligent robotics. In this paper, we analyze how agent technology is
presented in mathematical models of computation demonstrating how these
models are used in the novel distributed intelligent managed element
(DIME) network architecture (DNA), which extends the conventional
computational model of information processing networks, allowing
improvement of the efficiency and resiliency of computational processes.},
keywords={cloud computing;grid computing;software agents;DIME DNA;agent
systems;agent technology;computational processes;distributed
cloud;distributed grid;distributed intelligent managed element network
architecture;information processing networks;mathematical
models;Computational efficiency;Computational
modeling;Computers;DNA;Mathematical model;Network architecture;Turing
machines;Agent Technology;Cloud Computing;DIME Network Architecture;Grid
Computing;Intelligent Systems;Super Recursive Algorithms},
doi={10.1109/WETICE.2016.28},
month={June},}
@INPROCEEDINGS{7531776,
author={L. Jing and J. Fengshui and L. En},
booktitle={2016 Chinese Control and Decision Conference (CCDC)},
title={RGB-D sensor-based auto path generation method for arc welding
robot},
year={2016},
pages={4390-4395},
abstract={In this paper, an auto path generation method for arc welding
robot is proposed by using a RGB-D sensor. From the generated 3D point
cloud of the weld workpiece, the welding line is extracted and the path
points of the robot are generated by an auto path generation algorithm.
So the torch tip of the arc welding robot would walks along the welding
line. This path generation module is added to current widely used
ordinary arc welding robot while not changing it, used to assist people
when teaching arc welding robots. This maybe eventually replace operator
by automatically generate the path. The detail algorithm of the
recognizing space curve welding lines and the auto path point generation
method are emphasized in this paper. Experimental result shows the
application feasibility of the proposed method.},
keywords={arc welding;image colour analysis;image sensors;robotic
welding;3D point cloud;RGB-D sensor-based auto path generation
method;arc welding robot;path points;space curve welding lines;weld
workpiece;Feature extraction;Image edge detection;Robot sensing
systems;Three-dimensional displays;Two dimensional
displays;Welding;RGB-D;arc welding;image processing;path
generation;point cloud},
doi={10.1109/CCDC.2016.7531776},
month={May},}
@INPROCEEDINGS{7529965,
author={Y. Chen and G. D. Luca},
booktitle={2016 IEEE International Parallel and Distributed Processing
Symposium Workshops (IPDPSW)},
title={VIPLE: Visual IoT/Robotics Programming Language Environment for
Computer Science Education},
year={2016},
pages={963-971},
abstract={Microsoft released its Robotics Developer Studio (MSRDS) and
Visual Programming Language (VPL) in 2006. Microsoft VPL is
service-oriented, uses workflow-based visual programming, and has strong
support for parallel computing. It is a milestone and flagship in
software engineering and in computer science education. Many
universities and high schools have adopted VPL as a tool for teaching
computing and engineering concepts and for programming robots.
Unfortunately, as part of Microsoft's restructuring plan, the robotics
division of Microsoft Research was suspended on September 22, 2014,
leaving the Microsoft VPL community without updates and support. Arizona
State University (ASU) is among the schools that adopted VPL since its
first release in 2006. We started to find a solution to our VPL-based
curriculum in 2014. This paper presents our research and development of
a new visual programming language and its development environment: ASU
VIPLE (Visual IoT/Robotics Programming Language Environment). ASU VIPLE
extends the discontinued Microsoft VPL to sustain our curriculum and to
help the community with their VPL projects. ASU VIPLE supports LEGO EV3
and all IoT devices based on an open architecture. ASU VIPLE integrates
engineering design process, workflow, fundamental programming concepts,
control flow, parallel computing, event-driven programming seamlessly
into the curriculum. It has been pilot tested at Arizona State
University in summer 2015 and in spring 2016, as well as in several
other universities. ASU VIPLE software and documents can be freely
downloaded at: http://venus.eas.asu.edu/WSRepository/VIPLE/.},
keywords={Internet of Things;computer science education;control
engineering computing;control engineering education;parallel
processing;programming environments;robot programming;teaching;visual
programming;ASU VIPLE software;Arizona State University;LEGO
EV3;MSRDS;Microsoft VPL;Robotics Developer Studio;computer science
education;engineering design process;event-driven programming;open
architecture;parallel computing;software engineering
education;teaching;visual IoT/robotics programming language
environment;visual programming language;workflow-based visual
programming;Cloud computing;Internet of
things;Protocols;Robots;Visualization;Internet of Things;MSRDS
VPL;computer science education;parallel computing;robot;visual
programming},
doi={10.1109/IPDPSW.2016.55},
month={May},}
@INPROCEEDINGS{7529551,
author={O. O. Mumini and Ren Lingxue and K. Ivanov and Lei Wang},
booktitle={2016 IEEE International Conference on Cloud Computing and Big
Data Analysis (ICCCBDA)},
title={Mining brain features from schizophrenia studies with Shift-And
pattern matching},
year={2016},
pages={157-163},
abstract={An efficient prospect to medical procedures such as diagnosis
and therapy is by obtaining knowledge of medical experts from formal
reports. Several studies have been carried out on finding differences in
brain connectivity between schizophrenia patients and healthy controls
with their results reported in natural language with tables and figures.
In the area of biomedical research, natural language processing can be
employed to retrieve relevant information from articles by scientific
and medical experts, based on which a brain network characterizing
schizophrenia could be built. Hence, this study presents suitable text
mining model for retrieving information about brain region.
Meta-analysis is employed to integrate knowledge from different, while
relevant information is retrieved from scientific publications with
Shift-And Pattern Matching. Evaluation on a set of 1,525 scientific
literatures on schizophrenia shows the model has good recall of 73.7%.},
keywords={brain;data mining;medical information systems;natural language
processing;patient diagnosis;patient treatment;biomedical research;brain
connectivity;brain features;brain network;brain region;diagnosis;formal
reports;healthy controls;medical experts;medical
procedures;meta-analysis;natural language processing;pattern
matching;schizophrenia patients;schizophrenia studies;scientific
publications;text mining model;therapy;Brain modeling;Computational
modeling;Dictionaries;Medical diagnostic imaging;Servers;natural
language;pattern matching;research articles;schizophrenia;text mining},
doi={10.1109/ICCCBDA.2016.7529551},
month={July},}
@INPROCEEDINGS{7523645,
author={Jaesung Oh and Hyoin Bae and Jeongsoo Lim and Jun-Ho Oh},
booktitle={2016 6th IEEE International Conference on Biomedical Robotics
and Biomechatronics (BioRob)},
title={Development of autonomous laser toning system based on vision
recognition and robot manipulator},
year={2016},
pages={317-322},
abstract={In this paper, the design, implementation, and operation
method of the autonomous laser toning system are proposed, which is
called as MELON (Manipulator for Effective Laser tONing). The system can
recognize the accurate treatment points from the 3D point cloud data
obtained with the camera, and it is possible to emit the laser at the
desired position and orientation repeatedly, precisely, and accurately
using intuitive differential inverse kinematics of the robot
manipulator. The feasibility test of the MELON is conducted by using a
plaster cast of a woman's head, and then, we find that the manipulator
has a workspace to cover the entire face of the human inductively and
distribution of the laser emission is homogeneous on the face.
Therefore, we find the possibility of the autonomous laser toning using
MELON.},
keywords={computer graphics;manipulators;robot kinematics;robot
vision;3D point cloud;MELON;autonomous laser toning system;differential
inverse kinematics;laser emission;manipulator for effective laser
toning;plaster cast;robot manipulator;vision
recognition;Cameras;Face;Manipulators;Medical services;Surface emitting
lasers},
doi={10.1109/BIOROB.2016.7523645},
month={June},}
@ARTICLE{7515118,
author={V. Pankratius and J. Li and M. Gowanlock and D. M. Blair and C.
Rude and T. Herring and F. Lind and P. J. Erickson and C. Lonsdale},
journal={IEEE Intelligent Systems},
title={Computer-Aided Discovery: Toward Scientific Insight Generation
with Machine Support},
year={2016},
volume={31},
number={4},
pages={3-10},
abstract={The process of scientific discovery is traditionally assumed
to be entirely executed by humans. This article highlights how
increasing data volumes and human cognitive limits are challenging this
traditional assumption. Relevant examples are found in observational
astronomy and geoscience, disciplines that are undergoing transformation
due to growing networks of space-based and ground-based sensors. The
authors outline how intelligent systems for computer-aided discovery can
routinely complement and integrate human scientists in the insight
generation loop in scalable ways for next-generation science. The
pragmatics of model-based computer-aided discovery systems go beyond
feature detection in empirical data to answer fundamental questions,
such as how empirical detections fit into hypothesized models and model
variants to ease the scientist's work of placing large ensembles of
detections into a theoretical context. The authors demonstrate
successful applications of this paradigm in several areas, including
ionospheric studies, volcanics, astronomy, and planetary landing site
identification for spacecraft and robotic missions.},
keywords={cognitive systems;natural sciences computing;data
volumes;feature detection;geoscience;ground-based sensors;human
cognitive limits;intelligent systems;ionospheric studies;machine
support;model-based computer aided discovery systems;next generation
science;observational astronomy;planetary landing site
identification;robotic missions;scientific discovery;scientific insight
generation;space-based sensors;spacecraft;volcanics;Analytical
models;Computational modeling;Computer aided analysis;Data models;Global
Positioning System;Planets;Space research;big data;cloud
computing;computer-aided discovery;data mining;discovery
science;intelligent analytics;intelligent systems;machine learning},
doi={10.1109/MIS.2016.60},
ISSN={1541-1672},
month={July},}
@INPROCEEDINGS{7501280,
author={C. Negru and F. Pop and M. Mocanu and V. Cristea and A. Hangan
and L. Vacariu},
booktitle={2016 IEEE International Conference on Automation, Quality and
Testing, Robotics (AQTR)},
title={Cost-aware cloud storage service allocation for distributed data
gathering},
year={2016},
pages={1-5},
abstract={In today cyber-infrastructures, large datasets are produced in
real-time by different sources geographically distributed. These data
must be acquired and preserved for further use in knowledge extraction.
In the context of multi-cloud environments, the cost-efficient storage
service selection is a challenge. There are plenty of Cloud storage
providers offering multiple options so, it is crucial to select the best
solution in terms of cost and quality of service that meet customers
requirements. Due to its multi-objective nature, the process of optimal
service selection becomes a difficult problem. In this paper, we study
the multi-objective optimization problem for storage service selection.
We start from a real world case scenario and build our mathematical
model for the optimization problem. Then we propose an aggregated linear
programming technique to find a near optimal solution for the service
selection problem.},
keywords={cloud computing;data handling;feature selection;linear
programming;quality of service;resource allocation;cloud storage service
allocation;cost-efficient storage service selection;distributed data
gathering;linear programming;multicloud environment;multiobjective
optimization;quality of service;Cloud computing;Data models;Distributed
databases;Linear programming;Optimization;Resource management;Cloud
Computing;Cost optimization;Data Storage;Datacenters;Linear Programming},
doi={10.1109/AQTR.2016.7501280},
month={May},}
@INPROCEEDINGS{7501379,
author={C. Rink and S. Kriegel and J. Hasse and Z. C. Marton},
booktitle={2016 IEEE International Conference on Automation, Quality and
Testing, Robotics (AQTR)},
title={On-the-fly particle filter registration for laser data},
year={2016},
pages={1-6},
abstract={This work is focused on streaming particle filter registration
of surface models such as homogeneous triangle meshes and point clouds.
Part of the approach is a streaming curvature feature calculation. The
investigated approach utilizes a particle filter to incrementally update
pose estimates during data acquisition. The method is evaluated in real
data experiments with a high-precision laser striper system attached to
an industrial robot. During the laser scan, the data is integrated
on-the-fly in order to calculate features and based on these to estimate
the object's pose. Experiments show the method's competitiveness in
accuracy and reliability compared to state-of-the-art offline algorithms.},
keywords={data acquisition;image registration;particle filtering
(numerical methods);pose estimation;data acquisition;high-precision
laser striper system;homogeneous triangle meshes;industrial robot;laser
data;on-the-fly particle filter registration;point clouds;pose
estimation;streaming curvature feature calculation;surface models;Data
acquisition;Lasers;Particle filters;Pose estimation;Sensors;Streaming
media;Three-dimensional displays},
doi={10.1109/AQTR.2016.7501379},
month={May},}
@INPROCEEDINGS{7501155,
author={P. Olivka and M. Mihola and P. Novák and T. Kot and J. Babjak},
booktitle={2016 17th International Carpathian Control Conference (ICCC)},
title={The 3D laser range finder design for the navigation and mapping
for the coal mine robot},
year={2016},
pages={533-538},
abstract={This article describes the construction of the 3D laser range
finder (LRF) for the coal mine robot. The construction is based on our
practical experiences with previous indoor 3D LRF design. The coal mine
environment is very dirty. It is necessary to keep in mind this heavy
working condition during the whole design process. The process of design
started with definition of requirements. This specification is followed
by selection of suitable parts and the dynamic analysis of a selected
actuator. The suitable housing was designed for spatially arranged
internal parts and by the Rapid Prototyping method was made the fully
functional 3D LRF prototype. Finally are presented results from the 3D
LRF testing conducted in the real underground environment.},
keywords={coal;control engineering computing;industrial robots;laser
ranging;mining;path planning;rapid prototyping (industrial);robot
vision;3D laser range finder design;actuator dynamic analysis;coal mine
robot mapping;coal mine robot navigation;indoor 3D LRF design;rapid
prototyping;underground environment;Coal mining;Frequency
measurement;Prototypes;Robot sensing
systems;Servomotors;Three-dimensional displays;3D Mapping;Laser Range
Finder;Point Cloud;Positioning Sensor;Rapid Prototyping;Servo;TeleRescuer},
doi={10.1109/CarpathianCC.2016.7501155},
month={May},}
@INPROCEEDINGS{7501126,
author={T. Kot and P. Novák and J. Babjak},
booktitle={2016 17th International Carpathian Control Conference (ICCC)},
title={Visualization of point clouds built from 3D scanning in coal mines},
year={2016},
pages={372-377},
abstract={The article presents a way of visualizing point clouds created
by 3D scanning in a coal mine. The first part focuses on the choice of
individual algorithms for point cloud pre-processing (using the library
PCL - Point Cloud Library), namely voxelization, outlier removing and
smoothing. Then it is described the main rendering algorithm of the
software - the chosen way of point rendering and some more advanced
methods including shading and coloring. In the end there are mentioned
some optimizations of rendering speed and especially few new methods of
providing additional data to the user, especially related to distance
measurements.},
keywords={data visualisation;mining;rendering (computer
graphics);smoothing methods;3D scanning;Point Cloud Library;coal
mines;coloring;library PCL;outlier removal;point cloud
pre-processing;point clouds visualization;point rendering
algorithm;shading;smoothing method;voxelization;Cameras;Coal
mining;Image color analysis;Lighting;Rendering (computer
graphics);Robots;Three-dimensional displays;3D scanning;PCL;mobile
robot;point cloud;teleoperation;visualization},
doi={10.1109/CarpathianCC.2016.7501126},
month={May},}
@INPROCEEDINGS{7501348,
author={F. Rossi and A. Benso and S. Di Carlo and G. Politano and A.
Savino and P. L. Acutis},
booktitle={2016 IEEE International Conference on Automation, Quality and
Testing, Robotics (AQTR)},
title={FishAPP: A mobile App to detect fish falsification through image
processing and machine learning techniques},
year={2016},
pages={1-6},
abstract={Food forgery is one of the most articulated socio-economic
concerns, which contributed to increase people awareness on what they
eat. Identification of species represents a key aspect to expose
commercial frauds implemented by substitution of valuable species with
others of lower value. Fish species identification is mainly performed
by morphological identification of gross anatomical features of the
whole fish. However, the increasing presence on markets of new
little-known species makes morphological identification of species
difficult. In this paper we present FishAPP, a cloud-based
infrastructure for fish species recognition. FishAPP is composed of a
mobile application developed for the Android and the iOS mobile
operating system enabling the user to shot pictures of a whole fish and
submit them for remote analysis and a remote cloud-based processing
system that implements a complex image processing pipeline and a neural
network machine learning system able to analyze the obtained images and
to perform classification into predefined fish classes. Preliminary
results obtained from the available dataset provided encouraged results.},
keywords={Android (operating system);aquaculture;cloud computing;feature
extraction;image classification;learning (artificial
intelligence);neural nets;smart phones;Android;FishAPP;cloud-based
infrastructure;fish falsification detection;fish species
identification;fish species recognition;food forgery;iOS mobile
operating system;image classification;image processing;machine
learning;mobile application;neural network;Cloud computing;Feature
extraction;Mobile applications;Mobile communication;Servers;Smart phones},
doi={10.1109/AQTR.2016.7501348},
month={May},}
@INPROCEEDINGS{7487504,
author={T. Fischer and Y. Demiris},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Markerless perspective taking for humanoid robots in
unconstrained environments},
year={2016},
pages={3309-3316},
abstract={Perspective taking enables humans to imagine the world from
another viewpoint. This allows reasoning about the state of other
agents, which in turn is used to more accurately predict their behavior.
In this paper, we equip an iCub humanoid robot with the ability to
perform visuospatial perspective taking (PT) using a single depth camera
mounted above the robot. Our approach has the distinct benefit that the
robot can be used in unconstrained environments, as opposed to previous
works which employ marker-based motion capture systems. Prior to and
during the PT, the iCub learns the environment, recognizes objects
within the environment, and estimates the gaze of surrounding humans. We
propose a new head pose estimation algorithm which shows a performance
boost by normalizing the depth data to be aligned with the human head.
Inspired by psychological studies, we employ two separate mechanisms for
the two different types of PT. We implement line of sight tracing to
determine whether an object is visible to the humans (level 1 PT). For
more complex PT tasks (level 2 PT), the acquired point cloud is mentally
rotated, which allows algorithms to reason as if the input data was
acquired from an egocentric perspective. We show that this can be used
to better judge where object are in relation to the humans. The
multifaceted improvements to the PT pipeline advance the state of the
art, and move PT in robots to markerless, unconstrained environments.},
keywords={humanoid robots;image motion analysis;inference
mechanisms;object recognition;robot vision;egocentric
perspective;humanoid robots;iCub;marker-based motion capture
systems;markerless perspective taking;object recognition;point
cloud;reasoning;single depth camera;unconstrained
environments;visuospatial perspective taking;Cameras;Head;Humanoid
robots;Robot kinematics;Robot vision systems;Three-dimensional displays},
doi={10.1109/ICRA.2016.7487504},
month={May},}
@INPROCEEDINGS{7487710,
author={W. J. Beksi and N. Papanikolopoulos},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={3D point cloud segmentation using topological persistence},
year={2016},
pages={5046-5051},
abstract={In this paper, we present an approach to segment 3D point
cloud data using ideas from persistent homology theory. The proposed
algorithms first generate a simplicial complex representation of the
point cloud dataset. Next, we compute the zeroth homology group of the
complex which corresponds to the number of connected components.
Finally, we extract the clusters of each connected component in the
dataset. We show that this technique has several advantages over state
of the art methods such as the ability to provide a stable segmentation
of point cloud data under noisy or poor sampling conditions and its
independence of a fixed distance metric.},
keywords={image segmentation;sampling methods;3D point cloud data;3D
point cloud segmentation;fixed distance metric;persistent homology
theory;sampling conditions;topological persistence;zeroth homology
group;Clustering algorithms;Face;Image
segmentation;Robots;Sensors;Three-dimensional displays;Topology},
doi={10.1109/ICRA.2016.7487710},
month={May},}
@INPROCEEDINGS{7487402,
author={O. Kechagias-Stamatis and N. Aouf},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Histogram of distances for local surface description},
year={2016},
pages={2487-2493},
abstract={3D object recognition is proven superior compared to its 2D
counterpart with numerous implementations, making it a current research
topic. Local based proposals specifically, although being quite
accurate, they limit their performance on the stability of their local
reference frame or axis (LRF/A) on which the descriptors are defined.
Additionally, extra processing time is demanded to estimate the LRF for
each local patch. We propose a 3D descriptor which overrides the
necessity of a LRF/A reducing dramatically processing time needed. In
addition robustness to high levels of noise and non-uniform subsampling
is achieved. Our approach, namely Histogram of Distances is based on
multiple L2-norm metrics of local patches providing a simple and fast to
compute descriptor suitable for time-critical applications. Evaluation
on both high and low quality popular point clouds showed its promising
performance.},
keywords={object recognition;statistical analysis;2D object
recognition;3D object recognition;L2-norm metrics;LRF estimation;LRF/A
stability;histogram of distances approach;local reference frame or
axis;local surface description;point
clouds;Histograms;Measurement;Object recognition;Pattern
recognition;Robustness;Shape;Three-dimensional displays;3D Matching;3D
Object Recognition;Local Features;Point Cloud},
doi={10.1109/ICRA.2016.7487402},
month={May},}
@INPROCEEDINGS{7487400,
author={P. Abelha and F. Guerin and M. Schoeler},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={A model-based approach to finding substitute tools in 3D vision
data},
year={2016},
pages={2471-2478},
abstract={A robot can feasibly be given knowledge of a set of tools for
manipulation activities (e.g. hammer, knife, spatula). If the robot then
operates outside a closed environment it is likely to face situations
where the tool it knows is not available, but alternative unknown tools
are present. We tackle the problem of finding the best substitute tool
based solely on 3D vision data. Our approach has simple hand-coded
models of known tools in terms of superquadrics and relationships among
them. Our system attempts to fit these models to point clouds of unknown
tools, producing a numeric value for how good a fit is. This value can
be used to rate candidate substitutes. We explicitly control how closely
each part of a tool must match our model, under direction from
parameters of a target task. We allow bottom-up information from
segmentation to dictate the sizes that should be considered for various
parts of the tool. These ideas allow for a flexible matching so that
tools may be superficially quite different, but similar in the way that
matters. We evaluate our system's ratings relative to other approaches
and relative to human performance in the same task. This is an approach
to knowledge transfer, via a suitable representation and reasoning
engine, and we discuss how this could be extended to transfer in
planning.},
keywords={computer graphics;manipulators;robot vision;3D vision
data;flexible matching;hand-coded models;knowledge transfer;manipulation
activities;model-based approach;point clouds;reasoning
engine;robot;substitute tools;superquadrics;Computational modeling;Data
models;Numerical models;Robots;Shape;Solid modeling;Three-dimensional
displays},
doi={10.1109/ICRA.2016.7487400},
month={May},}
@INPROCEEDINGS{7487375,
author={C. A. Mueller and A. Birk},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Hierarchical graph-based discovery of non-primitive-shaped
objects in unstructured environments},
year={2016},
pages={2263-2270},
abstract={We present a hierarchical graph-based approach for unknown
object discovery in RGB-D point clouds captured with a Kinect-like
sensor from unstructured scenes. A two-step approach is proposed which
first extracts meaningful regions from an input scene through
over-segmentation. Secondly, a procedure is introduced to detect
compositions of such regions that can represent primitive-shaped object
candidates like boxes or cylinders. Complex-shaped objects are
interpreted as a composition of primitive-shaped objects, for instance,
a teddy bear can consist of two convex-shaped arms, legs, a
convex-shaped head and torso. An ensemble of classifiers is trained to
learn patterns from the appearances of such neighboring primitive shapes
that constitute complex-shaped objects. Therein the appearance is
described by a set of features from the texture and geometry domain. For
the experiments, a dataset was prepared which is publicly available,
containing a set of scenes which consists of 296 human-annotated object
instances in total. Experiments show that the proposed hierarchical
approach is capable to extract meaningful regions: an under-segmentation
rate of 2.6% has been achieved. Furthermore, objects are segmented with
a segmentation rate of 92.9% which reflects the capability of our
approach to detect potential object candidates within unstructured
scenes.},
keywords={graph theory;image classification;image colour analysis;image
segmentation;image sensors;image texture;object detection;robot
vision;Kinect-like sensor;RGB-D point clouds;classifier
ensemble;complex-shaped objects;convex-shaped arms;convex-shaped
head;geometry domain;hierarchical graph-based discovery;human-annotated
object instances;nonprimitive-shaped
objects;over-segmentation;primitive-shaped object
candidates;primitive-shaped objects;teddy bear;texture
domain;under-segmentation rate;unknown object discovery;unstructured
environments;unstructured scenes;Cognitive science;Color;Feature
extraction;Reliability;Robots;Shape;Three-dimensional displays},
doi={10.1109/ICRA.2016.7487375},
month={May},}
@INPROCEEDINGS{7487784,
author={S. Akkaladevi and M. Ankerl and C. Heindl and A. Pichler},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Tracking multiple rigid symmetric and non-symmetric objects in
real-time using depth data},
year={2016},
pages={5644-5649},
abstract={In this paper, a robust, real-time object tracking approach
capable of dealing with multiple symmetric and non-symmetric objects in
a real-time requirement setting is proposed. The approach relies only on
depth data to track multiple objects in a dynamic environment and uses
random-forest based learning to deal with problems like object
occlusion, motion-blur due to camera motion and clutter. We show that
the relation between object motion and the corresponding change in its
3D point cloud data can be learned using only 6 random forests. A
framework that unites object pose estimation and object pose tracking to
efficiently track multiple objects in 3D space is presented. The
approach is robust in tracking objects even in presence of motion blur
that causes noisy depth data and is capable of real-time performance
with 1.8ms per frame. The experimental evaluations demonstrate the
performance of the approach against robustness, accuracy and speed and
compare the approach with the state of the art. A publicly available
dataset with real-world data is also provided for future benchmarking.},
keywords={cameras;cloud computing;image denoising;image
restoration;learning (artificial intelligence);object tracking;pose
estimation;3D point cloud data;3D space;camera motion;motion
blur;multiple rigid nonsymmetric objects tracking;multiple rigid
symmetric object tracking;noisy depth data;object motion;object pose
tracking;pose estimation;random-forest based learning;real-time object
tracking approach;real-time requirement setting;Cameras;Object
tracking;Real-time systems;Robustness;Three-dimensional
displays;Vegetation},
doi={10.1109/ICRA.2016.7487784},
month={May},}
@INPROCEEDINGS{7487265,
author={L. Bose and A. Richards},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Fast depth edge detection and edge based RGB-D SLAM},
year={2016},
pages={1323-1330},
abstract={This paper presents a method of occluding depth edge-detection
targeted towards RGB-D video streams and explores the use of these and
other edge features in RGB-D SLAM. The proposed depth edge-detection
approach uses prior information obtained from the previous RGB-D video
frame to determine which areas of the current depth image are likely to
contain edges due to image similarity. By limiting the search for edges
to these areas a significant amount of computation time is saved
compared to searching the entire image. Pixels belonging to both the
depth and colour edges of an RGB-D image can be back projected using the
depth component to form 3D point clouds of edge points. Registration
between such edge point clouds is achieved using ICP and we present a
realtime RGB-D SLAM system utilizing such back projected edge features.
Experimental results are presented demonstrating the performance of both
the proposed depth edge-detection and SLAM system using publicly
available datasets.},
keywords={SLAM (robots);edge detection;image colour analysis;iterative
methods;mobile robots;robot vision;video streaming;3D point
clouds;ICP;RGB-D video streams;back projected edge features;computation
time;depth component;edge based RGB-D SLAM;edge features;fast depth edge
detection;image similarity;publicly available datasets;Cameras;Image
edge detection;Iterative closest point algorithm;Simultaneous
localization and mapping;Streaming media;Three-dimensional displays},
doi={10.1109/ICRA.2016.7487265},
month={May},}
@INPROCEEDINGS{7487623,
author={Youngsun Kwon and Donghyuk Kim and Sung-eui Yoon},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Super ray based updates for occupancy maps},
year={2016},
pages={4267-4274},
abstract={We present a novel approach, Super Ray, for efficiently
updating map representations such as grids and octrees with point
clouds. In this paper, we define a super ray for points as a
representative ray to them with an associated frustum. A super ray is
constructed in a way that updating those points has the same set of
cells accessed during the map update process. As a result, we can
perform the update process with a super ray in a single traversal on the
map, resulting in performance improvement without compromising any
representation accuracy of the map. For constructing super rays
efficiently, we propose mapping lines for handling 2-D and 3-D cases
from an observation that edges or grid points branch out the access
pattern of updating the map. Our method is general enough to be applied
for variety of occupancy map structures based on axis-aligned space
subdivisions such as grids and octrees. We test our method into indoor
and outdoor benchmarks, and achieve 2.5 times on average (up to 3.5
times) performance improvement over the state-of-the-art update method
for OctoMap and grid maps.},
keywords={computer graphics;octrees;OctoMap;axis-aligned space
subdivisions;frustum;grid maps;map representations;map update
process;mapping lines;occupancy maps;octrees;point clouds;representative
ray;super ray based updates;Memory management;Octrees;Path
planning;Robot sensing systems;Space heating;Three-dimensional displays},
doi={10.1109/ICRA.2016.7487623},
month={May},}
@INPROCEEDINGS{7487208,
author={C. Linegar and W. Churchill and P. Newman},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Made to measure: Bespoke landmarks for 24-hour, all-weather
localisation with a camera},
year={2016},
pages={787-794},
abstract={This paper is about camera-only localisation in challenging
outdoor environments, where changes in lighting, weather and season
cause traditional localisation systems to fail. Conventional approaches
to the localisation problem rely on point-features such as SIFT, SURF or
BRIEF to associate landmark observations in the live image with
landmarks stored in the map; however, these features are brittle to the
severe appearance change routinely encountered in outdoor environments.
In this paper, we propose an alternative to traditional point-features:
we train place-specific linear SVM classifiers to recognise distinctive
elements in the environment. The core contribution of this paper is an
unsupervised mining algorithm which operates on a single mapping dataset
to extract distinct elements from the environment for localisation. We
evaluate our system on 205km of data collected from central Oxford over
a period of six months in bright sun, night, rain, snow and at all times
of the day. Our experiment consists of a comprehensive N-vs-N analysis
on 22 laps of the approximately 10km route in central Oxford. With our
proposed system, the portion of the route where localisation fails is
reduced by a factor of 6, from 33.3% to 5.5%.},
keywords={cameras;data mining;feature extraction;image
classification;support vector machines;unsupervised
learning;BRIEF;N-vs-N analysis;SIFT;SURF;all-weather
localisation;camera-only localisation;distinctive elements;landmark
observations;outdoor environments;place-specific linear SVM
classifiers;unsupervised mining algorithm;Clouds;Detectors;Feature
extraction;Robots;Robustness;Sun;Training},
doi={10.1109/ICRA.2016.7487208},
month={May},}
@INPROCEEDINGS{7487648,
author={M. Velas and M. Spanel and A. Herout},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Collar Line Segments for fast odometry estimation from Velodyne
point clouds},
year={2016},
pages={4486-4495},
abstract={We present a novel way of odometry estimation from Velodyne
LiDAR point cloud scans. The aim of our work is to overcome the most
painful issues of Velodyne data - the sparsity and the quantity of data
points - in an efficient way, enabling more precise registration.
Alignment of the point clouds which yields the final odometry is based
on random sampling of the clouds using Collar Line Segments (CLS). The
closest line segment pairs are identified in two sets of line segments
obtained from two consequent Velodyne scans. From each pair of
correspondences, a transformation aligning the matched line segments
into a 3D plane is estimated. By this, significant planes (ground,
walls, ...) are preserved among aligned point clouds. Evaluation using
the KITTI dataset shows that our method outperforms publicly available
and commonly used state-of-the-art method GICP for point cloud
registration in both accuracy and speed, especially in cases where the
scene lacks significant landmarks or in typical urban elements. For such
environments, the registration error of our method is reduced by 75%
compared to the original GICP error.},
keywords={computer graphics;distance measurement;estimation
theory;optical radar;random processes;sampling methods;KITTI
dataset;Velodyne LiDAR point cloud scans;collar line segments;data point
quantity;data point sparsity;odometry estimation;point cloud
registration;random sampling;Cameras;Estimation;Image
segmentation;Iterative closest point algorithm;Laser radar;Robot sensing
systems;Three-dimensional displays},
doi={10.1109/ICRA.2016.7487648},
month={May},}
@INPROCEEDINGS{7487687,
author={T. Cieslewski and E. Stumm and A. Gawel and M. Bosse and S.
Lynen and R. Siegwart},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Point cloud descriptors for place recognition using sparse visual
information},
year={2016},
pages={4830-4836},
abstract={Place recognition is a core component in simultaneous
localization and mapping (SLAM), limiting positional drift over space
and time to unlock precise robot navigation. Determining which
previously visited places belong together continues to be a highly
active area of research as robotic applications demand increasingly
higher accuracies. A large number of place recognition algorithms have
been proposed, capable of consuming a variety of sensor data including
laser, sonar and depth readings. The best performing solutions, however,
have utilized visual information by either matching entire images or
parts thereof. Most commonly, vision based approaches are inspired by
information retrieval and utilize 3D-geometry information about the
observed scene as a post-verification step. In this paper we propose to
use the 3D-scene information from sparse-visual feature maps directly at
the core of the place recognition pipeline. We propose a novel
structural descriptor which aggregates sparse triangulated landmarks
from SLAM into a compact signature. The resulting 3D-features provide a
discriminative fingerprint to recognize places over seasonal and
viewpoint changes which are particularly challenging for approaches
based on sparse visual descriptors. We evaluate our system on publicly
available datasets and show how its complementary nature can provide an
improvement over visual place recognition.},
keywords={SLAM (robots);feature extraction;image matching;object
recognition;path planning;robot vision;visual
databases;3D-features;3D-geometry information;3D-scene
information;SLAM;image matching;information retrieval;point cloud
descriptors;positional drift;publicly available datasets;robot
navigation;seasonal changes;simultaneous localization and mapping;sparse
triangulated landmarks;sparse visual descriptors;sparse visual
information;sparse-visual feature maps;structural descriptor;viewpoint
changes;vision-based approach;visual information;visual place
recognition;Databases;Histograms;Pipelines;Simultaneous localization and
mapping;Three-dimensional displays;Visualization},
doi={10.1109/ICRA.2016.7487687},
month={May},}
@INPROCEEDINGS{7487373,
author={D. J. Lee and M. E. Campbell},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={An efficient probabilistic surface normal estimator},
year={2016},
pages={2248-2254},
abstract={An efficient surface normal estimation method is presented.
The new algorithm estimates surface normal direction for each cell in a
grid based on the occupancy information (both occupied and empty) of the
neighboring cells. This grid representation allows user-defined sizes
and scaling with the environment, not the number of measurements.
Recursive and batch formulations to obtain the posterior estimate are
presented, and compared. A computationally efficient implementation is
derived which provides consistent and accurate estimates as measurements
become available. Both simulation and experimental results are shown,
demonstrating comparable estimation performance to that of using Point
Cloud Library, but with significantly reduced computation time.},
keywords={probability;recursive estimation;robots;batch
formulations;grid representation;occupancy information;point cloud
library;probabilistic surface normal estimator efficiency;recursive
formulations;robotics;surface normal direction;Estimation;Mathematical
model;Object recognition;Probabilistic logic;Robots;Three-dimensional
displays;Time measurement},
doi={10.1109/ICRA.2016.7487373},
month={May},}
@INPROCEEDINGS{7487138,
author={A. Narr and R. Triebel and D. Cremers},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Stream-based Active Learning for efficient and adaptive
classification of 3D objects},
year={2016},
pages={227-233},
abstract={We present a new Active Learning approach for classifying
objects from streams of 3D point cloud data. The major problems here are
the non-uniform occurrence of class instances and the unbalanced numbers
of samples per class. We show that standard online learning methods
based on decision trees perform comparably bad for such data streams,
which are however particularly relevant for mobile robots that need to
learn semantics persistently. To address this, we use Mondrian forests
(MF), a recent online learning algorithm that is independent on the data
order. We present an extension of that algorithm and show that MF are
less overconfident than standard Random Forests. In experiments on the
KITTI benchmark, we show that this leads to a substantially improved
classification performance for data streams, rendering our approach very
attractive for lifelong robot learning applications.},
keywords={computer graphics;decision trees;image classification;learning
(artificial intelligence);mobile robots;object recognition;3D objects
adaptive classification;3D point cloud data;KITTI benchmark;MF;Mondrian
forests;data order;data streams;decision trees;mobile robots;online
learning algorithm;random forests;robot learning
applications;semantics;standard online learning methods;stream-based
active learning;Learning
systems;Robots;Semantics;Standards;Three-dimensional
displays;Training;Training data},
doi={10.1109/ICRA.2016.7487138},
month={May},}
@INPROCEEDINGS{7487145,
author={A. Renzaglia and C. Reymann and S. Lacroix},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Monitoring the evolution of clouds with UAVs},
year={2016},
pages={278-283},
abstract={We study the problem of monitoring the evolution of
atmospheric variables within low-altitude cumulus clouds with a fleet of
Unmanned Aerial Vehicles (UAVs). To tackle this challenge, two main
problems can be identified: i) creating on-line maps of the relevant
variables, based on sparse local measurements; ii) designing a planning
algorithm which exploits the obtained map to generate trajectories that
optimize the adaptive data sampling process, minimizing the uncertainty
in the map, while steering the vehicles within the air flows to generate
energetic-efficient flights. Our approach is based on Gaussian Processes
(GP) for the mapping, combined with a stochastic optimization scheme for
the trajectories generation. The system is tested in simulations carried
out using a realistic three-dimensional current field. Results for a
single UAV as well as for a fleet of multiple UAVs, sharing information
to cooperatively achieve the mission, are provided.},
keywords={Gaussian processes;autonomous aerial
vehicles;optimisation;sampling methods;steering systems;trajectory
control;GP;Gaussian processes;UAV;adaptive data sampling
process;atmospheric variables;cloud evolution;energetic-efficient
flights;low-altitude cumulus clouds;on-line maps;planning
algorithm;sparse local measurements;stochastic optimization
scheme;three-dimensional current field;trajectories generation;unmanned
aerial vehicles;vehicle steering;Atmospheric measurements;Atmospheric
modeling;Clouds;Covariance matrices;Kernel;Optimization;Trajectory},
doi={10.1109/ICRA.2016.7487145},
month={May},}
@INPROCEEDINGS{7487370,
author={J. Schlosser and C. K. Chow and Z. Kira},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Fusing LIDAR and images for pedestrian detection using
convolutional neural networks},
year={2016},
pages={2198-2205},
abstract={In this paper, we explore various aspects of fusing LIDAR and
color imagery for pedestrian detection in the context of convolutional
neural networks (CNNs), which have recently become state-of-art for many
vision problems. We incorporate LIDAR by up-sampling the point cloud to
a dense depth map and then extracting three features representing
different aspects of the 3D scene. We then use those features as extra
image channels. Specifically, we leverage recent work on HHA [9]
(horizontal disparity, height above ground, and angle) representations,
adapting the code to work on up-sampled LIDAR rather than Microsoft
Kinect depth maps. We show, for the first time, that such a
representation is applicable to up-sampled LIDAR data, despite its
sparsity. Since CNNs learn a deep hierarchy of feature representations,
we then explore the question: At what level of representation should we
fuse this additional information with the original RGB image channels?
We use the KITTI pedestrian detection dataset for our exploration. We
first replicate the finding that region-CNNs (R-CNNs) [8] can outperform
the original proposal mechanism using only RGB images, but only if
fine-tuning is employed. Then, we show that: 1) using HHA features and
RGB images performs better than RGB-only, even without any fine-tuning
using large RGB web data, 2) fusing RGB and HHA achieves the strongest
results if done late, but, under a parameter or computational budget, is
best done at the early to middle layers of the hierarchical
representation, which tend to represent midlevel features rather than
low (e.g. edges) or high (e.g. object class decision) level features, 3)
some of the less successful methods have the most parameters, indicating
that increased classification accuracy is not simply a function of
increased capacity in the neural network.},
keywords={feature extraction;image colour analysis;image fusion;image
representation;image sensors;neural nets;object detection;optical
radar;pedestrians;traffic engineering computing;3D scene;CNN;HHA
features;KITTI pedestrian detection dataset;LIDAR image fusion;Microsoft
Kinect depth maps;R-CNN;RGB Web data;RGB image channels;convolutional
neural networks;depth map;feature extraction;feature
representations;height above ground;horizontal disparity;mid-level
feature representation;object class decision;pedestrian
detection;region-CNN;up-sampled LIDAR data;Computer architecture;Feature
extraction;Laser radar;Neural networks;Proposals;Robot sensing
systems;Training},
doi={10.1109/ICRA.2016.7487370},
month={May},}
@INPROCEEDINGS{7487682,
author={J. Schneider and C. Eling and L. Klingbeil and H. Kuhlmann and
W. Förstner and C. Stachniss},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Fast and effective online pose estimation and mapping for UAVs},
year={2016},
pages={4784-4791},
abstract={Online pose estimation and mapping in unknown environments is
essential for most mobile robots. Especially autonomous unmanned aerial
vehicles require good pose estimates at comparably high frequencies. In
this paper, we propose an effective system for online pose and
simultaneous map estimation designed for light-weight UAVs. Our system
consists of two components: (1) real-time pose estimation combining
RTK-GPS and IMU at 100 Hz and (2) an effective SLAM solution running at
10 Hz using image data from an omnidirectional multi-fisheye-camera
system. The SLAM procedure combines spatial resection computed based on
the map that is incrementally refined through bundle adjustment and
combines the image data with raw GPS observations and IMU data on
keyframes. The overall system yields a real-time, georeferenced pose at
100 Hz in GPS-friendly situations. Additionally, we obtain a precise
pose and feature map at 10 Hz even in cases where the GPS is not
observable or underconstrained. Our system has been implemented and
thoroughly tested on a 5 kg copter and yields accurate and reliable pose
estimation at high frequencies. We compare the point cloud obtained by
our method with a model generated from georeferenced terrestrial laser
scanner.},
keywords={autonomous aerial vehicles;mobile robots;pose estimation;IMU
data;RTK-GPS;SLAM procedure;SLAM solution;autonomous unmanned aerial
vehicles;bundle adjustment;georeferenced terrestrial laser scanner;image
data;light-weight UAV;mapping;mobile robots;omnidirectional
multi-fisheye-camera system;online pose estimation;raw GPS
observations;real-time pose estimation;reliable pose
estimation;simultaneous map estimation;spatial resection;unknown
environments;Cameras;Global Positioning System;Real-time
systems;Receivers;Simultaneous localization and mapping},
doi={10.1109/ICRA.2016.7487682},
month={May},}
@INPROCEEDINGS{7487297,
author={Seongyong Koo and S. Behnke},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Focused online visual-motor coordination for a dual-arm robot
manipulator},
year={2016},
pages={1579-1586},
abstract={Coordination between visual sensors and robot manipulators is
necessary for successful manipulation. This paper proposes a novel
visual-motor coordination method that performs online parameter
estimation of an RGB-D camera mounted in a robot head without any
external markers. Through self-observation of a dual-arm robot
manipulator, the method updates parameters to reduce the discrepancy
between observed point cloud data and 3D mesh models of the current
robot configuration. With the estimated parameters at each time step,
visual data is adjusted to the focused workspace of the 14DOF dual-arm
robot manipulator. The online and realtime algorithm was developed by
using a GPU-based particle filtering method. Experimental results show
that our method outperforms state-of-the-art offline registration
methods in terms of accuracy and computation time. We also analyzed the
dependence of the results on prior parameters to demonstrate the online
capability of our method.},
keywords={computational complexity;graphics processing
units;manipulators;parameter estimation;particle filtering (numerical
methods);sensors;3D mesh models;DOF dual-arm robot manipulator;GPU-based
particle filtering method;RGB-D camera;computation time;focused online
visual-motor coordination;online parameter estimation;point cloud
data;real-time algorithm;robot head;self-observation;visual
sensors;Calibration;Cameras;Robot kinematics;Solid
modeling;Three-dimensional displays;Visualization},
doi={10.1109/ICRA.2016.7487297},
month={May},}
@INPROCEEDINGS{7487310,
author={H. F. M. Zaki and F. Shafait and A. Mian},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Convolutional hypercube pyramid for accurate RGB-D object
category and instance recognition},
year={2016},
pages={1685-1692},
abstract={Deep learning based methods have achieved unprecedented
success in solving several computer vision problems involving RGB
images. However, this level of success is yet to be seen on RGB-D images
owing to two major challenges in this domain: training data deficiency
and multi-modality input dissimilarity. We present an RGB-D object
recognition framework that addresses these two key challenges by
effectively embedding depth and point cloud data into the RGB domain. We
employ a convolutional neural network (CNN) pre-trained on RGB data as a
feature extractor for both color and depth channels and propose a rich
coarse-to-fine feature representation scheme, coined Hypercube Pyramid,
that is able to capture discriminatory information at different levels
of detail. Finally, we present a novel fusion scheme to combine the
Hypercube Pyramid features with the activations of fully connected
neurons to construct a compact representation prior to classification.
By employing Extreme Learning Machines (ELM) as non-linear classifiers,
we show that the proposed method outperforms ten state-of-the-art
algorithms for several tasks in terms of recognition accuracy on the
benchmark Washington RGB-D and 2D3D object datasets by a large margin
(upto 50% reduction in error rate).},
keywords={category theory;computer vision;convolution;image
classification;image colour analysis;image fusion;image
representation;learning (artificial intelligence);neural nets;object
recognition;CNN;ELM;RGB-D images;RGB-D object category;RGB-D object
recognition;classification;coarse-to-fine feature
representation;computer vision;convolutional hypercube
pyramid;convolutional neural network;deep learning;extreme learning
machines;fusion scheme;instance recognition;multimodality input
dissimilarity;nonlinear classifiers;point cloud data;training data
deficiency;Feature extraction;Hypercubes;Image color analysis;Object
recognition;Robots;Three-dimensional displays;Training},
doi={10.1109/ICRA.2016.7487310},
month={May},}
@INPROCEEDINGS{7485400,
author={J. Han and M. Kang and J. Wang and J. Kim},
booktitle={OCEANS 2016 - Shanghai},
title={Three-dimensional reconstruction of a semi-submersible offshore
platform with an unmanned surface vehicle},
year={2016},
pages={1-6},
abstract={This paper addresses the three-dimensional (3D) reconstruction
of a floating structure with an unmanned surface vehicle (USV). Onboard
lidar and sonar sensors are employed to collect a volumetric point cloud
of the structure both above and below the waterline. These measurements
are obtained in the vehicle-fixed frame; thus, for successful 3D
reconstruction, precision estimation of trajectory and attitude is
required. GPS signals are severely deteriorated or unavailable near and
under floating structures. Therefore, relative navigation with respect
to the planar surfaces of their hull structures is performed in the
framework of simultaneous localization and mapping (SLAM). This approach
enables high-precision navigation and mapping near and under a large
floating structure. A field experiment was performed in a
semi-submersible offshore platform environment and the results are
presented.},
keywords={Global Positioning System;SLAM (robots);image
reconstruction;navigation;offshore installations;optical radar;remotely
operated vehicles;sonar;GPS signals;SLAM;USV;floating
structure;high-precision navigation;hull structures;onboard
LIDAR;semi-submersible offshore platform;simultaneous localization and
mapping;sonar sensors;three-dimensional reconstruction;unmanned surface
vehicle;volumetric point cloud;Laser radar;Sensors;Sonar;Sonar
navigation;Surface reconstruction;Three-dimensional displays;Vehicles;3D
reconstruction;relative navigation;simultaneous localization and
mapping;unmanned surface vehicle},
doi={10.1109/OCEANSAP.2016.7485400},
month={April},}
@INPROCEEDINGS{7487738,
author={D. M. Lofaro and A. Asokan},
booktitle={2016 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Low latency bounty hunting and geographically adjacent server
configuration for real-time cloud control},
year={2016},
pages={5277-5282},
abstract={This paper explores the feasibility and proposes a method on
how to obtain high frequency real-time controllers operating in loop
with physical robot hardware over a geographically adjacent and bounty
hunting cloud server architecture. Having the cloud in the loop has many
purposes including increasing computation, decreasing “on robot” power
usage, reducing overall robot weight etc. Today when robots use the
cloud in loop it is typically for sharing information, high level
planning, and other non-real-time tasks. All of the balancing and
stability algorithms stay onboard the robot. What if we could run high
frequency real-time loops over the cloud? The better question is how
would we do that? As with any real-time system latency is a big factor
in the application and operation frequency. This paper shows that with a
geographically adjacent and bounty hunting server approach it is
feasible to obtain high frequency real-time control in loop with with a
physical robot over the cloud. The feasibility of such a system running
in-loop on humanoids and wheeled robots are explored and tested.},
keywords={cloud computing;control engineering computing;geographic
information systems;humanoid robots;real-time systems;bounty hunting
cloud server architecture;frequency real-time controllers;geographically
adjacent server configuration;humanoid robot;low latency bounty
hunting;physical robot;physical robot hardware;real-time cloud
control;robot weight;stability algorithms;Bandwidth;Cloud
computing;Computer architecture;Legged locomotion;Real-time
systems;Servers},
doi={10.1109/ICRA.2016.7487738},
month={May},}
@INPROCEEDINGS{7476140,
author={Z. Yuan and X. Huang and L. Sun and J. Jin},
booktitle={2016 IEEE International Conference on Control and Robotics
Engineering (ICCRE)},
title={Software defined mobile sensor network for micro UAV swarm},
year={2016},
pages={1-4},
abstract={This paper introduces novel mobile sensor networking
architecture for a swarm of micro unmanned vehicles (MAVs) using
software defined network (SDN) technology. The proposed architecture
aims to enhance the performance of user/control plane data transmission
between MAVs. SDN technique for mobile network is adopted to release the
computation burden of MAV nodes and improve the wireless channel
resource utilization by moving the complex network management operations
to cloud-based SDN controller.},
keywords={autonomous aerial vehicles;cloud computing;software defined
networking;wireless channels;MAV nodes;SDN technique;cloud-based SDN
controller;complex network management operations;data
transmission;microUAV swarm;microunmanned vehicles;mobile sensor
networking architecture;software defined mobile sensor network;wireless
channel resource utilization;Ad hoc networks;Computer
architecture;Mobile communication;Mobile
computing;Routing;Topology;Wireless communication;SDN;UAV;mobile sensor
network;swarm},
doi={10.1109/ICCRE.2016.7476140},
month={April},}
@ARTICLE{7299600,
author={M. Bilal and J. E. Nichol and M. Nazeer},
journal={IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing},
title={Validation of Aqua-MODIS C051 and C006 Operational Aerosol
Products Using AERONET Measurements Over Pakistan},
year={2016},
volume={9},
number={5},
pages={2074-2080},
abstract={The aim of this study was to evaluate the performance of the
Aqua-MODIS (MYD04) collections 5.1 (C051) and 6 (C006) operational
aerosol products over Pakistan. These include the Dark Target (DT) and
the Deep Blue (DB) C051/C006 aerosol optical depth (AOD) observations at
10-km spatial resolution, and which were validated using 7 years
(2007-2013) AOD measurements from two AERONET stations located in
Pakistan's two largest cities, Lahore and Karachi. Lahore is an inland
city, consisting of built-up areas and agricultural land and dominated
by mainly fine aerosol particles. Karachi is an urban-coastal city with
built-up areas and bare land, and mainly affected by coarse aerosol
particles. The retrieval uncertainties and accuracy were evaluated using
the expected error over land (EE : ±0.05 + 15%), the root-mean-square
error (RMSE), the mean absolute error (MAE), and the relative mean bias
(RMB). It was found that the DT C051 AOD retrievals were significantly
overestimated over both AERONET sites with mean overestimation of 21%
and 31% for Lahore and Karachi, respectively. On the other hand, the DB
C051 retrievals were underestimated by 10% and 35% for both cities.
Similar to the DT C051, the C006 AOD retrievals were overestimated over
Lahore, but significantly improved over Karachi, as the mean
overestimation reduced from 31% (RMB = 1.31) to 4% (RMB = 1.04) and the
percentage of retrievals within the EE increased from 38.46% to 63.33%.
However, the DB C006 AOD retrievals have similar errors (RMSE and MAE)
and the percentage of retrievals within the EE over both cities as C051,
but they have more mean underestimations. Spatio-temporal distributions
showed that the DT and DB C006 were well correlated with Karachi and
Lahore AERONET measurements, respectively. Therefore, these results
recommend the use of the DT C006 algorithm over Karachi and the DB C006
over Lahore for qualitative regional air quality applications due to
differences in land cover c- aracteristics and aerosol types.},
keywords={aerosols;air quality;atmospheric composition;atmospheric
techniques;estimation theory;mean square error methods;remote sensing;AD
2007 to 2013;AERONET measurement;Aqua-MODIS collection 5.1 operational
aerosol product;Aqua-MODIS collection 6 operational aerosol
product;AquaMODIS C006 operational aerosol product;AquaMODIS C051
operational aerosol product;DT C051 AOD
retrieval;Karachi;Lahore;Pakistan;aerosol optical depth;aerosol
particle;aerosol types;agricultural land;dark target aerosol optical
depth;deep blue aerosol optical depth;inland city;land cover
characteristics;mean absolute error;regional air quality
application;relative mean bias;root-mean-square error;spatial
resolution;spatiotemporal distribution;urban-coastal
city;Aerosols;Atmospheric measurements;Cities and
towns;Clouds;MODIS;Spatial resolution;Uncertainty;AOD;Aerosol robotic
network (AERONET);C006;Karachi;Lahore;MODIS},
doi={10.1109/JSTARS.2015.2481460},
ISSN={1939-1404},
month={May},}
@INPROCEEDINGS{7454548,
author={Y. Ma and D. Xiao and R. Li and Ruan Hang and Shan Zhao and
Junlong Zhao and Y. Zhang},
booktitle={2015 17th International Conference on E-health Networking,
Application Services (HealthCom)},
title={Android-based intelligent mobile robot for indoor healthcare},
year={2015},
pages={472-474},
abstract={The intelligent mobile robot platform based on Android
features multiple wireless communication functions, which, via Internet
and WLAN, may control the movement of the robot. The platform integrates
cloud speech recognition and offline speech recognition technologies to
control the movement of the robot and have simple man-machine
conversation. In addition, with a camera, this platform may realize the
remote real-time transmission of video. Since the platform is
characterized with sound hardware compatibility and expansibility, we
may conduct the research of the robot and rapidly develop an intelligent
mobile robot which applies to a specific application scenario on the
platform.},
keywords={Android (operating system);Internet;health care;human-robot
interaction;intelligent robots;medical robotics;mobile robots;speech
recognition;wireless LAN;Android-based intelligent mobile
robot;Internet;WLAN;camera;cloud speech recognition;indoor
healthcare;intelligent mobile robot;multiple wireless
communication;offline speech recognition technologies;remote real-time
transmission;robot movement;simple man-machine conversation;sound
hardware compatibility;sound hardware expansibility;video;Cloud
computing;Mobile robots;Robot sensing systems;Smart phones;Speech
recognition;Wireless communication},
doi={10.1109/HealthCom.2015.7454548},
month={Oct},}
@ARTICLE{7378304,
author={J. Li and Z. Teng and J. Xiao},
journal={IEEE Robotics and Automation Letters},
title={Can a Continuum Manipulator Fetch an Object in an Unknown
Cluttered Space?},
year={2017},
volume={2},
number={1},
pages={2-9},
abstract={Continuum manipulators are particularly suitable for
performing tasks in cluttered environments with limited space for
maneuvering. While there is progress on continuum grasping of an object
in a cluttered space, if the environment is unknown where the target
object is occluded and only partially visible, such as in a search and
rescue scenario, an open problem is how to determine if it is possible
to fetch the object. In this letter, we address this problem of online
determining whether a partially occluded object nested in an unknown
cluttered space can be fetched by a continuum manipulator based on
sensing the surrounding obstacles progressively with a distance sensor
attached to its tip, and if so, how to fetch the object autonomously.
Our method formulates constraints that can be quickly checked from
sensed information to decide if a solution exists for a multisection
spatial continuum manipulator to access and grasp an object in a
cluttered space. Examples using point clouds of unknown obstacles from
RGB-D sensing illustrate the effectiveness of our approach.},
keywords={manipulators;sensors;RGB-D sensing;cluttered space;continuum
grasping;distance sensor;multisection spatial continuum
manipulator;Belts;Grasping;Manipulators;Planning;Robot sensing
systems;Flexible Robots;Flexible robots;Manipulation
Planning;Manipulation in Unknown Environments;Perception-based
Navigation in Cluttered Space;manipulation in unknown
environments;manipulation planning;perception-based navigation in
cluttered space},
doi={10.1109/LRA.2016.2516589},
ISSN={2377-3766},
month={Jan},}
@INPROCEEDINGS{7442946,
author={A. Maligo and S. Lacroix},
booktitle={2015 IEEE International Symposium on Safety, Security, and
Rescue Robotics (SSRR)},
title={Classification of outdoor 3D lidar data based on unsupervised
Gaussian mixture models},
year={2015},
pages={1-7},
abstract={3D point clouds acquired with lidars are an important source
of data for the classification of outdoor environments by autonomous
terrestrial robots. We propose here a two-layer classification system.
The first layer consists of a Gaussian mixture model, issued from
unsupervised training, which defines a large set of data-oriented
classes. The second layer consists of a supervised, manual grouping of
the unsupervised classes into a smaller set of task-oriented classes.
Because it uses unsupervised learning at its core, the system does not
require any manual labelling of datasets. We evaluate the system on two
datasets of different nature, and the results show its capacity to adapt
to different data while providing classes which are exploitable in a
target task.},
keywords={Gaussian processes;mixture models;mobile robots;optical
radar;pattern classification;unsupervised learning;3D point
clouds;autonomous terrestrial robots;manual dataset labelling;outdoor 3D
lidar data classification;outdoor environment
classification;unsupervised classes;unsupervised gaussian mixture
models;unsupervised learning;unsupervised training;Data models;Feature
extraction;Labeling;Laser radar;Manuals;Shape;Three-dimensional displays},
doi={10.1109/SSRR.2015.7442946},
month={Oct},}
@INPROCEEDINGS{7428222,
author={A. C. Leite and T. B. Almeida-Antonio and P. J. From and F.
Lizarralde and L. Hsu},
booktitle={2015 IEEE International Workshop on Advanced Robotics and its
Social Impacts (ARSO)},
title={Control and obstacle collision avoidance method applied to
human-robot interaction},
year={2015},
pages={1-8},
abstract={In this work, we present a control and obstacle collision
avoidance method for redundant robot manipulators operating in partially
structured environments in the presence of humans. The control algorithm
is based on the concept of artificial potential fields and it uses the
pseudo-inverse of the Jacobian matrix with a weighting factor for the
mechanical joint limits, taking advantage of the robot redundancy for
the purpose of obstacle avoidance and control goal achievement. The
detection algorithm uses a depth sensor based on the structured light to
obtain a 2-1/2-D description of the surroundings from a point cloud.
Repulsive fields are created around the detected obstacles, allowing for
the robot to perform the task of interest without collisions. A
filtering methodology based on geometric elements is presented to filter
the RGB-D scene captured by the depth sensor, eliminating the robot body
and the obstacles located outside its workspace. Experimental results,
obtained with a Motoman DIA10 robot and a Microsoft KinectTM, illustrate
the feasibility of the proposed scheme.},
keywords={Jacobian matrices;collision avoidance;human-robot
interaction;image colour analysis;image filtering;mobile
robots;redundant manipulators;robot vision;Jacobian matrix;Microsoft
Kinect;Motoman DIA10 robot;RGB-D scene;artificial potential fields;depth
sensor;filtering methodology;geometric elements;goal
achievement;human-robot interaction;mechanical joint limits;obstacle
collision avoidance method;obstacle detection;redundant robot
manipulators;repulsive fields;robot redundancy;weighting
factor;Collision avoidance;Jacobian
matrices;Kinematics;Quaternions;Robot kinematics;Robot sensing systems},
doi={10.1109/ARSO.2015.7428222},
month={June},}
@INPROCEEDINGS{7418871,
author={C. Liu and D. Yuan and H. Zhao},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={3D point cloud denoising and normal estimation for 3D surface
reconstruction},
year={2015},
pages={820-825},
abstract={Denoising numerous large scale noise and preserving fine
features simultaneously remains a challenge to point-cloud-related
multiple view stereo (MVS) reconstruction approaches. The proposed
algorithm reuses the sparse point cloud which is often discarded after
the structure form motion (SfM) procedure in image based modeling to
guide the dense point cloud denoising. Furthermore, the utilization of
the octree division provides an efficient and simple denoising
mechanism. Experiments show that the proposed method successfully
removes the large scale noise points and presents a satisfactory
denoising result with detailed information preserved. In addition, the
normal of each point can be estimated fast and accurately as a
by-product of the denoising algorithm.},
keywords={computer graphics;estimation theory;image denoising;image
motion analysis;image reconstruction;octrees;stereo image processing;3D
point cloud denoising;3D surface reconstruction;dense point cloud
denoising;image based modeling;normal estimation;octree
division;point-cloud-related multiple view stereo reconstruction;sparse
point cloud;structure form motion procedure;Image reconstruction;Noise
reduction;Octrees;Surface fitting;Surface reconstruction;Surface
treatment;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7418871},
month={Dec},}
@INPROCEEDINGS{7418881,
author={J. Bao and Y. Jia and Y. Cheng and H. Tang and N. Xi},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Feedback of robot states for object detection in natural language
controlled robotic systems},
year={2015},
pages={875-880},
abstract={Controlling robots with natural language enables untrained
users to interact with them more easily. A significant challenge for
such systems is the mismatched visual perceptual capabilities between
humans and robots. Most existing methods try to improve the perceptual
ability of robots by either developing robust vision algorithms to
describe and identify objects more accurately, or refining the object
segmentation through human collaboration. In this paper, we present a
novel method to detect and track objects, and even discover previously
undetected objects (e.g. objects occluded by or stacked on other
objects) by incorporating feedback of robot states into the vision
module. By reasoning about the object states according to the
trajectories of robot states and then re-detecting the point clouds of
the objects, the representation of the environment can be efficiently
and accurately updated. Experimental results demonstrate the
effectiveness and advantages of the proposed method.},
keywords={human-robot interaction;image segmentation;mobile
robots;natural language processing;object tracking;robot vision;state
feedback;human collaboration;natural language;object detection;object
segmentation;object tracking;perceptual ability;robot control;robot
state feedback;vision module;visual perceptual capabilities;Natural
languages;Robot sensing systems;Semantics;Three-dimensional
displays;Trajectory;Visualization},
doi={10.1109/ROBIO.2015.7418881},
month={Dec},}
@INPROCEEDINGS{7418763,
author={R. Kuramachi and A. Ohsato and Y. Sasaki and H. Mizoguchi},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={G-ICP SLAM: An odometry-free 3D mapping system with robust 6DoF
pose estimation},
year={2015},
pages={176-181},
abstract={The paper proposes an odometry-free 3D mapping system that
combines a LIDAR and a inertial sensor. The proposed system achieved
robust 6DoF pose estimation for arbitrary motion and is implemented as a
hand-held unit to make use of simplified mobile mapping applications.
The pose estimation algorithm is based on "Velodyne SLAM" which is a
state of the art ICP based SLAM (Simultaneous Localization and Mapping)
method using only point cloud data. We added 3DoF inertial information
to process the point cloud correction and the position prediction.
Compared to the previous method, the proposed method is robust to rotary
motion and works for fast and large change of sensor position and
orientation. The results demonstrate effective operation in various
environments and we confirmed the improvement of the self-position
estimation and mapping performance.},
keywords={SLAM (robots);computer graphics;mobile robots;path
planning;pose estimation;position control;robot vision;3DoF inertial
information;G-ICP SLAM;ICP based SLAM;LIDAR;Velodyne SLAM;arbitrary
motion;hand-held unit;inertial sensor;mapping performance;mobile mapping
applications;odometry-free 3D mapping system;point cloud
correction;point cloud data;pose estimation algorithm;position
prediction;robust 6DoF pose estimation;rotary motion;self-position
estimation;simultaneous localization and mapping
method;Estimation;Iterative closest point
algorithm;Robustness;Simultaneous localization and
mapping;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7418763},
month={Dec},}
@INPROCEEDINGS{7418987,
author={P. Yin and Y. He and F. Gu and J. Han},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Multi-relation octomap based Heuristic ICP for air/surface robots
cooperation},
year={2015},
pages={1524-1529},
abstract={In this paper, we focus on the problem of fast and accurate
featureless registration of outdoor large scale 3D point-clouds which
possess great differences in the aspects of both resolution and view of
point. There are two main methods generally used to solve this problem:
feature based algorithm and point based one. However, feature based
method can only be used in very special environments with clear
geometric structure, while traditional point based method can only
obtain a relative coarse estimation and is sensitive to initial
alignment. Thus, in this paper, a registration algorithm, called Octree
Based Multiresolution Heuristic ICP, is proposed. Without relying on the
good initial registration and marked features, hybrid-ICP combines
different ICP algorithms, and improve the alignments using finer levels
of representation. In our outdoor riverside environments experiments,
our method outperform the classical point based registration algorithm
with an accuracy of 7 times better than classical Generalized-ICP and a
speedup 1.6 times.},
keywords={feature extraction;image registration;image
resolution;iterative methods;octrees;robot vision;air/surface robots
cooperation;feature based algorithm;hybrid-ICP algorithms;iterative
closest point;marked features;multirelation octomap based heuristic
ICP;octree based multiresolution heuristic ICP;outdoor large scale 3D
point-clouds;outdoor riverside environment;point based registration
algorithm;view of point;Covariance matrices;Iterative closest point
algorithm;Octrees;Robots;Shape;Standards;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7418987},
month={Dec},}
@INPROCEEDINGS{7418891,
author={W. Xiaojun and X. Shenghua and W. Jingyang},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={High accurate 3D reconstruction method using binocular stereo
based on multiple constraints},
year={2015},
pages={934-939},
abstract={In order to make an improvement of accuracy, completeness and
robustness of the image based 3D reconstruction algorithm, we present a
stereo matching approach under the condition of multi-resolution
combined with multi-constraints to compute the 3D model of an object in
this paper. In the first step, we build the image pyramid and stereo
matching happens from the top level to the lowest level. In each level,
we execute the following procedures: NCC matching, multi-constraints
checking, re-matching and weighted median filter, disparity refinement.
At last, we acquire an accurate and smooth disparity map in the bottom
level. Combine with the calibration parameters, we can compute the 3D
point cloud and use the Possion reconstruction method to generate the
mesh model. The contributions of this paper are the proposal of choosing
the pixel in the eight-neighborhood which has the highest match cost to
guide the matching of center pixel during re-match, and design an
anisotropic smoothing kernel to smooth the depth map and keep the detail
at the same time in the disparity refinement. Experimental results show
that the algorithm has the advantages of high precision, strong
robustness and good model integrity.},
keywords={image filtering;image matching;image reconstruction;image
resolution;stereo image processing;3D point cloud;NCC matching;Possion
reconstruction method;anisotropic smoothing kernel;binocular
stereo;center pixel;depth map smoothing;disparity refinement;high
accurate image based 3D reconstruction method;image pyramid;mesh
model;model integrity;multi-constraints checking;multi-resolution
condition;multiple constraints;stereo matching approach;stereo
rematching;weighted median filter;Calibration;Image
reconstruction;Optimization;Robustness;Smoothing methods;Solid
modeling;Three-dimensional displays;3D reconstruction;disparity
refinement;multi-constrains;multi-resolution;stereo matching},
doi={10.1109/ROBIO.2015.7418891},
month={Dec},}
@INPROCEEDINGS{7419070,
author={H. Liu and J. Luo and P. Wu and S. Xie and H. Li},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={People perception from RGB-D cameras for mobile robots},
year={2015},
pages={2020-2025},
abstract={Understanding how humans move through the scene is a key issue
of decision-making for an autonomous mobile robot in crown people zones.
So accurately detecting and tracking people from a mobile platform can
help improve interaction effective and efficient. In this paper, we
proposed a people detection and tracking system using combination of a
several new techniques for mobile robots, plan-view maps, depth weighted
histograms, and GNN data association. We proposed a spatial region of
interest based plan-view maps to detect human candidates. Firstly, point
cloud sub-clusters were segmented for candidate detection. Two different
plan-view maps, named occupancy map and height map, were employed to
identify human candidates from point cloud sub-clusters. Meanwhile, a
depth weighted histogram was extracted to feature a human candidate.
Then, a particle filter algorithm was adopted to track human's motion.
Finally, data association was set up to re-identify humans which were
tracked. Extensive experiments demonstrated the effectiveness and
robustness of our human detection and tracking system.},
keywords={cameras;feature extraction;image colour analysis;image motion
analysis;image segmentation;mobile robots;object detection;object
tracking;particle filtering (numerical methods);pattern clustering;robot
vision;sensor fusion;GNN data association;RGB-D cameras;autonomous
mobile robot;decision-making;depth weighted histograms;feature
extraction;height map;human candidates detection;human motion
tracking;humans reidentification;occupancy map;particle filter
algorithm;people detection and tracking system;people perception;point
cloud subclusters segmentation;spatial region of interest based
plan-view maps;Cameras;Feature extraction;Histograms;Mobile robots;Robot
vision systems;Three-dimensional displays;Tracking;Data
Association;Human Perception;Plan-view Maps;Point Cloud Clustering;RGB-D
Camera},
doi={10.1109/ROBIO.2015.7419070},
month={Dec},}
@INPROCEEDINGS{7416901,
author={R. I. Muhamedyev and Y. N. Amirgaliyev and M. N. Kalimoldayev
and A. N. Khamitov and A. Abdilmanova},
booktitle={2015 Twelve International Conference on Electronics Computer
and Computation (ICECCO)},
title={Selection of the most prominent lines of research in ICT domain},
year={2015},
pages={1-7},
abstract={The paper is devoted to selection of the most crucial
directions of research in ICT domain that could be implemented in the
Republic of Kazakhstan. In the paper we evaluated the dynamics of the
annual changes in the number of publications and convergence of ICT
sub-domains based on data of Scopus, EBSCO (Information Science &
Technology Abstracts, Academic Search Complete) and Google Scholar. To
analyze the place of Kazakhstan, we considered indexes shown in the
Global Competitiveness Report. As a result, the most rapidly developing
areas of research were revealed (big data, machine learning, 5G,
augmented reality, and etc.). The semantic network of the most modern
concepts of the ICT domain was constructed that visualizes the binary
relationship between the components and their relative importance. By
using comparative analysis of the number of publications in the leading
countries and some other countries including Kazakhstan, we selected
some key domains which need to be seriously improved onto the way of
development science in RK.},
keywords={Big Data;Internet of Things;augmented reality;cloud
computing;learning (artificial intelligence);mobile computing;research
initiatives;semantic networks;5G;EBSCO data;Google Scholar data;ICT
domain;Republic of Kazakhstan;Scopus data;augmented reality;big
data;machine learning;research selection;semantic network;5G mobile
communication;Augmented reality;Big data;Cloud computing;Mobile
computing;Semantics;5G;Augmented Reality;Big Data;Bioinformatics;Cloud
computing;Cyber-Physical systems;Embedded systems;Human-machine
systems;ICT domain;Information Security;Internet of Things;Machine
Learning;Machine to Machine;Mobile computing;Multi agent systems;Neural
Networks;Robotics;SDN;Visualization;e-Governance;scientometric
databases;taxonomy},
doi={10.1109/ICECCO.2015.7416901},
month={Sept},}
@INPROCEEDINGS{7419021,
author={H. Zhao and C. Zhang and G. Yang and C. Y. Chen},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={An intuitive teaching system using sensor fusion and 3D matching},
year={2015},
pages={1729-1734},
abstract={In this paper, a fast intuitive system for robot teaching
based on sensor fusion and 3D shapes matching technology is presented.
The off-line robot programming technique has been developed for a few
years. However, many intuitive ways are set up complex system which
always need relatively high initial investment in software and workers'
training. In this paper, we also present a system to program an UR robot
in an intuitive way, which allow users, even non-expert workers, to
program robot more simple and faster, what's more, this system which is
much easier to implement is simply and inexpensive, making it more
vulnerable to be widely used, especially in small and medium
enterprises. In this system, a human-robot interaction system has been
designed, a 3D registration algorithm is proposed for the matching
between virtual 3D model and real work piece. Once the registration is
completed, the system can convert any cloud point data from the virtual
3D model into the corresponding points on the real work piece. In
addition, the great deal of information included in the 3D model makes
the tool orientation teaching more easily. We aim to create an
easy-to-use robot programming system which could be accessible to anyone
without professional skills, and this system is required to be high
efficiency and precision. Additionally, it's safe because of users do
not need to be physically closed to the machine working environment.},
keywords={human-robot interaction;image matching;image
registration;laser ranging;robot kinematics;sensor fusion;shape
recognition;3D registration algorithm;3D shape matching;UR robot
programming;human-robot interaction system;intuitive robot teaching
system;laser range finder;sensor fusion;Cameras;Education;Robot
kinematics;Robot programming;Solid modeling;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7419021},
month={Dec},}
@INPROCEEDINGS{7418809,
author={D. Tran and E. Tadesse and P. Batapati and W. Sheng and L. Liu},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={A cloud based testbed for research and education in intelligent
transportation system},
year={2015},
pages={452-457},
abstract={The rapid evolution of technologies makes it possible to
create remote laboratory for research and educational purposes. This
paper presents a testbed that can conduct experiments following commands
from a remote client. It is a small-scaled intelligent transportation
system (ITS) testbed which can collect data, perform configuration and
control the radio-controlled (RC) car. The remote client can configure
the experiment by designing the RC car trajectories and selecting the
data to collect. The server in the testbed manages the testbed and
enables the transmission of the data per the client's request. The
testbed was validated with two case studies related to intelligent
transportation systems in which the client can detect driver drowsiness
and anomaly driving behavior using the data collected by the server from
this cloud-based testbed.},
keywords={automobiles;cloud computing;intelligent transportation
systems;mobile robots;road safety;telerobotics;ITS;RC car
trajectories;anomaly driving behavior;cloud-based testbed;driver
drowsiness;radio-controlled car;remote client;remote
laboratory;small-scaled intelligent transportation system testbed;Cloud
computing;Monitoring;Servers;Switches;Trajectory;Vehicles;Wheels},
doi={10.1109/ROBIO.2015.7418809},
month={Dec},}
@INPROCEEDINGS{7419118,
author={B. Li and X. Zhang and J. P. Munoz and J. Xiao and X. Rong and
Y. Tian},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Assisting blind people to avoid obstacles: An wearable obstacle
stereo feedback system based on 3D detection},
year={2015},
pages={2307-2311},
abstract={A wearable Obstacle Stereo Feedback (OSF) System for the Blind
people based on 3D space obstacle detection is presented to assist the
navigation. The OSF system embedded with a depth sensor to perceive the
in-front 3D spatial information in the form of point clouds. We
implemented the downsampling Random Sample Consensus (RANSAC) algorithm
to process the perceived point cloud, and detect the obstacles in front
of the user. Finally, Head-Related Transfer Functions (HRTF) are applied
to create the virtual stereo sound which represents the obstacles
according to its coordinate in the 3D space. The experiment shows that
OSF system can detect the obstacle in the indoor environment effectively
and provides a feasible auditory perception to indicate the in-front
safety zone for the blind user.},
keywords={handicapped aids;safety;stereo image processing;wearable
computers;3D space obstacle detection;3D spatial information;HRTF;OSF
system;RANSAC;auditory perception;blind people assistance;blind
user;downsampling random sample consensus algorithm;head-related
transfer functions;in-front safety zone;obstacle avoidance;perceived
point cloud;point clouds;virtual stereo sound;wearable obstacle stereo
feedback system;Cameras;Data models;Databases;Finite impulse response
filters;Floors;Navigation;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7419118},
month={Dec},}
@INPROCEEDINGS{7418964,
author={Y. He and L. Chen and J. Chen and M. Li},
booktitle={2015 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={A novel way to organize 3D LiDAR point cloud as 2D depth map
height map and surface normal map},
year={2015},
pages={1383-1388},
abstract={In this paper we focus on what meaningful 2D perceptual
information we can get from 3D LiDAR point cloud. Current work [1] [2]
[3] have demonstrated that the depth, height and local surface normal
value of a 3D data are useful features for improving Deep Neural
Networks (DNNs) based object detection. We thus propose to organise
LiDAR point as three different maps: dense depth map, height map and
surface normal map. Specifically, given a pair of RGB image and sparse
depth map projected from LiDAR point cloud, we propose a parameter
self-adaptive method to upgrade sparse depth map to dense depth map,
which is then passed to a convex optimisation framework to gain global
enhancement. Height map is obtained by reprojecting each pixel in dense
depth map into 3D coordinate, which enables us to record its height
value, surface normal map is obtained by a trilateral filter constructed
from depth map and RGB image. Finally, we validate our framework on both
KITTI tracking dataset and Middlebury dataset^1 . To the best of our
knowledge, we are the first to interpret 3D LiDAR point cloud as various
2D features and hope it will motivate more research on object detection
by combing RGB image and 3D LiDAR point cloud.},
keywords={convex programming;image colour analysis;image
filtering;neural nets;object detection;optical radar;radar
computing;radar imaging;2D depth map;2D perceptual information;3D LiDAR
point cloud;DNN based object detection;KITTI tracking dataset;Middlebury
dataset;RGB image;convex optimisation framework;deep neural network
based object detection;gain global enhancement;height map;parameter
self-adaptive method;surface normal map;trilateral
filter;Bandwidth;Color;Feature extraction;Image color analysis;Laser
radar;Object detection;Three-dimensional displays},
doi={10.1109/ROBIO.2015.7418964},
month={Dec},}
@INPROCEEDINGS{7408156,
author={D. Zehe and W. Cai and A. Knoll and H. Aydt},
booktitle={2015 Winter Simulation Conference (WSC)},
title={Tutorial on a modeling and simulation cloud service},
year={2015},
pages={103-114},
abstract={For large-scale urban system simulations the computing power
of traditional workstations is not sufficient. The move to High
Performance Computing clusters is a viable solution. Users of such
simulations are domain experts with little knowledge in computer science
and optimization of such simulations. The access to HPC resources is
also not available. Vendors have not sufficiently addressed this. This
leads to the conclusion of moving the computational demand to the cloud,
where the on-demand culture for resources has been expanding. In this
tutorial we will present an approach of how to work with an entirely
cloud-based solution for modeling and simulation, with an exemplary
implementation of an urban traffic simulation cloud service. Since the
computational offload from the workstation to a remote computing entity
also allows the use of novel user interfaces (design and devices),
through the use of RESTful interfaces, use-case applicable interfaces
for simulations can also be created.},
keywords={cloud computing;expert systems;parallel processing;traffic
information systems;user interfaces;HPC resource;RESTful
interface;cloud-based solution;computational demand;computational
offload;computer science;computing power;domain expert;high performance
computing cluster;large-scale urban system simulation;on-demand
culture;remote computing entity;urban traffic simulation cloud
service;use-case applicable interface;user interface;Cloud
computing;Computational modeling;Data
models;Hardware;Runtime;Workstations},
doi={10.1109/WSC.2015.7408156},
month={Dec},}
@ARTICLE{7384720,
author={J. Mahler and F. T. Pokorny and Z. McCarthy and A. F. van der
Stappen and K. Goldberg},
journal={IEEE Robotics and Automation Letters},
title={Energy-Bounded Caging: Formal Definition and 2-D Energy Lower
Bound Algorithm Based on Weighted Alpha Shapes},
year={2016},
volume={1},
number={1},
pages={508-515},
abstract={Caging grasps are valuable as they can be robust to bounded
variations in object shape and pose, do not depend on friction, and
enable transport of an object without full immobilization. Complete
caging of an object is useful but may not be necessary in cases where
forces such as gravity are present. This letter extends caging theory by
defining energy-bounded cages with respect to an energy field such as
gravity. This letter also introduces energy-bounded-cage-analysis-2-D
(EBCA-2-D), a sampling-based algorithm for planar analysis that takes as
input an energy function over poses, a polygonal object, and a
configuration of rigid fixed polygonal obstacles, e.g., a gripper, and
returns a lower bound on the minimum escape energy. In the special case
when the object is completely caged, our approach is independent of the
energy and can provably verify the cage. EBCA-2-D builds on recent
results in collision detection and the computational geometric theory of
weighted α-shapes and runs in O(s^2 + sn^2 ) time where s is the number
of samples, n is the total number of object and obstacle vertices, and
typically n <;<; s. We implemented EBCA-2-D and evaluated it with nine
parallel-jaw gripper configurations and four nonconvex obstacle
configurations across six nonconvex polygonal objects. We found that the
lower bounds returned by EBCA-2-D are consistent with intuition, and we
verified the algorithm experimentally with Box2-D simulations and RRT*
motion planning experiments that were unable to find escape paths with
lower energy. EBCA2-D required an average of 3 min per problem on a
single-core processor but has potential to be parallelized in a
cloud-based implementation. Additional proofs, data, and code are
available at: http://berkeleyautomation.github.io/caging/.},
keywords={collision avoidance;computational geometry;grippers;sampling
methods;2D energy lower bound algorithm;Box2D simulation;EBCA-2D;RRT
motion planning;collision detection;computational geometric
theory;energy-bounded caging;nonconvex obstacle configuration;nonconvex
polygonal object;parallel-jaw gripper;planar analysis;rigid fixed
polygonal obstacle;sampling-based algorithm;weighted alpha
shape;Approximation algorithms;Computational geometry;Grippers;Path
planning;Robots;Robustness;Computational Geometry;Grasping;Motion and
Path Planning;computational geometry;motion and path planning},
doi={10.1109/LRA.2016.2519145},
ISSN={2377-3766},
month={Jan},}
@INPROCEEDINGS{7402183,
author={J. C. P. Villota and A. H. R. Costa},
booktitle={2015 12th Latin American Robotics Symposium and 2015 3rd
Brazilian Symposium on Robotics (LARS-SBR)},
title={Aligning RGB-D Point Clouds through Adaptive Integration of Color
and Depth Cues},
year={2015},
pages={309-314},
abstract={One of the most important tasks in building environment maps
with partial information is to find a good alignment between pairs of
point clouds representing consecutive frames. RANSAC and ICP are widely
used algorithms to align pairs of frames: the former finds an initial
transformation which is refined by the latter. Decreasing the alignment
error in the first step can reduce the computational cost in the
subsequent step. A robot with a RGB-D camera navigating indoors can come
across scenes under different conditions. When the environment presents
a high degree of image texture and low degree of structure, better
results are achieved using only color images. Otherwise, depth
information is better suited. In this paper, we contribute a new
adaptive technique for the effective integration of color and depth
information in two consecutive frames in the alignment process. Our
proposal provides automatic means to weigh the relevance of each type of
information depending on the visual texture and structure of the scene.},
keywords={image colour analysis;image texture;ICP algorithm;RANSAC
algorithm;RGB-D camera;RGB-D point clouds alignment;color cue;color
image;depth cue;depth information;image texture;red-green-blue-depth
point cloud;visual texture;Color;Image color analysis;Iterative closest
point algorithm;Measurement;Robots;Three-dimensional
displays;Visualization;Descriptors;Image Structure;Image
texture;Pairwise alignment;RGB-D sensors;SLAM},
doi={10.1109/LARS-SBR.2015.38},
month={Oct},}
@ARTICLE{7378846,
author={F. T. Cheng and H. Tieng and H. C. Yang and M. H. Hung and Y. C.
Lin and C. F. Wei and Z. Y. Shieh},
journal={IEEE Robotics and Automation Letters},
title={Industry 4.1 for Wheel Machining Automation},
year={2016},
volume={1},
number={1},
pages={332-339},
abstract={Industry 4.0 is set to be one of the new manufacturing
objectives. The technologies involved to achieve Industry 4.0 are
Internet of Things (IoT), cyber physical systems (CPS), and cloud
manufacturing (CM). However, the current objectives defined by Industry
4.0 do not include zero defects; it only keeps the faith of achieving
nearly zero-defects state. The purpose of this paper is to propose a
platform denoted advanced manufacturing cloud of things (AMCoT) to not
only achieve the objectives of Industry 4.0 but also accomplish the goal
of zero defects by applying the technology of automatic virtual
metrology (AVM). As such, by applying Industry 4.0 together with AVM to
achieve the goal of zero defects, the era of Industry 4.1 is taking
place. The application of wheel machining automation is adopted in this
letter to illustrate how AMCoT and Industry 4.1 work.},
keywords={Internet of Things;cloud computing;cyber-physical
systems;flexible manufacturing systems;virtual
manufacturing;AMCoT;AVM;CPS;Internet of Things;IoT;advanced
manufacturing cloud of things;automatic virtual metrology;cloud
manufacturing;cyber physical system;industry 4.0;industry
4.1;manufacturing objective;wheel machining automation;zero
defect;Industries;Inspection;Machining;Metrology;Wheels;Automatic
Virtual Metrology (AVM);Industry 4.1;Intelligent and Flexible
Manufacturing, Factory Automation;Intelligent and flexible
manufacturing;Zero Defects;automatic virtual metrology (AVM);factory
automation;industry 4.1;zero defects},
doi={10.1109/LRA.2016.2517208},
ISSN={2377-3766},
month={Jan},}
@INPROCEEDINGS{7402166,
author={J. d. L. Silveira and E. O. Freire and L. Molina and E. A. N.
Carvalho},
booktitle={2015 12th Latin American Robotics Symposium and 2015 3rd
Brazilian Symposium on Robotics (LARS-SBR)},
title={A Simple Visual Odometry Approach Based on Point Clouds},
year={2015},
pages={204-209},
abstract={In this paper, a simple approach for estimating the ego-motion
of a vehicle is proposed. The system can estimate the motion based on a
sequence of images and point cloud data. The system is capable of
estimate the ego-motion through a 2-D to 3-D mapping of the detected
features on the image considering that the calibration parameters of the
sensors are available. Singular value decomposition is used to estimate
the rigid-body transformation between two point clouds, and we employ a
RANSAC outlier rejection method to estimate a more accurate ego-motion.
Datasets from the KITTI Benchmark Suite are used in order to evaluate
the proposed method. The experiments show that our approach performed
well with translational errors below 5%.},
keywords={calibration;distance measurement;feature extraction;image
sensors;image sequences;intelligent transportation systems;motion
estimation;random processes;road vehicles;sampling methods;singular
value decomposition;2D mapping;3D mapping;KITTI benchmark suite;RANSAC
outlier rejection method;calibration parameters;features detection;image
sequence;point clouds;random sample consensus;rigid-body transformation
estimation;sensors;singular value decomposition;vehicle ego-motion
estimation;visual odometry;Cameras;Feature extraction;Laser
radar;Sensors;Three-dimensional
displays;Vehicles;Visualization;LIDAR;Localization;Point Cloud;Visual
Odometry},
doi={10.1109/LARS-SBR.2015.39},
month={Oct},}
@INPROCEEDINGS{7404471,
author={U. Neettiyath and T. Sato and M. Sangekar and A. Bodenmann and
B. Thornton and T. Ura and A. Asada},
booktitle={OCEANS 2015 - MTS/IEEE Washington},
title={Identification of manganese crusts in 3D visual reconstructions
to filter geo-registered acoustic sub-surface measurements},
year={2015},
pages={1-6},
abstract={The volumetric distribution of cobalt-rich manganese crusts
(CRC) is of significant interest for mining and geology. Traditionally
studying underwater deposits of CRC involved physical sampling from
Remotely Operated Vehicles (ROV) or using dredges. Recently, acoustic
measurements of crust thickness have been demonstrated that can give a
significantly higher spatial resolution for measurement. The probe makes
high resolution measurements of sub-surface reflections to calculate the
thickness of the deposit. However, CRC coverage is often not continuous
and it is difficult to determine from the acoustic signals alone whether
an acoustic signal was measured over CRC or not. The authors propose a
method to filter these using visual information (3D colour map of the
seafloor) from the same region. After locating the acoustic beam on the
seafloor, points around the region are selected. A number of analyses
are performed on this point cloud to extract parameters that can
reliably discriminate between exposed CRC and other types of seafloor.
The proposed method was tested on two areas of seafloor - one which is
known to contain crust and one which does not contain crust. These two
sets of data were then used to train a Support Vector Machine (SVM)
classifier. The trained classifier was then tested with the training
sets and a test set containing both crust and non-crust regions to
verify if the CRC is being detected reliably. The results were
promising; in 85.4% of the cases, the detection was successful. The
performance will be verified by using larger sets of data. In our future
work, the results can be applied to estimate a volumetric distribution
of CRC in the region.},
keywords={geology;geophysical image processing;geophysics
computing;image reconstruction;mining;oceanic crust;oceanographic
equipment;oceanographic techniques;pattern classification;remotely
operated vehicles;seafloor phenomena;support vector machines;3D visual
reconstruction;CRC underwater deposit;CRC volumetric distribution;CRC
volumetric distribution estimation;ROV;SVM classifier;acoustic
beam;acoustic signal measurement;cobalt-rich manganese crust;crust
region;crust thickness acoustic
measurement;dredges;geology;georegistered acoustic subsurface
measurement filtering;higher spatial resolution;manganese crust
identification;mining;noncrust region;parameter extraction;physical
sampling;remotely operated vehicles;seafloor 3D colour map;seafloor
visual information colour map;subsurface reflection high resolution
measurement;support vector machine classifier;Acoustic
measurements;Acoustics;Cameras;Probes;Sea measurements;Shape;Training},
doi={10.23919/OCEANS.2015.7404471},
month={Oct},}
@INPROCEEDINGS{7402134,
author={A. Díaz and E. Caicedo and L. Paz and P. Piniés},
booktitle={2015 12th Latin American Robotics Symposium and 2015 3rd
Brazilian Symposium on Robotics (LARS-SBR)},
title={Dense Localization of a Monocular Camera Using Keyframes},
year={2015},
pages={13-18},
abstract={In this paper, we present a low cost localization system that
exploits dense image information to continuously track the position of a
camera in 6DOF. It leverages of the use of a set of selected "key
frames" separated in distance from which a depth map is available to
create a local 3D point cloud. In this way, we avoid the computational
overload caused by common dense sequential approaches. The system uses a
3D-2D technique to calculate an initial pose estimate for the
intermediate camera frames. A refinement step stated as a Non Linear
Least Squares (NLQs) optimisation is performed by minimising the
photo-consistency error. The NLQs cost function is defined by aligning a
warped image and an image associated to the closest key frame. The
minimum solution is calculated using the Levenberg-Marquardt method. To
validate the accuracy of our system, we conducted experiments using data
with perfect ground truth. Our assessment shows that our system is able
to achieve up to millimeter accuracy. Most of the expensive calculations
are carried out by exploiting parallel computing and GPGPU.},
keywords={SLAM (robots);cameras;graphics processing units;least squares
approximations;nonlinear programming;object tracking;parallel
processing;3D point cloud;3D-2D technique;GPGPU;Levenberg-Marquardt
method;NLQ optimisation;camera position tracking;dense image
information;dense monocular camera localization;depth
map;general-purpose graphics processing unit;ground truth;key frame
selection;nonlinear least square optimisation;parallel computing;pose
estimation;refinement step;warped image alignment;Cameras;Integrated
circuits;Optimization;Real-time systems;Simultaneous localization and
mapping;Three-dimensional displays;6 DOF;keyframes;localization using
dense techniques;non linear squares optimization;parallel computing},
doi={10.1109/LARS-SBR.2015.22},
month={Oct},}
@ARTICLE{7312465,
author={R. C. Luo and S. Y. Chen},
journal={IEEE Transactions on Industrial Informatics},
title={Human Pose Estimation in 3-D Space Using Adaptive Control Law
With Point-Cloud-Based Limb Regression Approach},
year={2016},
volume={12},
number={1},
pages={51-58},
abstract={This paper presents the algorithm of human pose estimation in
3-D space using adaptive control law with point-cloud-based limb
regression approach. The proposed approach is a data-driven method for
3-D pose estimation of human. In addition, we exploit the inverse
relationship between the estimated parameter and the pose of limb,
proposing a hybrid scheme, the combination of indirect adaptive scheme
in Cartesian coordinate, and indirect adaptive scheme in spherical
space. The experimental results show the ability of error tolerance,
even dealing with sparse, partial, and noisy limb data. The comparison
of the ground truth is provided with the standard dataset. The
computation speed is sufficient, as it is expected to be used in
real-time applications or as the 3-D feature in other recognition
frameworks. Finally, the proposed algorithm is demonstrated with the
point cloud sensing in real scene and the experimental results are
included in this paper.},
keywords={adaptive control;pose estimation;real-time systems;regression
analysis;3D pose estimation;3D space;Cartesian coordinate;adaptive
control law;human pose estimation;point-cloud-based limb regression
approach;real-time applications;Adaptation models;Adaptive
control;Cameras;Solid modeling;Three-dimensional displays;Adaptive
Control Law;Adaptive control law;Human Pose Estimation;Limb Regression
Approach;Point Cloud;human pose estimation;limb regression
approach;point cloud},
doi={10.1109/TII.2015.2496140},
ISSN={1551-3203},
month={Feb},}
@INPROCEEDINGS{7391355,
author={A. M. Kintsakis and S. E. Reppou and G. T. Karagiannis and P. A.
Mitkas},
booktitle={2015 E-Health and Bioengineering Conference (EHB)},
title={Robot-assisted cognitive exercise in mild cognitive impairment
patients: The RAPP approach},
year={2015},
pages={1-4},
abstract={Medical advances have increased life expectancy to the point
where senior citizens comprise a larger than ever percentage of the
general population. The RAPP project introduces socially interactive
robots that will not only physically assist, but also serve as a
companion to senior citizens. In this paper, a novel system is
presented, designed as part of the RAPP project that aims to assist
senior citizens with mild cognitive impairment in performing cognitive
tests as a means of exercise, while also keeping track of their
performance. Our goal is to provide an interactive and intriguing user
friendly alternative to the classic cognitive tests that will keep the
patient interested and thus increase compliance. Tracking the patient's
performance in the cognitive tests can prove invaluable as the data can
be further associated with medical treatment or lifestyle changes and
conclusively lead to better understanding of the nature of cognitive
decline.},
keywords={assisted living;cognition;geriatrics;interactive
devices;medical disorders;medical robotics;patient treatment;RAPP
approach;interactive user friendly;intriguing user friendly;medical
treatment;mild cognitive impairment patients;robot-assisted cognitive
exercise;senior citizen assistant;socially interactive robots;Computer
architecture;Knowledge based systems;Ontologies;Robot sensing
systems;Senior citizens;Speech;RAPP;cloud computing;cognitive
exercise;robotics;social inclusion},
doi={10.1109/EHB.2015.7391355},
month={Nov},}
@INPROCEEDINGS{7373477,
author={B. Gabbasov and I. Danilov and I. Afanasyev and E. Magid},
booktitle={2015 10th International Symposium on Mechatronics and its
Applications (ISMA)},
title={Toward a human-like biped robot gait: Biomechanical analysis of
human locomotion recorded by Kinect-based Motion Capture system},
year={2015},
pages={1-6},
abstract={This paper presents biomechanical analysis of human locomotion
recorded by Motion Capture (MoCap) system based on four Kinect 2 sensors
and iPi Soft markerless tracking and visualization technology. To
analyze multi-depth sensor video recordings we utilize iPi Mocap Studio
software and iPi Biomech Add-on plug-in, which provide us visual and
biomechanical human gait data: linear and angular joint coordinates,
velocity, acceleration, center of mass (CoM) position, skeleton and 3D
point cloud. The final analysis was performed in MATLAB environment,
calculating zero moment point (ZMP) and ground projection of the CoM
(GCoM) trajectories from human body dynamics by considering human body
as a single weight point. These were followed by GCoM and ZMP error
estimation. The further objective of our research is to reproduce the
obtained with our MoCap system human-like gait with Russian biped robot
AR-601M.},
keywords={control engineering computing;data visualisation;image motion
analysis;image sensors;legged locomotion;object tracking;AR-601M Russian
biped robot;GCoM error estimation;Kinect-based motion capture
system;MoCap system;ZMP error estimation;angular joint
coordinates;biomechanical analysis;center-of-mass;ground projection of
the CoM;human locomotion analysis;human-like biped robot gait;iPi
Biomech Add-on plug-in;iPi Mocap Studio software;iPi Soft markerless
tracking technology;linear joint coordinates;multidepth sensor video
recordings;visualization technology;zero moment point;Biological system
modeling;Biomechanics;Robot kinematics;Sensors;Three-dimensional
displays;Tracking},
doi={10.1109/ISMA.2015.7373477},
month={Dec},}
@INPROCEEDINGS{7367695,
author={T. Wissel and P. Stüber and J. Manit and R. Bruder and A.
Schweikard and F. Ernst},
booktitle={2015 IEEE 15th International Conference on Bioinformatics and
Bioengineering (BIBE)},
title={Predicting cutaneous features for marker-less optical
head-tracking in cranial radiotherapy},
year={2015},
pages={1-5},
abstract={In cranial radiotherapy highly accurate tumor localization is
required to spare healthy tissue while delivering sufficient dose to the
target. Here, marker-less optical tracking can support immobilization
devices and monitor head motion. However, recent studies have shown that
the spatial registration of the optically generated point cloud to a
reference is not robust enough. It may suffer from misalignments due to
local similarities and changes along the deformable surface geometry.
Irrespective from the registration algorithm, we propose to support the
spatial information by tissue thickness patterns across the surface.
These patterns are predicted by Gaussian Processes which process near
infrared optical backscatter from the skin. We first demonstrate for a
cohort of 30 volunteers that the tissue thickness can be determined with
errors of less than 0.22mm on average. We found high prediction accuracy
irrespective of gender, age or skin type. Second, we show that the
robustness of a standard iterative closest point (ICP) algorithm can be
improved when exploiting cutaneous patterns to identify point-to-point
correspondences. On average, tissue thickness support outperformed the
standard procedure for every subject and was capable of pushing 90% of
all misalignments exceeding a registration error of 1mm below that
threshold.},
keywords={Gaussian processes;iterative methods;optical
tracking;radiation therapy;skin;tumours;Gaussian process;cranial
radiotherapy;cutaneous patterns;dose delivery;head motion
monitoring;immobilization devices;marker-less optical head-tracking;near
infrared optical backscatter;registration algorithm;registration
error;skin;spatial information;standard iterative closest point
algorithm;tissue thickness patterns;tumor localization;Adaptive
optics;Backscatter;Integrated optics;Iterative closest point
algorithm;Skin;Surface treatment;Three-dimensional displays},
doi={10.1109/BIBE.2015.7367695},
month={Nov},}
@INPROCEEDINGS{7364621,
author={C. H. Wu and S. Y. Jiang and K. T. Song},
booktitle={2015 15th International Conference on Control, Automation and
Systems (ICCAS)},
title={CAD-based pose estimation for random bin-picking of multiple
objects using a RGB-D camera},
year={2015},
pages={1645-1649},
abstract={In this paper, we propose a CAD-based 6-DOF pose estimation
design for random bin-picking of multiple different objects using a
Kinect RGB-D sensor. 3D CAD models of objects are constructed via a
virtual camera, which generates a point cloud database for object
recognition and pose estimation. A voxel grid filter is suggested to
reduce the number of 3D point cloud of objects for reducing computing
time of pose estimation. A voting-scheme method was adopted for the
6-DOF pose estimation a swell as object recognition of different type
objects in the bin. Furthermore, an outlier filter is designed to filter
out bad matching poses and occluded ones, so that the robot arm always
picks up the upper object in the bin to increase pick up success rate. A
series of experiments on a Kuka 6-axis robot revels that the proposed
system works satisfactorily to pick up all random objects in the bin.
The average recognition rate of three different type objects is 93.9%
and the pickup success rate is 89.7%.},
keywords={CAD;image colour analysis;image filtering;image matching;image
sensors;object recognition;pose estimation;production engineering
computing;robot vision;robotic assembly;solid modelling;3D CAD
models;CAD-based 6-DOF pose estimation design;Kinect RGB-D sensor;Kuka
6-axis robot;RGB-D camera;automated assembly lines;computing time
reduction;multiple object random bin-picking;object recognition;point
cloud database generation;pose estimation;virtual camera;voting-scheme
method;voxel grid filter;Databases;Design automation;Grippers;Robot
sensing systems;Solid modeling;Yttrium;6-DOF pose estimation;Industrial
robot;random bin-picking},
doi={10.1109/ICCAS.2015.7364621},
ISSN={2093-7121},
month={Oct},}
@INPROCEEDINGS{7358823,
author={Youngji Kim and Hwasup Lim and Sang Chul Ahn},
booktitle={2015 12th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI)},
title={Multi-body ICP: Motion segmentation of rigid objects on dense
point clouds},
year={2015},
pages={532-536},
abstract={Motion is one of the crucial keys for segmenting moving
objects in dynamic environment. This paper proposes an approach of
segmenting moving parts in the sequence of point clouds. Different rigid
motions in dense point clouds are found and clustered by applying
segmentation schemes such as graph-cuts into Iterative Closest Point
(ICP) algorithm with initial segmentation from Generalized PCA (GPCA).
Result shows that we can estimate motions of rigidly moving objects and
segment parts having different motions simultaneously.},
keywords={computer graphics;graph theory;image segmentation;image
sequences;iterative methods;motion estimation;pattern
clustering;principal component analysis;GPCA;ICP algorithm;dense point
clouds;dynamic environment;generalized principal component
analysis;graph-cuts;iterative closest point;motion clustering;motion
segmentation;motions estimation;moving objects segmentation;moving parts
segmentation;multibody ICP;point clouds sequence;rigid motions;rigidly
moving objects;Clustering algorithms;Computer vision;Iterative closest
point algorithm;Labeling;Motion segmentation;Three-dimensional
displays;Trajectory;Iterative Closest Point (ICP);Motion
clustering;Motion segmentation},
doi={10.1109/URAI.2015.7358823},
month={Oct},}
@INPROCEEDINGS{7359498,
author={W. R. Green and H. Grobler},
booktitle={2015 Pattern Recognition Association of South Africa and
Robotics and Mechatronics International Conference (PRASA-RobMech)},
title={Normal distribution transform graph-based point cloud segmentation},
year={2015},
pages={54-59},
abstract={We present a graph-based algorithm for segmenting point cloud
scenes using criteria based on the combination of spatial, geometric,
and appearance features. An octree data structure is employed to
organize the point cloud data. The voxel space is used to create a
Normal Distribution Transform Feature Representation (NDT-FR) to model
the underlying sensor data and corresponding features in a probabilistic
manner. The proposed segmentation algorithm uses the Hellinger distance
calculated on local statistics stored in neighboring voxels to define
the edge weights of the graph. Rather than choosing a specific feature
for edge weight calculation, our approach has the ability to combine
multiple features into a single edge weight without the need to find an
appropriate normalization scheme. We verify our algorithm on multiple
indoor scenes and perform a qualitative evaluation. We also show how our
edge weighting scheme can increase the accuracy of object boundaries in
the final segmentation.},
keywords={data structures;image
segmentation;octrees;transforms;Hellinger distance;NDT-FR;edge weight
calculation;graph-based point cloud segmentation algorithm;indoor
scenes;local statistics;normal distribution transform feature
representation;normalization scheme;octree data structure;point cloud
data;point cloud scenes;sensor data;voxel space;Gaussian
distribution;Image color analysis;Image edge detection;Image
segmentation;Measurement;Octrees;Three-dimensional displays},
doi={10.1109/RoboMech.2015.7359498},
month={Nov},}
@INPROCEEDINGS{7347698,
author={F. Faion and A. Zea and J. Steinbring and M. Baum and U. D.
Hanebeck},
booktitle={2015 Sensor Data Fusion: Trends, Solutions, Applications (SDF)},
title={Recursive Bayesian pose and shape estimation of 3D objects using
transformed plane curves},
year={2015},
pages={1-6},
abstract={We consider the task of recursively estimating the pose and
shape parameters of 3D objects based on noisy point cloud measurements
from their surface. We focus on objects whose surface can be constructed
by transforming a plane curve, such as a cylinder that is constructed by
extruding a circle. However, designing estimators for such objects is
challenging, as the straightforward distance-minimizing approach cannot
observe all parameters, and additionally is subject to bias in the
presence of noise. In this article, we first discuss these issues and
then develop probabilistic models for cylinder, torus, cone, and an
extruded curve by adapting related approaches including Random
Hypersurface Models, partial likelihood, and symmetric shape models. In
experiments with simulated data, we show that these models yield
unbiased estimators for all parameters even in the presence of high
noise.},
keywords={Bayes methods;pose estimation;random processes;recursive
estimation;shape recognition;3D
objects;cone;cylinder;distance-minimizing approach;extruded curve;noisy
point cloud measurements;partial likelihood;probabilistic models;random
hypersurface models;recursive Bayesian pose estimation;recursive
Bayesian shape estimation;symmetric shape models;torus;transformed plane
curves;Adaptation models;Bayes methods;Noise measurement;Probabilistic
logic;Prototypes;Shape;Three-dimensional displays},
doi={10.1109/SDF.2015.7347698},
month={Oct},}
@INPROCEEDINGS{7347796,
author={J. Garstka and G. Peters},
booktitle={2015 12th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={Adaptive 3-D object classification with reinforcement learning},
year={2015},
volume={02},
pages={381-385},
abstract={We propose an adaptive approach to 3-D object classification.
In this approach appropriate 3-D feature descriptor algorithms for 3-D
point clouds are selected via reinforcement learning depending on
properties of the objects to be classified. This approach is supposed to
be able to learn strategies for an advantageous selection of 3-D point
cloud descriptor algorithms in an autonomous and adaptive way, and thus
is supposed to yield higher object classification rates in unfamiliar
environments than any of the single algorithms alone. In addition, we
expect our approach to be able to adapt to subsequently added 3-D
feature descriptor algorithms as well as to autonomously learn new
object categories when encountered in the environment without further
user assistance. We describe the 3-D object classification pipeline
based on local 3-D features and its integration into the reinforcement
learning environment.},
keywords={feature extraction;image classification;learning (artificial
intelligence);object detection;3D feature descriptor algorithms;3D
object classification pipeline;3D point cloud descriptor
algorithms;adaptive 3D object classification approach;reinforcement
learning environment;user assistance;Context;Histograms;Learning
(artificial intelligence);Object
recognition;Pipelines;Shape;Three-dimensional displays;3-D Object
Classification;Reinforcement Learning},
month={July},}
@INPROCEEDINGS{7347759,
author={J. Garstka and G. Peters},
booktitle={2015 12th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={Fast and robust keypoint detection in unstructured 3-D point
clouds},
year={2015},
volume={02},
pages={131-140},
abstract={In robot perception, as well as in other areas of 3-D computer
vision, keypoint detection is the first major step for an efficient and
accurate 3-D perception of the environment. Thus, a fast and robust
algorithm for an automatic identification of keypoints in unstructured
3-D point clouds is essential. The presented algorithm is designed to be
highly parallelizable and can be implemented on modern GPUs for fast
execution. The computation is based on a convolution of a voxel based
representation of the point cloud and a voxelized integral volume. The
generation of the voxel-based representation neither requires additional
surface information or normals nor needs to approximate them. The
proposed approach is robust against noise up to the mean distance
between the 3-D points. In addition, the algorithm provides moderate
scale invariance, i. e., it can approximate keypoints for lower
resolution versions of the input point cloud. This is particularly
useful, if keypoints are supposed to be used with any local 3-D point
cloud descriptor to recognize or classify point clouds at different
scales. We evaluate our approach in a direct comparison with
state-of-the-art keypoint detection algorithms in terms of repeatability
and computation time.},
keywords={graphics processing units;image representation;object
detection;robot vision;3D computer vision;GPU;automatic keypoint
identification;computation time;local 3D point cloud descriptor;robot
perception;robust keypoint detection;unstructured 3D point clouds;voxel
based point cloud representation;voxelized integral volume;Algorithm
design and analysis;Convolution;Detection
algorithms;Histograms;Kernel;Robustness;Three-dimensional displays;3-D
Computer Vision;3-D Keypoint Detection;3-D Recognition},
month={July},}
@INPROCEEDINGS{7347758,
author={P. Nicolai and J. Raczkowsky and H. Wörn},
booktitle={2015 12th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={Continuous pre-calculation of human tracking with time-delayed
ground-truth: A hybrid approach to minimizing tracking latency by
combination of different 3D cameras},
year={2015},
volume={02},
pages={121-130},
abstract={We present an approach to track a point cloud with a 3D camera
system with low latency and/or high frame rate, based on ground truth
provided by a second 3D camera system with higher latency and/or lower
frame rate. In particular, we employ human tracking based on Kinect
cameras and combine it with higher frame-rate/lower latency of
Time-of-Flight (ToF) cameras. We present the system setup, methods used
and evaluation results showing a very high accuracy in combination with
a latency reduction of up to factor 30.},
keywords={cameras;human-robot interaction;image
sensors;minimisation;object tracking;robot vision;3D camera
system;Kinect cameras;ToF cameras;continuous precalculation;frame
rate;human tracking;human-robot interaction;hybrid tracking latency
minimization approach;latency reduction;time-delayed
ground-truth;time-of-flight cameras;Cameras;Computer vision;Image motion
analysis;Optical imaging;Optical sensors;Target
tracking;Three-dimensional displays;3D Camera;Data Fusion;Probability
Propagation;Tracking},
month={July},}
@INPROCEEDINGS{7351230,
author={X. Ji and J. Cheng and D. Tao},
booktitle={2015 IEEE International Conference on Image Processing (ICIP)},
title={Local mean spatio-temporal feature for depth image-based speed-up
action recognition},
year={2015},
pages={2389-2393},
abstract={With the promptly growing population of the low-cost Microsoft
Kinect sensor, action recognition, which is a hard yet important problem
in computer vision, has been received substantial attention. However,
most existing approaches in action recognition spend much time on
feature detection even though these methods can achieve high recognition
rates. In this paper, we propose a local mean spatio-temporal feature
(LMSF) to speed up depth image based action recognition. In particular,
we solve the problem from three aspects: (1) associate the 4D normals by
a local mean spatio-temporal neighborhood; (2) extract motion frames by
detecting the differences between consecutive frames; (3) reduce
redundant normals extracted from depth cloud points by sparse coding.
The proposed approach is tested on two public benchmark datasets, i.e.,
MSRAction3D and MSRGesture3D. Experimental results demonstrate the
advantages of our improvement method and the state-of-the-art
performance on processing speed.},
keywords={computer vision;feature extraction;gesture recognition;image
motion analysis;4D normals;LMSF;MSRAction3D;MSRGesture3D;computer
vision;consecutive frame difference detection;depth cloud points;depth
image-based speed-up action recognition;feature detection;local mean
spatio-temporal neighborhood;local mean spatiotemporal feature;low-cost
Microsoft Kinect sensor;motion frame extraction;redundant normals;sparse
coding;Cameras;Encoding;Feature extraction;Image
recognition;Robustness;Skeleton;Visualization;Speed-up action
recognition;depth sequences;mean spatio-temporal neighborhood;polynormal},
doi={10.1109/ICIP.2015.7351230},
month={Sept},}
@ARTICLE{7271006,
author={D. Holz and A. E. Ichim and F. Tombari and R. B. Rusu and S.
Behnke},
journal={IEEE Robotics Automation Magazine},
title={Registration with the Point Cloud Library: A Modular Framework
for Aligning in 3-D},
year={2015},
volume={22},
number={4},
pages={110-124},
abstract={Registration is an important step when processing
three-dimensional (3-D) point clouds. Applications for registration
range from object modeling and tracking, to simultaneous localization
and mapping (SLAM). This article presents the open-source point cloud
library (PCL) and the tools available for point cloud registration. The
PCL incorporates methods for the initial alignment of point clouds using
a variety of local shape feature descriptors, as well as methods for
refining initial alignments using different variants of the well-known
iterative closest point (ICP) algorithm. This article provides an
overview on registration algorithms, usage examples of their PCL
implementations, and tips for their application. Since the choice and
parameterization of the right algorithm for a particular type of data is
one of the biggest problems in 3-D point cloud registration, we present
three complete examples of data (and applications) and the respective
registration pipeline in the PCL. These examples include dense
red-green-blue-depth (RGB-D) point clouds acquired by consumer color and
depth cameras, high-resolution laser scans from commercial 3-D scanners,
and low-resolution sparse point clouds captured by a custom lightweight
3-D scanner on a microaerial vehicle (MAV).},
keywords={computer graphics;feature extraction;image
registration;iterative methods;shape recognition;3D alignment;3D point
cloud registration;3D scanners;ICP algorithm;MAV;PCL;RGB-D point
clouds;SLAM;color cameras;dense red-green-blue-depth point clouds;depth
cameras;high-resolution laser scans;iterative closest point
algorithm;local shape feature descriptors;low-resolution sparse point
clouds;microaerial vehicle;object modeling;object tracking;open-source
point cloud library;point clouds alignment;simultaneous localization and
mapping;Cloud computing;Iterative closest point algorithm;Iterative
methods;Open source software;Robot sensing
systems;Sensors;Three-dimensional displays},
doi={10.1109/MRA.2015.2432331},
ISSN={1070-9932},
month={Dec},}
@INPROCEEDINGS{7340499,
author={H. Houshiar and A. Nüchter},
booktitle={2015 XXV International Conference on Information,
Communication and Automation Technologies (ICAT)},
title={3D point cloud compression using conventional image compression
for efficient data transmission},
year={2015},
pages={1-8},
abstract={Modern 3D laser scanners make it easy to collect large 3D
point clouds. In this paper we present the use of conventional image
based compression methods for 3D point clouds. We map the point cloud
onto panorama images to encode the range, reflectance and color value
for each point. An encoding method is presented to map the floating
point measured ranges on to a three channel image. The image compression
methods are used to compress the generated panorama images. We present
the results of several lossless compression methods and the lossy JPEG
on point cloud compression. Lossless compression methods are designed to
retain the original data. On the other hand lossy compression methods
sacrifice the details for higher compression ratio. This produces
artefacts in the recovered point cloud data. We study the effects of
these artefacts on encoded range data. A filtration process is presented
for determination of range outliers from uncompressed point clouds.},
keywords={data compression;image coding;3D laser scanner;3D point cloud
compression;color reflectance;color value;efficient data
transmission;image compression;lossless compression method;lossy
JPEG;panorama image;range encoding;Image coding;Image color
analysis;Laser radar;Octrees;Robots;Three-dimensional displays;Transform
coding;3D laser measurement systems;3D point clouds;lossless
compression;lossy compression;panorama images},
doi={10.1109/ICAT.2015.7340499},
month={Oct},}
@ARTICLE{7317787,
author={B. Peasley and S. Birchfield},
journal={IEEE Transactions on Robotics},
title={RGBD Point Cloud Alignment Using Lucas #x2013;Kanade Data
Association and Automatic Error Metric Selection},
year={2015},
volume={31},
number={6},
pages={1548-1554},
abstract={We propose to overcome a significant limitation of the
iterative closest point (ICP) algorithm used by KinectFusion, namely,
its sole reliance upon geometric information. Our approach uses both
geometric and color information in a direct manner that uses all the
data in order to accurately estimate camera pose. Data association is
performed by Lucas-Kanade to compute an affine warp between the color
images associated with two RGBD point clouds. A subsequent step then
estimates the Euclidean transformation between the point clouds using
either a point-to-point or point-to-plane error metric, with a novel
method based on a normal covariance test for automatically selecting
between them. Together, Lucas-Kanade data association with covariance
testing enables robust camera tracking through areas of low geometric
features, without sacrificing accuracy in environments in which the
existing ICP technique succeeds. Experimental results on several
publicly available datasets demonstrate the improved performance both
qualitatively and quantitatively.},
keywords={cameras;covariance analysis;feature extraction;geometry;image
colour analysis;iterative methods;object tracking;sensor
fusion;Euclidean transformation;ICP algorithm;KinectFusion;Lucas-Kanade
data association;RGBD point cloud alignment;automatic error metric
selection;camera pose estimation;color information;covariance
testing;geometric features;geometric information;iterative closest point
algorithm;point-to-plane error metrics;point-to-point error
metrics;robust camera tracking;Algorithm design and analysis;Image color
analysis;Iterative closest point algorithm;Robot sensing
systems;Three-dimensional displays;Camera
tracking;Kinect;KinectFusion;Lucas–Kanade;Lucas¿¿¿Kanade;iterative
closest point (ICP);mapping},
doi={10.1109/TRO.2015.2489479},
ISSN={1552-3098},
month={Dec},}
@INPROCEEDINGS{7335467,
author={M. Song and D. Huber},
booktitle={2015 International Conference on 3D Vision},
title={Automatic Recovery of Networks of Thin Structures},
year={2015},
pages={37-45},
abstract={Applications, such as construction monitoring and planning for
renovations, require the accurate recovery of existing conditions of
structures. Many types of infrastructure are primarily comprised of
arbitrarily-shaped thin structures (e.g., Truss bridges, steel frame
buildings under construction, and transmission towers), which existing
automatic modeling methods are incapable of handling. To address this
issue, this paper presents an approach to automatically recognize and
model beams, planes, and joints from a 3D point cloud containing a
complex network of thin structures, and to recover their topology. In
our approach, each beam is evolved from a seed by matching and aligning
the cross section images. This growing algorithm can model beams with
arbitrary cross sections. By performing the algorithm on a point
connectivity graph, we distinguish beams from joints and improve the
algorithm's robustness to closely spaced objects. In parallel, planes
and joints are also extracted and modeled. The connectivity graph of
these primitives allows for a compact, object-level understanding of the
entire structure. We demonstrate the capability and robustness of our
approach on both synthetic and real datasets.},
keywords={civil engineering computing;image processing;3D point
cloud;arbitrary cross sections;automatic modeling methods;automatic
recovery;cross section images;growing algorithm;object-level
understanding;point connectivity graph;Complex networks;Joints;Laser
beams;Shape;Solid modeling;Structural beams;Three-dimensional
displays;3D shape retrieval and recognition;Reverse
engineering;Robotics;Scene analysis},
doi={10.1109/3DV.2015.12},
month={Oct},}
@INPROCEEDINGS{7337215,
author={G. Shi and L. Zhao and K. Wang and Y. Gao and Y. Liu},
booktitle={2015 International Conference on Fluid Power and Mechatronics
(FPM)},
title={Indoor objects 3D modeling based on RGB-D camera for robot vision},
year={2015},
pages={750-755},
abstract={The 3D-object modeling has become one hottopic in robot vision
field for robots locating, grabbing and dropping operation. In this
paper, we propose an efficient method of object modeling based RGB-D
camera (Kinect) that can help the robot to carry out operation task. Our
method is divided into three stage: firstly, point clouds of scene
containing object are captured from the camera and plane-segmentation to
extract clusters of object; secondly, the preprocessed method is used to
filter noise and outlier points; Points pair-wise alignment by way of
our proposed pairs rejection strategy, global consistent alignment of
the complete object and object-centered representation easy to
manipulate. Besides, for the well-known loop closure problem, we
optimize the pose and eliminate the accumulations base on the ICP
improved algorithm. Finally, the proposed method is validated by the
modeling of several household objects.},
keywords={feature extraction;image capture;image representation;image
segmentation;mobile robots;optimisation;pose estimation;robot
vision;RGB-D camera;indoor object 3D modelling;object capture;object
cluster extraction;object-centered representation;pair rejection
strategy;plane segmentation;pose optimization;robot
vision;Cameras;Filtering algorithms;Iterative closest point
algorithm;Measurement;Robots;Solid modeling;Three-dimensional
displays;3D object modeling;ICP;loop optimization;noise reduction},
doi={10.1109/FPM.2015.7337215},
month={Aug},}
@INPROCEEDINGS{7321318,
author={S. Pacheco-Gutierrez and A. Stancu and M. Mustafa and E. Codres
and B. Codres},
booktitle={2015 19th International Conference on System Theory, Control
and Computing (ICSTCC)},
title={Three-dimensional interest point detection and description using
Speeded-Up Robust Features and histograms of oriented points},
year={2015},
pages={348-353},
abstract={This article presents a novel systematic methodology for the
detection of interest points in 3D point clouds and its corresponding
descriptors by using the information of an RGB camera and a
structured-light sensor. This is achieved by fusing Speeded-Up Robust
Features (SURF) in the image space, and histograms that statistically
represent the relationship of three dimensional geometric data around
the interest points. The SURF algorithm is implemented over an image
whose pixel coordinates have a direct corresponding 3D point, thus
allowing the fusion of both approaches. By combining both methodologies,
it is intent to define a set of interest points whose descriptors are
able to maintain the intrinsic characteristics of its constituent parts
such as repeatability, distinctiveness and robustness while remaining
compact and fast to compute. The detected points will be use for both,
localization and mapping of mobile robots in partially unknown
environments.},
keywords={SLAM (robots);cameras;feature extraction;geometry;image colour
analysis;image sensors;mobile robots;path planning;robot vision;3D point
clouds;RGB camera;SLAM;SURF algorithm;SURF fusion;histograms of oriented
points;image space;intrinsic characteristics;mobile robot
localization;mobile robot mapping;pixel coordinates;simultaneous
localization and mapping;speeded-up robust features;structured-light
sensor;three dimensional geometric data;three-dimensional interest point
description;three-dimensional interest point
detection;Histograms;Kernel;Robot kinematics;Simultaneous localization
and mapping;Three-dimensional displays;Computer vision;Landmark
Detection and Characterization;Robotics;SLAM},
doi={10.1109/ICSTCC.2015.7321318},
month={Oct},}
@INPROCEEDINGS{7222807,
author={S. Keshmiri and Y. Z. Tan and S. M. Ahmed and Y. Wu and C. M.
Chew and C. K. Pang},
booktitle={2015 IEEE International Conference on Advanced Intelligent
Mechatronics (AIM)},
title={3D reconstruction of complex weld geometry based on adaptive
sampling},
year={2015},
pages={1795-1800},
abstract={In this paper, an adaptive sampling algorithm is proposed for
reconstruction of a complex weld geometry based on the obtained 3D point
cloud. Several pivotal samples for reconstructing the weld geometry are
selected from the point cloud using a randomized strategy in the initial
stage of the iterative algorithm. Based on the pivotal samples, the
model is incrementally refined in the second stage by adaptive sampling
of the neighbors using three depth comparison functions. Our simulation
results using the proposed sampling algorithm demonstrate significant
improvements in total samples drawn, number of iterations, and
computation time while ensuring zero duplicate acceptance, as compared
to random sampling approaches with and without replacement.},
keywords={geometry;iterative methods;offshore installations;randomised
algorithms;robotic welding;sampling methods;welds;3D point cloud;3D
reconstruction;adaptive sampling algorithm;complex weld geometry;complex
weld geometry reconstruction;computation time;depth comparison
functions;incrementally refined model;iteration number;iterative
algorithm;randomized strategy;weld geometry
reconstruction;zero-duplicate acceptance;Adaptation models;Computational
modeling;Geometry;Joints;Solid modeling;Three-dimensional
displays;Welding},
doi={10.1109/AIM.2015.7222807},
ISSN={2159-6247},
month={July},}
@INPROCEEDINGS{7295752,
author={S. Starke and N. Hendrich and H. Bistry and Jianwei Zhang},
booktitle={2015 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Fast and robust detection and tracking of multiple persons on
RGB-D data fusing spatio-temporal information},
year={2015},
pages={95-101},
abstract={In this paper, we present an efficient and adaptive method for
detecting and tracking multiple persons while providing real-time
capability and high robustness to outlier noise. Given an RGB-D image
data sequence, our algorithm combines two independent approaches for
person detection. First, a cluster-based segmentation and classification
on RGB-D point clouds and second a face detection on RGB images, where
each method itself is post-processed by spatio-temporal filtering for
tracking and sensitivity purposes. Our analysis and experimental results
prove that the combined approach performs significantly better than the
individual solutions and greatly reduces the number of false positives
in situations where one detector fails.},
keywords={image classification;image colour analysis;image
segmentation;image sequences;object detection;object tracking;RGB-D
image data sequence;RGB-D point clouds;adaptive method;cluster-based
classification;cluster-based segmentation;false positives;person
detection;robust detection;robust tracking;sensitivity
purposes;spatio-temporal filtering;spatio-temporal
information;Clustering algorithms;Face;Face
detection;Robustness;Tracking;Computer Vision;Face Detection;Information
Fusion;Mobile Robotics;Pattern Recognition;Person Detection;RGB-D
Data;Tracking},
doi={10.1109/MFI.2015.7295752},
month={Sept},}
@INPROCEEDINGS{7295887,
author={T. Bhuvaneswari and J. T. H. Yao},
booktitle={2014 IEEE International Symposium on Robotics and
Manufacturing Automation (ROMA)},
title={Automated greenhouse},
year={2014},
pages={194-199},
abstract={Gardening is one of the popular hobbies among the people in
the midst of busy work culture and urban life style. Gardening seems to
release the stress, healthy spending of the leisure time effectively.
But the apartment living has no free space for gardening. As a result,
small scale greenhouse is now the hottest trend in the century.
Greenhouse is a structure that the user used to grow the plants. It is
built with a specific need for the type of plant they wish to grow. So
the structure varies depending on type of plant and scale of size.
Although it creates a perfect environment for plants, it needs human
care to control the optimum status of the house such as ventilation.
Automated greenhouse is to ease people when they wish to grow plants. It
helps to monitor the situation, when they are not at home. The main aim
of this paper is to minimize the human care needed for the plant by
automating the green house and monitor the in-house environment status.
A single unit of the greenhouse structure prototype has been constructed
and integrated with the sensors. The control system is designed with
Adriano Uno microcontroller. Servo motors have been used to push the
roof when there is rain detected. A 12 volt fan is also installed and
turns ON when the temperature is too high. The prototype developed is
simulated under five different places and the results are analyzed.},
keywords={control system
synthesis;gardening;greenhouses;microcontrollers;prototypes;roofs;servomotors;ventilation;Adriano
Uno microcontroller;automated greenhouse structure prototype;busy work
culture;control system design;fan;gardening;hobbies;human care;in-house
environment status monitoring;leisure time;optimum status
control;roof;sensors;servomotors;small scale greenhouse;urban life
style;ventilation;voltage 12 V;Clouds;Green products;Rain;Temperature
control;Automation;Greenhouse;Microcontroller;rain;sensors;temperature},
doi={10.1109/ROMA.2014.7295887},
month={Dec},}
@INPROCEEDINGS{7295857,
author={T. Calloway and D. B. Megherbi},
booktitle={2014 IEEE International Symposium on Robotics and
Manufacturing Automation (ROMA)},
title={Orientation estimation via low cost depth sensor ICP versus MEMS
gyroscope sensor fusion},
year={2014},
pages={27-32},
abstract={Working implementations of 3D simultaneous localization and
mapping (SLAM) using low cost depth sensors have exploded in popularity
in recent years but remain limited in some important ways. In
particular, they are not robust to rapid changes in orientation and can
accumulate significant error with just a single gradual turn into a new
scene. In this work we integrate and compare the Kinfu iterative closest
point (ICP) based SLAM implementation from the Point Cloud Library with
a hybrid optical-based inertial tracker (HObIT). In three separate
experiments we find the HObIT to be far more accurate and robust to both
slow and rapid changes in orientation. We therefore propose the
integration of precision calibrated MEMS inertial sensors into existing
low cost SLAM solutions for far more practical and robust solutions.},
keywords={SLAM (robots);gyroscopes;iterative methods;microsensors;sensor
fusion;3D simultaneous localization and mapping;MEMS gyroscope sensor
fusion;hybrid optical-based inertial tracker;iterative closest point;low
cost depth sensor;orientation estimation;point cloud
library;Calibration;Integrated optics;Iterative closest point
algorithm;Optical imaging;Optical
sensors;Software;Tracking;ICP;MEMS;RGB-D;SLAM;motion tracking},
doi={10.1109/ROMA.2014.7295857},
month={Dec},}
@INPROCEEDINGS{7295880,
author={S. Seçil and K. Turgut and O. Parlaktuna and M. Özkan},
booktitle={2014 IEEE International Symposium on Robotics and
Manufacturing Automation (ROMA)},
title={3-D visualization system for geometric parts Using a laser
profile sensor and an industrial robot},
year={2014},
pages={160-165},
abstract={This paper presents a framework that can be used for the
visualization of solid objects by using a laser profile sensor and
industrial robot arm. Many industrial applications like automated
inspection, geometric reverse engineering, autonomous planning and
object recognition require 3D points captured from the surface of
objects. This research proposes a framework that leads to the successful
visualization of geometric objects by using an industrial robot and a
laser profile sensor. Real applications are performed for a sample part
and the results are discussed.},
keywords={control engineering computing;data visualisation;industrial
manipulators;laser beam applications;production engineering computing;3D
geometric object visualization system;3D point cloud;automated
inspection;autonomous planning;geometric reverse engineering;industrial
robot arm;laser profile sensor;object recognition;solid object
visualization;Cameras;Charge coupled devices;Robot vision
systems;Service robots;Visualization;3-D visualization;industrial
robot;laser profile sensor},
doi={10.1109/ROMA.2014.7295880},
month={Dec},}
@INPROCEEDINGS{7295892,
author={M. Whaiduzzaman and A. Gani and A. Naveed},
booktitle={2014 IEEE International Symposium on Robotics and
Manufacturing Automation (ROMA)},
title={PEFC: Performance Enhancement Framework for Cloudlet in mobile
cloud computing},
year={2014},
pages={224-229},
abstract={Augmenting the computing capability to the distant cloud help
us to envision a new computing era named as mobile cloud computing
(MCC). By leveraging the cloud resources mobile user can computation
time and energy benefit. However, distant cloud has several limitations
such as communication delay and bandwidth make us to think for closer
cloud which brings the idea of proximate cloud of cloudlet. Cloudlet has
distinct advantages and is free from several limitations of distant
cloud. However, limited resources of cloudlet negatively impact the
cloudlet performance with the increasing number of substantial users. In
this paper, we propose a framework which helps to enhance the finite
resource cloudlet performance by increasing cloudlet resources. Our aim
is to increase the cloudlet performance with this limited cloudlet
resource and make the better user experience for the cloudlet user in
mobile cloud computing. We analyze and explain the each section of the
proposed framework. In addition, we also list the important features and
salient advantages of Performance Enhancement Framework of Cloudlet
(PEFC).},
keywords={cloud computing;mobile computing;resource allocation;software
performance evaluation;MCC;PEFC;cloud resource leveraging;mobile cloud
computing;performance enhancement framework for cloudlet;IEEE 802.11
Standard;Integrated circuits;Medical services;Performance
evaluation;Robots;Synchronization;Wireless
communication;cloudlet;computation offloading;mobile cloud
computing;performance enhancement;resources},
doi={10.1109/ROMA.2014.7295892},
month={Dec},}
@INPROCEEDINGS{7295818,
author={M. Camurri and S. Bazeille and D. G. Caldwell and C. Semini},
booktitle={2015 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Real-time depth and inertial fusion for local SLAM on dynamic
legged robots},
year={2015},
pages={259-264},
abstract={We present a real-time SLAM system that combines an improved
version of the Iterative Closest Point (ICP) and inertial dead reckoning
to localize our dynamic quadrupedal machine in a local map. Despite the
strong and fast motions induced by our 80 kg hydraulic legged robot, the
SLAM system is robust enough to keep the position error below 5% within
the local map that surrounds the robot. The 3D map of the terrain,
computed at the camera frame rate is suitable for vision based planned
locomotion. The inertial measurements are used before and after the ICP
registration, to provide a good initial guess, to correct the output and
to detect registration failures which can potentially corrupt the map.
The performance in terms of time and accuracy are also doubled by
preprocessing the point clouds with a background subtraction prior to
performing the ICP alignment. Our local mapping approach, in spite of
having a global frame of reference fixed onto the ground, aligns the
current map to the body frame, and allows us to push the drift away from
the most recent camera scan. The system has been tested on our robot by
performing a trot around obstacles and validated against a motion
capture system.},
keywords={SLAM (robots);computer graphics;legged locomotion;3D map;ICP
registration;dynamic legged robots;dynamic quadrupedal machine;hydraulic
legged robot;inertial dead reckoning;inertial fusion;iterative closest
point;local SLAM;motion capture system;point clouds;real-time SLAM
system;real-time depth;vision based planned locomotion;Iterative closest
point algorithm;Legged locomotion;Simultaneous localization and
mapping;Three-dimensional displays;Transforms},
doi={10.1109/MFI.2015.7295818},
month={Sept},}
@INPROCEEDINGS{7296112,
author={M. Goldhoorn and R. Hartanto},
booktitle={2014 International Conference on Computer Graphics Theory and
Applications (GRAPP)},
title={Semantic labelling of 3D point clouds using spatial object
constraints},
year={2014},
pages={1-6},
abstract={The capability of dealing with knowledge from the real human
environment is required for autonomous systems to perform complex tasks.
The robot must be able to extract the objects from the sensors' data and
give them a meaningful semantic description. In this paper a novel
method for semantic labelling is presented. The method is based on the
idea of connecting spatial information about the objects to their
spatial relations to other entities. In this approach, probabilistic
methods are used to deal with incomplete knowledge, caused by noisy
sensors and occlusions. The process is divided into two stages. First,
the spatial attributes of the objects are extracted and used for the
object pre-classification. Second, the spatial constraints are taken
into account for the semantic labelling process. Finally, we show that
the use of spatial object constraints improves the recognition results.},
keywords={Feature extraction;Labeling;Object recognition;Probabilistic
logic;Robots;Semantics;Three-dimensional displays;3D Scene
Interpretation;Clouds;Feature Extraction;Object
Recognition;Probabilistic Methods;Scene Understanding;Semantic Labelling},
month={Jan},}
@INPROCEEDINGS{7281991,
author={L. Neto and J. Reis and D. Guimarães and G. Gonçalves},
booktitle={2015 IEEE 13th International Conference on Industrial
Informatics (INDIN)},
title={Sensor cloud: SmartComponent framework for reconfigurable
diagnostics in intelligent manufacturing environments},
year={2015},
pages={1706-1711},
abstract={Sensor networks that consist of a variety of sensor nodes,
with capability for distributed storage and analysis, interoperable and
delay-tolerant communication, will pave the way for a truly scalable
network of sensors which will support adaptable plug-and-produce
assembly stations. The concept of Sensor Cloud has emerged as the
cornerstone for enabling the integration of nearly real time data
sources into Service Oriented Architectures and as one of the enablers
for Reconfigurable Manufacturing Systems. Hitherto there are still some
challenges that need to be tackled, namely the on-the-fly instantiation
and update of services at the system level and the dynamic
(re)organization of the services created. This paper presents the first
steps in the development of a framework (taking advantage of several
technologies like UPnP, OSGi and iPOJO) to address these challenges and
make Sensor Clouds a reality in the shop floor. The results from the
first implementation reveal that the performance of the system is linear
in terms of scalability, and from these scalability tests, we proved the
framework's robustness and consistent responsiveness.},
keywords={cloud computing;fault diagnosis;intelligent manufacturing
systems;production engineering computing;SmartComponent
framework;intelligent manufacturing environments;reconfigurable
diagnostics;reconfigurable manufacturing systems;scalability
tests;sensor cloud concept;sensor networks;service oriented
architecture;shop floor;Computer architecture;Context;Manufacturing
systems;Monitoring;Robot sensing systems;Scalability;Software},
doi={10.1109/INDIN.2015.7281991},
ISSN={1935-4576},
month={July},}
@INPROCEEDINGS{7284031,
author={M. Figat and T. Kornuta and M. Szlenk and C. Zieliński},
booktitle={2015 20th International Conference on Methods and Models in
Automation and Robotics (MMAR)},
title={Distributed, reconfigurable architecture for robot companions
exemplified by a voice-mail application},
year={2015},
pages={1092-1097},
abstract={The limitations of the computational capabilities of robots'
on-board computers necessitate the distribution of their control
software in a cloud. This paper presents a distributed, reconfigurable
control architecture based on the decomposition into agents, two of
which are executed on the robot: the core agent is a fixed part of the
controller, providing the task-independent robot capabilities, whereas
the dynamic agent, loaded from the cloud when required, executes the
task dependent program. The two mentioned agents execute the task,
additionally utilising the capabilities of the cloud. The system is
presented on an exemplary application of a NAO robot sending voice-mails.},
keywords={cloud computing;control engineering computing;reconfigurable
architectures;robots;voice mail;NAO robot;cloud computing;distributed
control architecture;reconfigurable control architecture;voice-mail
application;Computer architecture;Computers;Electronic mail;Robot
kinematics;Servers;Speech recognition},
doi={10.1109/MMAR.2015.7284031},
month={Aug},}
@ARTICLE{7165662,
author={P. Ondrúška and P. Kohli and S. Izadi},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={MobileFusion: Real-Time Volumetric Surface Reconstruction and
Dense Tracking on Mobile Phones},
year={2015},
volume={21},
number={11},
pages={1251-1258},
abstract={We present the first pipeline for real-time volumetric surface
reconstruction and dense 6DoF camera tracking running purely on
standard, off-the-shelf mobile phones. Using only the embedded RGB
camera, our system allows users to scan objects of varying shape, size,
and appearance in seconds, with real-time feedback during the capture
process. Unlike existing state of the art methods, which produce only
point-based 3D models on the phone, or require cloud-based processing,
our hybrid GPU/CPU pipeline is unique in that it creates a connected 3D
surface model directly on the device at 25Hz. In each frame, we perform
dense 6DoF tracking, which continuously registers the RGB input to the
incrementally built 3D model, minimizing a noise aware photoconsistency
error metric. This is followed by efficient key-frame selection, and
dense per-frame stereo matching. These depth maps are fused
volumetrically using a method akin to KinectFusion, producing compelling
surface models. For each frame, the implicit surface is extracted for
live user feedback and pose estimation. We demonstrate scans of a
variety of objects, and compare to a Kinect-based baseline, showing on
average ~ 1.5cm error. We qualitatively compare to a state of the art
point-based mobile phone method, demonstrating an order of magnitude
faster scanning times, and fully connected surface models.},
keywords={graphics processing units;image capture;image colour
analysis;image fusion;image reconstruction;image registration;mobile
computing;object tracking;pose estimation;solid modelling;stereo image
processing;KinectFusion;MobileFusion;RGB input registration;cloud-based
processing;connected 3D surface model;dense 6DoF camera tracking;dense
per-frame stereo matching;dense tracking;embedded RGB camera;hybrid
GPU/CPU pipeline;image capture;implicit surface extraction;key-frame
selection;live user feedback;mobile phones;noise aware photoconsistency
error metric;object appearance;object scanning;object shape;object
size;point-based 3D model;point-based mobile phone method;pose
estimation;real-time feedback;real-time volumetric surface
reconstruction;scanning time;volumetric depth map
fusion;Cameras;Computational modeling;Mobile
handsets;Pipelines;Real-time systems;Surface
reconstruction;Three-dimensional displays;3D object scanning;mobile
computing;surface reconstruction;0},
doi={10.1109/TVCG.2015.2459902},
ISSN={1077-2626},
month={Nov},}
@INPROCEEDINGS{7274608,
author={H. T. N. Dung and S. Lee},
booktitle={2015 IEEE 7th International Conference on Cybernetics and
Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation
and Mechatronics (RAM)},
title={Outlier removal based on boundary order and shade information in
structured light 3D camera},
year={2015},
pages={124-129},
abstract={Structured light 3D camera is considered as one of the most
reliable techniques for 3D reconstruction by producing high accuracy 3D
point cloud. However, in case that the camera is slant with object
surface, the reflected light into camera may be weakened, resulting the
occurrence of outliers (i.e. wrong decoded pixels) in 3D point cloud.
State-of-the-art methods to deal with this problem have made remarkable
improvements, but still depended on the assumption that the surface to
be captured is smooth. In this paper, instead of operating on the point
coordinators only, we propose a new method to identify outliers based on
the shade information and camera-projector correspondence. To enhance
our solution, we also introduce a new shade detection algorithm that
using the variation of gray colors from captured patterns. Compared with
other methods, the experimental results have proved that the proposed
method is more effective in removing outliers, even when the surface is
not smooth. Besides, our method is also able to detect clusters of
density outliers, which is one of limitation of existing approaches.},
keywords={computer graphics;image colour analysis;image
reconstruction;3D point cloud;3D reconstruction;boundary
order;camera-projector correspondence;captured pattern;gray
color;outlier removal;point coordinator;shade detection algorithm;shade
information;structured light 3D
camera;Accuracy;Cameras;Conferences;Decoding;Detection
algorithms;Lighting;Three-dimensional displays},
doi={10.1109/ICCIS.2015.7274608},
ISSN={2326-8123},
month={July},}
@INPROCEEDINGS{7274616,
author={Q. Yuan and T. S. Lembono and Y. Zou and I. M. Chen},
booktitle={2015 IEEE 7th International Conference on Cybernetics and
Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation
and Mechatronics (RAM)},
title={Automatic robot taping: Auto-path planning and manipulation},
year={2015},
pages={175-180},
abstract={Many industrial applications, such as painting, plasma
spraying, require people to work long hours to cover the uninvolved part
with masking tapes. In this paper, we introduce an automatic robot
taping system which can reconstruct the 3D model of the taping part, do
path planning and complete the surface covering of the parts using
masking tapes automatically. The system uses a Kinect as the 3D scanner
to acquire the 3D point clouds of the taping part. Based on the digital
3D points cloud model of the part, we introduces a path planning method
to generate the moving path of the systems. A rotary platform and a
6-DOF robot arm collaborate together to execute the robot taping
process. For continuous taping, the system and method can handle Tubular
shapes. Experiment of taping a cylindroid shape is conducted to validate
the system and method. For parts with more complex geometry, further
development on the automatic initial tape attaching and cutting
mechanism is needed. With such system, the taping work can be much less
troublesome. Also, the path planning method applies to applications such
as drawing or writing on different surfaces etc. as well.},
keywords={cutting;industrial manipulators;path planning;3D model;3D
scanner;6-DOF robot arm;Kinect;auto-path manipulation;auto-path planning
method;automatic initial tape attaching mechanism;automatic robot taping
system;complex geometry;continuous taping;cutting mechanism;cylindroid
shape;digital 3D points cloud model;masking tapes;rotary
platform;tubular shapes;Path planning;Service robots;Shape;Solid
modeling;Surface treatment;Three-dimensional displays},
doi={10.1109/ICCIS.2015.7274616},
ISSN={2326-8123},
month={July},}
@INPROCEEDINGS{7271333,
author={F. Bonin-Font and A. Cosic and P. L. Negre and M. Solbach and G.
Oliver},
booktitle={OCEANS 2015 - Genova},
title={Stereo SLAM for robust dense 3D reconstruction of underwater
environments},
year={2015},
pages={1-6},
abstract={The virtual reconstruction of underwater environments in 3
dimensions can be of great utility for scientific or industrials
applications, being the use of Autonomous Underwater Vehicles (AUVs)
equipped with cameras an invaluable tool with progressive improvements.
However, a highly accurate vehicle localization process is fundamental
to place every portion of that 3D model in its real position with
respect to the origin of the world system of coordinates, and to join
properly the overall reconstructed area, especially in large surveys.
Simultaneous Localization And Mapping (SLAM) techniques constitute the
most precise localization approach using only data provided by the
navigation sensors installed on board the underwater robot. However, the
real challenge consists in applying these techniques in underwater
environments where the imaging conditions are usually limited or
degraded. This paper presents the comparison of two different SLAM
approaches based on stereo-vision, a graph-SLAM and an EKF SLAM, applied
to localize, in real time, an AUV moving in underwater environments.
Both algorithms deal with pure 3D data, (x, y, z) for the vehicle
position and a quaternion to represent its orientation. The aim of this
work is to assess and compare the performance of both solutions in terms
of accuracy in the estimation of the robot pose. First experiments were
conducted in controlled aquatic scenarios where it was feasible to build
a highly reliable ground truth, showing how the graph-SLAM approach
outperforms its EKF counterpart under the same working and environmental
conditions. A second set of experiments was conducted in the sea,
showing the same tendency in the results. The SLAM pose estimates, given
with respect to a global world frame, corresponding to the approach with
less accumulated error are used to recreated virtual 3D maps of the
environment based on the concatenation of successive stereo visual point
clouds placed in the corresponding SLAM locations.},
keywords={autonomous underwater vehicles;stereo image
processing;underwater equipment;EKF SLAM;autonomous underwater
vehicles;controlled aquatic scenarios;graph-SLAM;pure 3D data;robot
pose;robust dense 3D reconstruction;simultaneous localization and
mapping techniques;stereo SLAM;stereo-vision;underwater
environments;vehicle position;Robot kinematics;Simultaneous localization
and mapping;Three-dimensional displays;Trajectory;Vehicles;Visualization},
doi={10.1109/OCEANS-Genova.2015.7271333},
month={May},}
@INPROCEEDINGS{7271587,
author={A. Palomer and P. Ridao and D. Ribas and G. Vallicrosa},
booktitle={OCEANS 2015 - Genova},
title={Multi-beam terrain/object classification for underwater
navigation correction},
year={2015},
pages={1-5},
abstract={Building accurate bathymetries of the seabed has been a focus
of study in the last decade. For this purpose seabed point cloud
registration has been a focus for some researchers. Some of this
registration methods are based on gathering the points of the cloud that
contain more information for the registration (i.e. the ones that flat
or smooth, normally being the seabed) and using them as part of
ICP-derived methods. For this point picking purpose, we present a
segmentation technique that distinguish between objects (interesting for
registration) and ground (smooth and not interesting for registration).
The method proposed here uses difference of normals for object's border
detection and a variation of the Density-Based Spatial Clustering of
Application with Noise for object clustering. Once the objects
boundaries are detected and the points are clustered the rest of the
points are classified as object or ground. This classification is done
by taking all the points that lie within the object's border and
checking it's depth compare to its closes border point. The method is
evaluated using a multi-beam dataset gathered on the La Lune shipwreck,
a site of archaeological interest.},
keywords={marine navigation;object detection;oceanographic techniques;La
Lune shipwreck;density-based spatial clustering;multi-beam
dataset;multibeam terrain/object classification;object border
detection;object clustering;registration methods;seabed point cloud
registration;segmentation technique;underwater navigation
correction;Benchmark testing;Clustering
algorithms;Noise;Robots;Sonar;Three-dimensional displays;Underwater
vehicles},
doi={10.1109/OCEANS-Genova.2015.7271587},
month={May},}
@INPROCEEDINGS{7251468,
author={C. Y. Lin and Z. A. Abebe and S. H. Chang},
booktitle={2015 International Conference on Advanced Robotics (ICAR)},
title={Advanced spraying task strategy for bicycle-frame based on
geometrical data of workpiece},
year={2015},
pages={277-282},
abstract={Path planning and trajectory generation are the primary tasks
for spray painting applications. However, for the most efficient
spraying application, the detailed dimensional information of the
workpiece along the generated path is essential. This paper introduces
the shape signature approach for automatic identification and
measurement of cross-sectional information of a bicycle frame model for
advanced spraying task of 6-DOF robot. The path planning is generated
from skeletal line of the point cloud model of the bicycle frame which
is reconstructed by Kinect Fusion based algorithm. Cross-sectional shape
and dimensions are autonomously identified along the generated spray-gun
path and subsequently the corresponding spray task is applied in each
segment. The spray-gun orientation and painting condition such as the
spray speed, volume rate and internal and external pressures are
automatically adjusted according to the cross-sectional information and
nature of the path at the current pose of the end effector.},
keywords={bicycles;end effectors;image fusion;industrial
manipulators;painting;robot vision;spraying;6-DOF robot;Kinect fusion
based algorithm;advanced spraying task strategy;bicycle frame
model;bicycle-frame;cross-sectional shape;dimensional information;end
effector;geometrical data;path planning;point cloud model;shape
signature approach;spray painting;spray task;spray-gun
orientation;spray-gun path;trajectory generation;Painting;Path
planning;Robots;Shape;Solid modeling;Spraying;Three-dimensional
displays;3D model construction;6DOF robot;Shape signature},
doi={10.1109/ICAR.2015.7251468},
month={July},}
@INPROCEEDINGS{7237901,
author={L. Li and W. Wang and Y. Su and Z. Du},
booktitle={2015 IEEE International Conference on Mechatronics and
Automation (ICMA)},
title={A data-driven grasp planning method based on Gaussian Process
Classifier},
year={2015},
pages={2626-2631},
abstract={This paper presents a grasp planning method for grasping novel
objects from point clouds provided by the Kinect camera. By applying
machine learning, the planning method can generate two points which
represent the contact point and direction of grasp. This method is based
on three components: 1) grasp configuration which can present the
location of contact points and the direction of grasp, 2) features which
take force closure and grasp stability into account, and 3) Gaussian
Process Classifier which is used to calculate the grasp quality by using
the features of each grasp configuration. Two experiments are carried
out to verify our method. The results demonstrate that the robot using
this approach can successfully grasp objects with partial point clouds.},
keywords={Gaussian processes;grippers;learning (artificial
intelligence);robot vision;stability;Gaussian process classifier;Kinect
camera;data-driven grasp planning method;grasp stability;machine
learning;object grasping;point clouds;Feature extraction;Gaussian
processes;Grasping;Grippers;Probability;Robots;Three-dimensional
displays;Gaussian Process;force-closure;grasp planning;point
clouds;robotic grasping},
doi={10.1109/ICMA.2015.7237901},
ISSN={2152-7431},
month={Aug},}
@INPROCEEDINGS{7222736,
author={Y. G. Moon and S. J. Go and K. H. Yu and M. C. Lee},
booktitle={2015 IEEE International Conference on Advanced Intelligent
Mechatronics (AIM)},
title={Development of 3D laser range finder system for object recognition},
year={2015},
pages={1402-1405},
abstract={Over the last years, object recognition has become a more and
more active field of research in robotics. 3D perception is a promising
technology for automatic control, manufacturing and robotics. Compared
to 2D vision systems, 3D range sensors can provide direct geometric
information of the environment. This study presents a high quality, low
cost 3D laser range finder designed for object recognition. The 3D laser
is built on the base of a 2D laser range finder by the extension with a
servo motor and a rotation module. The servo is controlled by an
embedded computer running Linux. The survey yield a digital data set,
which is essentially a dense “point cloud”, where each point is
represented by a coordinate in 3D space. A complete 3D scan data of an
area of 360° with 48,598 points are grabbed in 0.5 seconds. A systematic
error is 5 cm. While scanning, different online algorithms for line and
surface detection are applied to the data. Object segmentation and
detection are done in real-time after the scan. The implemented software
modules detect objects.},
keywords={image processing equipment;laser ranging;object
recognition;optical scanners;3D laser range finder;3D
perception;Linux;dense point cloud;embedded computer;object
recognition;object segmentation;servo motor;Business;Object
recognition;Robot sensing systems;Sensor
systems;Servomotors;Three-dimensional displays},
doi={10.1109/AIM.2015.7222736},
ISSN={2159-6247},
month={July},}
@INPROCEEDINGS{7139887,
author={K. Huang and L. T. Jiang and J. R. Smith and H. J. Chizeck},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Sensor-aided teleoperated grasping of transparent objects},
year={2015},
pages={4953-4959},
abstract={This paper presents a method of augmenting streaming point
cloud data with pretouch proximity sensor information for the purposes
of teleoperated grasping of transparent targets. When using commercial
RGB-Depth (RGB-D) cameras, material properties can significantly affect
depth measurements. In particular, transparent objects are difficult to
perceive with RGB images and commercially available depth sensors.
Geometric information of such objects needs to be gathered with
additional sensors, and in many scenarios, it is of interest to gather
this information without physical contact. In this work, a non-contact
pretouch sensor fixed to the robot end effector is used to sense and
explore physical geometries previously unobserved. Thus, the point cloud
representation of an unknown, transparent grasp target, can be enhanced
through telerobotic exploration in real-time. Furthermore, real-time
haptic rendering algorithms and haptic virtual fixtures used in
combination with the augmented streaming point clouds assist the
teleoperator in collision avoidance during exploration. Theoretical
analyses are performed to design virtual fixtures suitable for pretouch
sensing, and experiments show the effectiveness of this method to gather
geometry data without collision and eventually to successfully grasp a
transparent object.},
keywords={collision avoidance;control engineering computing;haptic
interfaces;image colour analysis;image representation;rendering
(computer graphics);sensors;telerobotics;RGB images;RGB-D
camera;augmented streaming point cloud;collision avoidance;commercial
RGB-Depth camera;depth measurement;depth sensor;geometric
information;haptic virtual fixture;material property;oncontact pretouch
sensor;point cloud representation;pretouch proximity sensor
information;pretouch sensing;real-time haptic rendering algorithm;robot
end effector;sensor-aided teleoperated grasping;streaming point cloud
data;teleoperator;telerobotic exploration;theoretical
analysis;transparent grasp target;transparent objects;transparent
target;Cameras;End effectors;Haptic interfaces;Robot sensing
systems;Three-dimensional displays},
doi={10.1109/ICRA.2015.7139887},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139616,
author={T. Linder and S. Wehner and K. O. Arras},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Real-time full-body human gender recognition in (RGB)-D data},
year={2015},
pages={3039-3045},
abstract={Understanding social context is an important skill for robots
that share a space with humans. In this paper, we address the problem of
recognizing gender, a key piece of information when interacting with
people and understanding human social relations and rules. Unlike
previous work which typically considered faces or frontal body views in
image data, we address the problem of recognizing gender in RGB-D data
from side and back views as well. We present a large, gender-balanced,
annotated, multi-perspective RGB-D dataset with full-body views of over
a hundred different persons captured with both the Kinect v1 and Kinect
v2 sensor. We then learn and compare several classifiers on the Kinect
v2 data using a HOG baseline, two state-of-the-art deep-learning
methods, and a recent tessellation-based learning approach. Originally
developed for person detection in 3D data, the latter is able to learn
the best selection, location and scale of a set of simple point cloud
features. We show that for gender recognition, it outperforms the other
approaches for both standing and walking people while being very
efficient to compute with classification rates up to 150 Hz.},
keywords={human-robot interaction;image classification;image
sensors;learning (artificial intelligence);object detection;object
recognition;HOG baseline;Kinect v1 sensor;Kinect v2 sensor;RGB-D
data;annotated RGB-D dataset;deep-learning methods;gender-balanced RGB-D
dataset;human social relations;human social rules;image
data;multiperspective RGB-D dataset;person detection;point cloud
features;real-time full-body human gender recognition;tessellation-based
learning approach;Accuracy;Legged locomotion;Robot sensing
systems;Support vector machines;Three-dimensional displays;Training},
doi={10.1109/ICRA.2015.7139616},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139713,
author={A. H. Quispe and B. Milville and M. A. Gutiérrez and C. Erdogan
and M. Stilman and H. Christensen and H. B. Amor},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Exploiting symmetries and extrusions for grasping household
objects},
year={2015},
pages={3702-3708},
abstract={In this paper we present an approach for creating complete
shape representations from a single depth image for robot grasping. We
introduce algorithms for completing partial point clouds based on the
analysis of symmetry and extrusion patterns in observed shapes.
Identified patterns are used to generate a complete mesh of the object,
which is, in turn, used for grasp planning. The approach allows robots
to predict the shape of objects and include invisible regions into the
grasp planning step. We show that the identification of shape patterns,
such as extrusions, can be used for fast generation and optimization of
grasps. Finally, we present experiments performed with our humanoid
robot executing pick-up tasks based on single depth images and discuss
the applications and shortcomings of our approach.},
keywords={humanoid robots;image representation;manipulators;mobile
robots;path planning;robot vision;shape recognition;autonomous
robots;complete shape representations;extrusion exploitation;extrusion
pattern analysis;grasp planning step;household object grasping;humanoid
robot;partial point cloud completion;pick-up tasks;robot grasping;shape
pattern identification;symmetries exploitation;symmetry pattern
analysis;Approximation
methods;Cameras;Optimization;Planning;Robots;Shape;Three-dimensional
displays},
doi={10.1109/ICRA.2015.7139713},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139611,
author={T. Fäulhammer and A. Aldoma and M. Zillich and M. Vincze},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Temporal integration of feature correspondences for enhanced
recognition in cluttered and dynamic environments},
year={2015},
pages={3003-3009},
abstract={We propose a method for recognizing rigid object instances in
RGB-D point clouds by accumulating low-level information from keypoint
correspondences over multiple observations. Compared to existing
multi-view approaches, we make fewer assumptions on the recognition
problem, dealing with cluttered and partially dynamic environments as
well as covering a wide range of objects. Evaluation on the publicly
available TUW and Willow datasets showed that our method achieves
state-of-the-art recognition performance for challenging sequences of
static environments and a significant improvement for environments
partially changing during the observation.},
keywords={clutter;object recognition;RGB-D point cloud;TUW
dataset;Willow dataset;low-level information;object
recognition;partially dynamic environment;temporal
integration;Cameras;Databases;Merging;Robot vision
systems;Three-dimensional displays},
doi={10.1109/ICRA.2015.7139611},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139782,
author={K. Wu and R. Ranasinghe and G. Dissanayake},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Active recognition and pose estimation of household objects in
clutter},
year={2015},
pages={4230-4237},
abstract={This paper presents an active object recognition and pose
estimation system for household objects in a highly cluttered
environment. A sparse feature model, augmented with the characteristics
of features when observed from different viewpoints is used for
recognition and pose estimation while a dense point cloud model is used
for storing geometry. This strategy makes it possible to accurately
predict the expected information available during the Next-Best-View
planning process as both the visibility as well as the likelihood of
feature matching can be considered simultaneously. Experimental
evaluations of the active object recognition and pose estimation with an
RGB-D sensor mounted on a Turtlebot are presented.},
keywords={image matching;image sensors;pose estimation;robot
vision;RGB-D sensor;Turtlebot;active object recognition;clutter;dense
point cloud model;feature matching;household objects;next-best-view
planning process;pose estimation system;sparse feature
model;Cameras;Estimation;Feature extraction;Object recognition;Robot
sensing systems;Three-dimensional displays},
doi={10.1109/ICRA.2015.7139782},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139334,
author={P. Ozog and G. Troni and M. Kaess and R. M. Eustice and M.
Johnson-Roberson},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Building 3D mosaics from an Autonomous Underwater Vehicle,
Doppler velocity log, and 2D imaging sonar},
year={2015},
pages={1137-1143},
abstract={This paper reports on a 3D photomosaicing pipeline using data
collected from an autonomous underwater vehicle performing simultaneous
localization and mapping (SLAM). The pipeline projects and blends 2D
imaging sonar data onto a large-scale 3D mesh that is either given a
priori or derived from SLAM. Compared to other methods that generate a
2D-only mosaic, our approach produces 3D models that are more
structurally representative of the environment being surveyed.
Additionally, our system leverages recent work in underwater SLAM using
sparse point clouds derived from Doppler velocity log range returns to
relax the need for a prior model. We show that the method produces
reasonably accurate surface reconstruction and blending consistency,
with and without the use of a prior mesh. We experimentally evaluate our
approach with a Hovering Autonomous Underwater Vehicle (HAUV) performing
inspection of a large underwater ship hull.},
keywords={Doppler shift;SLAM (robots);autonomous underwater
vehicles;image reconstruction;image segmentation;mesh generation;robot
vision;sonar imaging;surface reconstruction;2D imaging sonar
data;2D-only mosaic generation;3D photomosaicing pipeline;Doppler
velocity log;HAUV;SLAM;blending consistency;hovering autonomous
underwater vehicle;large underwater ship hull;large-scale 3D
mesh;simultaneous localization and mapping;sparse point clouds;surface
reconstruction;Cameras;Design automation;Simultaneous localization and
mapping;Solid modeling;Sonar;Three-dimensional displays},
doi={10.1109/ICRA.2015.7139334},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139996,
author={K. Yamamoto and T. Shitaka},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Maximal Output Admissible set for limit cycle controller of
humanoid robot},
year={2015},
pages={5690-5697},
abstract={This paper addresses a novel computation method of the Maximal
Output Admissible (MOA) set for a limit cycle controller and its
application to motion transition. By approximately calculating the MOA
set via sample point cloud, we can obtain an analytic form of the MOA
set even on a nonlinear system. Formulation provided in this paper can
be applicable to various types of controllers. Using the MOA set, we
demonstrate a motion transition from a standing posture to steady
walking. Switching two types of controllers based on the MOA set
realizes motion transition with the COP constraint satisfied. The
validity of proposed method is verified with a simulation.},
keywords={humanoid robots;legged locomotion;motion control;COP
constraint;MOA set;center-of-pressure constraint;humanoid robot;limit
cycle controller;maximal output admissible set;motion
transition;nonlinear system;sample point cloud;Aerospace
electronics;Legged locomotion;Limit-cycles;Switches;Three-dimensional
displays;Trajectory},
doi={10.1109/ICRA.2015.7139996},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139780,
author={S. M. Prakhya and L. Bingbing and L. Weisi and U. Qayyum},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Sparse Depth Odometry: 3D keypoint based pose estimation from
dense depth data},
year={2015},
pages={4216-4223},
abstract={This paper presents Sparse Depth Odometry (SDO) to
incrementally estimate the 3D pose of a depth camera in indoor
environments. SDO relies on 3D keypoints extracted on dense depth data
and hence can be used to augment the RGB-D camera based visual odometry
methods that fail in places where there is no proper illumination. In
SDO, our main contribution is the design of the keypoint detection
module, which plays a vital role as it condenses the input point cloud
to a few keypoints. SDO differs from existing depth alone methods as it
does not use the popular signed distance function and can run online,
even without a GPU. A new keypoint detection module is proposed via
keypoint selection, and is based on extensive theoretical and
experimental evaluation. The proposed keypoint detection module
comprises of two existing keypoint detectors, namely SURE [1] and NARF
[2]. It offers reliable keypoints that describe the scene more
comprehensively, compared to others. Finally, an extensive performance
evaluation of SDO on benchmark datasets with the proposed keypoint
detection module is presented and compared with the state-of-the-art.},
keywords={Cameras;Computational modeling;Covariance
matrices;Detectors;Estimation;Three-dimensional displays;Visualization},
doi={10.1109/ICRA.2015.7139780},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139615,
author={Jing Huang and Suya You},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Pole-like object detection and classification from urban point
clouds},
year={2015},
pages={3032-3038},
abstract={This paper focuses on detecting and classifying pole-like
objects from point clouds obtained in urban areas. To achieve our goal,
we propose a system consisting of three stages: localization,
segmentation and classification. The localization algorithm based on
slicing, clustering, pole seed generation and bucket augmentation takes
advantage of the unique characteristics of pole-like objects and avoids
heavy computation on the feature of every point in traditional methods.
Then, the bucket-shaped neighborhood of the segments is integrated and
trimmed with region growing algorithms, reducing the noises within
candidate's neighborhood. Finally, we introduce a representation of six
attributes based on the height and five point classes closely related to
the pole categories and apply SVM to classify the candidate objects into
4 categories, including 3 pole categories light, utility pole and sign,
and the non-pole category. The performance of our method is demonstrated
through comparison with previous works on a large-scale urban dataset.},
keywords={image classification;image denoising;image filtering;image
representation;image segmentation;object detection;pattern
clustering;statistical analysis;support vector machines;SVM;attribute
representation;bucket augmentation;bucket-shaped
neighborhood;clustering;light pole;localization algorithm;noise
reducing;nonpole category;object classification;object filtering;object
localization;object segmentation;pole seed generation;pole-like object
detection;region growing algorithm;sign pole;slicing;statistical pole
descriptor;urban area;urban point clouds;utility pole
pole;Classification algorithms;Clustering algorithms;Shape;Smoothing
methods;Support vector machines;Three-dimensional displays;Wires},
doi={10.1109/ICRA.2015.7139615},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139882,
author={J. Mahler and S. Patil and B. Kehoe and J. van den Berg and M.
Ciocarlie and P. Abbeel and K. Goldberg},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={GP-GPIS-OPT: Grasp planning with shape uncertainty using Gaussian
process implicit surfaces and Sequential Convex Programming},
year={2015},
pages={4919-4926},
abstract={Computing grasps for an object is challenging when the object
geometry is not known precisely. In this paper, we explore the use of
Gaussian process implicit surfaces (GPISs) to represent shape
uncertainty from RGBD point cloud observations of objects. We study the
use of GPIS representations to select grasps on previously unknown
objects, measuring grasp quality by the probability of force closure.
Our main contribution is GP-GPIS-OPT, an algorithm for computing grasps
for parallel-jaw grippers on 2D GPIS object representations.
Specifically, our method optimizes an approximation to the probability
of force closure subject to antipodal constraints on the parallel jaws
using Sequential Convex Programming (SCP). We also introduce GPIS-Blur,
a method for visualizing 2D GPIS models based on blending shape samples
from a GPIS. We test the algorithm on a set of 8 planar objects with
transparency, translucency, and specularity. Our experiments suggest
that GP-GPIS-OPT computes grasps with higher probability of force
closure than a planner that does not consider shape uncertainty on our
test objects and may converge to a grasp plan up to 5.7×faster than
using Monte-Carlo integration, a common method for grasp planning under
shape uncertainty. Furthermore, initial experiments on the Willow Garage
PR2 robot suggest that grasps selected with GP-GPIS-OPT are up to 90%
more successful than those planned assuming a deterministic shape. Our
dataset, code, and videos of our experiments are available at
http://rll.berkeley.edu/icra2015grasping/.},
keywords={Gaussian processes;approximation theory;convex
programming;grippers;mobile robots;probability;2D GPIS object
representations;GP-GPIS-OPT;Gaussian process implicit surfaces;RGBD
point cloud observations;SCP;Willow Garage PR2 robot;force closure;grasp
planning;parallel-jaw grippers;probability;sequential convex
programming;Approximation
methods;Force;Grippers;Sensors;Shape;Three-dimensional
displays;Uncertainty},
doi={10.1109/ICRA.2015.7139882},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139221,
author={G. N. Wilson and A. Ramirez-Serrano and Q. Sun},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Vehicle state prediction for outdoor autonomous high-speed
off-road UGVs},
year={2015},
pages={467-472},
abstract={This paper describes a method of vehicle state prediction for
an autonomous high-speed off-road Unmanned Ground Vehicle (UGV).
Effective vehicle state prediction will allow a UGV to plan its
navigation such that states (such as vertical acceleration induced by
the terrain roughness) never exceed a desired threshold. In this paper a
model of an n-wheeled generic vehicle is used determine its dynamics.
Using a known terrain input profile the vehicle's output states are
predicted using the developed n-wheel model. Simulated results of this
vehicle state prediction approach are presented, as well as experimental
tests using a UGV platform called Loc8. The experimental results used a
3D point cloud to determine the terrain input profile. Methods from the
literature are tested against the developed n-wheeled vehicle state
prediction method. Results show this n-wheel technique presents both
advantages and disadvantages in comparison with existing techniques. The
proposed approach predicts the average absolute acceleration much closer
to the measured average absolute acceleration than existing approaches.},
keywords={mobile robots;navigation;remotely operated vehicles;road
traffic control;vehicle dynamics;wheels;3D point cloud;Loc8 UGV
platform;n-wheeled generic vehicle;n-wheeled vehicle state prediction
method;outdoor autonomous high-speed off-road UGVs;terrain input
profile;unmanned ground vehicle;vehicle output
states;Acceleration;Cameras;Mathematical model;Predictive
models;Suspensions;Three-dimensional displays;Vehicles},
doi={10.1109/ICRA.2015.7139221},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139702,
author={J. Behley and V. Steinhage and A. B. Cremers},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Efficient radius neighbor search in three-dimensional point clouds},
year={2015},
pages={3625-3630},
abstract={Finding all neighbors of a point inside a given radius is an
integral part in many approaches using three-dimensional laser range
data. We present novel insights to significantly improve the runtime
performance of radius neighbor search using octrees. Our contributions
are as follows: (1) We propose an index-based organization of the point
cloud such that we can efficiently store start and end indexes of points
inside every octant and (2) exploiting this representation, we can use
pruning of irrelevant subtrees in the traversal to facilitate highly
efficient radius neighbor search. We show significant runtime
improvements of our proposed octree representation over state-of-the-art
neighbor search implementations on three different urban datasets.},
keywords={laser ranging;octrees;search problems;index-based
organization;laser range data;octree representation;radius neighbor
search;three-dimensional point clouds;urban datasets;Artificial neural
networks;Buildings;Indexes;Lasers;Octrees;Runtime;Three-dimensional
displays},
doi={10.1109/ICRA.2015.7139702},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139756,
author={P. Oettershagen and A. Melzer and T. Mantel and K. Rudin and R.
Lotz and D. Siebenmann and S. Leutenegger and K. Alexis and R. Siegwart},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={A solar-powered hand-launchable UAV for low-altitude multi-day
continuous flight},
year={2015},
pages={3986-3993},
abstract={This paper presents the conceptual design, detailed
development and flight testing of AtlantikSolar, a 5.6m-wingspan
solar-powered Low-Altitude Long-Endurance (LALE) Unmanned Aerial Vehicle
(UAV) designed and built at ETH Zurich. The UAV is required to provide
perpetual endurance at a geographic latitude of 45°N in a 4-month window
centered around June 21^st . An improved conceptual design method is
presented and applied to maximize the perpetual flight robustness with
respect to local meteorological disturbances such as clouds or winds.
Airframe, avionics hardware, state estimation and control method
development for autonomous flight operations are described. Flight test
results include a 12-hour flight relying solely on batteries to
replicate night-flight conditions. In addition, we present flight
results from Search-And-Rescue field trials where a camera and
processing pod were mounted on the aircraft to create high-fidelity
3D-maps of a simulated disaster area.},
keywords={autonomous aerial vehicles;rescue robots;state
estimation;AtlantikSolar;ETH Zurich;LALE;aircraft;airframe;autonomous
flight operations;avionics hardware;camera;clouds;conceptual
design;conceptual design method;control method development;flight
testing;high-fidelity 3D-maps;local meteorological
disturbances;low-altitude long-endurance unmanned aerial
vehicle;low-altitude multiday continuous flight;night-flight
conditions;perpetual flight robustness;processing pod;search-and-rescue
field trials;simulated disaster area;solar-powered hand-launchable
UAV;state estimation;winds;Aerospace
electronics;Airplanes;Batteries;Clouds;Design
methodology;Propulsion;Robustness},
doi={10.1109/ICRA.2015.7139756},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139023,
author={J. M. O'Kane and D. A. Shell},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Automatic design of discreet discrete filters},
year={2015},
pages={353-360},
abstract={We address the problem of deciding what information a robot
should transmit to the outside world, by exploring a setting where some
information (e.g., current status of the task) must be shared in order
for the robot to be useful, but where, simultaneously, we wish to impose
limits which ensure certain information is never divulged. These sorts
of conditions arise in several circumstances of increasing relevance:
robots that can provide some guarantee of privacy to their users,
controllers which safely use untrusted “cloud” services or smart-space
infrastructure, or robots that act as inspection devices in
information-sensitive contexts (e.g., factories, nuclear plants, etc.)
We introduce an algorithm which takes as input an arbitrary
combinatorial filter, expressed as a transition graph, and a set of
constraints, constituting both upper and lower bounds, that specify the
desired informational properties. The algorithm produces a coarser
version of the input filter which possesses the desired informational
properties, if and only if such a filter exists. We show that
determining whether it is possible to satisfy both the distinguishablity
and indistinguishablity constraints is NP-hard. The hardness result
helps justify the worst-case running time of the algorithm. We describe
an implementation of the algorithm along with empirical results showing
that, beyond some minimum problem complexity, the algorithm is faster
than naïve filter enumeration, albeit with greater memory requirements.},
keywords={filtering theory;graph theory;NP-hard;arbitrary combinatorial
filter;automatic design;discreet discrete filters;distinguishablity
constraints;indistinguishablity constraints;information-sensitive
contexts;informational properties;inspection devices;memory
requirements;naϊve filter enumeration;robots;smart-space
infrastructure;transition graph;untrusted cloud services;Algorithm
design and analysis;Color;Observers;Privacy;Robot kinematics;Robot
sensing systems},
doi={10.1109/ICRA.2015.7139023},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139593,
author={C. Pfitzner and S. May and C. Merkl and L. Breuer and M.
Köhrmann and J. Braun and F. Dirauf and A. Nüchter},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Libra3D: Body weight estimation for emergency patients in
clinical environments with a 3D structured light sensor},
year={2015},
pages={2888-2893},
abstract={This paper describes the application of a weight estimation
method for emergency patients in clinical environments. The approach
applies established algorithms for point cloud processing and filtering
to data from a low-cost, structured light sensor. A patient's volume is
estimated on the basis of their visible front surface. The approach is
currently being tested in the workflow of the emergency room at the
Universitätsklinikum Erlangen, Germany. Preliminary results show the
accuracy of the approach in relation to other conservative means of
weight measurements, for example, by physicians and anthropometric
measurements.},
keywords={anthropometry;biomedical measurement;image filtering;medical
image processing;optical sensors;weighing;3D structured light
sensor;Libra3D;anthropometric measurements;clinical environments;point
cloud processing;weight estimation method;weight
measurements;Accuracy;Back;Density measurement;Estimation;Medical
services;Three-dimensional displays;Weight measurement},
doi={10.1109/ICRA.2015.7139593},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139875,
author={D. Wolf and J. Prankl and M. Vincze},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Fast semantic segmentation of 3D point clouds using a dense CRF
with learned parameters},
year={2015},
pages={4867-4873},
abstract={In this paper, we present an efficient semantic segmentation
framework for indoor scenes operating on 3D point clouds. We use the
results of a Random Forest Classifier to initialize the unary potentials
of a densely interconnected Conditional Random Field, for which we learn
the parameters for the pairwise potentials from training data. These
potentials capture and model common spatial relations between class
labels, which can often be observed in indoor scenes. We evaluate our
approach on the popular NYU Depth datasets, for which it achieves
superior results compared to the current state of the art. Exploiting
parallelization and applying an efficient CRF inference method based on
mean field approximation, our framework is able to process full
resolution Kinect point clouds in half a second on a regular laptop,
more than twice as fast as comparable methods.},
keywords={image classification;image segmentation;inference
mechanisms;learning (artificial intelligence);3D point clouds;CRF
inference method;Kinect point clouds;NYU Depth datasets;conditional
random field;indoor scenes;mean field approximation;random forest
classifier;semantic segmentation
framework;Accuracy;Kernel;Labeling;Semantics;Three-dimensional
displays;Training;Training data},
doi={10.1109/ICRA.2015.7139875},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139443,
author={W. J. Beksi and N. Papanikolopoulos},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Object classification using dictionary learning and RGB-D
covariance descriptors},
year={2015},
pages={1880-1885},
abstract={In this paper, we introduce a dictionary learning framework
using RGB-D covariance descriptors on point cloud data for performing
object classification. Dictionary learning in combination with RGB-D
covariance descriptors provides a compact and flexible description of
point cloud data. Furthermore, the proposed framework is ideal for
updating and sharing dictionaries among robots in a decentralized or
cloud network. This work demonstrates the increased performance of 3D
object classification utilizing covariance descriptors and dictionary
learning over previous results with experiments performed on a publicly
available RGB-D database.},
keywords={image classification;image colour analysis;learning
(artificial intelligence);object recognition;robot vision;RGB-D
covariance descriptors;cloud network;computer vision;decentralized
network;dictionary learning framework;dictionary sharing;dictionary
updating;object classification;object recognition;point cloud
data;robotics;Covariance
matrices;Databases;Dictionaries;Robots;Shape;Three-dimensional
displays;Visualization},
doi={10.1109/ICRA.2015.7139443},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139486,
author={J. Zhang and S. Singh},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Visual-lidar odometry and mapping: low-drift, robust, and fast},
year={2015},
pages={2174-2181},
abstract={Here, we present a general framework for combining visual
odometry and lidar odometry in a fundamental and first principle method.
The method shows improvements in performance over the state of the art,
particularly in robustness to aggressive motion and temporary lack of
visual features. The proposed on-line method starts with visual odometry
to estimate the ego-motion and to register point clouds from a scanning
lidar at a high frequency but low fidelity. Then, scan matching based
lidar odometry refines the motion estimation and point cloud
registration simultaneously.We show results with datasets collected in
our own experiments as well as using the KITTI odometry benchmark. Our
proposed method is ranked #1 on the benchmark in terms of average
translation and rotation errors, with a 0.75% of relative position
drift. In addition to comparison of the motion estimation accuracy, we
evaluate robustness of the method when the sensor suite moves at a high
speed and is subject to significant ambient lighting changes.},
keywords={distance measurement;image matching;image registration;motion
estimation;optical radar;KITTI odometry benchmark;aggressive
motion;ambient lighting changes;ego-motion estimation;first principle
method;point cloud registration;scan matching based lidar
odometry;temporary lack of visual features;visual-lidar
mapping;visual-lidar odometry;Cameras;Distortion;Feature
extraction;Laser radar;Motion estimation;Three-dimensional
displays;Visualization},
doi={10.1109/ICRA.2015.7139486},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139679,
author={D. Maturana and S. Scherer},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={3D Convolutional Neural Networks for landing zone detection from
LiDAR},
year={2015},
pages={3471-3478},
abstract={We present a system for the detection of small and potentially
obscured obstacles in vegetated terrain. The key novelty of this system
is the coupling of a volumetric occupancy map with a 3D Convolutional
Neural Network (CNN), which to the best of our knowledge has not been
previously done. This architecture allows us to train an extremely
efficient and highly accurate system for detection tasks from raw
occupancy data. We apply this method to the problem of detecting safe
landing zones for autonomous helicopters from LiDAR point clouds.
Current methods for this problem rely on heuristic rules and use simple
geometric features. These heuristics break down in the presence of low
vegetation, as they do not distinguish between vegetation that may be
landed on and solid objects that should be avoided. We evaluate the
system with a combination of real and synthetic range data. We show our
system outperforms various benchmarks, including a system integrating
various hand-crafted point cloud features from the literature.},
keywords={aerospace computing;air safety;autonomous aerial
vehicles;helicopters;neural nets;object detection;optical radar;3D
convolutional neural networks;CNN;LiDAR point clouds;autonomous
helicopters;geometric features;hand-crafted point cloud
features;heuristic rules;landing zone detection;potentially obscured
obstacle detection;raw occupancy data;real range data;safe landing zone
detection;synthetic range data;vegetated terrain;volumetric occupancy
map coupling;Blades;Laser radar;Neural networks;Safety;Solid
modeling;Three-dimensional displays;Vegetation mapping},
doi={10.1109/ICRA.2015.7139679},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7140013,
author={E. Olson},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={M3RSM: Many-to-many multi-resolution scan matching},
year={2015},
pages={5815-5821},
abstract={We describe a new multi-resolution scan matching method that
makes exhaustive (and thus local-minimum-proof) matching practical, even
for large positional uncertainties. Unlike earlier multi-resolution
methods, in which putative matches at low-resolutions can lead the
matcher to an incorrect solution, our method generates exactly the same
answer as a brute-force full-resolution method. We provide a proof of
this. Novelly, our method allows decimation of both the look-up table
and in the point cloud, yielding a 10x speedup versus contemporary
correlative methods. When a robot closes a large-scale loop, it must
often consider many loop-closure candidates. In this paper, we describe
an approach for posing a scan matching query over these candidates
jointly, finding the best match(es) between a particular pose and a set
of candidate poses (“one-to-many”), or the best match between two sets
of poses (“many-to-many”). This mode of operation finds the first loop
closure as much as 45x faster than traditional “one-to-one” scan
matching.},
keywords={SLAM (robots);image matching;image resolution;image
retrieval;robot vision;M3RSM;SLAM systems;brute-force full-resolution
method;contemporary correlative methods;large-scale
loop;local-minimum-proof matching;look-up table;loop-closure
candidates;many-to-many multiresolution scan matching method;one-to-one
scan matching;point cloud;positional uncertainty;robot;scan matching
query;simultaneous localization and mapping systems;Cost function;Laser
radar;Lasers;Simultaneous localization and mapping;Three-dimensional
displays;Uncertainty},
doi={10.1109/ICRA.2015.7140013},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7139407,
author={L. M. Paz and P. Piniés and P. Newman},
booktitle={2015 IEEE International Conference on Robotics and Automation
(ICRA)},
title={A variational approach to online road and path segmentation with
monocular vision},
year={2015},
pages={1633-1639},
abstract={In this paper we present an online approach to segmenting
roads on large scale trajectories using only a monocular camera mounted
on a car. We differ from popular 2D segmentation solutions which use
single colour images and machine learning algorithms that require
supervised training on huge image databases. Instead, we propose a novel
approach that fuses 3D geometric data with appearance-based segmentation
of 2D information in an automatic system. Our contribution is twofold:
first, we propagate labels from frame to frame using depth priors of the
segmented road avoiding user interaction most of the time; second, we
transfer the segmented road labels to 3D laser point clouds. This
reduces the complexity of state-of-the-art segmentation algorithms
running on 3D Lidar data. Segmentation fails is in only 3% of the cases
over a sequence of 13,600 monocular images spanning an urban trajectory
of more than 10km.},
keywords={computer vision;image colour analysis;image
segmentation;variational techniques;2D segmentation solutions;3D
geometric data;3D laser point clouds;3D lidar data;appearance-based
segmentation;machine learning algorithms;monocular camera;monocular
vision;single colour images;urban trajectory;variational
approach;Cameras;Image
segmentation;Lasers;Optimization;Roads;Three-dimensional
displays;Vehicles},
doi={10.1109/ICRA.2015.7139407},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7125588,
author={C. M. Costa and H. M. Sobreira and A. J. Sousa and G. M. Veiga},
booktitle={2015 IEEE International Conference on Industrial Technology
(ICIT)},
title={Robust and accurate localization system for mobile manipulators
in cluttered environments},
year={2015},
pages={3308-3313},
abstract={Autonomous robots play a pivotal role in improving
productivity and reducing operational costs. They excel at both
precision and speed in repetitive jobs and can cooperate with humans in
complex tasks within dynamic environments. Self-localization is critical
to any robot that must navigate or manipulate the environment. To solve
this problem, a modular localization system suitable for mobile
manipulators was developed. By using LIDAR data the proposed system is
capable of achieving less than a centimeter in translation error and
less than a degree in rotation error while requiring only 5 to 25
milliseconds of processing time. The system was tested in two different
robot platforms at different velocities and in several cluttered and
dynamic environments. It demonstrated high accuracy while performing
pose tracking and high reliability when estimating the initial pose
using feature matching. No artificial landmarks are required and it is
able to adjust its operation rate in order to use very few hardware
resources when a mobile robot is not moving.},
keywords={feature extraction;image matching;manipulators;mobile
robots;optical radar;pose estimation;robot vision;LIDAR data;accurate
localization system;autonomous robots;cluttered environments;dynamic
environments;feature matching;mobile manipulators;mobile robot;modular
localization system;operation rate;operational costs;pose
tracking;productivity improvement;robot platforms;robust localization
system;Estimation;Lasers;Measurement;Robot sensing
systems;Three-dimensional displays;Iterative Closest Point;Normal
Distributions Transform;Point Cloud Library;Robot Operating
System;Self-localization;feature matching;initial pose estimation;point
cloud registration;pose tracking},
doi={10.1109/ICIT.2015.7125588},
month={March},}
@ARTICLE{7124626,
author={K. Bekris and R. Shome and A. Krontiris and A. Dobson},
journal={IEEE Robotics Automation Magazine},
title={Cloud Automation: Precomputing Roadmaps for Flexible Manipulation},
year={2015},
volume={22},
number={2},
pages={41-50},
abstract={The goal of this article is to highlight the benefits of cloud
automation for industrial adopters and some of the research challenges
that must be addressed in this process. The focus is on the use of cloud
computing for efficiently planning the motion of new robot manipulators
designed for flexible manufacturing floors. In particular, different
ways that a robot can interact with a computing cloud are considered,
where an architecture that splits computation between the remote cloud
and the robot appears advantageous. Given this synergistic robot-cloud
architecture, this article describes how solutions from the recent
literature can be employed on the cloud during a periodically updated
preprocessing phase to efficiently answer manipulation queries on the
robot given changes in the workspace. In this setup, interesting
tradeoffs arise between path quality and computational efficiency, which
are evaluated through simulation. These tradeoffs motivate further
research on how motion planning should be executed given access to a
computing cloud.},
keywords={cloud computing;flexible manipulators;flexible manufacturing
systems;industrial manipulators;path planning;cloud automation;cloud
computing;computational efficiency;flexible manipulation;flexible
manufacturing floor;industrial adopters;path quality;remote cloud;robot
manipulator motion planning;synergistic robot-cloud
architecture;Automation;Cloud computing;Flexible electronics;Robot
kinematics;Robot sensing systems},
doi={10.1109/MRA.2015.2401291},
ISSN={1070-9932},
month={June},}
@INPROCEEDINGS{7090346,
author={J. Liu and H. Liang and Z. Wang},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={A framework for detecting road curb on-line under various road
conditions},
year={2014},
pages={297-302},
abstract={Quick and accurate understanding of the ambient environment is
critical in the development of the intelligent vehicle. As the most
important element consisting the ambient environment, road curb
detecting is a fundamental and vital work for the development of
intelligent vehicle. In this paper, a framework was presented to detect
the road quickly and robustly using the velodyne lidar, which provides a
massive and unstructured information of the environment in the format of
point cloud. Three-sigma rule in the field of error processing and
reliability theory was adopted to detect road curb which adapts various
road condition. After then, the point cloud was grided and outlier
points were removed. Lastly, by utilizing the road continuous
information, a search method consists of eight search templates was
utilized to search the road curb automatically. The framework we
presented has been proved to be able to detect straight or curve road
curbs robustly and quickly in most road conditions automatically.
Compared to the former works, this paper detects the the road curb
depending on a adaptive threshold, which means it can be adaptive to
different road conditions. Besides, the search model is adaptive to the
changing of the road curb direction, which improve the search speed and
increase the road curb detection average accuracy.},
keywords={intelligent transportation systems;object detection;optical
radar;adaptive threshold;ambient environment;error
processing;intelligent vehicle;point cloud;reliability theory;road
conditions;road curb detection;search method;search
templates;three-sigma rule;velodyne lidar;Feature extraction;Intelligent
vehicles;Laser radar;Reliability theory;Roads;Three-dimensional
displays;Vegetation},
doi={10.1109/ROBIO.2014.7090346},
month={Dec},}
@INPROCEEDINGS{7090627,
author={S. i. Akamatsu and N. Shimaji and T. Tomizawa},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Development of a person counting system using a 3D laser scanner},
year={2014},
pages={1983-1988},
abstract={The purpose of the present study is to develop a person
counting system using a 3D laser scanner. The system consists of a
person-detection component, a tracking component, and a counting
component. The person-detection component detects human bodies as a
point cloud, even if a number of people are close to each other. To
achieve this, we use a method of grouping the point cloud towards the
bottom from the top. The tracking component tracks the detected person
using a Kalman filter. The counting component counts the number of
people who enter and exit a structure. Experimental results obtained in
the present study reveal that the proposed system can recognize
individual pedestrians and count the number of people passing through
crowded outdoor environments.},
keywords={Kalman filters;object detection;object tracking;optical
scanners;3D laser scanner;Kalman filter;counting component;crowded
outdoor environment;person counting system;person-detection
component;point cloud;tracking component;Kalman filters;Laser
beams;Lasers;Measurement by laser beam;Sensors;Three-dimensional
displays;Tracking},
doi={10.1109/ROBIO.2014.7090627},
month={Dec},}
@INPROCEEDINGS{7090530,
author={L. Ma and E. C. H. Cheung and W. Newman},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Using planar features for fast localization in indoor environments},
year={2014},
pages={1404-1409},
abstract={A means for fast and accurate indoor localization using
point-cloud data is presented. A sparse environment model is proposed,
comprised of a list of planar patches. It is shown that, with this
simplified model, incoming point-cloud data can be associated rapidly
with one (or none) of the constituent planes, and the environment model
can be fit to the clustered sample points algebraically in a
least-squares sense in real time.},
keywords={computer graphics;control engineering computing;indoor
navigation;least squares approximations;mobile robots;pattern
clustering;clustered sample points;constituent planes;fast
localization;indoor environments;least-squares sense;planar
features;planar patches;point-cloud data;sparse environment
model;Computational modeling;Mathematical model;Robot sensing
systems;Solid modeling;Three-dimensional displays;Transforms},
doi={10.1109/ROBIO.2014.7090530},
month={Dec},}
@INPROCEEDINGS{7090443,
author={E. C. H. Cheung and C. Chao and W. S. Newman},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Initial pose estimation using cross-section contours},
year={2014},
pages={878-883},
abstract={This paper presents a means to approximate an object's pose,
suitable for initialization of the Iterative Closest Point algorithm.
The class of problems considered is objects lying stably on a planar
surface, for which a relatively small number of pose types are possible.
Within each pose type, the registration problem is reduced to 3
dimensions. Using contours computed from horizontal slices, it is shown
that relatively noisy point-cloud samples can yield good estimates of
pose. Experimental results using an Atlas robot are presented. The
proposed method offers an efficient means to initialize point-cloud
fitting, resulting in faster, more reliable convergence.},
keywords={approximation theory;iterative methods;manipulators;pose
estimation;robot vision;Atlas robot;cross-section contours;horizontal
slice;iterative closest point algorithm;object pose approximation;planar
surface;point-cloud fitting;pose estimation;Convergence;Data
models;Educational robots;Estimation;Iterative closest point
algorithm;Robot sensing systems},
doi={10.1109/ROBIO.2014.7090443},
month={Dec},}
@INPROCEEDINGS{7090660,
author={X. Chen and J. Li and X. Yuan and C. Zhao},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Terrain classification based on variable-scale three-dimensional
grid map},
year={2014},
pages={2179-2184},
abstract={According to the distribution characteristics of lidar
collection points, dense in the vicinity and sparse in the distance, a
terrain classification method based on variable-scale three-dimensional
grid map is proposed to classify an unknown terrain into four
categories, which includes roads, lawns, buildings and trees. First, we
establish a variable-scale three-dimensional grid map. Then the
algorithm uses the point cloud feature extraction methods to extract the
features of voxels. A robust outlier detection algorithm is proposed to
estimate reliable local saliency features. Finally, we employ the
classifiers based on TWSVM to classify the voxels into four categories.
Experimental results show that our algorithm can reduce the number of
voxels while ensuring accuracy, reduce the noise and have good
classification results on real data sets.},
keywords={feature extraction;image classification;image denoising;mobile
robots;optical radar;radar imaging;robot vision;terrain mapping;LIDAR
collection points;TWSVM;autonomous robot navigation;distribution
characteristics;local saliency feature estimation;noise
reduction;outlier detection algorithm;point cloud feature extraction
methods;unknown terrain classification;variable-scale three-dimensional
grid map;voxel classification;voxel feature extraction;Buildings;Feature
extraction;Image color analysis;Laser radar;Principal component
analysis;Roads;Three-dimensional displays;Terrain classification;Twin
Support Vector Machine;robust feature extraction;variable-scale grid map},
doi={10.1109/ROBIO.2014.7090660},
month={Dec},}
@INPROCEEDINGS{7090590,
author={K. Tanaka and H. Ishii and S. Kinoshita and Q. Shi and H. Sugita
and S. Okabayashi and Y. Sugahara and A. Takanishi},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Design of operating software and electrical system of mobile
robot for environmental monitoring},
year={2014},
pages={1763-1768},
abstract={Environmental monitoring robot is a new method for approaching
environmental monitoring. The main issue with the development of this is
to develop accessible its system. Several studies have been conducted on
the autonomous monitoring system and they performed well, however,
almost they are costly and complex therefore be not considered well to
be accessible design. The purpose of our work was to develop an
accessible system and implement to root for other field's researchers.
The objectives of the work were to design operating software and
electrical system that could be easily used to operate the robot and
monitor the environment as a monitoring system. The middle sections of
the article introduce and describe specific information about these
objectives. We also explain the results of its experimental
implementations to an autonomous environmental monitoring robot named
Waseda Animal Monitoring robot (WAMOT). An operating software and
electrical system were successfully designed by using the existing
infrastructure and product, especially, smartphone and public cloud
server were used for making the robot easy to use. The developed method
is significant, not only because it fills a gap in the mobile robot, but
also because it makes possible to adapt to the conventional monitoring
method such as monitoring post. Therefore, achieves the core concept of
accessibility.},
keywords={cloud computing;environmental monitoring
(geophysics);environmental science computing;mobile robots;robot
programming;smart phones;WAMOT;Waseda animal monitoring robot;autonomous
environmental monitoring robot;autonomous monitoring system;electrical
system;mobile robot;operating software design;public cloud
server;smartphone;Environmental monitoring;Legged locomotion;Robot
sensing systems;Servers;Software},
doi={10.1109/ROBIO.2014.7090590},
month={Dec},}
@INPROCEEDINGS{7090401,
author={Y. Fu and G. Jiang and W. Feng and Y. Zhou and Y. Ou},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={On real-time obstacle avoidance using 3-D point clouds},
year={2014},
pages={631-636},
abstract={Obstacle avoidance is a key problem required in the mobile
robot navigation technology. In this paper, we build up a 3D environment
in an intelligent surveillance robot through registration of point
clouds and present a method to deal with the obstacle avoidance problem.
Firstly, the framework of the intelligent robot is given. Then we
register the point clouds from an RGB-D Kinect by applying an ICP
(Iterative Closest Point) algorithm. The online processing is done using
the ROS system. Next, we propose an approach to avoid obstacles by
projecting 3D data to a plane thus 2D technologies which need less
computation can be used. Finally, some experimental results are
presented to be valid.},
keywords={collision avoidance;computer graphics;control engineering
computing;image colour analysis;image registration;image
sensors;intelligent robots;iterative methods;mobile robots;operating
systems (computers);robot vision;surveillance;2D technologies;3D data
projection;3D environment;3D point clouds;ICP;RGB-D Kinect;ROS
system;intelligent surveillance robot;iterative closest point
algorithm;mobile robot navigation technology;online processing;point
clouds registration;real-time obstacle avoidance;robot operating
system;Collision avoidance;Iterative closest point algorithm;Robot
kinematics;Robot sensing systems;Surveillance;Three-dimensional displays},
doi={10.1109/ROBIO.2014.7090401},
month={Dec},}
@INPROCEEDINGS{7090597,
author={M. Liu},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Efficient segmentation and plane modeling of point-cloud for
structured environment by normal clustering and tensor voting},
year={2014},
pages={1805-1810},
abstract={In this paper, we introduce an efficient point-cloud
segmentation algorithm, inspired by efficient segmentation (also named
as super-pixel extraction). It uses parameterised “normal words” as
distance measures, which are obtained by clustering of surface normals.
We estimate the surface normals by the sparse tensor voting framework,
which enables adaptive structural extraction, even for the case of
missing points. The output result is consist of labeled point
representations regarding plane assumptions, which is validated by
metrics based on information theory. We show the quality of the
segmentation results by experiments on real datasets, and demonstrate
its potentials in aiding 2.5D topological navigation for structured
environments.},
keywords={pattern clustering;tensors;adaptive structural
extraction;distance measures;efficient point-cloud segmentation
algorithm;information theory;labeled point representations;normal
clustering;parameterised normal words;point-cloud plane modeling;sparse
tensor voting framework;structured environment;surface normal
clustering;tensor voting;Clustering algorithms;Entropy;Estimation;Image
segmentation;Information theory;Robots;Tensile stress},
doi={10.1109/ROBIO.2014.7090597},
month={Dec},}
@INPROCEEDINGS{7090729,
author={L. Wang and Z. Zhou and J. Wu and Y. Liu and X. Zhao},
booktitle={2014 IEEE International Conference on Robotics and
Biomimetics (ROBIO 2014)},
title={Support-plane estimation for floor detection to understand
regions' spatial organization},
year={2014},
pages={2576-2581},
abstract={Plane fitting plays an important role in image processing and
computer vision. It is challenging because of the outliers that do not
follow the plane pattern. In this work, we address the problem of
support-plane fitting for room floor detection from point clouds that
are generated from depth image. Based on the geometric layout of data,
an optimization problem is derived to estimate the support-plane.
Algorithms are also proposed to deal with data noise. The floor
detection is achieved by support-plane fitting, and is employed as a
reference to analyze the spatial organization of room scene. A
projection method is presented to form the organization map. Experiments
demonstrate the proposed method is more robust, and it achieves
remarkable performance in understanding the spatial organization.},
keywords={computer vision;object detection;optimisation;computer
vision;data noise;image processing;optimization problem;organization
map;plane pattern;region spatial organization;room floor detection;room
scene;support-plane estimation;support-plane
fitting;Estimation;Floors;Image
segmentation;Noise;Organizations;Robustness;Three-dimensional displays},
doi={10.1109/ROBIO.2014.7090729},
month={Dec},}
@ARTICLE{7058453,
author={M. Stenmark and J. Malec and K. Nilsson and A. Robertsson},
journal={IEEE Transactions on Automation Science and Engineering},
title={On Distributed Knowledge Bases for Robotized Small-Batch Assembly},
year={2015},
volume={12},
number={2},
pages={519-528},
abstract={The flexibility demands in manufacturing are severe, e.g., for
rapid-change-over to new product variants, while robots are flexible
machines that potentially can be adapted to a large variety of
production tasks. Task definitions such as explicit robot programs are
hardly reusable from an application point-of-view. To improve the
situation, a knowledge-based approach exploiting distributed declarative
information and cloud computing offers many possibilities for knowledge
exchange and reuse, and it has the potential to facilitate new business
models for industrial solutions. However, there are many unresolved
questions yet, e.g., those related to reliability, consistency, or legal
responsibility. To investigate some of these issues, different
knowledge-based architectures have been prototyped and evaluated by
confronting the solution candidates with key application demands. The
conclusion is that distributed cloud-based approaches offer many
possibilities, but there is still a need for further research and better
infrastructure before this approach can become industrially attractive.},
keywords={batch processing (industrial);cloud computing;knowledge based
systems;production control;production engineering computing;robotic
assembly;cloud computing;distributed declarative information;distributed
knowledge bases;industrial robot;knowledge-based architectures;robotized
small-batch assembly;Computer architecture;Knowledge based
systems;Ontologies;Robot kinematics;Robot sensing
systems;Software;Cognitive robotics;online knowledge bases;reuse of
knowledge;robot programming;web services for robots},
doi={10.1109/TASE.2015.2408264},
ISSN={1545-5955},
month={April},}
@ARTICLE{7015601,
author={L. Riazuelo and M. Tenorth and D. Di Marco and M. Salas and D.
Gálvez-López and L. Mösenlechner and L. Kunze and M. Beetz and J. D.
Tardós and L. Montano and J. M. M. Montiel},
journal={IEEE Transactions on Automation Science and Engineering},
title={RoboEarth Semantic Mapping: A Cloud Enabled Knowledge-Based
Approach},
year={2015},
volume={12},
number={2},
pages={432-443},
abstract={The vision of the RoboEarth project is to design a
knowledge-based system to provide web and cloud services that can
transform a simple robot into an intelligent one. In this work, we
describe the RoboEarth semantic mapping system. The semantic map is
composed of: 1) an ontology to code the concepts and relations in maps
and objects and 2) a SLAM map providing the scene geometry and the
object locations with respect to the robot. We propose to ground the
terminological knowledge in the robot perceptions by means of the SLAM
map of objects. RoboEarth boosts mapping by providing: 1) a subdatabase
of object models relevant for the task at hand, obtained by semantic
reasoning, which improves recognition by reducing computation and the
false positive rate; 2) the sharing of semantic maps between robots; and
3) software as a service to externalize in the cloud the more intensive
mapping computations, while meeting the mandatory hard real time
constraints of the robot. To demonstrate the RoboEarth cloud mapping
system, we investigate two action recipes that embody semantic map
building in a simple mobile robot. The first recipe enables semantic map
building for a novel environment while exploiting available prior
information about the environment. The second recipe searches for a
novel object, with the efficiency boosted thanks to the reasoning on a
semantically annotated map. Our experimental results demonstrate that,
by using RoboEarth cloud services, a simple robot can reliably and
efficiently build the semantic maps needed to perform its quotidian
tasks. In addition, we show the synergetic relation of the SLAM map of
objects that grounds the terminological knowledge coded in the ontology.},
keywords={SLAM (robots);Web services;cloud computing;control engineering
computing;intelligent robots;mobile robots;object recognition;ontologies
(artificial intelligence);RoboEarth semantic mapping system;SLAM map;Web
services;cloud enabled knowledge-based approach;cloud
services;intelligent robot;mobile robot;object locations;object model
subdatabase;ontology;scene geometry;semantic reasoning;software as a
service;Knowledge based systems;Navigation;Search
problems;Semantics;Simultaneous localization and
mapping;Visualization;Cloud mapping;knowledge representation;object
recognition;semantic mapping;visual SLAM},
doi={10.1109/TASE.2014.2377791},
ISSN={1545-5955},
month={April},}
@INPROCEEDINGS{7067263,
author={A. Dobrucki and M. Walczyński and W. Bożejko},
booktitle={2014 Signal Processing: Algorithms, Architectures,
Arrangements, and Applications (SPA)},
title={Parallel LMS-based adaptive algorithms of echo cancellation},
year={2014},
pages={13-18},
abstract={This paper deals with a number of parallel LMS family
algorithms in the context of their use in the acoustic echo
cancellation. The most complex parts of the LMS-based algorithms were
determined and parallelized. A number of parallelization methods were
designed taking under consideration varied types of architectures of
modern concurrent computing environment, such as GPUs, clusters of
workstations and cloud computing.},
keywords={echo suppression;graphics processing units;least mean squares
methods;parallel algorithms;GPU;acoustic echo cancellation;adaptive
algorithms;cloud computing;concurrent computing environment;parallel LMS
algorithms;parallelization method;workstation clusters;Computational
modeling;Microphones;Phase change random access
memory;Runtime;Speech;GPU;adaptive algorithms;echo cancellation;parallel
algorithms},
ISSN={2326-0262},
month={Sept},}
@INPROCEEDINGS{7064564,
author={K. Sullivan and W. Lawson and D. Sofge},
booktitle={2014 13th International Conference on Control Automation
Robotics Vision (ICARCV)},
title={Fusing laser reflectance and image data for terrain
classification for small autonomous robots},
year={2014},
pages={1656-1661},
abstract={Knowing the terrain is vital for small autonomous robots
traversing unstructured outdoor environments. We present a technique
using 3D laser point clouds combined with RGB camera images to classify
terrain into four pre-defined classes: grass, sand, concrete, and metal.
Our technique first segments the point cloud into distinct regions and
then applies a simple classifier to determine the classification of each
region. We demonstrate three classification and four segmentation
algorithms on five outdoor environments. Classification and segmentation
algorithms which use more information outperform information poor
combinations.},
keywords={image classification;image colour analysis;image fusion;image
segmentation;image sensors;mobile robots;path planning;3D laser point
clouds;RGB camera images;concrete class;grass class;image data
fusion;laser reflectance fusion;metal class;sand class;segmentation
algorithm;small autonomous robots;terrain classification;unstructured
outdoor environments;Accuracy;Classification algorithms;Image color
analysis;Image segmentation;Lasers;Robots;Three-dimensional displays},
doi={10.1109/ICARCV.2014.7064564},
month={Dec},}
@INPROCEEDINGS{7064381,
author={A. Sinha and P. Papadakis and M. R. Elara},
booktitle={2014 13th International Conference on Control Automation
Robotics Vision (ICARCV)},
title={A staircase detection method for 3D point clouds},
year={2014},
pages={652-656},
abstract={Staircase detection in an important ability required by indoor
robots, allowing for multi-floor exploration in 3D environments. We
present an algorithm for stair-case detection from point-cloud data
based on a new minimal 3D map representation and the estimation of
step-like features that are grouped based on adjacency in order to
emerge dominant staircase structures. Experiments performed using noisy
RGB-D sensor data from robot exploration trials showed a reliable
detection performance under varying conditions.},
keywords={computer graphics;estimation theory;image colour
analysis;object detection;3D environment;3D map representation;3D point
clouds;detection performance;indoor robot;multifloor exploration;noisy
RGB-D sensor data;point-cloud data;robot exploration trial;staircase
detection method;staircase structure;step-like feature
estimation;varying condition;Conferences;Feature extraction;Image color
analysis;Robot kinematics;Robot sensing systems;Three-dimensional
displays},
doi={10.1109/ICARCV.2014.7064381},
month={Dec},}
@INPROCEEDINGS{7057438,
author={H. C. Roh and Y. Choe and J. Jung and H. Na and Y. Cho and M. J.
Chung},
booktitle={2014 11th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI)},
title={Position estimation of landmark using 3D point cloud and
trilateration method},
year={2014},
pages={298-302},
abstract={We demonstrate the appropriacy of using laser distance meter
as the accurate, cost-effective, simple solution for position estimation
within precise 3D point cloud map in urban environment scenario. Our
approach treats trilateration method with minimum error selection
algorithm for better position accuracy of landmark. We validate the
performance of our approach through 22 landmarks measured using RTK GPS
for error analysis of proposed position estimation method based on
trilateration.},
keywords={Global Positioning System;cartography;cloud computing;error
analysis;3D point cloud map;GPS;error analysis;landmark position
estimation;laser distance meter;position estimation method;trilateration
method;Estimation;Global Positioning System;Measurement by laser
beam;Robots;Sensors;Three-dimensional displays;Vehicles;3D point
cloud;Laser distance meter;Trilateration;Urban environment},
doi={10.1109/URAI.2014.7057438},
month={Nov},}
@INPROCEEDINGS{7053321,
author={G. Jiang and M. U. Syed and L. Liu and Y. Ou and Y. Xu and M. S.
Malik and A. A. Chandio},
booktitle={Proceeding of the 11th World Congress on Intelligent Control
and Automation},
title={A novel approach for localization of an indoor autonomous mobile
surveillance robot},
year={2014},
pages={3636-3641},
abstract={Localization of an autonomous mobile robot has always been a
topic of interest in Robotics community. The constraints of working
under different environments are constantly a challenge in designing an
efficient localization system. We propose a localization algorithm for a
mobile robot, which will be used for home security. The objective is to
perform indoor localization efficiently and keeping the cost as low as
possible so as to make it commercializable. Here, we introduce an
approach based on recognizing a pattern generated by laser pointers on
the wall, which is captured by an infrared camera. The image of this
pattern is used to locate the position of a robot in the room. The
problem of working under black ceiling has been overcomed because the
projection is on the wall and is moving which also increases the range
of operation. As the pattern is unique, the interference of strong
sunlight has also been fixed to some extent. Experimental results show
that the proposed approach is effective in real environment and opens
new doors for future research.},
keywords={SLAM (robots);cameras;ceilings;image recognition;infrared
imaging;mobile robots;robot vision;video surveillance;black ceiling;home
security;indoor autonomous mobile surveillance robot
localization;infrared camera;laser pointers;operation range;pattern
recognition;projection;real environment;robot position
localization;sunlight interference;working constraints;Cameras;Charging
stations;Mobile robots;Robot kinematics;Robot vision
systems;Localization;Mobile robot;Robot navigation;Surveillance robot},
doi={10.1109/WCICA.2014.7053321},
month={June},}
@INPROCEEDINGS{7053264,
author={Shuang Ma and C. Zhou and L. Zhang and W. Hong and Y. Tian},
booktitle={Proceeding of the 11th World Congress on Intelligent Control
and Automation},
title={Depth image denoising and key points extraction for manipulation
plane detection},
year={2014},
pages={3315-3320},
abstract={The handling of twist-locks has been a heavy burden for the
container industry. To address this challenge, we are developing a
customized mobile manipulator for handling the twist-locks. In this
paper, we propose a fast normal computation algorithm for depth image,
which is able to use normal deviation along eight directions to extract
key points for segmenting points into objects on manipulation support
plane in an unstructured table top scene. Before further processing
point clouds, a bilateral filter is used to denoise depth images. To
evaluate the effectiveness of the bilateral filter, eight direction
angles are also used to observe the effectiveness of filter. To further
evaluate the proposed approach, a median filter is also used for
comparison with the bilateral filter. Experimental results show that the
fast surface normal computation based on depth image and eight
directions to determine a point are feasible for plane detection.},
keywords={containers;image denoising;image segmentation;median
filters;bilateral filter;container industry;customized mobile
manipulator;depth image denoising;fast normal computation;fast surface
normal computation;key points extraction;manipulation plane
detection;manipulation support plane;median filter;normal
deviation;point clouds;segmenting points;twist-lock
handling;unstructured table top scene;Estimation;Filtering theory;Image
segmentation;Noise;Robots;Three-dimensional displays;Vectors},
doi={10.1109/WCICA.2014.7053264},
month={June},}
@INPROCEEDINGS{7049580,
author={D. Eriksson and E. Shellshear},
booktitle={2014 11th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={Approximate distance queries for path-planning in massive point
clouds},
year={2014},
volume={02},
pages={20-28},
abstract={In this paper, algorithms have been developed that are capable
of efficiently pre-processing massive point clouds for the rapid
computation of the shortest distance between a point cloud and other
objects (e.g. triangulated, point-based, etc.). This is achieved by
exploiting fast distance computations between specially structured
subsets of a simplified point cloud and the other object. This approach
works for massive point clouds even with a small amount of RAM and was
able to speed up the computations, on average, by almost two orders of
magnitude. Given only 8 GB of RAM, this resulted in shortest distance
computations of 30 frames per second for a point cloud originally having
1 billion points. The findings and implementations will have a direct
impact for the many companies that want to perform path-planning
applications through massive point clouds since the algorithms are able
to produce real-time distance computations on a standard PC.},
keywords={Computational modeling;Data structures;Design automation;Load
modeling;Random access memory;Solid modeling;Three-dimensional
displays;Distance Computation;Path-Planning;Point Clouds;Simplification},
doi={10.5220/0005002000200028},
month={Sept},}
@INPROCEEDINGS{7048870,
author={P. Galambos and P. Baranyi and I. J. Rudas},
booktitle={IECON 2014 - 40th Annual Conference of the IEEE Industrial
Electronics Society},
title={Merged physical and virtual reality in collaborative virtual
workspaces: The VirCA approach},
year={2014},
pages={2585-2590},
abstract={Recently emerging paradigms of the so called Future Internet
induce significant changes in consumer and industrial ICT applications.
Remote collaboration in mixed physical and virtual realities made
possible thanks to the increasing network bandwidth further empowered by
the achievements of Internet of Things, Cloud Computing and Internet of
Services. This enticing vision brings benefits for several application
fields ranging from STEM education to smart factories. The paper
discusses some new possibilities through the VirCA (Virtual
Collaboration Arena) framework as a pilot realization. A brief
introduction is given to VirCA focusing on the basic concepts and
features that make it well suited for collaborative work in mixed
virtual and physical reality. Through a concrete life-like example, the
paper illustrates the way of involving real industrial devices into
remote collaboration scenarios and reviews the typical uses of such a
shared infrastructure considering the relationship of the virtual and
real entities.},
keywords={Internet of Things;cloud computing;groupware;virtual
reality;Internet of Things;Internet of services;STEM education;VirCA
approach;cloud computing;collaborative virtual workspaces;consumer ICT
application;future Internet;industrial ICT application;mixed physical
reality;mixed virtual reality;real entities;remote collaboration
scenarios;shared infrastructure;smart factories;virtual collaboration
arena framework;virtual entities;Collaboration;Engines;Internet;Robot
sensing systems;Service robots;Future Internet;Networked Robotics;Remote
Collaboration;Remote Laboratories;Virtual Laboratories;Virtual Reality},
doi={10.1109/IECON.2014.7048870},
ISSN={1553-572X},
month={Oct},}
@INPROCEEDINGS{7049851,
author={K. Yoshida and K. Kawasue},
booktitle={2014 11th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={Flexible shape measurement system for chemical plant using
magnetic sensors},
year={2014},
volume={01},
pages={758-763},
abstract={We propose a flexible computer vision system using magnetic
sensors. The system enables a flexible free scanning of a CCD camera and
a laser slit using 3D magnetic sensors. Many numbers of views of each
model from different angles can be taken on measuring the configuration
between a CCD camera and a laser slit projector simultaneously. The
information of different views is combined to reconstruct the 3D object
on a computer display. In this paper, the application for pipe
measurement is introduced. Experimental results show the feasibility of
our system.},
keywords={Cameras;Charge coupled devices;Magnetic field
measurement;Measurement by laser beam;Receivers;Temperature
measurement;Three-dimensional displays;CCD Camera;Computer
Vision;Magnetic Sensor;Pipe Measurement;Point Cloud},
doi={10.5220/0005097707580763},
month={Sept},}
@INPROCEEDINGS{7049619,
author={F. Maturana and L. Mann and J. Asenjo and S. Chatrola and R.
Staron},
booktitle={2014 11th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={Information framework for energy systems using agent-based cloud
computing technology},
year={2014},
volume={02},
pages={338-345},
abstract={We present a novel idea to combine agent technology and cloud
computing for monitoring an energy system. We describe how a cloud
infrastructure can be useful in storing and processing near real-time
sensor data. This paper also presents the implementation of the cloud
computing capability as Software-as-a-Service (SaaS) and reporting
capabilities in cloud throughout a network of worker role and procedural
assemblies. This system generates alarms based on various conditions
emerging from the sensor data of an energy generation system.},
keywords={Assembly;Big data;Cloud computing;Computer architecture;Data
analysis;Measurement;Agent;Cloud Computing;Database;Energy
Systems;Manifest;Reporting;Trends},
month={Sept},}
@INPROCEEDINGS{7049641,
author={A. W. Alhashimi and R. Hostettler and T. Gustafsson},
booktitle={2014 11th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={An improvement in the observation model for Monte Carlo
Localization},
year={2014},
volume={02},
pages={498-505},
abstract={Accurate and robust mobile robot localization is very
important in many robot applications. Monte Carlo localization (MCL) is
one of the robust probabilistic solutions to robot localization
problems. The sensor model used in MCL directly influence the accuracy
and robustness of the pose estimation process. The classical beam models
assumes independent noise in each individual measurement beam at the
same scan. In practice, the noise in adjacent beams maybe largely
correlated. This will result in peaks in the likelihood measurement
function. These peaks leads to incorrect particles distribution in the
MCL. In this research, an adaptive sub-sampling of the measurements is
proposed to reduce the peaks in the likelihood function. The sampling is
based on the complete scan analysis. The specified measurement is
accepted or not based on the relative distance to other points in the 2D
point cloud. The proposed technique has been implemented in ROS and
stage simulator. The result shows that selecting suitable value of
distance between accepted scans can improve the localization error and
reduce the required computations effectively.},
keywords={Atmospheric measurements;Laser beams;Monte Carlo
methods;Noise;Particle measurements;Robot sensing
systems;Localization;Monte Carlo Localization;Observation Model;Particle
Filter;Robotics;Sensor Model},
doi={10.5220/0005065604980505},
month={Sept},}
@INPROCEEDINGS{7049590,
author={J. Ćesić and I. Marković and S. Jurić-Kavelj and I. Petrović},
booktitle={2014 11th International Conference on Informatics in Control,
Automation and Robotics (ICINCO)},
title={Detection and tracking of dynamic objects using 3D laser range
sensor on a mobile platform},
year={2014},
volume={02},
pages={110-119},
abstract={In this paper we present an algorithm for detection,
extraction and tracking of moving objects using a 3D laser range sensor.
First, ground extraction is performed using random sample consensus for
model parameter estimation. Afterwards, to downsample the point cloud, a
voxel grid filtering is executed and octree data structure is used. This
data structure enables an efficient detection of differences between two
consecutive point clouds, based on which clustering of dynamic parts of
the cloud is performed. The obtained clusters are then expanded over the
set of static voxels in order to cover entire objects. In order to
account for ego-motion an iterative closest point registration technique
with an initial transformation guess obtained by odometry of the
platform is used. As the final step, we present a tracking algorithm
based on joint probabilistic data association (JPDA) filter with
variable process and measurement noise taking into account velocity and
position of the tracked objects. However, JPDA filter assumes a constant
and known number of objects in the scene, and therefore we use track
management based on entropy. Experiments are performed using a setup
consisting of a Velodyne HDL-32E mounted on top of a mobile platform in
order to verify the developed algorithms.},
keywords={Data structures;Heuristic algorithms;Noise measurement;Object
detection;Pipelines;Robot sensing systems;Three-dimensional
displays;Detection of Moving Objects;JPDA Filter;Laser Range
Sensor;Tracking},
month={Sept},}
@ARTICLE{7042718,
journal={Computer},
title={News},
year={2015},
volume={48},
number={2},
pages={14-20},
abstract={Topics include hackers attacking the Internet naming
authority, a new Bluetooth version offering online connectivity and
showing promise for the Internet of Things, companies looking for ways
to enable mobile apps to communicate with one another, the US planning
the world's most powerful supercomputer, a set of prosthetic arms that
users can control with their thoughts, hackers hitting a German steel
factory and a South Korean nuclear power plant, NASA using apes as the
model for a disaster-response robot, Sony inventing a system that
converts regular glasses into smart spectacles, researchers finding a
critical flaw in an important cloud technology, a new approach letting
cellular towers function like inexpensive radar, and a program teaching
California prisoners to program.},
keywords={Bluetooth;Companies;Computer hacking;Electronic
mail;Internet;Robots;6LoWPAN;APL;App Index;Applied Physics
Laboratory;Bluetooth;Bluetooth 4.2;Bluetooth Smart
sensors;CZDS;California Institute of Technology;California Prison
Industry Authority;Caltech;Code.7370;DARPA Robotics
Challenge;DoE;Docker;Facebook;Famous Industries;Fraunhofer Institute for
Communication;Google;Google Glass;ICANN;IPv6 over Low power Wireless
Personal Area Networks;Information Processing;Internet Corporation for
Assigned Names and Numbers;Internet of Things;IoT;JPL;Jet Propulsion
Laboratory;Johns Hopkins University;Korea Hydro &amp; Nuclear
Power;Lawrence Livermore National Laboratory;Les Baugh;Lidar;Modular
Prosthetic Limbs;NASA;Oak Ridge National Laboratory;PCL;Red Hat Product
Security team;Revolutionizing Prosthetics program;RoboSimian;San Quentin
State Prison;Sierra;Single-Lens Display Module;Smart
EyeGlass;Sony;Summit;Taunis Tiigi;The Last Mile;Twitter;Twitter Cards;US
Department of Energy;University of California at Santa Barbara;and
Ergonomics;cellular towers;centralized zone data system;cloud
computing;cyberattack;deep linking;exascale computing;flaw;mobile
apps;passive coherent location;programming classes;radar;security;smart
glasses;spearfishing;supercomputer;universal Web addresses;wearable
technology;wireless},
doi={10.1109/MC.2015.53},
ISSN={0018-9162},
month={Feb},}
@INPROCEEDINGS{7013289,
author={K. Yoshida and K. Kawasue},
booktitle={2014 IEEE Symposium on Computational Intelligence for
Multimedia, Signal and Vision Processing (CIMSIVP)},
title={Self-localization method for three-dimensional handy scanner
using multi spot laser},
year={2014},
pages={1-5},
abstract={On the computer vision system, if the shape of the object
includes complex parts, unmeasurable area exists for occlusions of the
part on its surface in many cases. The area where camera can observe in
a frame is also limited and the limitation causes the unmeasurable area.
In order to reduce the unmeasurable area, scanning the measurement
device is required. Many numbers of views of each model from different
position (orientation) have to be taken to reconstruct the whole shape
of the model. The point cloud data (surface data) obtained by the
measurement device are connected to reconstruct the model. The
connection of the data is performed by considering the movement of the
measurement system (Self-localization) or using ICP (Iterative Closest
Point) algorithm. Accuracy of the connection influences the result of
the model reconstructions. Reliable and accurate self-localization of
measurement device is introduced in this paper.},
keywords={computer vision;computerised instrumentation;image
reconstruction;optical scanners;ICP algorithm;computer vision
system;iterative closest point algorithm;model reconstructions;multispot
laser;self-localization method;three-dimensional handy
scanner;Accuracy;Area measurement;Cameras;Iterative closest point
algorithm;Measurement by laser beam;Reliability;Three-dimensional
displays;KINECT;computer visiont;self-localization;three-dimensional
sensing},
doi={10.1109/CIMSIVP.2014.7013289},
month={Dec},}
@INPROCEEDINGS{7029986,
author={J. A. Bernabé and J. Felip and A. P. del Pobil and A. Morales},
booktitle={2013 13th IEEE-RAS International Conference on Humanoid
Robots (Humanoids)},
title={Contact localization through robot and object motion from point
clouds},
year={2013},
pages={268-273},
abstract={In this paper we address the problem of detecting contacts
between a robot hand and an object during the approach and execution
phases of manipulation tasks as a complement of touch perception. The
motion produced on objects by unnoticed contacts is exploited to trigger
the contact localization. Our method combines robot motion with tracking
of target objects by means of point cloud to probabilistically estimate
the location of contacts on a occupancy grid map. No previous knowledge
on the object shape is required. The proposed approach is implemented on
a real platform and experimentally validated in several cases.},
keywords={manipulators;contact localization;execution
phases;manipulation tasks;object motion;object shape;occupancy grid
map;point clouds;robot hand;robot motion;touch perception;unnoticed
contacts;Estimation;Manipulators;Tactile sensors;Three-dimensional
displays},
doi={10.1109/HUMANOIDS.2013.7029986},
ISSN={2164-0572},
month={Oct},}
@INPROCEEDINGS{7031314,
author={M. Sato and J. Kaneko and K. Kojima},
booktitle={2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE)},
title={Development of pavement surface inspection system for wheel chair
comfortability},
year={2014},
pages={219-220},
abstract={This paper proposes a pavement surface inspection system for
wheel chair comfortability. Our proposed wheel chair, which can
recognize its position and gather pavement surface point clouds,
displays barriers on the pavement to wheel chair users after analyzing
gathered data. We present our concept and discuss a visualization method
on this system.},
keywords={computerised instrumentation;ergonomics;handicapped
aids;inspection;roads;gathered data analysis;pavement surface inspection
system development;pavement surface point clouds;position
recognition;visualization method;wheel chair comfortability;Data
visualization;Educational institutions;Electronic mail;Global
Positioning System;Inspection;Real-time systems;Wheels},
doi={10.1109/GCCE.2014.7031314},
ISSN={2378-8143},
month={Oct},}
@INPROCEEDINGS{7024277,
author={J. C. P. Villota and A. H. R. Costa},
booktitle={2014 Joint Conference on Robotics: SBR-LARS Robotics
Symposium and Robocontrol},
title={Adaptive Selection of Color Images or Depth to Align RGB-D Point
Clouds},
year={2014},
pages={175-180},
abstract={Alignment of pair wise image point clouds is an important task
in building environment maps with partial information. The combination
of depth information and images provided by RGB-D cameras are often used
to improve such alignment. However, when the environment is structured
and its images show little texture, depth information is more reliable,
on the other hand, when the images of the environment have enough
texture, better results are achieved when texture information is used.
In this paper, we propose a new adaptive approach to make the most
effective selection of image or depth information in order to find a
better alignment of points and thus better define the rigid
transformation between two point clouds. Our approach uses an adaptive
parameter based on the degree of texture of the scene, selecting not
only FPFH and SURF descriptors, but also weighting the iterative ICP
process. Datasets containing RGB-D data with textured and non textured
images are used to validate our proposal.},
keywords={computational geometry;image colour analysis;image
texture;iterative methods;FPFH descriptor;RGB-D camera;RGB-D point
cloud;SURF descriptor;adaptive selection;color images;depth
information;iterative ICP process;pairwise image point cloud
alignment;Measurement;Simultaneous localization and
mapping;Three-dimensional displays;Visualization;Descriptors;Image
texture;Pairwise alignment;RGB-D sensors;SLAM},
doi={10.1109/SBR.LARS.Robocontrol.2014.40},
month={Oct},}
@INPROCEEDINGS{7026171,
author={M. Martinez and R. Stiefelhagen},
booktitle={2014 IEEE International Conference on Image Processing (ICIP)},
title={Kinect unbiased},
year={2014},
pages={5791-5795},
abstract={Since its release, Kinect has been the de facto standard for
low-cost RGB-D sensors. An infrared laser ray shot through an
holographic diffraction grating projects a fixed dot pattern which is
captured using an infrared camera. The pseudo-random pattern ensures
that a simple block matching algorithm suffices to provide reliable
depth estimates, allowing a cost-effective implementation. In this
paper, we analyze the software limitations of Kinect's method, which
allows us to propose algorithms that provide better precision. First, we
analyze the dot pattern: we measure its pincushion distortion and its
effect on the dot density, which is smaller towards the edges of the
image. Then, we analyze the behavior of Block Matching algorithms, we
show how Kinect's Block Matching implementation is; in general; limited
by the dot density of the pattern, and a significant spatial bias is
introduced as a result. We propose an efficient approach to estimate the
disparity of each dot, allowing us to produce a point cloud with better
spatial resolution than Block Matching algorithms.},
keywords={image colour analysis;image matching;image resolution;image
sensors;Kinect method;block matching algorithm;depth estimates;dot
density;fixed dot pattern;holographic diffraction grating;image
edges;infrared camera;infrared laser ray shot;low-cost RGB-D
sensors;pincushion distortion;point cloud;pseudorandom pattern;software
limitations;spatial bias;spatial resolution;Algorithm design and
analysis;Cameras;Image edge detection;Sensors;Spatial
resolution;Three-dimensional displays;Kinect;RGB-D;bias;pattern},
doi={10.1109/ICIP.2014.7026171},
ISSN={1522-4880},
month={Oct},}
@INPROCEEDINGS{7017673,
author={G. R. Jang and Y. D. Shin and J. H. Park and M. H. Baeg},
booktitle={2014 IEEE International Symposium on Safety, Security, and
Rescue Robotics (2014)},
title={Real-time point-cloud data transmission for teleoperation using
H.264/AVC},
year={2014},
pages={1-6},
abstract={This paper presents an algorithm that offers real-time
point-cloud transmission for teleoperation using H.264/AVC. Because the
3D-visualized point-cloud data provides metric information regarding the
remote workplace, the teleoperator can observe the remote workplace from
several viewpoints. The size of point-cloud data is a limitation when
transmitting in poor and unstable network environments in real-time.
Therefore, to perform robust teleoperation, it is necessary to compress
point-cloud data at the remote location and transmit it in real-time.
The feasibility of the proposed algorithm is verified by applying it in
indoor and outdoor mapping with a 3G network system. Real-time
teleoperation is effectively realized by applying the suggested
algorithm.},
keywords={3G mobile communication;cloud computing;data
communication;data compression;data visualisation;telerobotics;video
coding;3D-visualized point-cloud data;3G network system;H.264/AVC;indoor
mapping;outdoor mapping;real-time point-cloud data
transmission;real-time teleoperation;robust teleoperation;Channel
estimation;Employment;Encoding;Image coding;Real-time systems;Video
coding;Workstations;H.264/AVC;point cloud compression;teleoperation},
doi={10.1109/SSRR.2014.7017673},
ISSN={2374-3247},
month={Oct},}
@INPROCEEDINGS{7017650,
author={M. Coatsworth and J. Tran and A. Ferworn},
booktitle={2014 IEEE International Symposium on Safety, Security, and
Rescue Robotics (2014)},
title={A hybrid lossless and lossy compression scheme for streaming
RGB-D data in real time},
year={2014},
pages={1-6},
abstract={Mobile and aerial robots used in urban search and rescue
(USAR) operations have shown the potential for allowing us to explore,
survey and assess collapsed structures effectively at a safe distance.
RGB-D cameras, such as the Microsoft Kinect, allow us to capture 3D
depth data in addition to RGB images, providing a significantly richer
user experience than flat video, which may provide improved situational
awareness for first responders. However, the richer data comes at a
higher cost in terms of data throughput and computing power
requirements. In this paper we consider the problem of live streaming
RGB-D data over wired and wireless communication channels, using
low-power, embedded computing equipment. When assessing a disaster
environment, a range camera is typically mounted on a ground or aerial
robot along with the onboard computer system. Ground robots can use both
wireless radio and tethers for communications, whereas aerial robots can
only use wireless communication. We propose a hybrid lossless and lossy
streaming compression format designed specifically for RGB-D data and
investigate the feasibility and usefulness of live-streaming this data
in disaster situations.},
keywords={aerospace robotics;cameras;data compression;image colour
analysis;rescue robots;robot vision;video streaming;3D depth data
capture;Microsoft Kinect;RGB images;RGB-D cameras;RGB-D data
streaming;USAR operations;aerial robots;computing power
requirements;data throughput;disaster environment;live
streaming;lossless compression scheme;lossy compression scheme;low-power
embedded computing equipment;mobile robots;red-green-blue-depth
data;tethers;urban search and rescue;wireless
radio;Computers;Hardware;Image coding;Robots;Servers;Three-dimensional
displays;Wireless communication;3D;USAR;compression;point cloud;response
robot;streaming;video},
doi={10.1109/SSRR.2014.7017650},
ISSN={2374-3247},
month={Oct},}
@INPROCEEDINGS{7017644,
author={M. Pelka and K. Majek and J. Bedkowski and P. Musialik and A.
Maslowski and G. de Cubber and H. Balta and A. Coelho and R. Goncalves
and R. Baptista and J. M. Sanchez and S. Govindaraj},
booktitle={2014 IEEE International Symposium on Safety, Security, and
Rescue Robotics (2014)},
title={Training and Support system in the Cloud for improving the
situational awareness in Search and Rescue (SAR) operations},
year={2014},
pages={1-6},
abstract={In this paper, a Training and Support system for Search and
Rescue operations is described. The system is a component of the ICARUS
project (http://www.fp7-icarus.eu) which has a goal to develop sensor,
robotic and communication technologies for Human Search And Rescue
teams. The support system for planning and managing complex SAR
operations is implemented as a command and control component that
integrates different sources of spatial information, such as maps of the
affected area, satellite images and sensor data coming from the unmanned
robots, in order to provide a situation snapshot to the rescue team who
will make the necessary decisions. Support issues will include planning
of frequency resources needed for given areas, prediction of coverage
conditions, location of fixed communication relays, etc. The training
system is developed for the ICARUS operators controlling UGVs (Unmanned
Ground Vehicles), UAVs (Unmanned Aerial Vehicles) and USVs (Unmanned
Surface Vehicles) from a unified Remote Control Station (RC2). The
Training and Support system is implemented in SaaS model (Software as a
Service). Therefore, its functionality is available over the Ethernet.
SAR ICARUS teams from different countries can be trained simultaneously
on a shared virtual stage. In this paper we will show the multi-robot 3D
mapping component (aerial vehicle and ground vehicles). We will
demonstrate that these 3D maps can be used for Training purpose. Finally
we demonstrate current approach for ICARUS Urban SAR (USAR) and Marine
SAR (MSAR) operation training.},
keywords={cloud computing;command and control systems;control
engineering computing;emergency management;learning (artificial
intelligence);multi-robot systems;rescue robots;road vehicles;ICARUS
project;MSAR;RC2;SAR operations;SaaS
model;UAV;UGV;USAR;USV;cloud;command and control component;communication
technologies;marine SAR;multirobot 3D mapping component;remote control
station;robotic technologies;search and rescue operations;situational
awareness;software as a service;support system;training;unmanned aerial
vehicles;unmanned ground vehicles;unmanned surface vehicles;urban
SAR;Graphics processing units;Iterative closest point algorithm;Mobile
communication;Robots;Semantics;Three-dimensional displays;Training},
doi={10.1109/SSRR.2014.7017644},
ISSN={2374-3247},
month={Oct},}
@INPROCEEDINGS{7017652,
author={A. Bock and A. Kleiner and J. Lundberg and T. Ropinski},
booktitle={2014 IEEE International Symposium on Safety, Security, and
Rescue Robotics (2014)},
title={An interactive visualization system for urban search amp; rescue
mission planning},
year={2014},
pages={1-7},
abstract={We present a visualization system for incident commanders in
urban search & rescue scenarios that supports the inspection and access
path planning in post-disaster structures. Utilizing point cloud data
acquired from unmanned robots, the system allows for assessment of
automatically generated paths, whose computation is based on varying
risk factors, in an interactive 3D environment increasing immersion. The
incident commander interactively annotates and reevaluates the acquired
point cloud based on live feedback. We describe design considerations,
technical realization, and discuss the results of an expert evaluation
that we conducted to assess our system.},
keywords={data visualisation;disasters;emergency
management;inspection;path planning;rescue robots;risk management;access
path planning;incident commanders;inspection;interactive 3D
environment;interactive visualization system;live feedback;point cloud
data;post-disaster structures;risk factors;unmanned robots;urban search
& rescue mission planning;Buildings;Educational
institutions;Hazards;Integrated circuits;Rendering (computer
graphics);Robots;Three-dimensional displays},
doi={10.1109/SSRR.2014.7017652},
ISSN={2374-3247},
month={Oct},}
@INPROCEEDINGS{6982405,
author={I. C. Resceanu and C. F. Reşceanu and S. M. Simionescu},
booktitle={2014 18th International Conference on System Theory, Control
and Computing (ICSTCC)},
title={SaaS solutions for small-medium businesses: Developer's
perspective on creating new SaaS products},
year={2014},
pages={140-144},
abstract={Software as a Service is a method of licensing and delivering
software on-demand based on a centralized hosting solution which
absolves the client of the hardware costs that come with high-end
client-server server hosted solutions, the maintenance and hot-spare
availability of that hardware and the continuous renewal costs. While
small-medium businesses (SMBs) fit well in the client perspective of
SaaS, the provider perspective is not easily available due to high
hardware and development costs of SaaS. This paper provides an overview
of how SMBs can become SaaS providers easier, with the help of IaaS and
software templates.},
keywords={client-server systems;cloud computing;small-to-medium
enterprises;SaaS products;SaaS solutions;centralized
hosting;client-server server hosted solutions;continuous renewal
costs;small-medium business;software as a service;software
on-demand;software
templates;Companies;Hardware;Licenses;Servers;Software as a
service;IaaS;SaaS;client-server;small-medium business;web applications},
doi={10.1109/ICSTCC.2014.6982405},
month={Oct},}
@INPROCEEDINGS{6973581,
author={C. Cai and H. Jiang},
booktitle={2013 International Conference on Information Science and
Cloud Computing Companion},
title={Performance Comparisons of Evolutionary Algorithms for Walking
Gait Optimization},
year={2013},
pages={129-134},
abstract={To investigate the performance of different evolutionary
algorithms on walking gait optimization, we designed an optimization
framework. There are four bio-inspired methods in the framework, which
include Genetic Algorithm (GA), Covariance Matrix Adaption Evolution
Strategy (CMA-ES), Particle Swarm Optimization (PSO) and Differential
Evolution (DE). In the learning process of each method, we employed
three learning tasks to optimize the walking gait, which are aiming at
generating a gait with higher speed, stability and flexibility
respectively. We analyzed the gaits optimized by each four methods
separately. According to the comparison of these results, it indicates
that DE performs better than the other three algorithms. The comparison
also shows that the gaits learned by CMA-ES and PSO are acceptable, but
there exist drawbacks compared to DE. And among these methods, GA
presents weak performance on gait optimization.},
keywords={covariance matrices;gait analysis;genetic algorithms;humanoid
robots;legged locomotion;particle swarm
optimisation;stability;CMA-ES;DE;GA;PSO;bio-inspired methods;covariance
matrix adaption evolution strategy;differential evolution;evolutionary
algorithms;flexibility;genetic algorithm;learning process;particle swarm
optimization;performance comparisons;stability;walking gait
optimization;Foot;Genetic algorithms;Legged
locomotion;Optimization;Sociology;Trajectory;CMA-ES;DE;Gait
Optimization;Humanoid Robotics;PSO},
doi={10.1109/ISCC-C.2013.100},
month={Dec},}
@INPROCEEDINGS{6957753,
author={A. Hata and D. Wolf},
booktitle={17th International IEEE Conference on Intelligent
Transportation Systems (ITSC)},
title={Road marking detection using LIDAR reflective intensity data and
its application to vehicle localization},
year={2014},
pages={584-589},
abstract={A correct perception of road signalizations is required for
autonomous cars to follow the traffic codes. Road marking is a
signalization present on road surfaces and commonly used to inform the
correct lane cars must keep. Cameras have been widely used for road
marking detection, however they are sensible to environment
illumination. Some LIDAR sensors return infrared reflective intensity
information which is insensible to illumination condition. Existing road
marking detectors that analyzes reflective intensity data focus only on
lane markings and ignores other types of signalization. We propose a
road marking detector based on Otsu thresholding method that make
possible segment LIDAR point clouds into asphalt and road marking. The
results show the possibility of detecting any road marking (crosswalks,
continuous lines, dashed lines). The road marking detector has also been
integrated with Monte Carlo localization method so that its performance
could be validated. According to the results, adding road markings onto
curb maps lead to a lateral localization error of 0.3119 m.},
keywords={Global Positioning System;Monte Carlo methods;intelligent
transportation systems;object detection;optical radar;LIDAR reflective
intensity data;LIDAR sensors;Monte Carlo localization method;Otsu
thresholding method;asphalt marking;environment illumination;infrared
reflective intensity information;lane markings;lateral localization
error;road marking detection;road signalization;vehicle
localization;Asphalt;Detectors;Histograms;Laser
radar;Roads;Three-dimensional displays;Vehicles},
doi={10.1109/ITSC.2014.6957753},
ISSN={2153-0009},
month={Oct},}
@INPROCEEDINGS{6943141,
author={C. Premebida and J. Carreira and J. Batista and U. Nunes},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Pedestrian detection combining RGB and dense LIDAR data},
year={2014},
pages={4112-4117},
abstract={Why is pedestrian detection still very challenging in
realistic scenes? How much would a successful solution to monocular
depth inference aid pedestrian detection? In order to answer these
questions we trained a state-of-the-art deformable parts detector using
different configurations of optical images and their associated 3D point
clouds, in conjunction and independently, leveraging upon the recently
released KITTI dataset. We propose novel strategies for depth upsampling
and contextual fusion that together lead to detection performance which
exceeds that of the RGB-only systems. Our results suggest depth cues as
a very promising mid-level target for future pedestrian detection
approaches.},
keywords={image fusion;image sampling;object detection;optical
radar;pedestrians;3D point clouds;KITTI dataset;RGB-only
systems;contextual fusion;deformable part detector;dense LIDAR
data;depth upsampling;mid-level target detection;monocular depth
inference;optical images;pedestrian detection;Cameras;Deformable
models;Detectors;Feature extraction;Laser radar;Three-dimensional
displays},
doi={10.1109/IROS.2014.6943141},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6943077,
author={J. Aleotti and D. L. Rizzini and R. Monica and S. Caselli},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Global registration of mid-range 3D observations and short range
next best views},
year={2014},
pages={3668-3675},
abstract={This work proposes a method for autonomous robot exploration
of unknown objects by sensor fusion of 3D range data. The approach aims
at overcoming the physical limitation of the minimum sensing distance of
range sensors. Two range sensors are used with complementary
characteristics mounted in eye-in-hand configuration on a robot arm. The
first sensor operates at mid-range and is used in the initial phase of
exploration when the environment is unknown. The second sensor, which
provides short-range data, is used in the following phase where the
objects are explored at close distance through next best view planning.
Next best view planning is performed using a volumetric representation
of the environment. A complete point cloud model of each object is
finally computed by global registration of all object observations
including mid-range and short range views. In experiments performed in
environments with multiple rigid objects the global registration
algorithm has proven more accurate than a standard sequential
registration approach.},
keywords={image registration;mobile robots;path planning;robot
vision;sensor fusion;3D range data;autonomous robot
exploration;eye-in-hand configuration;global registration;midrange 3D
observation;next best view planning;point cloud model;range sensor;robot
arm;sensor fusion;volumetric representation;Algorithm design and
analysis;Manipulators;Planning;Robot sensing systems;Three-dimensional
displays},
doi={10.1109/IROS.2014.6943077},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6942685,
author={R. Roberts and F. Dellaert},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Direct superpixel labeling for mobile robot navigation using
learned general optical flow templates},
year={2014},
pages={1032-1037},
abstract={Towards the goal of autonomous obstacle avoidance for mobile
robots, we present a method for superpixel labeling using optical flow
templates. Optical flow provides a rich source of information that
complements image appearance and point clouds in determining
traversability. While much past work uses optical flow towards
traversability in a heuristic manner, the method we present here instead
classifies flow according to several optical flow templates that are
specific to the typical environment shape. Our first contribution over
prior work in superpixel labeling using optical flow templates is large
improvements in accuracy and efficiency by inference directly from
spatiotemporal gradients instead of from independently-computed optical
flow, and from improved optical flow modeling for obstacles. Our second
contribution over the same is extending superpixel labeling methods to
arbitrary camera optics without the need to calibrate the camera, by
developing and demonstrating a method for learning optical flow
templates from unlabeled video. Our experiments demonstrate successful
obstacle detection in an outdoor mobile robot dataset.},
keywords={collision avoidance;image sensors;image sequences;mobile
robots;robot vision;arbitrary camera optics;autonomous obstacle
avoidance;camera sensors;image appearance;mobile robot
navigation;optical flow template learning;point clouds;superpixel
labeling;traversability determination;Biomedical optical
imaging;Cameras;Labeling;Optical imaging;Optical sensors;Robot vision
systems},
doi={10.1109/IROS.2014.6942685},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6942632,
author={Z. Fang and S. Scherer},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Experimental study of odometry estimation methods using RGB-D
cameras},
year={2014},
pages={680-687},
abstract={Lightweight RGB-D cameras that can provide rich 2D visual and
3D point cloud information are well suited to the motion estimation of
indoor micro aerial vehicles (MAVs). In recent years, several RGB-D
visual odometry methods which process data from the sensor in different
ways have been proposed. However, it is unclear which methods are
preferable for online odometry estimation on a computation-limited, fast
moving MAV in practical indoor environments. This paper presents a
detailed analysis and comparison of several state-of-the-art real-time
odometry estimation methods in a variety of challenging scenarios, with
a special emphasis on the trade-off among accuracy, robustness and
computation speed. An experimental comparison is conducted using public
available benchmark datasets and author-collected datasets including
long corridors, illumination changing environments and fast motion
scenarios. Experimental results present both quantitative and
qualitative differences among these methods and provide some guidelines
on choosing the “right” algorithm for an indoor MAV according to the
quality of the RGB-D data and environment characteristics.},
keywords={autonomous aerial vehicles;cameras;distance measurement;image
colour analysis;indoor environment;lighting;motion estimation;robot
vision;2D visual cloud information;3D point cloud information;RGB-D
cameras;RGB-D visual odometry methods;fast motion scenario;illumination
changing environments;indoor MAV;indoor environments;indoor microaerial
vehicles;long corridor scenario;motion estimation;online odometry
estimation;real-time odometry estimation
method;Cameras;Estimation;Feature extraction;Iterative closest point
algorithm;Robustness;Three-dimensional displays;Visualization},
doi={10.1109/IROS.2014.6942632},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6942634,
author={M. Menna and M. Gianni and F. Ferri and F. Pirri},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Real-time autonomous 3D navigation for tracked vehicles in rescue
environments},
year={2014},
pages={696-702},
abstract={The paper presents a novel framework for 3D autonomous
navigation for tracked vehicles. The framework takes care of clustering
and segmentation of point clouds, traversability analysis, autonomous 3D
path planning, motion planning and flippers control. Results illustrated
in an experiment section show that the framework is promising to face
harsh terrains. Robot performance is proved in three main experiments
taken in a training rescue area, on fire escape stairs and in a
non-planar testing environment, built ad-hoc to prove 3D path planning
functionalities. Performance tests are also presented.},
keywords={navigation;path planning;rescue robots;tracked
vehicles;autonomous 3D path planning;flippers control;motion
planning;real-time autonomous 3D navigation;rescue environments;tracked
vehicles;traversability
analysis;Estimation;Labeling;Navigation;Noise;Robots;Sensors;Three-dimensional
displays},
doi={10.1109/IROS.2014.6942634},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6926306,
author={X. Zhao and A. M. Naguib and S. Lee},
booktitle={The 23rd IEEE International Symposium on Robot and Human
Interactive Communication},
title={Kinect based calling gesture recognition for taking order service
of elderly care robot},
year={2014},
pages={525-530},
abstract={This paper proposes a Kinect-based calling gesture recognition
scenario for taking order service of an elderly care robot. The proposed
scenarios are designed mainly for helping non expert users like elderly
to call service robot for their service request. In order to facilitate
elderly service, natural calling gestures are designed to interact with
the robot. Our challenge here is how to make the natural calling gesture
recognition work in a cluttered and randomly moving objects. In this
approach, there are two modes of our calling gesture recognition:
Skeleton based gesture recognition and Octree based gesture recognition.
Individual people is segmented out from 3D point cloud acquired by
Microsoft Kinect, skeleton is generated for each segment, face detection
is applied to identify whether the segment is human or not, specific
natural calling gestures are designed based on skeleton joints. For the
case that user is sitting on a chair or sofa, correct skeleton cannot be
generated, Octree based gesture recognition procedure is used to
recognize the gesture, in which human segments with head and hand are
identified by face detection as well as specific geometrical constrains
and skin color evidence. The proposed method has been implemented and
tested on “HomeMate”, a service robot developed for elderly care. The
performance and results are given.},
keywords={control engineering computing;face recognition;gesture
recognition;handicapped aids;human-robot interaction;image colour
analysis;image segmentation;medical robotics;mobile robots;robot
vision;service robots;3D point cloud;HomeMate;Microsoft Kinect based
calling gesture recognition;Octree based gesture recognition;elderly
care robot;face detection;geometrical constraints;human-robot
interaction;image segmentation;service robots;skeleton based gesture
recognition;skin color evidence;Face detection;Gesture
recognition;Octrees;Senior citizens;Sensors;Skeleton},
doi={10.1109/ROMAN.2014.6926306},
ISSN={1944-9445},
month={Aug},}
@INPROCEEDINGS{6916262,
author={M. P. Gerardo-Castro and T. Peynot and F. Ramos and R. Fitch},
booktitle={17th International Conference on Information Fusion (FUSION)},
title={Robust multiple-sensing-modality data fusion using Gaussian
Process Implicit Surfaces},
year={2014},
pages={1-8},
abstract={The ability to build high-fidelity 3D representations of the
environment from sensor data is critical for autonomous robots.
Multi-sensor data fusion allows for more complete and accurate
representations. Furthermore, using distinct sensing modalities (i.e.
sensors using a different physical process and/or operating at different
electromagnetic frequencies) usually leads to more reliable perception,
especially in challenging environments, as modalities may complement
each other. However, they may react differently to certain materials or
environmental conditions, leading to catastrophic fusion. In this paper,
we propose a new method to reliably fuse data from multiple sensing
modalities, including in situations where they detect different targets.
We first compute distinct continuous surface representations for each
sensing modality, with uncertainty, using Gaussian Process Implicit
Surfaces (GPIS). Second, we perform a local consistency test between
these representations, to separate consistent data (i.e. data
corresponding to the detection of the same target by the sensors) from
inconsistent data. The consistent data can then be fused together, using
another GPIS process, and the rest of the data can be combined as
appropriate. The approach is first validated using synthetic data. We
then demonstrate its benefit using a mobile robot, equipped with a laser
scanner and a radar, which operates in an outdoor environment in the
presence of large clouds of airborne dust and smoke.},
keywords={Gaussian processes;sensor fusion;GPIS process;Gaussian process
implicit surfaces;airborne dust;consistent data;distinct continuous
surface representations;inconsistent data;laser scanner;local
consistency test;mobile robot;multiple-sensing-modality data
fusion;radar;smoke;Data integration;Laser radar;Robot sensing
systems;Robustness;Surface treatment;Uncertainty},
month={July},}
@INPROCEEDINGS{6907211,
author={D. Kim and J. Lee and S. e. Yoon},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Cloud RRT^#x2217; : Sampling Cloud based RRT^#x2217; },
year={2014},
pages={2519-2526},
abstract={We present a novel biased sampling technique, Cloud RRT*, for
efficiently computing high-quality collision-free paths, while
maintaining the asymptotic convergence to the optimal solution. Our
method uses sampling cloud for allocating samples on promising regions.
Our sampling cloud consists of a set of spheres containing a portion of
the C-space. In particular, each sphere projects to a collision-free
spherical region in the workspace. We initialize our sampling cloud by
conducting a workspace analysis based on the generalized Voronoi graph.
We then update our sampling cloud to refine the current best solution,
while maintaining the global sampling distribution for exploring
understudied other homotopy classes. We have applied our method to a 2D
motion planning problem with kinematic constraints, i.e., the Dubins
vehicle model, and compared it against the state-of-the-art methods. We
achieve better performance, up to three times, over prior methods in a
robust manner.},
keywords={cloud computing;collision avoidance;computational
geometry;control engineering computing;robot dynamics;sampling
methods;vehicle dynamics;2D motion planning problem;Dubins vehicle
model;asymptotic convergence;cloud based RRT;collision-free spherical
region;generalized Voronoi graph;global sampling
distribution;high-quality collision-free paths;kinematic
constraints;sampling cloud;workspace analysis;Collision
avoidance;Convergence;Mobile robots;Planning;Trajectory;Vehicles},
doi={10.1109/ICRA.2014.6907211},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907298,
author={K. Lai and L. Bo and D. Fox},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Unsupervised feature learning for 3D scene labeling},
year={2014},
pages={3050-3057},
abstract={This paper presents an approach for labeling objects in 3D
scenes. We introduce HMP3D, a hierarchical sparse coding technique for
learning features from 3D point cloud data. HMP3D classifiers are
trained using a synthetic dataset of virtual scenes generated using CAD
models from an online database. Our scene labeling system combines
features learned from raw RGB-D images and 3D point clouds directly,
without any hand-designed features, to assign an object label to every
3D point in the scene. Experiments on the RGB-D Scenes Dataset v.2
demonstrate that the proposed approach can be used to label indoor
scenes containing both small tabletop objects and large furniture pieces.},
keywords={CAD;image colour analysis;solid modelling;unsupervised
learning;virtual reality;3D point cloud data;3D scene labeling;CAD
model;HMP3D classifiers;RGB-D images;RGB-D scenes dataset v.2;furniture
pieces;hand-designed feature;hierarchical sparse coding technique;indoor
scenes;learning features;object label;online database;scene labeling
system;synthetic dataset;tabletop objects;unsupervised feature
learning;virtual scenes;Dictionaries;Feature
extraction;Labeling;Matching pursuit algorithms;Solid
modeling;Three-dimensional displays;Videos},
doi={10.1109/ICRA.2014.6907298},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6906977,
author={W. Huang and X. Gong and Z. Xiang},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Road scene segmentation via fusing camera and lidar data},
year={2014},
pages={1008-1013},
abstract={This paper presents an approach for pixel-wise object
segmentation for road scenes based on the integration of a color image
and an aligned 3D point cloud. In light of the advantage of range
information in object discovery, we first produce initial object
hypotheses by clustering the sparse 3D point cloud. The image pixels
registered to the clustered 3D points are taken as samples to learn each
object's prior knowledge. The priors are represented by Gaussian Mixture
Models (GMMs) of color and 3D location information only, requiring no
high-level features. We further formulate the segmentation problem
within a Conditional Random Field (CRF) framework, which incorporates
the learned prior models, together with hard constraints placed on the
registered pixels and pairwise spatial constraints to achieve final
results. Our algorithm is validated on the challenging KITTI dataset
which contains diverse complicated road scenarios. Both qualitative and
quantitative evaluation results show the superiority of our algorithm.},
keywords={Gaussian processes;image colour analysis;image
segmentation;mixture models;object recognition;optical radar;radar
imaging;3D point cloud;CRF framework;GMM;Gaussian mixture models;KITTI
dataset;LIDAR data;color image;conditional random field;fusing
camera;object discovery;pixel-wise object segmentation;road scene
segmentation;Color;Image color analysis;Image segmentation;Laser
radar;Roads;Three-dimensional displays;Vehicles},
doi={10.1109/ICRA.2014.6906977},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907502,
author={F. B. Carlson and N. D. Vuong and R. Johansson},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Polynomial reconstruction of 3D sampled curves using auxiliary
surface data},
year={2014},
pages={4411-4416},
abstract={This paper proposes a method for structural enhancement of a
3D sampled curve. The curve is assumed to be organized, but corrupted
with low frequency noise. The proposed method approaches the notion of
curve reconstruction in a novel way, where information about the
structure in a scanned surface is used to reconstruct the curve.
Principal Component Analysis is carried out on successive neighborhoods
along the curve to estimate reduced dimensionality spaces, which allows
polynomial reconstruction. The effectiveness of the proposed method is
verified by both simulations and experiments.},
keywords={computational geometry;curve fitting;polynomials;principal
component analysis;3D sampled curves;auxiliary surface data;curve
reconstruction;low frequency noise;polynomial reconstruction;principal
component analysis;reduced dimensionality space estimation;structural
enhancement method;Noise;Polynomials;Principal component
analysis;Programming;Robots;Surface reconstruction;Three-dimensional
displays;3D sampled curve;Polynomial reconstruction;point cloud;smoothing},
doi={10.1109/ICRA.2014.6907502},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907098,
author={S. Morante and J. G. Victores and A. Jardón and C. Balaguer},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Action effect generalization, recognition and execution through
Continuous Goal-Directed Actions},
year={2014},
pages={1822-1827},
abstract={Programming by demonstration (PbD) allows matching the
kinematic movements of a robot with those of a human. The presented
Continuous Goal-Directed Actions (CGDA) is able to additionally encode
the effects of a demonstrated action, which are not encoded in PbD. CGDA
allows generalization, recognition and execution of action effects on
the environment. In addition to analyzing kinematic parameters (joint
positions/velocities, etc.), CGDA focuses on changes produced on the
object due to an action (spatial, color, shape, etc.). By tracking
object features during action execution, we create a trajectory in an
n-dimensional feature space that represents object temporal states.
Discretized action repetitions provide us with a cloud of points. Action
generalization is accomplished by extracting the average point of each
sequential temporal interval of the point cloud. These points are
interpolated using Radial Basis Functions, obtaining a generalized
multidimensional object feature trajectory. Action recognition is
performed by comparing the trajectory of a query sample with the
generalizations. The trajectories discrepancy score is obtained by using
Dynamic Time Warping (DTW). Robot joint trajectories for execution are
computed in a simulator through evolutionary computation. Object
features are extracted from sensors, and each evolutionary individual
fitness is measured using DTW, comparing the simulated action with the
generalization.},
keywords={automatic programming;generalisation (artificial
intelligence);image recognition;learning (artificial
intelligence);radial basis function networks;robot kinematics;robot
programming;robot vision;action effect execution;action effect
generalization;action effect recognition;continuous goal-directed
actions;discretized action repetitions;dynamic time warping;evolutionary
computation;evolutionary individual fitness;generalized multidimensional
object feature trajectory;interpolation;n-dimensional feature
space;object feature extraction;object feature tracking;object temporal
states;point cloud;programming by demonstration;radial basis
functions;robot joint trajectories;robot kinematic movements;sequential
temporal interval;Color;Feature
extraction;Joints;Kinematics;Paints;Robots;Trajectory},
doi={10.1109/ICRA.2014.6907098},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907302,
author={Q. Zhang and X. Song and X. Shao and H. Zhao and R. Shibasaki},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Start from minimum labeling: Learning of 3D object models and
point labeling from a large and complex environment},
year={2014},
pages={3082-3089},
abstract={A large category model base can provide object-level knowledge
for various perception tasks of the intelligent vehicle system. The
automatic and efficient construction of such a model base is highly
desirable but challenging. This paper presents a novel semi-supervised
approach to discover possible prototype models of 3D object structures
from the point cloud of a large and complex environment, given a limited
number of seeds in an object category. Our method incrementally trains
the models while simultaneously collecting object samples. Considering
the bias problem of model learning caused by bias accumulation in a
sample collection, we propose to gradually differentiate the standard
category model into several sub-category models to represent different
intra-category structural styles. Thus, new sub-categories are
discovered and modeled, old models are improved, and redundant models
for similar structures are deleted iteratively during the learning
process. This multiple-model strategy provides several interactive
options for the category boundary to deal with the bias problem.
Experimental results demonstrate the effectiveness and high efficiency
of our approach to model mining from “big point cloud data”.},
keywords={Big Data;data mining;intelligent transportation
systems;learning (artificial intelligence);solid modelling;3D object
model learning;3D object structures;bias problem;big point cloud
data;category boundary;category model base;intelligent vehicle
system;intracategory structural styles;minimum labeling;model
mining;multiple-model strategy;object category;object-level
knowledge;perception tasks;point labeling;semisupervised
approach;Computational modeling;Data
mining;Labeling;Reliability;Shape;Solid modeling;Three-dimensional
displays},
doi={10.1109/ICRA.2014.6907302},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6906886,
author={F. T. Pokorny and Y. Bekiroglu and D. Kragic},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Grasp moduli spaces and spherical harmonics},
year={2014},
pages={389-396},
abstract={In this work, we present a novel representation which enables
a robot to reason about, transfer and optimize grasps on various objects
by representing objects and grasps on them jointly in a common space. In
our approach, objects are parametrized using smooth differentiable
functions which are obtained from point cloud data via a spectral
analysis. We show how, starting with point cloud data of various
objects, one can utilize this space consisting of grasps and smooth
surfaces in order to continuously deform various surface/grasp
configurations with the goal of synthesizing force closed grasps on
novel objects. We illustrate the resulting shape space for a collection
of real world objects using multidimensional scaling and show that our
formulation naturally enables us to use gradient ascent approaches to
optimize and simultaneously deform a grasp from a known object towards a
novel object.},
keywords={manipulators;grasp moduli spaces;multidimensional
scaling;object representation;point cloud data;shape space;smooth
differentiable functions;smooth surfaces;spectral analysis;spherical
harmonics;Harmonic analysis;Robot kinematics;Shape;Surface
reconstruction;Three-dimensional displays;Vectors},
doi={10.1109/ICRA.2014.6906886},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907777,
author={A. Eilering and V. Yap and J. Johnson and K. Hauser},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Identifying support surfaces of climbable structures from 3D
point clouds},
year={2014},
pages={6226-6231},
abstract={This paper presents a probabilistic technique for identifying
support surfaces like floors, walls, stairs, and rails from unstructured
3D point cloud scans. A Markov random field is employed to model the
joint probability of point labels, which can take on a number of
user-defined surface classes. The probability of a point depends on both
local spatial features of the point cloud around the point as well as
the classifications of points in its neighborhood. The training step
estimates joint and pairwise potentials from labeled point cloud
datasets, and the prediction step aims to maximize the joint probability
of all labels using a hill-climbing procedure. The method is applied to
stair and ladder detection from noisy and partial scans using three
types of sensors: a sweeping laser sensor, time-offlight depth camera,
and a Kinect depth camera. The resulting classifier achieves
approximately 75% accuracy and is robust to variations in point density.},
keywords={Markov processes;computer graphics;floors;rails;structural
engineering computing;walls;3D point clouds;Kinect depth camera;Markov
random field;climbable structures;floors;hill-climbing procedure;joint
probability;labeled point cloud datasets;ladder detection;noisy
scans;partial scans;probabilistic technique;rails;stairs;support surface
identification;support surfaces;sweeping laser sensor;time-offlight
depth camera;unstructured 3D point cloud scans;user-defined surface
classes;walls;Accuracy;Labeling;Rails;Robots;Sensors;Three-dimensional
displays;Training},
doi={10.1109/ICRA.2014.6907777},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907804,
author={N. Sommer and M. Li and A. Billard},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Bimanual compliant tactile exploration for grasping unknown
objects},
year={2014},
pages={6400-6407},
abstract={Humans have an incredible capacity to learn properties of
objects by pure tactile exploration with their two hands. With robots
moving into human-centred environment, tactile exploration becomes more
and more important as vision may be occluded easily by obstacles or fail
because of different illumination conditions. In this paper, we present
our first results on bimanual compliant tactile exploration, with the
goal to identify objects and grasp them. An exploration strategy is
proposed to guide the motion of the two arms and fingers along the
object. From this tactile exploration, a point cloud is obtained for
each object. As the point cloud is intrinsically noisy and un-uniformly
distributed, a filter based on Gaussian Processes is proposed to smooth
the data. This data is used at runtime for object identification.
Experiments on an iCub humanoid robot have been conducted to validate
our approach.},
keywords={Gaussian processes;compliance control;humanoid
robots;manipulators;motion control;Gaussian processes;bimanual compliant
tactile exploration;iCub humanoid robot;object identification;point
cloud;robots;unknown object grasping;Robot kinematics;Tactile
sensors;Three-dimensional displays;Training;Uncertainty},
doi={10.1109/ICRA.2014.6907804},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907395,
author={Y. Ogawa and Z. Wang and T. Wada and Y. Hirata and K. Kosuge},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Generating human motion transition map in indoor environment and
analyzing human behavior by geographical clustering},
year={2014},
pages={3700-3705},
abstract={In recent years, robots working in human living space with
human-robot interactions are actively studied. To these robots, it is
important to perform environmental cognition not only building
environment map for autonomous motion of the robots but also estimating
presences of human around the robots. In this study, by utilizing human
state estimation function and SLAM based mapping technology, a concept
and architecture of Human Motion Map by representing human behavior in
the human living space as a hybrid map system are proposed. Beyond the
conventional map which represents the existence of wall and objects,
Human Motion Map represents not only the existence of humans in a
particular location but also motion distributions. With recent
improvements of the cloud computing technology, Human Motion Map can be
accumulated as a kind of big data while measurements of robots are
performed continuingly while it is moving around. In this paper, we
propose a motion feature classification algorithm for clustering human
motions geographically. Some experiment result of basic motion feature
extraction, geographical clustering, and human motion behavior analyzing
are provided for illustrating the validity of proposed algorithm.},
keywords={SLAM (robots);human-robot interaction;mobile robots;motion
control;SLAM based mapping technology;autonomous robots
motion;environment map;environmental cognition;geographical
clustering;human behavior analysis;human living space;human motion
map;human motion transition map generation;human state estimation
function;human-robot interactions;indoor environment;motion feature
classification algorithm;Buildings;Estimation;Hidden Markov
models;Legged locomotion;Robot sensing systems},
doi={10.1109/ICRA.2014.6907395},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907535,
author={E. Ilg and R. Ku¨mmerle and W. Burgard and T. Brox},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Reconstruction of rigid body models from motion distorted laser
range data using optical flow},
year={2014},
pages={4627-4632},
abstract={The setup of tilting a 2D laser range finder up and down is a
widespread strategy to acquire 3D point clouds. This setup requires that
the scene is static while the robot takes a 3D scan. If an object moves
through the scene during the measurement process and one does not take
into account these movements, the resulting model will get distorted.
This paper presents an approach to reconstruct the 3D model of a moving
rigid object from the inconsistent set of 2D measurements by the help of
a camera. Our approach utilizes optical flow in the camera images to
estimate the motion in the image plane and point-line constraints to
compensate the missing information about the motion in depth. We combine
multiple sweeps and/or views into to a single consistent model using a
point-to-plane ICP approach and optimize single sweeps by smoothing the
resulting trajectory. Experiments obtained in real outdoor scenarios
with moving cars demonstrate that our approach yields accurate models.},
keywords={image reconstruction;image sensors;image sequences;laser
ranging;motion estimation;2D laser range finder;2D measurements;3D point
clouds;3D scan;camera images;cars;image plane;motion distorted laser
range data;motion estimation;moving rigid object;optical flow;point-line
constraints;point-to-plane ICP approach;rigid body model
reconstruction;Calibration;Cameras;Integrated optics;Laser modes;Optical
imaging;Three-dimensional displays;Trajectory},
doi={10.1109/ICRA.2014.6907535},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907236,
author={A. Hermans and G. Floros and B. Leibe},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Dense 3D semantic mapping of indoor scenes from RGB-D images},
year={2014},
pages={2631-2638},
abstract={Dense semantic segmentation of 3D point clouds is a
challenging task. Many approaches deal with 2D semantic segmentation and
can obtain impressive results. With the availability of cheap RGB-D
sensors the field of indoor semantic segmentation has seen a lot of
progress. Still it remains unclear how to deal with 3D semantic
segmentation in the best way. We propose a novel 2D-3D label transfer
based on Bayesian updates and dense pairwise 3D Conditional Random
Fields. This approach allows us to use 2D semantic segmentations to
create a consistent 3D semantic reconstruction of indoor scenes. To this
end, we also propose a fast 2D semantic segmentation approach based on
Randomized Decision Forests. Furthermore, we show that it is not needed
to obtain a semantic segmentation for every frame in a sequence in order
to create accurate semantic 3D reconstructions. We evaluate our approach
on both NYU Depth datasets and show that we can obtain a significant
speed-up compared to other methods.},
keywords={Bayes methods;decision trees;image colour analysis;image
reconstruction;image segmentation;mobile robots;statistical
distributions;3D semantic reconstruction;Bayesian updates;RGB-D
images;conditional random fields;dense 3D semantic mapping;indoor
semantic segmentation;mobile robotics;randomized decision
forests;Accuracy;Image reconstruction;Image segmentation;Kernel;Resource
description framework;Semantics;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907236},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907663,
author={D. Fehr and W. J. Beksi and D. Zermas and N. Papanikolopoulos},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={RGB-D object classification using covariance descriptors},
year={2014},
pages={5467-5472},
abstract={In this paper, we introduce a new covariance based feature
descriptor to be used on “colored” point clouds gathered by a mobile
robot equipped with an RGB-D camera. Although many recent descriptors
provide adequate results, there is not yet a clear consensus on how to
best tackle “colored” point clouds. We present the notion of a
covariance on RGB-D data. Covariances have not only been proven to be
successful in image processing, but in other domains as well. Their main
advantage is that they provide a compact and flexible description of
point clouds. Our work is a first step towards demonstrating the
usability of the concept of covariances in conjunction with RGB-D data.
Experiments performed on an RGB-D database and compared to previous
results show the increased performance of our method.},
keywords={covariance analysis;feature extraction;image
classification;image colour analysis;mobile robots;robot vision;video
cameras;RGB-D camera;RGB-D database;RGB-D object classification;colored
point cloud;covariance based feature descriptor;image processing;mobile
robot;Accuracy;Cameras;Covariance matrices;Databases;Image color
analysis;Shape;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907663},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907757,
author={H. Alismail and L. D. Baker and B. Browning},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Continuous trajectory estimation for 3D SLAM from actuated lidar},
year={2014},
pages={6096-6101},
abstract={We extend the Iterative Closest Point (ICP) algorithm to
obtain a method for continuous-time trajectory estimation (CICP)
suitable for SLAM from actuated lidar. Traditional solutions to SLAM
from actuated lidar rely heavily on the accuracy of an auxiliary pose
sensor to form rigid frames. These frames are then used with ICP to
obtain accurate pose estimates. However, since lidar records a single
range sample at time, any error in inter-sample sensor motion must be
accounted for. This is not possible if the frame is treated as a rigid
point cloud. In this work, instead of ICP we estimate a continuous-time
trajectory that takes into account inter-sample pose errors. The
trajectory is represented as a linear combination of basis functions and
formulated as a solution to a (sparse) linear system without restrictive
assumptions on sensor motion. We evaluate the algorithm on synthetic and
real data and show improved accuracy in open-loop SLAM in comparison to
state-of-the-art rigid registration methods.},
keywords={continuous time systems;iterative methods;optical radar;pose
estimation;3D SLAM;actuated lidar;continuous time trajectory
estimation;intersample sensor motion;iterative closest point
algorithm;Laser radar;Simultaneous localization and mapping;Splines
(mathematics);Three-dimensional displays;Trajectory},
doi={10.1109/ICRA.2014.6907757},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907518,
author={M. Munaro and A. Basso and A. Fossati and L. Van Gool and E.
Menegatti},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={3D reconstruction of freely moving persons for re-identification
with a depth sensor},
year={2014},
pages={4512-4519},
abstract={In this work, we describe a novel method for creating 3D
models of persons freely moving in front of a consumer depth sensor and
we show how they can be used for long-term person re-identification. For
overcoming the problem of the different poses a person can assume, we
exploit the information provided by skeletal tracking algorithms for
warping every point cloud frame to a standard pose in real time. Then,
the warped point clouds are merged together to compose the model.
Re-identification is performed by matching body shapes in terms of whole
point clouds warped to a standard pose with the described method. We
compare this technique with a classification method based on a
descriptor of skeleton features and with a mixed approach which exploits
both skeleton and shape features. We report experiments on two datasets
we acquired for RGB-D re-identification which use different skeletal
tracking algorithms and which are made publicly available to foster
research in this new research branch.},
keywords={bone;cameras;image classification;image colour analysis;image
sensors;object tracking;orthopaedics;3D freely moving person
reconstruction;RGB-D reidentification;consumer depth sensor;long-term
person reidentification;skeletal tracking algorithm;skeleton feature
descriptor;warped point cloud;Joints;Shape;Solid
modeling;Standards;Three-dimensional displays;Training},
doi={10.1109/ICRA.2014.6907518},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907453,
author={M. Liu and R. Siegwart},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Navigation on point-cloud #x2014; A Riemannian metric approach},
year={2014},
pages={4088-4093},
abstract={Mobile wheeled- or tracked-robots drive in 2.5-dimensional
(2.5D) environments, where the traversable surface can be considered as
a 2D-manifold embedded in a three-dimensional (3D) ambient space. In
this work, we aim at solving the 2.5D navigation problem solely on
point-cloud. The proposed method is independent of traditional surface
parametrization or reconstruction methods, such as a meshing process,
which generally has high computational complexity. Instead, we utilize
the output of 3D tensor voting framework (TVF) using raw point-clouds. A
novel local Riemannian metric is defined based on the saliency
components of TVF, which helps the modeling of the latent traversable
surface. Using this metric, we prove that the geodesic in the 3D tensor
space leads to rational path-planning results. Compared to traditional
methods, the results reveal the advantages of the proposed method in
terms of facilitating the robot maneuver with minimum movement.},
keywords={mobile robots;path planning;robot vision;2.5D navigation
problem;2D-manifold;3D ambient space;3D tensor voting
framework;Riemannian metric approach;TVF;latent traversable
surface;meshing process;mobile wheeled robots;path planning;point-cloud
navigation;reconstruction method;robot maneuver;surface parametrization
method;three dimensional ambient
space;tracked-robots;Measurement;Planning;Robots;Tensile
stress;Three-dimensional displays;Trajectory;Vectors},
doi={10.1109/ICRA.2014.6907453},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907351,
author={B. Zheng and Y. Zhao and J. C. Yu and K. Ikeuchi and S. C. Zhu},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Detecting potential falling objects by inferring human action and
natural disturbance},
year={2014},
pages={3417-3424},
abstract={Detecting potential dangers in the environment is a
fundamental ability of living beings. In order to endure such ability to
a robot, this paper presents an algorithm for detecting potential
falling objects, i.e. physically unsafe objects, given an input of 3D
point clouds captured by the range sensors. We formulate the falling
risk as a probability or a potential that an object may fall given human
action or certain natural disturbances, such as earthquake and wind. Our
approach differs from traditional object detection paradigm, it first
infers hidden and situated “causes (disturbance) of the scene, and then
introduces intuitive physical mechanics to predict possible “effects
(falls) as consequences of the causes. In particular, we infer a
disturbance field by making use of motion capture data as a rich source
of common human pose movement. We show that, by applying various
disturbance fields, our model achieves a human level recognition rate of
potential falling objects on a dataset of challenging and realistic
indoor scenes.},
keywords={image motion analysis;image sensors;mobile robots;object
detection;object recognition;pose estimation;robot vision;disturbance
field;falling object detection;human action;human level
recognition;human pose movement;motion capture data;natural
disturbance;range sensors;safety surveillance robot;Earthquakes;Energy
barrier;Force;Kinetic energy;Potential energy;Robots;Three-dimensional
displays},
doi={10.1109/ICRA.2014.6907351},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907326,
author={K. B. Kaldestad and S. Haddadin and R. Belder and G. Hovland and
D. A. Anisi},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Collision avoidance with potential fields based on parallel
processing of 3D-point cloud data on the GPU},
year={2014},
pages={3250-3257},
abstract={In this paper we present an experimental study on real-time
collision avoidance with potential fields that are based on 3D point
cloud data and processed on the Graphics Processing Unit (GPU). The
virtual forces from the potential fields serve two purposes. First, they
are used for changing the reference trajectory. Second they are
projected to and applied on torque control level for generating
according nullspace behavior together with a Cartesian impedance main
control loop. The GPU algorithm creates a map representation that is
quickly accessible. In addition, outliers and the robot structure are
efficiently removed from the data, and the resolution of the
representation can be easily adjusted. Based on the 3D robot
representation and the remaining 3D environment data, the virtual forces
that are fed to the trajectory planning and torque controller are
calculated. The algorithm is experimentally verified with a 7-Degree of
Freedom (DoF) torque controlled KUKA/DLR Lightweight Robot for static
and dynamic environmental conditions. To the authors knowledge, this is
the first time that collision avoidance is demonstrated in real-time on
a real robot using parallel GPU processing.},
keywords={collision avoidance;control engineering computing;graphics
processing units;industrial robots;mobile robots;parallel
processing;torque control;trajectory control;3D-point cloud
data;Cartesian impedance main control loop;GPU;KUKA/DLR lightweight
robot;collision avoidance;graphics processing unit;industrial
manufacturing;parallel processing;potential fields;reference trajectory
planning;torque control level;Collision avoidance;Force;Graphics
processing units;Robot sensing systems;Service robots;Three-dimensional
displays},
doi={10.1109/ICRA.2014.6907326},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907660,
author={M. Schröder and J. Maycock and H. Ritter and M. Botsch},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Real-time hand tracking using synergistic inverse kinematics},
year={2014},
pages={5447-5454},
abstract={We present a method for real-time bare hand tracking that
utilizes natural hand synergies to reduce the complexity and improve the
plausibility of the hand posture estimation. The hand pose and posture
are estimated by fitting a virtual hand model to the 3D point cloud
obtained from a Kinect camera using an inverse kinematics approach. We
use real human hand movements captured with a Vicon motion tracking
system as the ground truth for deriving natural hand synergies based on
principal component analysis. These synergies are integrated in the
tracking scheme by optimizing the posture in a reduced parameter space.
Tracking in this reduced space combined with joint limit avoidance
constrains the posture estimation to natural hand articulations. The
information loss associated with dimension reduction can be dealt with
by employing a hierarchical optimization scheme. We show that our
synergistic hand tracking approach improves runtime performance and
increases the quality of the posture estimation.},
keywords={control engineering computing;manipulator kinematics;motion
control;object tracking;optimisation;pose estimation;principal component
analysis;virtual reality;3D point cloud;Kinect camera;Vicon motion
tracking system;dimension reduction;hand pose estimation;hand posture
estimation;hierarchical optimization scheme;human hand movements;joint
limit avoidance constrains;natural hand articulations;natural hand
synergies;posture optimization;principal component analysis;real-time
bare hand tracking;synergistic hand tracking approach;synergistic
inverse kinematics;tracking scheme;virtual hand
model;Estimation;Joints;Kinematics;Optimization;Real-time
systems;Three-dimensional displays;Tracking},
doi={10.1109/ICRA.2014.6907660},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907668,
author={G. Arbeiter and S. Fuchs and J. Hampp and R. Bormann},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Efficient segmentation and surface classification of range images},
year={2014},
pages={5502-5509},
abstract={Derivation of geometric structures from point clouds is an
important step towards scene understanding for mobile robots. In this
paper, we present a novel method for segmentation and surface
classification of ordered point clouds. Data from RGB-D cameras are used
as input. Normal based region growing segments the cloud and point
feature descriptors classify each segment. Not only planar segments can
be described but also curved surfaces. In an evaluation on indoor scenes
we show the performance of our approach as well as give a comparison to
state of the art methods.},
keywords={cameras;image classification;image segmentation;RGB-D
cameras;curved surfaces;efficient segmentation;indoor scenes;normal
based region;ordered point clouds;point feature descriptors;range
images;surface classification;Accuracy;Cameras;Image
segmentation;Principal component analysis;Robustness;Surface
treatment;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907668},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907679,
author={D. I. Kim and G. S. Sukhatme},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Semantic labeling of 3D point clouds with object affordance for
robot manipulation},
year={2014},
pages={5578-5584},
abstract={When a robot is deployed it needs to understand the nature of
its surroundings. In this paper, we address the problem of semantic
labeling 3D point clouds by object affordance (e.g., `pushable',
`liftable'). We propose a technique to extract geometric features from
point cloud segments and build a classifier to predict associated object
affordances. With the classifier, we have developed an algorithm to
enhance object segmentation and reduce manipulation uncertainty by
iterative clustering, along with minimizing labeling entropy. Our
incremental multiple view merging technique shows improved object
segmentation. The novel feature of our approach is the semantic labeling
that can be directly applied to manipulation planning. In our
experiments with 6 affordance labels, an average of 81.8% accuracy of
affordance prediction is achieved. We demonstrate refined object
segmentation by applying the classifier to data from the PR2 robot using
a Microsoft Kinect in an indoor office environment.},
keywords={entropy;feature extraction;image classification;image
segmentation;iterative methods;mobile robots;object recognition;path
planning;pattern clustering;robot vision;3D point cloud semantic
labeling;Microsoft Kinect;PR2 robot;associated object affordance
prediction;classifier;geometric feature extraction;incremental multiple
view merging technique;indoor office environment;iterative
clustering;labeling entropy minimization;manipulation
planning;manipulation uncertainty reduction;object segmentation;point
cloud segments;robot manipulation;Entropy;Image segmentation;Object
segmentation;Robots;Semantics;Three-dimensional displays;Vectors},
doi={10.1109/ICRA.2014.6907679},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6906998,
author={M. B. Horowitz and N. Matni and J. W. Burdick},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Convex relaxations of SE(2) and SE(3) for visual pose estimation},
year={2014},
pages={1148-1154},
abstract={This paper proposes a new method for rigid body pose
estimation based on spectrahedral representations of the tautological
orbitopes of SE(2) and SE(3). The approach can use dense point cloud
data from stereo vision or an RGB-D sensor (such as the Microsoft
Kinect), as well as visual appearance data as input. The method is a
convex relaxation of the classical pose estimation problem, and is based
on explicit linear matrix inequality (LMI) representations for the
convex hulls of SE(2) and SE(3). Given these representations, the
relaxed pose estimation problem can be framed as a robust least squares
problem with the optimization variable constrained to these convex sets.
Although this formulation is a relaxation of the original problem,
numerical experiments indicates that it is indeed exact - i.e. its
solution is a member of SE(2) or SE(3) - in many interesting settings.
We additionally show that this method is guaranteed to be exact for a
large class of pose estimation problems.},
keywords={computer vision;convex programming;least squares
approximations;linear matrix inequalities;pose estimation;stereo image
processing;LMI representations;RGB-D sensor;SE(2) convex hull;SE(3)
convex hull;convex relaxation;dense point cloud data;linear matrix
inequality;optimization variable;red-green-blue-depth sensor;rigid body
pose estimation;robust least squares problem;spectrahedral
representation;stereo vision;tautological orbitopes;visual appearance
data;visual pose estimation;Estimation;Least squares
approximations;Optimization;Robot sensing
systems;Robustness;Visualization},
doi={10.1109/ICRA.2014.6906998},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907317,
author={G. Pandey and J. R. McBride and S. Savarese and R. M. Eustice},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Toward mutual information based place recognition},
year={2014},
pages={3185-3192},
abstract={This paper reports on a novel mutual information (MI) based
algorithm for robust place recognition. The proposed method provides a
principled framework for fusing the complementary information obtained
from 3D lidar and camera imagery for recognizing places within an a
priori map of a dynamic environment. The visual appearance of the
locations in the map can be significantly different due to changing
weather, lighting conditions and dynamical objects present in the
environment. Various 3D/2D features are extracted from the textured
point clouds (scans) and each scan is represented as a collection of
these features. For two scans acquired from the same location, the high
value of MI between the features present in the scans indicates that the
scans are captured from the same location. We use a non-parametric
entropy estimator to estimate the true MI from the sparse marginal and
joint histograms of the features extracted from the scans. Experimental
results using seasonal datasets collected over several years are used to
validate the robustness of the proposed algorithm.},
keywords={feature extraction;object recognition;optical radar;3D
lidar;camera imagery;complementary information;feature
extraction;nonparametric entropy estimator;novel mutual information
based place recognition algorithm;seasonal datasets;textured point
clouds;Cameras;Feature extraction;Laser radar;Random
variables;Robots;Sensors;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907317},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6907648,
author={I. Gori and U. Pattacini and V. Tikhanoff and G. Metta},
booktitle={2014 IEEE International Conference on Robotics and Automation
(ICRA)},
title={Three-finger precision grasp on incomplete 3D point clouds},
year={2014},
pages={5366-5373},
abstract={We present a novel method for three-finger precision grasp and
its implementation in a complete grasping tool-chain. We start from
binocular vision to recover the partial 3D structure of unknown objects.
We then process the incomplete 3D point clouds searching for good
triplets according to a function that accounts for both the feasibility
and the stability of the solution. In particular, while stability is
determined using the classical force-closure approach, feasibility is
evaluated according to a new measure that includes information about the
possible configuration shapes of the hand as well as the hand's inverse
kinematics. We finally extensively assess the proposed method using the
stereo vision and the kinematics of the iCub robot.},
keywords={computer graphics;dexterous manipulators;grippers;humanoid
robots;robot vision;stereo image processing;binocular
vision;force-closure approach;grasping tool-chain;iCub robot;incomplete
3D point cloud;inverse kinematics;partial 3D structure;stereo
vision;three-finger precision
grasp;Friction;Grasping;Kinematics;Robots;Shape;Stability
analysis;Three-dimensional displays},
doi={10.1109/ICRA.2014.6907648},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6871493,
author={H. Y. Chen and C. Y. Lin},
booktitle={2014 International Conference on Advanced Robotics and
Intelligent Systems (ARIS)},
title={RGB-D sensor based real-time 6DoF-SLAM},
year={2014},
pages={61-65},
abstract={A robust 6 Dof SLAM algorithm using an RGB-D sensor has been
developed, which can be used in autonomous navigation and localization
for robots in an indoor scenario. The SLAM algorithm developed is a
graph-based approach in which the nodes of the graph represents the
position and orientation of the camera, and the edges of the graph
represents the constraints between the nodes. To simplify calculation
and achieve real-time application, SURF matching and RANSAC are first
applied to consecutive frames from the RGB camera to obtain good
corresponding keypoints. Afterwards, ICP is applied to the point cloud
of these keypoints, calculating the transformation matrix, and thus
obtaining visual odometry. The nodes, edges, and scenery data are then
recorded in a graph. Finally, ELCH is applied to eliminate
dead-reckoning upon loop detections to ensure global consistency. The
resulting 3D model of the surrounding is presented in a point cloud
format, consisting of both color and depth information.},
keywords={SLAM (robots);cameras;image sensors;iterative methods;matrix
algebra;navigation;pattern matching;ELCH;ICP;RANSAC;RGB camera;RGB-D
sensor;SURF matching;autonomous navigation;cloud format;color
information;depth information;graph-based approach;robust 6 Dof SLAM
algorithm;simultaneous localization and mapping;transformation
matrix;visual odometry;Cameras;Feature extraction;Libraries;Simultaneous
localization and mapping;Three-dimensional displays},
doi={10.1109/ARIS.2014.6871493},
month={June},}
@INPROCEEDINGS{6857834,
author={R. Cucu and C. Avram and A. Astilean and I. G. Fărcaş and J.
Machado},
booktitle={2014 IEEE International Conference on Automation, Quality and
Testing, Robotics},
title={E-health decision support system for differential diagnosis},
year={2014},
pages={1-6},
abstract={A new experimental system, capable to use the combined
facilities offered by mobile communications, cloud computing and
artificial intelligence, to assist the professional formation and
specialization of medical staff and to offer up to date information for
differential diagnosis, is proposed. To demonstrate the feasibility of
the proposed approach, a proof-of-concept system was developed. An
application in which two expert systems are used for the differential
diagnosis of hypertension is presented. These systems aim to facilitate
the diagnosis process of primary, endocrine and renal hypertension. A
Naive Bayes Classifier and a Fuzzy Inference System were designed and
implemented in order to differentiate the presented types of
hypertension. The application was designed based on the client-server
architecture, using Cloud Computing techniques and Android programming.
The system take as inputs the preliminary medical information and
investigation results that are sent from the Android client and outputs
the precise risk of having a certain type of hypertension.},
keywords={Android (operating system);Bayes methods;client-server
systems;cloud computing;decision support systems;expert systems;fuzzy
reasoning;medical information systems;mobile computing;patient
diagnosis;pattern classification;Android client;Android
programming;artificial intelligence;client-server architecture;cloud
computing;differential diagnosis;e-health decision support
system;endocrine hypertension;experimental system;expert systems;fuzzy
inference system;medical staff specialization;mobile
communications;naive Bayes classifier;primary hypertension;professional
formation;proof-of-concept system;renal hypertension;Androids;Cloud
computing;Expert systems;Fuzzy logic;Humanoid
robots;Hypertension;Medical diagnostic imaging;Cloud
Computing;E-health;Expert System;Fuzzy Logic;Hypertension;Naive Bayes
Classifier;mobile communication},
doi={10.1109/AQTR.2014.6857834},
month={May},}
@INPROCEEDINGS{6857843,
author={N. Jazdi},
booktitle={2014 IEEE International Conference on Automation, Quality and
Testing, Robotics},
title={Cyber physical systems in the context of Industry 4.0},
year={2014},
pages={1-4},
abstract={We are currently experiencing the fourth Industrial Revolution
in terms of cyber physical systems. These systems are industrial
automation systems that enable many innovative functionalities through
their networking and their access to the cyber world, thus changing our
everyday lives significantly. In this context, new business models, work
processes and development methods that are currently unimaginable will
arise. These changes will also strongly influence the society and
people. Family life, globalization, markets, etc. will have to be
redefined. However, the Industry 4.0 simultaneously shows
characteristics that represent the challenges regarding the development
of cyber-physical systems, reliability, security and data protection.
Following a brief introduction to Industry 4.0, this paper presents a
prototypical application that demonstrates the essential aspects.},
keywords={Internet of Things;computer network reliability;computer
network security;data protection;industrial control;production
engineering computing;Industry 4.0;Internet of Things;business
models;cyber physical systems;data protection;data reliability;data
security;development methods;industrial automation systems;industrial
revolution;work processes;Actuators;Automation;Embedded
systems;Industries;Intelligent sensors;Internet;CPS;Cloud
Technology;Industry 4.0;Internet of Things},
doi={10.1109/AQTR.2014.6857843},
month={May},}
@INPROCEEDINGS{6857854,
author={A. Coleşa},
booktitle={2014 IEEE International Conference on Automation, Quality and
Testing, Robotics},
title={Fast creation of short-living virtual machines using
copy-on-write RAM-disks},
year={2014},
pages={1-6},
abstract={We propose a method to rapidly launch new virtual machines
(VM), starting from a prepared copy-on-write (COW) snapshot image. W e
use RAM-disks instead of stored ones and apply the COW technique on
them, also. A VM can extend its RAM-disk on non-volatile storage area
when it grows over an established threshold, just to avoid overloading
the RAM. Our solution is appropriate in situations like testing
environments with short-living VMs, where the VM persistence is not a
requirement, b u t one just needs to start very fast new running VMs in
a known safe state. W e implemented our solution in QEMU/KVM. The
performance tests showed an improvement in the total VM's creation and
running time of about 15% over the basic HDD-based method.},
keywords={cloud computing;random-access storage;storage
management;virtual machines;virtualisation;COW snapshot image;COW
technique;HDD-based method;QEMU-KVM;RAM overloading;VM persistence;cloud
computing;copy-on-write RAM-disks;copy-on-write snapshot
image;nonvolatile storage area;running time;short-living virtual machine
creation;virtualization;Cloning;Elasticity;Memory management;Random
access memory;Testing;Virtual machine monitors;Virtualization},
doi={10.1109/AQTR.2014.6857854},
month={May},}
@INPROCEEDINGS{6840196,
author={J. Salmeron-Garcia and P. Inigo-Blasco and F. Diaz-del-Rio and
D. Cagigas-Muniz},
booktitle={ISR/Robotik 2014; 41st International Symposium on Robotics},
title={Mobile robot motion planning based on Cloud Computing stereo
vision processing},
year={2014},
pages={1-6},
abstract={Nowadays, the limitations of robot embedded hardware (which
cannot be upgraded easily) make difficult to perform computationally
complex tasks such as those of high level artificial vision. However,
instead of disposing these "out-dated" embedded systems, Cloud
technologies for computation offloading can be used. In this paper we
present and analyze an example of computation offloading in the context
of artifical vision: point cloud extraction for stereo images. A
prototype prepared for exploiting the cloud's unique capabilities (such
as elasticity) has been developed, and the inherent issues that appears
are explained and addressed.},
month={June},}
@INPROCEEDINGS{6840176,
author={H. Surmann and R. Worst and E. Zimmermann and S. Wilkes and T.
M. Liebelt and C. Eulering},
booktitle={ISR/Robotik 2014; 41st International Symposium on Robotics},
title={Simple Mobile Robots and Self-AdaptiveWireless Networks},
year={2014},
pages={1-6},
abstract={Disaster areas require mobile robots with extreme
capabilities. This paper presents an approach for setting up a network
infrastructure to operate such mobile robots. We present a waterproof
netserver box as a main component and mobile router robots (RC cars) to
extend the network capabilities. The simple mobile robot consists of
cheap standard RC components. It provides a view into the disaster area
with its sensors, e.g., cameras, and in addition and very important, it
sets up a persistent communication between all mobile robots and
rescuers. All components use ROS as a middleware and can be integrated
in the overall system. In addition to infrastructure networks, mesh
networks are also supported to replacce the destroyed network
infrastructure. Furthermore, the network and robots are prepared for
cloud computing.},
month={June},}
@INPROCEEDINGS{6840123,
author={A. Dargazany and K. Berns},
booktitle={ISR/Robotik 2014; 41st International Symposium on Robotics},
title={Stereo-based Terrain Traversability Estimation using Surface
Normals},
year={2014},
pages={1-7},
abstract={A stereo-based terrain classification for traversability
estimation of all terrains in offroad mobile robots is presented. The
proposed method defines the roughness of the surrounding terrain for
every single pixel in the image or point in a point cloud using surface
normals and also explains how this is applied to all terrain by knowing
the kinematics capability, mechanical constraints and size of the UGV.
Roughness estimation using surface normals helps us model the whole
terrain with a virtual function which increases the accuracy of the
resulting terrain travesability map considerably.},
month={June},}
@INPROCEEDINGS{6840170,
author={D. Holz and S. Behnke},
booktitle={ISR/Robotik 2014; 41st International Symposium on Robotics},
title={Registration of Non-Uniform Density 3D Point Clouds using
Approximate Surface Reconstruction},
year={2014},
pages={1-7},
abstract={3D laser scanners composed of a rotating 2D laser range
scanner exhibit different point densities within and between individual
scan lines. Such non-uniform point densities influence neighbor searches
which in turn may negatively affect feature estimation and scan
registration. To reliably register such scans, we extend a
state-of-the-art registration algorithm to include topological
information from approximate surface reconstructions. We show that our
approach outperforms related approaches in both refining a good initial
pose estimate and registering badly aligned point clouds if no such
estimate is available. In an example application, we demonstrate local
3D mapping with a micro aerial vehicle by registering sequences of
non-uniform density point clouds acquired in-flight with a continuously
rotating lightweight 3D scanner.},
month={June},}
@INPROCEEDINGS{6840126,
author={K. Schneider and W. Weber and A. Weigl-Seitz and K. Kleinmann},
booktitle={ISR/Robotik 2014; 41st International Symposium on Robotics},
title={Interactive Path Editor for Industrial Robots using a
3D-Simulation Environment},
year={2014},
pages={1-6},
abstract={This paper presents a path editor for the interactive creation
of complex robot motion programs. It combines specific capabilities that
can be found in commercial offline robot programming tools, CAD, VR and
AR software as well as drawing tools. The user can intuitively edit
point-cloud-based waypoints generated through various input channels and
is provided with online visual and acoustic feedback on the feasibility
of the desired robot motion. The paper discusses the system architecture
and presents usability considerations.},
month={June},}
@INPROCEEDINGS{6826158,
author={A. C. Caminero and A. Robles-Gómez and S. Ros and L. Tobarra and
R. Hernandez and R. Pastor and M. Castro},
booktitle={2014 IEEE Global Engineering Education Conference (EDUCON)},
title={Deconstructing remote laboratories to create Laboratories as a
Service (LaaS)},
year={2014},
pages={623-629},
abstract={The creation and publication of utilities as services (the
most widely known being Infrastructure as a Service, IaaS, Platform as a
Service, PaaS, and Software as a Service, SaaS) has been a hot topic of
research and development for the recent years. They allow easy creation
and deployment of infrastructures and applications which increase the
versatility and usefulness of the Information Technology (IT) budgets of
institutions that implement them. Therefore, this paper proposes the
development of Laboratories as a Service (LaaS), which allow users of
remote laboratories create versatile experiments adapted to their needs.
These will be based on the deconstruction of remote laboratories,
creation of clients, and selection of a container. An aeolian laboratory
based on the Lego Mindstorms robotic kit is used as an example.},
keywords={cloud computing;distance learning;information
technology;laboratories;research and development;IaaS;LaaS;Lego
Mindstorms robotic kit;PaaS;SaaS;aeolian laboratory;information
technology;infrastructure as a service;laboratories as a
service;platform as a service;remote laboratories;research and
development;software as a service;Containers;Cultural
differences;Educational institutions;Electronic mail;Remote
laboratories;Customization;Distance education;Laboratories as a Service
(LaaS);Remote laboratories},
doi={10.1109/EDUCON.2014.6826158},
ISSN={2165-9559},
month={April},}
@INPROCEEDINGS{6739848,
author={D. Xiong and H. Lu and Z. Zeng and Z. Zheng},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Topological localization based on key-frames selection and
vocabulary tree for mobile robots},
year={2013},
pages={2505-2510},
abstract={It is significant to realize topological localization for
improving the ability of autonomous navigation for mobile robots in the
unstructured environment. In this paper, we introduce vocabulary tree
technique, which is popular and has been applied successfully in pattern
recognition community, to the topological self-localization for mobile
robots, and propose a topological localization algorithm based on
key-frames selection and vocabulary tree. In the off-line phase for
building robot's topological map, we firstly propose a key-frames
selection algorithm based on vocabulary tree, and pick up distinctive
and discriminative keyframes. Then we extract local visual features from
key-frames to build a vocabulary tree again, and obtain feature vectors
which can sparsely represent local visual features of key-frames through
vocabulary tree. These vectors are considered as the nodes of the
topological map. In the on-line localization phase, we draw on a method
of image retrieval based on vocabulary tree to realize robot
self-localization. Finally, we use the COLD database to perform
topological localization experiments, choose the local visual feature
which is robust and can be run in real-time, and determine the best
algorithm parameters. The effectiveness of the proposed algorithm is
validated by the experimental results.},
keywords={image retrieval;mobile robots;navigation;pattern
recognition;trees (mathematics);visual databases;COLD
database;autonomous navigation;image retrieval;key-frames
selection;mobile robots;pattern recognition;topological
localization;topological self-localization;vocabulary
tree;Buildings;Clouds;Feature
extraction;Robots;Vectors;Visualization;Vocabulary},
doi={10.1109/ROBIO.2013.6739848},
month={Dec},}
@INPROCEEDINGS{6739800,
author={T. Xie and J. Yang and M. Li and W. Zhao},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Place recognition based on Latent Dirichlet Allocation using
Markov chain Monte Carlo method},
year={2013},
pages={2225-2230},
abstract={Place or scene recognition is an important competence of
mobile robots for operating in a real dynamic environment. Latent
Dirichlet Allocation (LDA), a popular probabilistic model, can achieve
outstanding performance in image recognition, and it has been attracted
the attention of a large number of researchers. Parameter estimation is
a key step for the learning procedure of LDA. In this paper, we propose
a novel place recognition approach based on LDA using Markov chain Monte
Carlo (MCMC) for approximate inference technique. Firstly, the training
images of each category are represented as a set of different themes.
And MCMC is employed to estimate parameters of LDA instead of
variational inference. Then an unknown test image can be recognized
according to its themes distribution. Experimental results show that our
method can perform better than the variational inference algorithm over
IDOL2 database and our own image set captured under different imaging
conditions in our campus.},
keywords={Markov processes;Monte Carlo methods;image
recognition;inference mechanisms;learning (artificial
intelligence);mobile robots;parameter estimation;probability;robot
vision;IDOL2 database;LDA;MCMC;Markov chain Monte Carlo
method;approximate inference technique;image recognition;imaging
conditions;latent Dirichlet allocation;learning procedure;mobile
robots;parameter estimation;place recognition;probabilistic model;scene
recognition;themes distribution;training images;Clouds;Databases;Feature
extraction;Image sequences;Inference algorithms;Markov processes;Training},
doi={10.1109/ROBIO.2013.6739800},
month={Dec},}
@INPROCEEDINGS{6739790,
author={P. Thumbunpeng and M. Ruchanurucks and A. Khongma},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Surface area calculation using Kinect's filtered point cloud with
an application of burn care},
year={2013},
pages={2166-2169},
abstract={This paper presents surface area calculation using point cloud
from Kinect. The point cloud from Kinect is erroneous due to
characteristics of this low cost device. Applying marching cube directly
from such this point cloud, the resulting triangular mesh has multiple
layers; despite the fact that their original surface has only one layer.
In this paper, we bypass marching cube, which is ineffective when used
with Kinect as mentioned, and calculate an area size directly using
quadrangular mesh. The resulting area calculation is more precise. We
even make the precision higher by filtering the point cloud using a
spatial filter. We aim to apply this method for humans' burn area ratio
calculation.},
keywords={computer graphics;medical computing;mesh generation;patient
care;sensors;spatial filters;Kinect filtered point cloud;burn area ratio
calculation;burn care;marching cube;quadrangular mesh;spatial
filter;surface area calculation;triangular
mesh;Calibration;Cameras;Color;Image
segmentation;Sensors;Skin;Three-dimensional displays},
doi={10.1109/ROBIO.2013.6739790},
month={Dec},}
@INPROCEEDINGS{6739535,
author={Z. Li and Z. Xia and G. Chen and Y. Gan and Y. Hu and J. Zhang},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Kinect-based robotic manipulation for door opening},
year={2013},
pages={656-660},
abstract={A general framework of robot manipulation planning for door
opening task is proposed in this paper. First grasp pattern, including
grasp point and method for various door handles, is defined. Second
door-opening pattern, including task planning and trajectory planning,
is also proposed. This paper utilizes Kinect as a vision sensor to
collect point cloud data of the environment. Viewpoint Feature Histogram
(VFH) descriptors are used to recognize the type of the door handle.
Then corresponding parameters of grasp pattern and door-opening pattern
are extracted using Point Cloud processing algorithms. The robot follows
the corresponding grasp pattern to grasp the door handle and repeats the
motion trajectory to finish each sub operation. Two experiments were
conducted to validate the proposed framework.},
keywords={image sensors;manipulators;motion estimation;path
planning;robot vision;service robots;Kinect-based robotic
manipulation;VFH;door-opening pattern;grasp pattern;grasp point;motion
trajectory;point cloud processing algorithms;robot manipulation
planning;task planning;trajectory planning;viewpoint feature
histogram;vision sensor;Conferences;Grippers;Planning;Robot sensing
systems;Service robots;Trajectory},
doi={10.1109/ROBIO.2013.6739535},
month={Dec},}
@INPROCEEDINGS{6739720,
author={C. Jaramillo and I. Dryanovski and R. G. Valenti and J. Xiao},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={6-DoF pose localization in 3D point-cloud dense maps using a
monocular camera},
year={2013},
pages={1747-1752},
abstract={We present a 6-degree-of-freedom (6-DoF) pose localization
method for a monocular camera in a 3D point-cloud dense map prebuilt by
depth sensors (e.g., RGB-D sensor, laser scanner, etc.). We employ fast
and robust 2D feature detection on the real camera to be matched against
features from a virtual view. The virtual view (color and depth images)
is constructed by projecting the map's 3D points onto a plane using the
previous localized pose of the real camera. 2D-to-3D point
correspondences are obtained from the inherent relationship between the
real camera's 2D features and their matches on the virtual depth image
(projected 3D points). Thus, we can solve the Perspective-n-Point (PnP)
problem in order to find the relative pose between the real and virtual
cameras. With the help of RANSAC, the projection error is minimized even
further. Finally, the real camera's pose is solved with respect to the
map by a simple frame transformation. This procedure repeats for each
time step (except for the initial case). Our results indicate that a
monocular camera alone can be localized within the map in real-time (at
QVGA-resolution). Our method differentiates from others in that no chain
of poses is needed or kept. Our localization is not susceptible to drift
because the history of motion (odometry) is mostly independent over each
PnP + RANSAC solution, which throws away past errors. In fact, the
previous known pose only acts as a region of interest to associate 2D
features on the real image with 3D points in the map. The applications
of our proposed method are various, and perhaps it is a solution that
has not been attempted before.},
keywords={cameras;distance measurement;feature extraction;image
matching;optical scanners;3D point-cloud dense maps;6-DoF pose
localization;PnP problem;QVGA-resolution;RANSAC;RGB-D
sensor;degree-of-freedom;depth sensors;feature detection;laser
scanner;localized pose;monocular camera;odometry;perspective-n-point
problem;real image;virtual cameras;virtual depth image matching;virtual
view;Cameras;Feature extraction;Image color analysis;Robot vision
systems;Three-dimensional displays;Visualization},
doi={10.1109/ROBIO.2013.6739720},
month={Dec},}
@INPROCEEDINGS{6739536,
author={T. Kotthäuser and M. Divband Soorati and B. Mertsching},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Automatic reconstruction of polygonal room models from 3D point
clouds},
year={2013},
pages={661-667},
abstract={The implementation of context-bound tasks at a higher level of
abstraction often demands a semantic representation of the environment
and the enclosed objects. A fundamental step towards the creation of
rich semantic models is the abstraction of information from the raw and
oftentimes unordered sensor data. In this work, we introduce a novel
method for the abstraction of indoor environments as polyhedral models
of interior rooms yielding to 3D floor plans. 3D laser scans that are
acquired from a mobile robot serve as inputs for our method. First, the
point cloud is segmented and polygonized. The final model is extracted
by relating individual segments and determining the best configuration
for the given room. With the help of a room model, we can easily
discriminate between points that are located within the room and those
that are part of the room boundaries. This facilitates succeeding tasks
such as recognition of objects and furniture, detection of doorways as
well as place recognition.},
keywords={image segmentation;image sensors;mobile robots;object
recognition;robot vision;solid modelling;3D floor plans;3D laser
scans;3D point clouds;context-bound tasks;doorways detection;furniture
recognition;indoor environments abstraction;information abstraction
level;mobile robot;objects recognition;place recognition;point cloud
polygonization;point cloud segmentation;polygonal room models;polyhedral
models;room boundaries;semantic models;sensor data;Clutter;Layout;Robot
sensing systems;Semantics;Solid modeling;Three-dimensional displays},
doi={10.1109/ROBIO.2013.6739536},
month={Dec},}
@INPROCEEDINGS{6739534,
author={Q. Zhang and L. Kong and J. Zhao},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Real-time general object recognition for indoor robot based on PCL},
year={2013},
pages={651-655},
abstract={We present a system for real-time general object recognition
(gor) for indoor robot in complex scenes. A point cloud image containing
the object to be recognized from a Kinect sensor, for general object at
will, must be extracted a point cloud model of the object with the
Cluster Extraction method, and then we can compute the global features
of the object model, making up the model database after processing many
frame images. Here the global feature we used is Clustered Viewpoint
Feature Histogram (CVFH) feature from Point Cloud Library (PCL). For
real-time gor we must preprocess all the point cloud images streamed
from the Kinect into clusters based on a clustering threshold and the
min-max cluster sizes related to the size of the model, for reducing the
amount of the clusters and improving the processing speed, and also
compute the CVFH features of the clusters. For every cluster of a frame
image, we search the several nearer features from the model database
with the KNN method in the feature space, and we just consider the
nearest model. If the strings of the model name contain the strings of
the object to be recognized, it can be considered that we have
recognized the general object; otherwise, we compute another cluster
again and perform the above steps. The experiments showed that we had
achieved the real-time recognition, and ensured the speed and accuracy
for the gor.},
keywords={feature extraction;object recognition;pattern clustering;robot
vision;visual databases;CVFH feature;GOR;KNN method;Kinect
sensor;PCL;cluster extraction method;clustered viewpoint feature
histogram;clustering threshold;frame image processing;global
feature;indoor robot;min-max cluster size;model database;point cloud
image;point cloud library;real-time general object
recognition;Computational modeling;Databases;Histograms;Object
recognition;Real-time systems;Robots;Three-dimensional displays},
doi={10.1109/ROBIO.2013.6739534},
month={Dec},}
@INPROCEEDINGS{6739625,
author={X. Zhang and L. Li and D. Tu},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Point cloud registration with 2D and 3D fusion information on
mobile robot integrated vision system},
year={2013},
pages={1187-1192},
abstract={Multi-view point clouds registration method based on the 2D
and 3D fusion information is proposed. First the joint calibration of
the CCD camera and 3D laser radar system is conducted to fuse the image
information and range information. Then, matching feature points in the
images are obtained with the SIFT algorithm. Based on the joint
calibration results and the images feature matching points, the point
clouds feature matching points are determined. After that, the initial
value of the different view point cloud space transform is calculated
with the RANSAC, and ICP algorithm is adopted to optimize the results.
Finally, the proposed method was implemented on a mobile robot and
verified effective in the experiment.},
keywords={cameras;feature extraction;image fusion;image matching;image
registration;iterative methods;mobile robots;robot vision;2D fusion
information;3D fusion information;3D laser radar system;CCD camera;ICP
algorithm;RANSAC;SIFT algorithm;charge-coupled device camera;feature
points matching;image information;iterative closest point
algorithm;mobile robot integrated vision system;multiview point clouds
registration method;point cloud space transform;range
information;scale-invariant feature transform;Calibration;Cameras;Laser
fusion;Laser radar;Radar imaging;Three-dimensional displays},
doi={10.1109/ROBIO.2013.6739625},
month={Dec},}
@INPROCEEDINGS{6739518,
author={M. Jin and H. Gu and S. Fan and Y. Zhang and H. Liu},
booktitle={2013 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Object shape recognition approach for sparse point clouds from
tactile exploration},
year={2013},
pages={558-562},
abstract={In this paper a novel approach is proposed for tactile shape
recognition, which uses tactile point location and normal information.
Superquadric functions are applied to construct several shape primitives
and k-means unsupervised clustering method is used to partition the
objects as several patches. By extracting geometrical features from each
patch and rearranging features, object feature vectors are constructed
for Gaussian process (GP) classifier to identify object shapes.
Simulations results prove that our approach can achieve a high
recognition rate in object shape classification task from sparse and
noisy tactile point clouds.},
keywords={Gaussian processes;computer graphics;feature
extraction;geometry;haptic interfaces;image classification;object
recognition;pattern clustering;shape recognition;GP classifier;Gaussian
process classifier;geometrical feature extraction;k-means unsupervised
clustering method;noisy tactile point clouds;object feature
vectors;object partition;object shape classification task;object shape
identification;object shape recognition;recognition rate;shape
primitives;sparse tactile point clouds;superquadric functions;tactile
exploration;tactile point location;tactile shape
recognition;Accuracy;Feature extraction;Shape;Tactile sensors;Vectors},
doi={10.1109/ROBIO.2013.6739518},
month={Dec},}
@INPROCEEDINGS{6766588,
author={Yu-Cheol Lee and Seung-Hwan Park},
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)},
title={3D map building method with mobile mapping system in indoor
environments},
year={2013},
pages={1-7},
abstract={This paper presents three dimensional (3D) map building method
for the intelligent vehicles based on accurate indoor localization using
a mobile mapping system (MMS) that is equipped with perception sensors
consist of a wheel odometer, a laser range finder (LRF), and two
projected texture stereo (PTS) cameras. The environmental data measured
by perception sensors are stored in the node units according to a
certain distance interval. In order to estimate the positions of the MMS
using the relationship among nodes, the localization method is divided
into two parts, front-end (map-based scan matching) and back-end
(graph-based optimization). The estimated positions are used to build
the grid-based map and the point cloud dataset, respectively as the 2D
and the 3D maps through the mapping process (Bayesian model). An
experiment has been performed in office environment (indoor) to verify
the effectiveness of the proposed method. Experimental results show the
high precision of 3D point cloud dataset that can be used for various
applications including navigation of intelligent vehicles and
pedestrians in indoor evironments.},
keywords={Bayes methods;SLAM (robots);cameras;graph theory;laser
ranging;mobile robots;optimisation;robot vision;3D map building
method;3D point cloud dataset;Bayesian model;LRF;MMS;PTS
cameras;environmental data;graph-based optimization;grid-based
map;indoor environments;indoor localization;intelligent vehicles;laser
range finder;mapping process;mobile mapping
system;pedestrians;perception sensors;texture stereo cameras;wheel
odometer;Buildings;Feature extraction;Mobile
communication;Optimization;Sensors;Three-dimensional displays;Vehicles},
doi={10.1109/ICAR.2013.6766588},
month={Nov},}
@INPROCEEDINGS{6766587,
author={H. Houshiar and D. Borrmann and J. Elseberg and A. Nüchter},
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)},
title={Panorama based point cloud reduction and registration},
year={2013},
pages={1-8},
abstract={To reconstruct environments 3D point clouds acquired by laser
scanners are registered. This is an important but also time consuming
part of any mapping system for mobile robots. The time needed for
mapping is drastically reduced when the size of the input data is
reduced. This paper examines different ways of reducing the size of
point clouds without losing vital information for the matching process.
We present novel point cloud reduction methods on the basis of panorama
images. It is shown that the reduced point clouds are ideally suited for
feature based registration on panorama images. We evaluate the presented
reduction methods based on their effect on the performance of the
registration algorithm.},
keywords={SLAM (robots);image matching;image reconstruction;image
registration;mobile robots;optical scanners;3D point cloud
reconstruction;feature based registration;laser scanners;mapping
system;matching process;mobile robots;panorama based point cloud
reduction;panorama based point cloud registration;panorama
images;Feature extraction;Image resolution;Laser modes;Measurement by
laser beam;Octrees;Surface emitting lasers;Three-dimensional displays},
doi={10.1109/ICAR.2013.6766587},
month={Nov},}
@INPROCEEDINGS{6766575,
author={L. A. Contreras and A. Pacheco-Ortega and J. I. Figueroa and W.
W. Mayol-Cuevas and J. Savage},
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)},
title={Object detection via receptive field co-occurrence and spatial
cloud-point data},
year={2013},
pages={1-8},
abstract={The use of image and spatial information together in mobile
robots systems it is a promising field, due to the enhanced level of
discrimination and efficiency that can be gained. In this paper we
employ an RGB-D camera for object detection and clustering and develop
methods that combine the two strands of information: first we cluster
potential objects by means of their spatial position and then link
geometry and co-occurrence histograms to enable reliable object
detection. Experiments and design parameters are presented for example
scenarios of object detection under clutter.},
keywords={mobile robots;object detection;robot vision;RGB-D
camera;cooccurrence histograms;link geometry;mobile robots
systems;receptive field cooccurrence;reliable object detection;spatial
cloud point data;spatial information;spatial position;Cameras;Feature
extraction;Histograms;Object detection;Reliability;Three-dimensional
displays;Vectors},
doi={10.1109/ICAR.2013.6766575},
month={Nov},}
@INPROCEEDINGS{6766550,
author={G. G. Demisse and D. Borrmann and A. Nüchter},
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)},
title={Interpreting thermal 3D models of indoor environments for energy
efficiency},
year={2013},
pages={1-8},
abstract={In recent years, 3D models of buildings are used in
maintenance and inspection, preservation, and other building related
applications. However, the usage of these models is limited, because
most models are pure representations with no or little associated
semantics. In this paper, we present a pipeline of techniques used for
interior interpretation, object detection, and adding energy related
semantics to windows of a 3D thermal model. A sequence of algorithms is
presented for building the fundamental semantics of a 3D model.
Furthermore, a Markov Random Field is used to model the temperature
distribution of detected windows to further label the windows as either
open, closed or damaged.},
keywords={Markov processes;energy conservation;inspection;maintenance
engineering;power engineering computing;solid modelling;3D thermal
model;Markov random field;buildings 3D models;energy efficiency;indoor
environments;interior interpretation;object detection;temperature
distribution;Cameras;Clouds;Data models;Floors;Heating;Labeling;Solid
modeling;3D thermal model;Boltzmann distribution;Energy
efficiency;energy function;window detection},
doi={10.1109/ICAR.2013.6766550},
month={Nov},}
@INPROCEEDINGS{6766495,
author={I. Gori and U. Pattacini and V. Tikhanoff and G. Metta},
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)},
title={Ranking the good points: A comprehensive method for humanoid
robots to grasp unknown objects},
year={2013},
pages={1-7},
abstract={We propose a grasping pipeline to deal with unknown objects in
the real world. We focus on power grasp, which is characterized by large
areas of contact between the object and the surfaces of the palm and
fingers. Our method seeks object regions that match the curvature of the
robot's palm. The entire procedure relies on binocular vision, which
provides a 3D point cloud of the visible part of the object. The
obtained point cloud is segmented in smooth surfaces. A score function
measures the quality of the graspable points on the basis of the surface
they belong to. A component of the score function is learned from
experience and it is used to map the curvature of the object surfaces to
the curvature of the robot's hand.The user can further provide top-down
information on the preferred grasping regions. We guarantee the
feasibility of a chosen hand configuration by measuring its
manipulability. We prove the effectiveness of the proposed approach by
tasking a humanoid robot to grasp a number of unknown real objects.},
keywords={dexterous manipulators;humanoid robots;image
segmentation;object detection;robot vision;3D point cloud;binocular
vision;comprehensive method;curvature mapping;grasping pipeline;hand
configuration;humanoid robots;manipulability;point cloud
segmentation;power grasp;robot palm curvature;score function;unknown
object
grasping;Grasping;Joints;Pipelines;Reliability;Robots;Shape;Three-dimensional
displays},
doi={10.1109/ICAR.2013.6766495},
month={Nov},}
@INPROCEEDINGS{6758553,
author={S. Brossette and J. Vaillant and F. Keith and A. Escande and A.
Kheddar},
booktitle={2013 6th IEEE Conference on Robotics, Automation and
Mechatronics (RAM)},
title={Point-cloud multi-contact planning for humanoids: Preliminary
results},
year={2013},
pages={19-24},
abstract={We present preliminary results in porting our multi-contact
non-gaited motion planning framework to operate in real environments
where the surroundings are acquired using an embedded camera together
with a depth map sensor. We consider the robot to have no a priori
knowledge of the environment, and propose a scheme to extract the
information relevant for planning from an acquired point cloud. This
yield the basis of an egocentric on-the-fly multi-contact planner. We
then demonstrate its capacity with two simulation scenarios involving an
HRP-2 robot in various environment before discussing some issues to be
addressed in our quest to achieve a close loop between planning and
execution in an environment explored through embedded sensors.},
keywords={cameras;humanoid robots;image sensors;path planning;HRP-2
robot;depth map sensor;egocentric on-the-fly multi-contact
planner;embedded camera;embedded sensors;humanoid robots;information
extraction;multicontact nongaited motion planning framework;point-cloud
multicontact planning;Cameras;Planning;Robot sensing systems;Solid
modeling;Surface treatment;Three-dimensional displays},
doi={10.1109/RAM.2013.6758553},
ISSN={2158-2181},
month={Nov},}
@INPROCEEDINGS{6758559,
author={D. Gao and T. F. Lu and S. Grainger},
booktitle={2013 6th IEEE Conference on Robotics, Automation and
Mechatronics (RAM)},
title={Post identification and location derivation in vineyards through
point clouds using cylinder extraction and density clustering},
year={2013},
pages={55-60},
abstract={An automatic pruning machine is desirable due to the
limitations and drawbacks of current grapevine pruning methods. It
mitigates the issue of skilled worker shortages and reduces overall
labour cost. To achieve autonomous grapevine pruning accurately and
effectively, it is crucial to identify and locate posts, cordons and
canes, which are the main objects for automatic pruning operations. In
this paper, a new method is proposed to automatically identify the post
and derive its location using point clouds. This method adopted the
advantages of cylinder extraction and density clustering, and combined
the features of cylinder and density for identification purposes. The
results of applying this method to different data sets in vineyards are
presented and its effectiveness is illustrated.},
keywords={agricultural machinery;feature extraction;image
recognition;pattern clustering;automatic pruning machine;automatic
pruning operations;autonomous grapevine pruning;cylinder
extraction;density clustering;point clouds;post automatic
identification;post location derivation;vineyards;Accuracy;Feature
extraction;Machine vision;Manuals;Noise;Pipelines;Sensors;cylinder
extraction;density clustering;grapevine pruning;point clouds},
doi={10.1109/RAM.2013.6758559},
ISSN={2158-2181},
month={Nov},}
@INPROCEEDINGS{6729280,
author={K. Salhi and A. M. Alimi},
booktitle={2013 International Conference on Individual and Collective
Behaviors in Robotics (ICBR)},
title={Fuzzy-PID hybrid controller for mobile robot using point cloud
and low cost depth sensor},
year={2013},
pages={92-97},
abstract={In mobile robots, motion control systems play an important
role to assume trajectory planning and obstacle avoidance.
Proportional-Integral-Derivative (PID) controllers are the most popular
controller used in industrial control systems including mobile robots.
The PID controller is developed based on the linear control theory but
it gives inconsistent performance for different condition. In order to
overcome this problem, we propose a Fuzzy-tuned PID controller in which
the PID parameters are learned, adapted and changed thanks to the fuzzy
system. The PID inputs are given by the Kinect sensor after being
processed by the point cloud library. The effectiveness of this method
is evaluated experimentally in real time using the mobile robot iRobot
Create.},
keywords={collision avoidance;fuzzy control;fuzzy systems;image
sensors;mobile robots;motion control;three-term control;trajectory
control;Kinect sensor;PID parameters;fuzzy PID hybrid controller;fuzzy
system;fuzzy tuned PID controller;industrial control systems;linear
control theory;low cost depth sensor;mobile robot iRobot Create;mobile
robots;motion control systems;obstacle avoidance;point cloud
library;proportional integral derivative controllers;trajectory
planning;Libraries;Process control;Fuzzy Logic Controller;Irobot Create
robot;Kinect;Point Cloud;Proportional Integral Derivative controller},
doi={10.1109/ICBR.2013.6729280},
month={Dec},}
@INPROCEEDINGS{6722575,
author={V. M. Cedeno-Campos and P. A. Trodden and T. J. Dodd and J. Heley},
booktitle={2013 IEEE International Conference on Systems, Man, and
Cybernetics},
title={Highly Flexible Self-Reconfigurable Systems for Rapid Layout
Formation to Offer Manufacturing Services},
year={2013},
pages={4819-4824},
abstract={Due to the changing trend in manufacturing field from mass
production to mass customization it is important to offer new products
at low cost and fast time to market to keep competitive. This paper
addresses the problem of manufacturing services with fast production
time, for multiple products and capacity for low and high production
volumes. To provide these services, a novel framework, INTREPID, is
proposed based on the idea of Self-Reconfigurable Manufacturing Systems
(S-RMS). A factory that facilitates manufacturing layout reconfiguration
and mobile production resources that can self-reconfigure depending on
the product is proposed. The full scope of this framework is to provide
global services through the cloud and a public manufacturing network.
This will allow production in any country without huge investment for
entities like SMEs, startups and even entrepreneurships.},
keywords={cloud computing;investment;mass production;product
customisation;INTREPID;S-RMS;SME;cloud
computing;entrepreneurship;flexible self-reconfigurable systems;global
services;investment;manufacturing field;manufacturing layout
reconfiguration;manufacturing services;mass customization;mass
production;mobile production resources;production time;production
volumes;public manufacturing network;rapid layout
formation;self-reconfigurable manufacturing
systems;self-reconfigure;startups;Layout;Manufacturing;Materials;Mobile
communication;Production facilities;Robots;Reconfigurable manufacturing
systems;cloud manufacturing;customized make-to-order products;fast time
to market;production layout self-reconfiguration;robotic mobile
manipulator;robots self-organization;service-oriented manufacturing},
doi={10.1109/SMC.2013.820},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{6723262,
author={S. V. Salinas and B. N. Chew and A. Müller and B. N. Holben and
S. C. Liew},
booktitle={2013 IEEE International Geoscience and Remote Sensing
Symposium - IGARSS},
title={First results from AERONET mini-dragon photometer network set-up
at Singapore},
year={2013},
pages={2238-2241},
abstract={We report our first photometric measurements of aerosol
optical depth from AERONET's mini-DRAGON sites at Singapore performed
over the months of August and September 2012. Multi-spectral
measurements of aerosol optical depth provide essential spectral
information to obtain and/or retrieve the so-called Angstrom exponent
number which is an essential parameter for inferring aerosol particle
size regime. Based on the range of variability of Angstrom exponent
number and aerosol optical depth, various aerosol types present in the
local environment, can be identified. Special emphasis is placed on
detecting the possible presence of external sources of aerosols such as
from trans-boundary smoke originating from regional biomass burning
episodes which is prevalent during this time of the year.},
keywords={aerosols;atmospheric optics;photometers;remote sensing;AD 2012
08;AD 2012 09;AERONET miniDRAGON photometer network;Angstrom exponent
number;Singapore;aerosol optical depth;aerosol particle size
regime;multispectral measurement;photometric measurement;transboundary
smoke;Aerosols;Biomass;Biomedical optical imaging;Clouds;Optical
imaging;Optical sensors;Optical variables control;Aerosols;South East
Asia;biomass burning;photometry},
doi={10.1109/IGARSS.2013.6723262},
ISSN={2153-6996},
month={July},}
@INPROCEEDINGS{6719358,
author={B. Cafaro and M. Gianni and F. Pirri and M. Ruiz and A. Sinha},
booktitle={2013 IEEE International Symposium on Safety, Security, and
Rescue Robotics (SSRR)},
title={Terrain traversability in rescue environments},
year={2013},
pages={1-8},
abstract={3D Terrain understanding and structure estimation is a crucial
issue for robots navigating rescue scenarios. Large scale 3D point
clouds, even if crisp and yielding a detailed representation of the
scene, provide no information about what is ground, and what is top,
what can be surmounted and what can be not, what can be crossed, and
what is too deep to be traversed. In this work, we propose a new
preliminary method for point cloud structuring, leading to the
definition of a traversability map labeled with a cost that specifies
how far is the considered region from a traversable one. The
representation comes with a real-time algorithm that can be used for the
safe navigation of a specific robot, according to its own limitations or
constraints. Here, by robot constraints, we mean the length, height,
weight of the robot, together with its kinematics constraints (in terms
of ground mobility). We present results of the method with experiments
taken on different scenarios, furthermore we illustrate the pros and
contras of relying only on points cloud data set, without resorting to a
surface reconstruction.},
keywords={image representation;natural scenes;navigation;rescue
robots;robot kinematics;terrain mapping;3D point cloud partitioning;3D
point cloud structure;3D terrain traversability map;real-time
algorithm;rescue environments;robot kinematics constraints;robot
navigation;scene representation;Clouds;Estimation;Path
planning;Real-time systems;Robots;Stability analysis;Three-dimensional
displays;3D point cloud partitioning;3D point cloud structure;terrain
analysis;terrain cost map;terrain traversability map},
doi={10.1109/SSRR.2013.6719358},
ISSN={2374-3247},
month={Oct},}
@INPROCEEDINGS{6706719,
author={S. Orts-Escolano and V. Morell and J. García-Rodríguez and M.
Cazorla},
booktitle={The 2013 International Joint Conference on Neural Networks
(IJCNN)},
title={Point cloud data filtering and downsampling using growing neural
gas},
year={2013},
pages={1-8},
abstract={3D sensors provide valuable information for mobile robotic
tasks like scene classification or object recognition, but these sensors
often produce noisy data that makes impossible applying classical
keypoint detection and feature extraction techniques. Therefore, noise
removal and downsampling have become essential steps in 3D data
processing. In this work, we propose the use of a 3D filtering and
downsampling technique based on a Growing Neural Gas (GNG) network. GNG
method is able to deal with outliers presents in the input data. These
features allows to represent 3D spaces, obtaining an induced Delaunay
Triangulation of the input space. Experiments show how GNG method yields
better input space adaptation to noisy data than other filtering and
downsampling methods like Voxel Grid. It is also demonstrated how the
state-of-the-art keypoint detectors improve their performance using
filtered data with GNG network. Descriptors extracted on improved
keypoints perform better matching in robotics applications as 3D scene
registration.},
keywords={feature extraction;image classification;image
registration;image sensors;mesh generation;mobile robots;neural
nets;object recognition;robot vision;3D filtering;3D scene
registration;3D sensors;3D spaces;Delaunay triangulation;GNG;feature
extraction techniques;growing neural gas;keypoint detection;mobile
robotic tasks;noisy data;object recognition;point cloud data
downsampling;point cloud data filtering;robotics applications;scene
classification;voxel grid;Detectors;Feature
extraction;Neurons;Noise;Solid modeling;Three-dimensional displays},
doi={10.1109/IJCNN.2013.6706719},
ISSN={2161-4393},
month={Aug},}
@INPROCEEDINGS{6696944,
author={T. Looi and B. Yeung and M. Umasthan and J. Drake},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={KidsArm #x2014; An image-guided pediatric anastomosis robot},
year={2013},
pages={4105-4110},
abstract={Minimally invasive surgery (MIS) revolutionized surgery by
drastically reducing patient recovery times by allowing surgeons to
perform procedures through a series of small incisions. However, MIS has
also increased the complexity of the tasks as tools did not have the
same degrees of freedom and dexterity compared to open procedures. In
particular, pediatric patients pose a unique challenge as they have
smaller volumes and different tissue properties. Our group designed
KidsArm, an image-guided pediatric surgical robot, to automate
anastomosis. KidsArm is single port anastomosis tool that uses a pair of
stereo cameras to generate a 3D point cloud to guide the tool tip and
apply a series of sutures. The system was designed to be minimally
invasive and constrained by standard pediatric trocar sizes while also
being automated. An image processing system was created to extract and
track surface features on the simulated tissue samples while providing
feedback to the robot controller. The system was tested on two
scenarios: side-to-side and end-to-end silicone samples. KidsArm
successfully applied 3 sutures autonomously on the side-to-side scenario
however the end-to-end scenario proved to be more difficult due to
greater deformation and workspace restrictions. However, KidsArm
demonstrates that it is feasible for a robot to autonomously perform
anastomosis. More work will be required to accelerate the process and
characterize the behavior with tissue samples.},
keywords={image sensors;medical robotics;robot vision;stereo image
processing;surgery;3D point cloud;KidsArm;MIS;Minimally invasive
surgery;end-to-end silicone samples;image processing system;image-guided
pediatric anastomosis robot;image-guided pediatric surgical robot;open
procedures;patient recovery times;pediatric patients;pediatric trocar
sizes;robot controller;side-to-side silicone samples;single port
anastomosis tool;small incisions;stereo cameras;surface features;tissue
properties;tissue samples;Cameras;Feature
extraction;Joints;Robots;Surgery;Three-dimensional displays},
doi={10.1109/IROS.2013.6696944},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6697124,
author={J. Zhang and A. Chambers and S. Maeta and M. Bergerman and S.
Singh},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={3D perception for accurate row following: Methodology and results},
year={2013},
pages={5306-5313},
abstract={Rows of trees such as in orchards, planted in straight
parallel lines can provide navigation cues for autonomous machines that
operate in between them. When the tree canopies are well managed, tree
rows appear similar to corridor walls and a simple 2D sensing scheme
suffices. However, when the tree canopies are three dimensional, or
ground vegetation occludes tree trunks, it is necessary to use a three
dimensional sensing mode. An additional complication in prolific
canopies is that GPS is not reliable and hence is not suitable to
register data from sensors onboard a traversing vehicle. Here, we
present a method to register 3D data from a lidar sensor onboard a
vehicle that must accurately determine its pose relative to the rows. We
first register point cloud into a common reference frame and then
determine the position of tree rows and trunks in the vicinity to
determine the vehicle pose. Our method is tested online and with data
from commercial orchards. Experimental results show that the accuracy is
sufficient to enable accurate traversal between tree rows even when tree
canopies do not approximate planar walls.},
keywords={computer graphics;mobile robots;optical information
processing;optical radar;optical sensors;path planning;remote sensing by
laser beam;vegetation;2D sensing scheme suffix;3D data registration;3D
perception;3D sensing mode;accurate row following;autonomous
machine;commercial orchards;ground vegetation;lidar sensor
onboard;navigation cues;prolific canopy;register point cloud;tree
canopies;tree rows position determination;tree trunk occlusion;trunks
position determination;vehicle pose determination;Laser
radar;Lasers;Measurement by laser beam;Roads;Three-dimensional
displays;Vegetation;Vehicles},
doi={10.1109/IROS.2013.6697124},
ISSN={2153-0858},
month={Nov},}
@INPROCEEDINGS{6681257,
author={Y. Cai and Z. Chen and H. Min},
booktitle={2013 Eighth International Conference on P2P, Parallel, Grid,
Cloud and Internet Computing},
title={Improving Particle Swarm Optimization Algorithm for Distributed
Sensing and Search},
year={2013},
pages={373-379},
abstract={Distributed coordination is critical for a multi-robot system
in collective cleanup task under a dynamic environment. In traditional
methods, robots easily drop into premature convergence. In this paper,
we propose a swarm intelligence based algorithm to reduce the
expectation time for searching targets and removing. We modify the
traditional PSO algorithm with a random factor to tackle premature
convergence problem, and it can achieve a significant improvement in
multi-robot system. The proposed method has been implemented on
self-developed simulator for searching task. The simulation results
demonstrate the feasibility, robustness, and scalability of our proposed
method than previous methods.},
keywords={convergence;multi-robot systems;particle swarm
optimisation;rescue robots;PSO algorithm;collective cleanup
task;distributed coordination;distributed sensing and search;multi-robot
system;particle swarm optimization algorithm;premature convergence
problem;self-developed simulator;swarm intelligence based
algorithm;Convergence;Heuristic algorithms;Multi-robot systems;Particle
swarm optimization;Robot kinematics;Robot sensing systems;Collective
cleanup;PSO algorithm;Swarm intelligence;Swarm robotics},
doi={10.1109/3PGCIC.2013.64},
month={Oct},}
@INPROCEEDINGS{6669006,
author={T. Tsuji and N. Kurita and M. Yamada and S. Sakaino and Y. Kaneko},
booktitle={2013 IEEE Region 10 Humanitarian Technology Conference},
title={Rehabilitation database based on haptic signal processing},
year={2013},
pages={13-18},
abstract={The growth of the internet brings up new keywords for the
information technology, such as cloud computing and big data. The
increasing data in the information society has a potential to offer new
insight into the real world. On the other hand, the increasing demand
for rehabilitation of aged people has resulted in increased demand for
physical therapy robots. Therefore, this paper proposes the construction
of rehabilitation database extending the concept of medical cloud
technologies. We discuss the possibility of establishing a new
validation methodology by generating database created using the data
collected with rehabilitation equipment. Statistical processing was
applied to the data collected using the rehabilitation equipment, for
investigating an example of a validation method. The results show that a
new knowledge for physical therapy can be extracted through a
statistical evaluation with database compiled from quantitative sensor
information.},
keywords={cloud computing;database management systems;haptic
interfaces;medical information systems;medical robotics;patient
treatment;sensor fusion;signal processing;statistical
analysis;Internet;aged people rehabilitation;haptic signal
processing;information society;information technology;medical cloud
technologies;physical therapy robots;quantitative sensor
information;rehabilitation database;rehabilitation equipment;statistical
evaluation;statistical processing;Databases;Force;Medical
treatment;Muscles;Pneumatic systems;Robot sensing systems;biarticular
muscle;haptics;movement therapy;physical therapy robot;pneumatic
artificial muscle},
doi={10.1109/R10-HTC.2013.6669006},
month={Aug},}
@INPROCEEDINGS{6664326,
author={L. Xie and S. Ni and Z. Wang},
booktitle={2012 IEEE 2nd International Conference on Cloud Computing and
Intelligence Systems},
title={Research and designed of a structured scalable robot control
system based on real-time bus},
year={2012},
volume={02},
pages={994-998},
abstract={Aimed at the problems of poor transferring capabilities and
maintainability in heterogeneous robot platforms, a structured scalable
robot control system based on real-time bus, such as RS485bus is
proposed, which can be adapted to a number of heterogeneous robot
platform. The hardware and software design is elaborated and using this,
a Guide Robot is developed to prove the interchangeability, adaptation
and extensibility of the system.},
keywords={control engineering computing;field buses;robots;software
architecture;RS485 bus;guide robot;hardware design;heterogeneous robot
platform;software design;structured scalable robot control system;DC
motors;Hardware;Image color analysis;Robots;Servomotors;Software;Speech
recognition;Interaction interface;Modularized hardware;Reusable
software;Robotic architecture},
doi={10.1109/CCIS.2012.6664326},
ISSN={2376-5933},
month={Oct},}
@INPROCEEDINGS{6650496,
author={S. Cockrell and G. Lee and W. Newman},
booktitle={2013 IEEE 13th International Conference on Rehabilitation
Robotics (ICORR)},
title={Determining navigability of terrain using point cloud data},
year={2013},
pages={1-6},
abstract={This paper presents an algorithm to identify features of the
navigation surface in front of a wheeled robot. Recent advances in
mobile robotics have brought about the development of smart wheelchairs
to assist disabled people, allowing them to be more independent. These
robots have a human occupant and operate in real environments where they
must be able to detect hazards like holes, stairs, or obstacles.
Furthermore, to ensure safe navigation, wheelchairs often need to locate
and navigate on ramps. The algorithm is implemented on data from a
Kinect and can effectively identify these features, increasing occupant
safety and allowing for a smoother ride.},
keywords={collision avoidance;handicapped aids;image sensors;medical
robotics;mobile robots;robot vision;Kinect;disabled people assist;human
occupant;mobile robotics;navigation surface feature;point cloud
data;robot navigation;smart wheelchairs;terrain navigability;wheeled
robot;Cameras;Collision avoidance;Floors;Mobile
robots;Roads;Wheelchairs;Kinect;drivable surfaces;mobile
robotics;obstacle detection;1},
doi={10.1109/ICORR.2013.6650496},
ISSN={1945-7898},
month={June},}
@INPROCEEDINGS{6646095,
author={F. Oleari and D. Lodi Rizzini and S. Caselli},
booktitle={2013 IEEE 9th International Conference on Intelligent
Computer Communication and Processing (ICCP)},
title={A low-cost stereo system for 3D object recognition},
year={2013},
pages={127-132},
abstract={In this paper, we present a low-cost stereo vision system
designed for object recognition with FPFH point feature descriptors.
Image acquisition is performed using a pair of consumer market UVC
cameras costing less than 80 Euros, lacking synchronization signal and
without customizable optics. Nonetheless, the acquired point clouds are
sufficiently accurate to perform object recognition using FPFH features.
The recognition algorithm compares the point cluster extracted from the
current image pair with the models contained in a dataset. Experiments
show that the recognition rate is above 80% even when the object is
partially occluded.},
keywords={object recognition;stereo image processing;visual
perception;3D object recognition;FPFH point feature descriptors;consumer
market UVC cameras;image acquisition;low-cost stereo vision system;point
cluster;synchronization signal;Computational modeling;Feature
extraction;Object recognition;Sensors;Three-dimensional displays;Webcams},
doi={10.1109/ICCP.2013.6646095},
month={Sept},}
@INPROCEEDINGS{6631095,
author={P. Moghadam and M. Bosse and R. Zlot},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Line-based extrinsic calibration of range and image sensors},
year={2013},
pages={3685-3691},
abstract={Creating rich representations of environments requires
integration of multiple sensing modalities with complementary
characteristics such as range and imaging sensors. To precisely combine
multisensory information, the rigid transformation between different
sensor coordinate systems (i.e., extrinsic parameters) must be
estimated. The majority of existing extrinsic calibration techniques
require one or multiple planar calibration patterns (such as
checkerboards) to be observed simultaneously from the range and imaging
sensors. The main limitation of these approaches is that they require
modifying the scene with artificial targets. In this paper, we present a
novel algorithm for extrinsically calibrating a range sensor with
respect to an image sensor with no requirement of external artificial
targets. The proposed method exploits natural linear features in the
scene to precisely determine the rigid transformation between the
coordinate frames. First, a set of 3D lines (plane intersection and
boundary line segments) are extracted from the point cloud, and a set of
2D line segments are extracted from the image. Correspondences between
the 3D and 2D line segments are used as inputs to an optimization
problem which requires jointly estimating the relative translation and
rotation between the coordinate frames. The proposed method is not
limited to any particular types or configurations of sensors. To
demonstrate robustness, efficiency and generality of the presented
algorithm, we include results using various sensor configurations.},
keywords={calibration;distance measurement;feature extraction;image
sensors;natural scenes;nonlinear programming;sensor fusion;2D line
segments;3D lines;boundary line segments;coordinate frames;image
sensors;line-based extrinsic calibration;multiple sensing
modalities;multisensory information;natural linear features;optimization
problem;planar calibration patterns;plane intersection;point cloud;range
sensors;relative translation estimation;rigid transformation;rigid
transformation determination;rotation estimation;sensor coordinate
system estimation;Calibration;Cameras;Feature extraction;Image
segmentation;Robot sensing systems;Three-dimensional displays;Vectors},
doi={10.1109/ICRA.2013.6631095},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630569,
author={T. Mörwald and A. Richtsfeld and J. Prankl and M. Zillich and M.
Vincze},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Geometric data abstraction using B-splines for range image
segmentation},
year={2013},
pages={148-153},
abstract={With the availability of cheap and powerful RGB-D sensors
interest in 3D point cloud based methods has drastically increased. One
common prerequisite of these methods is to abstract away from raw point
cloud data, e.g. to planar patches, to reduce the amount of data and to
handle noise and clutter. We present a novel method to abstract RGB-D
sensor data to parametric surface models described by B-spline surfaces
and associated boundaries. Data is first pre-segmented into smooth
patches before B-spline surfaces are fitted. The best surface
representations of these patches are selected in a merging procedure.
Furthermore, we show how curve fitting estimates smooth boundaries and
improves the given sensor information compared to hand-labelled ground
truth annotation when using colour in addition to depth information. All
parts of the framework are open-source^1 and are evaluated on the object
segmentation database (OSD) also available online, showing accuracy and
usability of the proposed methods.},
keywords={clutter;curve fitting;image colour analysis;image
segmentation;splines (mathematics);stereo image processing;3D point
cloud based method;B-spline surfaces;OSD;RGB-D sensor data
abstraction;RGB-D sensors;clutter;curve fitting;depth
information;geometric data abstraction;hand-labelled ground truth
annotation;image colour;merging procedure;noise handling;object
segmentation database;parametric surface model;patch surface
representation;planar patches;range image segmentation;sensor
information;smooth boundaries;Data models;Image color analysis;Image
edge detection;Image segmentation;Sensors;Splines
(mathematics);Three-dimensional displays},
doi={10.1109/ICRA.2013.6630569},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6631252,
author={P. Morton and B. Douillard and J. Underwood},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Multi-sensor identity tracking with event graphs},
year={2013},
pages={4742-4748},
abstract={The ability to track moving objects is a key part of
autonomous robot operation in real-world environments. Whilst for many
tasks knowing the positions of objects may be sufficient, tracking the
identity of targets may also be desirable. When objects are well
separated preserving identities is trivial, however, the identities of
objects that pass close to one another may become confused. This paper
considers methods to maintain the identities of tracked objects using a
combination of LIDAR and video data. When objects are well separated,
they are tracked using location information from the LIDAR. When objects
move together and their identities cannot be resolved, interactions are
recorded and later resolved using appearance models. A vision based
approach is adapted for use with LIDAR data and a new method for
identity reasoning is proposed. The methods are validated on a dataset
comprising a total of 37906 manually labelled point cloud segments.},
keywords={graph theory;image fusion;image motion analysis;image
segmentation;mobile robots;optical radar;radar imaging;robot
vision;target tracking;LIDAR data;appearance models;autonomous robot
operation;event graphs;identity reasoning method;location
information;manually-labelled point cloud segments;moving object
tracking;multisensor identity tracking;object positions;real-world
environments;target identity tracking;video data;vision-based
approach;Cameras;Cognition;Image color analysis;Laser radar;Target
tracking;Trajectory},
doi={10.1109/ICRA.2013.6631252},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6631081,
author={J. Pan and I. A. Şucan and S. Chitta and D. Manocha},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Real-time collision detection and distance computation on point
cloud sensor data},
year={2013},
pages={3593-3599},
abstract={Most prior techniques for proximity computations are designed
for synthetic models and assume exact geometric representations.
However, real robots construct representations of the environment using
their sensors, and the generated representations are more cluttered and
less precise than synthetic models. Furthermore, this sensor data is
updated at high frequency. In this paper, we present new collision- and
distance-query algorithms, which can efficiently handle large amounts of
point cloud sensor data received at real-time rates. We present two
novel techniques to accelerate the computation of broad-phase data
structures: 1) we present a progressive technique that incrementally
computes a high-quality dynamic AABB tree for fast culling, and 2) we
directly use an octree representation of the point cloud data as a
proximity data structure. We assign a probability value to each leaf
node of the tree, and the algorithm computes the nodes corresponding to
high collision probability. In practice, our new approaches can be an
order of magnitude faster than previous methods. We demonstrate the
performance of the new methods on both synthetic data and on sensor data
collected using a Kinect™ for motion planning for a mobile manipulator
robot.},
keywords={collision avoidance;manipulators;mobile
robots;probability;broad-phase data structure;collision-query
algorithm;distance computation;distance-query algorithm;dynamic AABB
tree;geometric representation;mobile manipulator robot;motion
planning;octree representation;point cloud sensor data;probability
value;proximity computation;proximity data structure;real-time collision
detection;Collision avoidance;Heuristic
algorithms;Octrees;Pipelines;Robot sensing systems},
doi={10.1109/ICRA.2013.6631081},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630732,
author={M. Derry and B. Argall},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Automated doorway detection for assistive shared-control
wheelchairs},
year={2013},
pages={1254-1259},
abstract={This work presents an algorithm for rapid, automated detection
of open doorways using 3D point cloud data. The algorithm has been
developed in the context of shared-control powered wheelchairs in which
adaptive assistance is provided to individuals who otherwise might not
possess the fine motor control necessary to handle potentially
challenging activities, such as doorway traversal. In this context it is
important to go beyond the 2D laser scanner for open doorway detection,
for both safety reasons as well as opportunities for improved
shared-control behavior development. We evaluate the doorway detection
by systematically testing the performance on several doors and door
configurations from varied view points, using point clouds generated by
a Microsoft Kinect.},
keywords={handicapped aids;mobile robots;optical scanners;wheelchairs;2D
laser scanner;3D point cloud data;Microsoft Kinect;adaptive
assistance;assistive shared-control wheelchairs;automated
detection;automated doorway detection;doorway traversal;fine motor
control;open doorway detection;performance testing;robot;shared-control
behavior development;shared-control powered
wheelchairs;Cameras;Context;Navigation;Robot sensing
systems;Three-dimensional displays;Wheelchairs},
doi={10.1109/ICRA.2013.6630732},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630825,
author={P. Dames and V. Kumar},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Cooperative multi-target localization with noisy sensors},
year={2013},
pages={1877-1883},
abstract={This paper addresses the task of searching for an unknown
number of static targets within a known obstacle map using a team of
mobile robots equipped with noisy, limited field-of-view sensors. Such
sensors may fail to detect a subset of the visible targets or return
false positive detections. These measurement sets are used to localize
the targets using the Probability Hypothesis Density, or PHD, filter.
Robots communicate with each other on a local peer-to-peer basis and
with a server or the cloud via access points, exchanging measurements
and poses to update their belief about the targets and plan future
actions. The server provides a mechanism to collect and synthesize
information from all robots and to share the global, albeit
time-delayed, belief state to robots near access points. We design a
decentralized control scheme that exploits this communication
architecture and the PHD representation of the belief state.
Specifically, robots move to maximize mutual information between the
target set and measurements, both self-collected and those available by
accessing the server, balancing local exploration with sharing knowledge
across the team. Furthermore, robots coordinate their actions with other
robots exploring the same local region of the environment.},
keywords={cooperative systems;decentralised control;knowledge
management;mobile robots;multi-robot systems;object
detection;peer-to-peer computing;probability;sensors;PHD
representation;access points;action coordination;albeit time-delayed
state;belief state;communication architecture;cooperative multitarget
localization;decentralized control scheme design;global
state;information collection;information synthesis;knowledge
sharing;limited field-of-view sensors;local exploration;local
peer-to-peer communication;mobile robots;mutual information
maximization;noisy sensors;probability hypothesis density filters;return
false positive detections;static targets;subset detection;Robot sensing
systems},
doi={10.1109/ICRA.2013.6630825},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630715,
author={D. Held and J. Levinson and S. Thrun},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Precision tracking with sparse 3D and dense color 2D data},
year={2013},
pages={1138-1145},
abstract={Precision tracking is important for predicting the behavior of
other cars in autonomous driving. We present a novel method to combine
laser and camera data to achieve accurate velocity estimates of moving
vehicles. We combine sparse laser points with a high-resolution camera
image to obtain a dense colored point cloud. We use a color-augmented
search algorithm to align the dense color point clouds from successive
time frames for a moving vehicle, thereby obtaining a precise estimate
of the tracked vehicle's velocity. Using this alignment method, we
obtain velocity estimates at a much higher accuracy than previous
methods. Through pre-filtering, we are able to achieve near real time
results. We also present an online method for real-time use with
accuracies close to that of the full method. We present a novel approach
to quantitatively evaluate our velocity estimates by tracking a parked
car in a local reference frame in which it appears to be moving relative
to the ego vehicle. We use this evaluation method to automatically
quantitatively evaluate our tracking performance on 466 separate tracked
vehicles. Our method obtains a mean absolute velocity error of 0.27 m/s
and an RMS error of 0.47 m/s on this test set. We can also qualitatively
evaluate our method by building color 3D car models from moving
vehicles. We have thus demonstrated that our method can be used for
precision car tracking with applications to autonomous driving and
behavior modeling.},
keywords={automobiles;cameras;filtering theory;image colour
analysis;image motion analysis;image resolution;mean square error
methods;object tracking;optical radar;radar imaging;search
problems;traffic engineering computing;RMS error;automatic quantitative
evaluation;autonomous driving;camera data;car behavior prediction;color
3D car models;color-augmented search algorithm;dense color 2D data;dense
colored point cloud alignment;ego vehicle;high-resolution camera
image;laser data;local reference frame;mean absolute velocity
error;moving vehicle velocity estimation;precision car
tracking;prefiltering;sparse 3D data;sparse laser points;time
frames;tracking
performance;Accuracy;Cameras;Interpolation;Lasers;Three-dimensional
displays;Tracking;Vehicles},
doi={10.1109/ICRA.2013.6630715},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630714,
author={J. Schulman and A. Lee and J. Ho and P. Abbeel},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Tracking deformable objects with point clouds},
year={2013},
pages={1130-1137},
abstract={We introduce an algorithm for tracking deformable objects from
a sequence of point clouds. The proposed tracking algorithm is based on
a probabilistic generative model that incorporates observations of the
point cloud and the physical properties of the tracked object and its
environment. We propose a modified expectation maximization algorithm to
perform maximum a posteriori estimation to update the state estimate at
each time step. Our modification makes it practical to perform the
inference through calls to a physics simulation engine. This is
significant because (i) it allows for the use of highly optimized
physics simulation engines for the core computations of our tracking
algorithm, and (ii) it makes it possible to naturally, and efficiently,
account for physical constraints imposed by collisions, grasping
actions, and material properties in the observation updates. Even in the
presence of the relatively large occlusions that occur during
manipulation tasks, our algorithm is able to robustly track a variety of
types of deformable objects, including ones that are one-dimensional,
such as ropes; two-dimensional, such as cloth; and three-dimensional,
such as sponges. Our implementation can track these objects in real time.},
keywords={expectation-maximisation algorithm;object tracking;deformable
object tracking;manipulation tasks;maximum a posteriori
estimation;modified expectation maximization algorithm;physical
constraints;physics simulation engine;point clouds;probabilistic
generative model;Computational modeling;Inference
algorithms;Mathematical model;Noise;Physics;Probabilistic logic;Solid
modeling},
doi={10.1109/ICRA.2013.6630714},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6631180,
author={B. Kehoe and A. Matsukawa and S. Candido and J. Kuffner and K.
Goldberg},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Cloud-based robot grasping with the google object recognition
engine},
year={2013},
pages={4263-4270},
abstract={Rapidly expanding internet resources and wireless networking
have potential to liberate robots and automation systems from limited
onboard computation, memory, and software. “Cloud Robotics” describes an
approach that recognizes the wide availability of networking and
incorporates open-source elements to greatly extend earlier concepts of
“Online Robots” and “Networked Robots”. In this paper we consider how
cloud-based data and computation can facilitate 3D robot grasping. We
present a system architecture, implemented prototype, and initial
experimental data for a cloud-based robot grasping system that
incorporates a Willow Garage PR2 robot with onboard color and depth
cameras, Google's proprietary object recognition engine, the Point Cloud
Library (PCL) for pose estimation, Columbia University's GraspIt!
toolkit and OpenRAVE for 3D grasping and our prior approach to
sampling-based grasp analysis to address uncertainty in pose. We report
data from experiments in recognition (a recall rate of 80% for the
objects in our test set), pose estimation (failure rate under 14%), and
grasping (failure rate under 23%) and initial results on recall and
false positives in larger data sets using confidence measures.},
keywords={cloud computing;end effectors;object recognition;pose
estimation;public domain software;robot vision;3D robot
grasping;Columbia University;Google object recognition
engine;GraspIt!;OpenRAVE;PCL;Willow Garage PR2 robot;cloud-based
computation;cloud-based data;cloud-based robot grasping system;color
cameras;depth cameras;false positives;open-source toolkits;point cloud
library;pose estimation;pose uncertainty;sampling-based grasp
analysis;system architecture;Estimation;Google;Object
recognition;Robots;Servers;Three-dimensional displays;Training},
doi={10.1109/ICRA.2013.6631180},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630888,
author={Hanzhang Hu and D. Munoz and J. A. Bagnell and M. Hebert},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Efficient 3-D scene analysis from streaming data},
year={2013},
pages={2297-2304},
abstract={Rich scene understanding from 3-D point clouds is a
challenging task that requires contextual reasoning, which is typically
computationally expensive. The task is further complicated when we
expect the scene analysis algorithm to also efficiently handle data that
is continuously streamed from a sensor on a mobile robot. Hence, we are
typically forced to make a choice between 1) using a precise
representation of the scene at the cost of speed, or 2) making fast,
though inaccurate, approximations at the cost of increased
misclassifications. In this work, we demonstrate that we can achieve the
best of both worlds by using an efficient and simple representation of
the scene in conjunction with recent developments in structured
prediction in order to obtain both efficient and state-of-the-art
classifications. Furthermore, this efficient scene representation
naturally handles streaming data and provides a 300% to 500% speedup
over more precise representations.},
keywords={computer graphics;image representation;image sensors;inference
mechanisms;mobile robots;robot vision;3D point clouds;3D scene
analysis;contextual reasoning;continuously streamed data handling;mobile
robot;precise representation;rich scene understanding;scene analysis
algorithm;scene representation;streaming data;Algorithm design and
analysis;Data structures;Image analysis;Inference algorithms;Prediction
algorithms;Robot sensing systems},
doi={10.1109/ICRA.2013.6630888},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6631244,
author={P. Drews and S. C. da Silva Filho and L. F. Marcolino and P.
Núñez},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Fast and adaptive 3D change detection algorithm for autonomous
robots based on Gaussian Mixture Models},
year={2013},
pages={4685-4690},
abstract={Nowadays, the advance of the technology allows robots to
acquire dense point clouds decreasing the price and increasing the
performance. However, it is a hard task to deal with due to the large
amount of points, the redundancy and the noise. This paper proposes an
adaptable system to build a 3D feature model of point clouds using
Gaussian Mixture Models. These 3D models are used in order to detect
changes in the autonomous robot's working environment. The presented
work describes an efficient change detection system based on two
consecutive stages. First, a top-down approach estimates features using
Gaussian Mixture Models. The presented new approach improves the
performance of previous related works in terms of computational load and
robustness, nevertheless the system is selection criteria dependent.
Thus, the efficiency of different selection criteria are evaluated and
compared in this paper. Experimental results demonstrate that the
Minimum Distance Length (MDL) criteria outperforms the other studied
methods. In the second stage, a change detection method is performed
using the previously estimate Mixture of Gaussians. The proposed full
system is able to detect changes using Gaussian Mixture Models with a
reduced computational cost in relation to state-of-art algorithms.},
keywords={Gaussian processes;SLAM (robots);adaptive systems;mobile
robots;redundancy;3D feature model;3D models;Gaussian mixture models;MDL
criteria;adaptable system;adaptive 3D change detection
algorithm;autonomous robots;change detection method;change detection
system;dense point clouds;minimum distance length;redundancy;selection
criteria;Computational modeling;Navigation;Noise;Robots;Software;Solid
modeling;Three-dimensional displays},
doi={10.1109/ICRA.2013.6631244},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6631178,
author={R. P. de Figueiredo and P. Moreno and A. Bernardino and J.
Santos-Victor},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Multi-object detection and pose estimation in 3D point clouds: A
fast grid-based Bayesian Filter},
year={2013},
pages={4250-4255},
abstract={We address the problem of object detection and pose estimation
using 3D dense data in a multiple object library scenario.
State-of-the-art object detection and pose estimation methods are able
cope with background clutter and occlusion with acceptable noise levels
in the single object scenario. However, with multiple object libraries,
even moderate amount of noise lead to frequent object identity switches
and serious pose estimation errors. To attenuate these effects, we
propose a joint object-id and pose filtering approach using grid-based
Recursive Bayesian Filters (RBF). The grid method considers as state
variables the object label and its pose, and models the dynamics of the
filter with two “inertia” parameters: one for the object label and the
other for the object pose. Sensor noise characteristics are taken into
account with an observation noise parameter. To allow real-time
functionality we propose a selective update approach that dynamically
reduces the set of hypotheses evaluated at run time. We present results
in realistic scenarios and compare our approach with state-of-the-art
approaches in a three object problem, with significant performance
improvements.},
keywords={Bayes methods;computer graphics;filtering theory;image
sensors;object detection;pose estimation;recursive estimation;3D dense
data;3D point clouds;background clutter;fast grid-based Bayesian
filter;grid-based RBF;grid-based recursive Bayesian filters;inertia
parameters;joint object-id and pose filtering approach;multiobject
detection;multiple object library scenario;noise parameter;object
label;occlusion;performance improvements;pose estimation
errors;selective update approach;sensor noise characteristics;state
variables;Bayes methods;Computational
modeling;Detectors;Estimation;Mathematical model;Noise;Three-dimensional
displays},
doi={10.1109/ICRA.2013.6631178},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630654,
author={H. Almqvist and M. Magnusson and T. Stoyanov and A. J. Lilienthal},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Improving point-cloud accuracy from a moving platform in field
operations},
year={2013},
pages={733-738},
abstract={This paper presents a method for improving the quality of
distorted 3D point clouds made from a vehicle equipped with a laser
scanner moving over uneven terrain. Existing methods that use 3D
point-cloud data (for tasks such as mapping, localisation, and object
detection) typically assume that each point cloud is accurate. For
autonomous robots moving in rough terrain, it is often the case that the
vehicle moves a substantial amount during the acquisition of one point
cloud, in which case the data will be distorted. The method proposed in
this paper is capable of increasing the accuracy of 3D point clouds,
without assuming any specific features of the environment (such as
planar walls), without resorting to a “stop-scan-go” approach, and
without relying on specialised and expensive hardware. Each new point
cloud is matched to the previous using normal-distribution-transform
(NDT) registration, after which a mini-loop closure is performed with a
local, per-scan, graph-based SLAM method. The proposed method increases
the accuracy of both the measured platform trajectory and the point
cloud. The method is validated on both real-world and simulated data.},
keywords={SLAM (robots);graph theory;normal distribution;3D point-cloud
data;NDT registration;autonomous robots;graph-based SLAM
method;mini-loop closure;normal-distribution-transform
registration;point-cloud accuracy;stop-scan-go
approach;Accuracy;Clouds;Laser radar;Robot sensing
systems;Three-dimensional displays;Vehicles;Wheels},
doi={10.1109/ICRA.2013.6630654},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630640,
author={B. Peasley and S. Birchfield},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Replacing Projective Data Association with Lucas-Kanade for
KinectFusion},
year={2013},
pages={638-645},
abstract={We propose to overcome a significant limitation of the
KinectFusion algorithm, namely, its sole reliance upon geometric
information to estimate camera pose. Our approach uses both geometric
and color information in a direct manner that uses all the data in order
to perform the association of data between two RGBD point clouds. Data
association is performed by aligning the two color images associated
with the two point clouds by estimating a projective warp using the
Lucas-Kanade algorithm. This projective warp is then used to create a
correspondence map between the two point clouds, which is then used as
the data association for a point-to-plane error minimization. This
approach to correspondence allows camera tracking to be maintained
through areas of low geometric features. We show that our proposed LKDA
data association technique enables accurate scene reconstruction in
environments in which low geometric texture causes the existing approach
to fail, while at the same time demonstrating that the new technique
does not adversely affect results in environments in which the existing
technique succeeds.},
keywords={cameras;image colour analysis;image fusion;image
reconstruction;image texture;object tracking;pose
estimation;KinectFusion algorithm;LKDA data association
technique;Lucas-Kanade algorithm;RGBD point clouds;camera pose
estimation;camera tracking;color images;color information;correspondence
map;geometric features;geometric information;geometric
texture;point-to-plane error minimization;projective data
association;projective warp estimation;scene
reconstruction;Cameras;Feature extraction;Image color analysis;Iterative
closest point algorithm;Measurement;Personal digital
assistants;Three-dimensional displays},
doi={10.1109/ICRA.2013.6630640},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630713,
author={K. Hausman and F. Balint-Benczedi and D. Pangercic and Z. C.
Marton and R. Ueda and K. Okada and M. Beetz},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Tracking-based interactive segmentation of textureless objects},
year={2013},
pages={1122-1129},
abstract={This paper describes a textureless object segmentation
approach for autonomous service robots acting in human living
environments. The proposed system allows a robot to effectively segment
textureless objects in cluttered scenes by leveraging its manipulation
capabilities. In our pipeline, the cluttered scenes are first statically
segmented using state-of-the-art classification algorithm and then the
interactive segmentation is deployed in order to resolve this possibly
ambiguous static segmentation. In the second step the RGBD (RGB + Depth)
sparse features, estimated on the RGBD point cloud from the Kinect
sensor, are extracted and tracked while motion is induced into a scene.
Using the resulting feature poses, the features are then assigned to
their corresponding objects by means of a graph-based clustering
algorithm. In the final step, we reconstruct the dense models of the
objects from the previously clustered sparse RGBD features. We evaluated
the approach on a set of scenes which consist of various textureless
flat (e.g. box-like) and round (e.g. cylinder-like) objects and the
combinations thereof.},
keywords={dexterous manipulators;feature extraction;graph
theory;human-robot interaction;image classification;image
reconstruction;image segmentation;image sensors;interactive
systems;natural scenes;pattern clustering;robot vision;service
robots;Kinect sensor;RGBD point cloud;RGBD sparse features;autonomous
service robots;box-like objects;classification algorithm;cylinder-like
objects;dense object model reconstruction;feature poses;graph-based
clustering algorithm;human living environments;manipulation
capabilities;round objects;statically segmented cluttered
scenes;textureless flat;tracking-based interactive textureless object
segmentation;Clustering algorithms;Computational modeling;Feature
extraction;Motion segmentation;Three-dimensional displays;Tracking},
doi={10.1109/ICRA.2013.6630713},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6631103,
author={K. Harada and K. Nagata and T. Tsuji and N. Yamanobe and A.
Nakamura and Y. Kawai},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Probabilistic approach for object bin picking approximated by
cylinders},
year={2013},
pages={3742-3747},
abstract={This paper proposes a method for bin-picking for objects
without assuming the precise geometrical model of objects. We consider
the case where the shape of objects are not uniform but are similarly
approximated by cylinders. By using the point cloud of a single object,
we extract the probabilistic properties with respect to the difference
between an object and a cylinder and consider applying the probabilistic
properties to the pick-and-place motion planner of an object stacked on
a table. By using the probabilistic properties, we can also realize the
contact state where a finger maintain contact with the target object
while avoiding contact with other objects. We further consider
approximating the region occupied by fingers by a rectangular
parallelepiped. The pick-and-place motion is planned by using a set of
regions in combination with the probabilistic properties. Finally, the
effectiveness of the proposed method is confirmed by some numerical
examples and experimental result.},
keywords={bin packing;geometry;manipulators;path
planning;probability;object bin picking;pick-and-place motion
planner;precise geometrical model;probabilistic approach;rectangular
parallelepiped;Collision avoidance;Grasping;Planning;Probabilistic
logic;Robots;Shape;Vectors},
doi={10.1109/ICRA.2013.6631103},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630874,
author={A. Das and J. Servos and S. L. Waslander},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={3D scan registration using the Normal Distributions Transform
with ground segmentation and point cloud clustering},
year={2013},
pages={2207-2212},
abstract={The Normal Distributions Transform (NDT) scan registration
algorithm models the environment as a set of Gaussian distributions and
generates the Gaussians by discretizing the environment into voxels.
With the standard approach, the NDT algorithm has a tendency to have
poor convergence performance for even modest initial transformation
error. In this work, a segmented greedy cluster NDT (SGC-NDT) variant is
proposed, which uses natural features in the environment to generate
Gaussian clusters for the NDT algorithm. By segmenting the ground plane
and clustering the remaining features, the SGC-NDT approach results in a
smooth and continuous cost function which guarantees that the
optimization will converge. Experiments show that the SGC-NDT algorithm
results in scan registrations with higher accuracy and better
convergence properties when compared against other state-of-the- art
methods for both urban and forested environments.},
keywords={computer graphics;image registration;image representation;3D
scan registration;Gaussian distributions;NDT scan registration algorithm
models;SGC NDT variant;convergence performance;ground
segmentation;normal distributions transform;point cloud clustering;scan
registrations;segmented greedy cluster NDT;transformation
error;Accuracy;Clustering algorithms;Cost function;Gaussian
distribution;Iterative closest point algorithm;Lasers},
doi={10.1109/ICRA.2013.6630874},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630710,
author={F. T. Pokorny and J. A. Stork and D. Kragic},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={Grasping objects with holes: A topological approach},
year={2013},
pages={1100-1107},
abstract={This work proposes a topologically inspired approach for
generating robot grasps on objects with `holes'. Starting from a noisy
point-cloud, we generate a simplicial representation of an object of
interest and use a recently developed method for approximating shortest
homology generators to identify graspable loops. To control the movement
of the robot hand, a topologically motivated coordinate system is used
in order to wrap the hand around such loops. Finally, another concept
from topology - namely the Gauss linking integral - is adapted to serve
as evidence for secure caging grasps after a grasp has been executed. We
evaluate our approach in simulation on a Barrett hand using several
target objects of different sizes and shapes and present an initial
experiment with real sensor data.},
keywords={approximation theory;dexterous manipulators;integral
equations;topology;Barrett hand;Gauss linking integral;graspable loop
identification;noisy point-cloud;object grasping;robot grasp
generation;robot hand movement control;secure caging grasps;shortest
homology generator approximation;topologically motivated coordinate
system;topologically-inspired approach;Jacobian matrices;Noise
measurement;Robots},
doi={10.1109/ICRA.2013.6630710},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6630787,
author={K. C. Chan and C. K. Koh and C. S. G. Lee},
booktitle={2013 IEEE International Conference on Robotics and Automation},
title={A 3D-point-cloud feature for human-pose estimation},
year={2013},
pages={1623-1628},
abstract={Estimating human poses is an important step towards developing
robots that can understand human motions and improving their cognitive
capabilities. This paper presents a geometric feature for estimating
human poses from a 3D point cloud input. The proposed feature can be
considered as an extension of the idea of visual features, such as
color/edge, of color/grayscale images, and it contains the geometric
structure of the point cloud. It is derived by arranging the 3D points
into a tree structure, which preserves the global and local properties
of the 3D points. Shown experimentally, the tree structure (spatial
ordering) is particularly important for estimating human poses (i.e.,
articulated objects). The 3D orientation (pan, tilt and yaw angles) and
shape features are then extracted from each node in the tree to describe
the geometric distribution of the 3D points. The proposed feature has
been evaluated on a benchmark dataset and compared with two existing
geometric features. Experimental results show that the proposed feature
has the lowest overall error in human-pose estimation.},
keywords={feature extraction;pose estimation;robot vision;trees
(mathematics);3D-point-cloud feature;cognitive capabilities;geometric
distribution;geometric feature;global properties;human
motions;human-pose estimation;local properties;shape feature
extraction;tree structure;Estimation;Feature
extraction;Joints;Sensors;Shape;Three-dimensional displays;Vegetation},
doi={10.1109/ICRA.2013.6630787},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6629630,
author={C. C. T. Mendes and F. S. Osório and D. F. Wolf},
booktitle={2013 IEEE Intelligent Vehicles Symposium (IV)},
title={An efficient obstacle detection approach for organized point
clouds},
year={2013},
pages={1203-1208},
abstract={An accurate and efficient method for obstacle detection is a
key component of a robotic navigation system. Concerning indoor
environments, the ground surface can be modeled as a plane (or a set of)
and once estimated it can be employed for obstacle detection, e.g.
points lying above and below are considered obstacles. The same does not
hold for off-road and urban scenarios where one cannot expect planar
surfaces or obvious structural patterns. In 2002, Talukder et al.
presented a method to deal with such environments. Their method is based
on the height difference and “slope” between three-dimensional points.
Despite having been used successfully on several occasions, the method
has a high computational cost. We propose the use of a Graphics
Processing Unit (GPU) to enable its execution in real time. Experiments
were performed using a stereo camera and an RGB-D sensor, where the GPU
implementation has been compared to multi-core and single-core CPU
implementations. The results reveal a significant gain in computational
performance, reaching a speedup of almost 80 times in a specific
instance.},
keywords={graphics processing units;mobile robots;multiprocessing
systems;object detection;RGB-D sensor;computational performance;graphics
processing unit;multicore implementation;obstacle detection;off-road
scenario;organized point clouds;robotic navigation system;singlecore CPU
implementations;stereo camera;urban scenario;Approximation
methods;Cameras;Computational efficiency;Graphics processing
units;Kernel;Multicore processing},
doi={10.1109/IVS.2013.6629630},
ISSN={1931-0587},
month={June},}
@INPROCEEDINGS{6618173,
author={A. Saudabayev and Y. Khassanov and A. Shintemirov and H. A. Varol},
booktitle={2013 IEEE International Conference on Mechatronics and
Automation},
title={An intelligent object manipulation framework for industrial tasks},
year={2013},
pages={1708-1713},
abstract={This paper presents an intelligent object manipulation
framework for industrial tasks, which integrates a sensor-rich
multi-fingered robot hand, an industrial robot manipulator, a conveyor
belt and employs machine learning algorithms. The framework software
architecture is implemented using a Windows 7 operating system with RTX
real-time extension for synchronous handling of peripheral devices. The
framework uses Scale Invariant Feature Transform (SIFT) image processing
algorithm, Support Vector Machine (SVM) machine learning algorithm and
3D point cloud techniques for intelligent object recognition based on
RGB camera and laser rangefinder information from the robot hand end
effector. The objective is automated manipulation of objects with
different shapes and poses with minimum programming effort applied by a
user.},
keywords={belts;control engineering computing;conveyors;dexterous
manipulators;end effectors;feature extraction;intelligent robots;laser
ranging;learning (artificial intelligence);materials handling;operating
systems (computers);production engineering computing;software
architecture;support vector machines;RTX realtime extension;SIFT image
processing algorithm;SVM machine learning algorithm;Windows 7 operating
system;automated object manipulation;conveyor belt;framework software
architecture;industrial robot manipulator;industrial tasks;intelligent
object manipulation framework;laser rangefinder information;peripheral
device synchronous handling;robot hand end effector;scale invariant
feature transform;sensor-rich multifingered robot hand;support vector
machines;Feature extraction;Manipulators;Robot kinematics;Robot sensing
systems;Service robots;Training;LIDAR;Object Recognition;Real-Time
Operating System;Robot Manipulation},
doi={10.1109/ICMA.2013.6618173},
ISSN={2152-7431},
month={Aug},}
@INPROCEEDINGS{6606213,
author={H. C. Roh and T. J. Oh and Y. Choe and M. J. Chung},
booktitle={2013 9th Asian Control Conference (ASCC)},
title={Satellite map based quantitative analysis for 3D world modeling
of urban environment},
year={2013},
pages={1-6},
abstract={Here we present 3D world modeling and its quantitative
analysis methods in urban environment. If the expensive RTK GPS cannot
be prepared, it is difficult to measure the accuracy of the 3D world
model due to the blackout of GPS particularly in urban environment. To
cope with this difficulty, we combine to process both satellite image
and point cloud to compare each other in order to represents accuracy of
3D world model. We also introduce 3D world modeling method through
localization algorithm and global registration method in order to
validate our quantitative analysis. In the experimental result, we
describe our sensor system and evaluate the proposed quantitative
analysis method using 3 different localization algorithm. Our framework
is suitable of mobile mapping system in urban environment in terms of
cost.},
keywords={cartography;image registration;image segmentation;solid
modelling;3D world modeling;Global Positioning System;RTK GPS;global
registration method;localization algorithm;mobile mapping system;point
cloud processing;satellite image processing;satellite map based
quantitative analysis;urban environment;Accuracy;Buildings;Global
Positioning System;Satellites;Solid modeling;Three-dimensional
displays;Urban areas;3D world modeling;laser scanner;localization;point
cloud;satellite map;urban environment},
doi={10.1109/ASCC.2013.6606213},
month={June},}
@INPROCEEDINGS{6599620,
author={M. M. Manafzade and A. Harati},
booktitle={2013 21st Iranian Conference on Electrical Engineering (ICEE)},
title={Point cloud registration using MSSIR: Maximally stable shape
index regions},
year={2013},
pages={1-6},
abstract={Range image registration is one of the fundamental tasks in 3D
computer vision and robotics which is gaining more attention with
availability of affordable range cameras. Existing recent research has
considered application or extension of well known point features like
SIFT to the range data; examples include Shape index SIFT and 2.5D SIFT.
Compared to RGB image, the quality of range measurement is much worse in
sensors like Kinect. This is expected and inherent due to the exploited
structured light technique. Therefore, point features may easily
mismatched as a result of higher noise level. In this paper we show how
using region based features may overcome this challenge. MSER features
are extracted from shape index image obtained from the input range
image. A SIFT-like descriptor is then proposed to encode major smooth
regions of the scene as stable features invariant to scale, rotation and
affine transformations. Experimental results are obtained using range
image databases of Ohio State University and Stuttgart University which
show improvement on the percentage of correct matched features and
stability of detected features.},
keywords={cameras;feature extraction;image matching;image
registration;transforms;visual databases;2.5D SIFT;MSER feature
extraction;MSSIR;Ohio State University;RGB image;SIFT-like
descriptor;Stuttgart University;affine transformation;feature
detection;feature matching;maximally stable shape index regions;point
cloud registration;range cameras;range image databases;range image
registration;range measurement quality;region based features;rotation
transformation;scale transformation;shape index SIFT;shape index
image;structured light technique;Detectors;Equations;Feature
extraction;Indexes;Iterative closest point
algorithm;Shape;Three-dimensional displays;MSER;maximally stable shape
index regions;point cloud registration;range image},
doi={10.1109/IranianCEE.2013.6599620},
ISSN={2164-7054},
month={May},}
@ARTICLE{6583249,
author={H. Zhang and C. Reardon and L. E. Parker},
journal={IEEE Transactions on Cybernetics},
title={Real-Time Multiple Human Perception With Color-Depth Cameras on a
Mobile Robot},
year={2013},
volume={43},
number={5},
pages={1429-1441},
abstract={The ability to perceive humans is an essential requirement for
safe and efficient human-robot interaction. In real-world applications,
the need for a robot to interact in real time with multiple humans in a
dynamic, 3-D environment presents a significant challenge. The recent
availability of commercial color-depth cameras allow for the creation of
a system that makes use of the depth dimension, thus enabling a robot to
observe its environment and perceive in the 3-D space. Here we present a
system for 3-D multiple human perception in real time from a moving
robot equipped with a color-depth camera and a consumer-grade computer.
Our approach reduces computation time to achieve real-time performance
through a unique combination of new ideas and established techniques. We
remove the ground and ceiling planes from the 3-D point cloud input to
separate candidate point clusters. We introduce the novel information
concept, depth of interest, which we use to identify candidates for
detection, and that avoids the computationally expensive scanning-window
methods of other approaches. We utilize a cascade of detectors to
distinguish humans from objects, in which we make intelligent reuse of
intermediary features in successive detectors to improve computation.
Because of the high computational cost of some methods, we represent our
candidate tracking algorithm with a decision directed acyclic graph,
which allows us to use the most computationally intense techniques only
where necessary. We detail the successful implementation of our novel
approach on a mobile robot and examine its performance in scenarios with
real-world challenges, including occlusion, robot motion, nonupright
humans, humans leaving and reentering the field of view (i.e., the
reidentification challenge), human-object and human-human interaction.
We conclude with the observation that the incorporation of the depth
information, together with the use of modern techniques in new ways, we
are able to create an acc- rate system for real-time 3-D perception of
humans by a mobile robot.},
keywords={computer graphics;directed graphs;human-robot
interaction;image colour analysis;image sensors;mobile robots;object
detection;object tracking;robot vision;3D multiple human perception;3D
point cloud input;3D space;candidate point clusters;color-depth
cameras;computation time reduction;consumer-grade computer;decision
directed acyclic graph;depth
dimension;depth-of-interest;field-of-view;human-human
interaction;human-object interaction;human-robot interaction;humans
leaving;humans reentering;information concept;mobile robot;nonupright
humans;occlusion;real-time multiple human perception;robot
motion;scanning-window methods;Cameras;Detectors;Image color
analysis;Real-time systems;Robot vision systems;Target tracking;3-D
vision;RGB-D camera application;depth of interest;human detection and
tracking;human perception;Actigraphy;Algorithms;Artificial
Intelligence;Color;Colorimetry;Computer Peripherals;Computer
Simulation;Computer Systems;Humans;Image Enhancement;Imaging,
Three-Dimensional;Motion;Pattern Recognition,
Automated;Robotics;Transducers;Video Games;Whole Body Imaging},
doi={10.1109/TCYB.2013.2275291},
ISSN={2168-2267},
month={Oct},}
@INPROCEEDINGS{6581476,
author={M. Djehaich and H. Ziane and N. Achour and R. Tiar and N. Ouadah},
booktitle={2013 11th International Symposium on Programming and Systems
(ISPS)},
title={SLAM-ICP with a Boolean method applied on a car-like robot},
year={2013},
pages={116-121},
abstract={Scan matching is a popular way of recovering a mobile robot's
motion and constitutes the basis of many localization and mapping
approaches. Consequently, a variety of scan matching algorithms have
been proposed in the past. All these algorithms share one common
attribute: They match pairs of scans to obtain spatial relations between
two robot poses. The work presented in this paper consists in the
implementation of a SLAM algorithm (Simultaneous Localization and
Mapping) on a car-like vehicle. Our algorithm is based on a measurement
alignment method called “Iterative Closest Points” (ICP) using binary
weighted method (Boolean). It helps find the rigid transformation that
minimizes the distance between two clouds of points. The developed
algorithm (SLAM-ICP) has been implemented and tested on the mobile
robot. Experimental results given at the end of this paper are compared
to classical localization technique (odometry) and SLAM-ICP with the
recursive method that is already implemented on the Robucar.},
keywords={Boolean functions;SLAM (robots);distance measurement;image
matching;iterative methods;mobile robots;pose estimation;recursive
estimation;robot vision;transforms;Boolean method;ICP;Robucar;SLAM
algorithm;SLAM-ICP;binary weighted method;car-like robot;distance
minimization;iterative closest points;localization approach;mapping
approach;measurement alignment method;mobile robot motion
recovery;recursive method;scan matching algorithms;simultaneous
localization-and-mapping;Iterative closest point algorithm;Lasers;Mobile
robots;Motion estimation;Robot kinematics;Simultaneous localization and
mapping;ICP;Measures alignment;SLAM;Scan matching},
doi={10.1109/ISPS.2013.6581476},
month={April},}
@INPROCEEDINGS{6558468,
author={M. Price and J. Green and J. Dickens},
booktitle={2012 5th Robotics and Mechatronics Conference of South Africa},
title={Point-cloud registration using 3D shape contexts},
year={2012},
pages={1-5},
abstract={The problem of aligning scans from a range sensor is central
to 3D mapping for robots. In previous work we demonstrated a
light-weight descriptor-based registration method that is suitable for
creating maps from range images produced by devices such as the XBOX
Kinect. For computational reasons, simple descriptors were used based
only on the distribution of distances between points. In this paper, we
present an alternative approach using 3D Shape Contexts that also
retains angular information thereby producing descriptors that are more
unique. Although this increases the computational load, intrinsic
properties of the descriptor facilitate keypoint selection, leading to a
more robust registration framework. This also provides greater
flexibility when applying the method to sparse point clouds such as
those produced by laser range scanners. Results are shown for
registering new data acquired from an underground mine environment.},
keywords={image registration;image sensors;path planning;robot vision;3D
robot mapping;3D shape context;XBOX Kinect;laser range
scanner;lightweight descriptor-based registration method;point cloud
registration;range sensor;sparse point
cloud;Azimuth;Cameras;Context;Robot sensing systems;Robustness;Shape},
doi={10.1109/ROBOMECH.2012.6558468},
ISSN={2329-6429},
month={Nov},}
@INPROCEEDINGS{6556369,
author={M. Eich},
booktitle={2013 IEEE Conference on Technologies for Practical Robot
Applications (TePRA)},
title={An application of fuzzy DL-based semantic perception to soil
container classification},
year={2013},
pages={1-6},
abstract={Semantic perception and object labeling are key requirements
for robots interacting with objects on a higher level. Symbolic
annotation of objects allows the usage of planning algorithms for object
interaction, for instance in a typical fetchand-carry scenario. In
current research, perception is usually based on 3D scene reconstruction
and geometric model matching, where trained features are matched with a
3D sample point cloud. In this work we propose a semantic perception
method which is based on spatio-semantic features. These features are
defined in a natural, symbolic way, such as geometry and spatial
relation. In contrast to point-based model matching methods, a spatial
ontology is used where objects are rather described how they "look
like", similar to how a human would described unknown objects to another
person. A fuzzy based reasoning approach matches perceivable features
with a spatial ontology of the objects. The approach provides a method
which is able to deal with senor noise and occlusions. Another advantage
is that no training phase is needed in order to learn object features.
The use-case of the proposed method is the detection of soil sample
containers in an outdoor environment which have to be collected by a
mobile robot. The approach is verified using real world experiments.},
keywords={feature extraction;fuzzy reasoning;fuzzy set theory;image
matching;learning (artificial intelligence);mobile robots;object
recognition;ontologies (artificial intelligence);robot vision;stereo
image processing;3D sample point cloud;3D scene reconstruction;feature
matching;fetch-and-carry scenario;fuzzy DL-based semantic
perception;fuzzy based reasoning approach;geometric model
matching;mobile robot;object feature learning;object labeling;object
spatial ontology;object symbolic annotation;occlusion;outdoor
environment;robot-object interaction;semantic perception method;sensor
noise;soil container classification;spatial relation;spatio-semantic
feature;Robots},
doi={10.1109/TePRA.2013.6556369},
ISSN={2325-0526},
month={April},}
@INPROCEEDINGS{6558458,
author={T. Ratshidaho and J. R. Tapamo and J. Claassens and N. Govender},
booktitle={2012 5th Robotics and Mechatronics Conference of South Africa},
title={ToF camera ego-motion estimation},
year={2012},
pages={1-6},
abstract={In this paper, three approaches for ego-motion estimation
using Time-of-Flight (ToF) camera data are evaluated. Ego-motion is
defined as a process of estimating a camera's pose relative to some
initial pose using the camera's image sequence. The ToF camera is
characterised with a number error models. These models are used to
design several filters that are applied on point cloud data. Iterative
Closest Point (ICP) is applied on the consecutive range images of the
ToF camera to estimate relative pose transform which is used for
egomotion estimation. We implemented two variants of ICP namely
point-to-point and point-to-plane. A feature based ego-motion approach
that detects and tracks features on the amplitude images and use their
corresponding 3D points to estimate the relative transformation is
implemented. These approaches are evaluated using the groundtruth
provided by the vicon system.},
keywords={cameras;feature extraction;image sequences;iterative
methods;mobile robots;motion estimation;object tracking;path
planning;pose estimation;robot vision;ToF camera ego-motion
estimation;amplitude images;camera image sequence;camera pose
estimation;consecutive range images;feature based ego-motion
approach;feature detection;feature tracking;iterative closest
point;mobile robotics;number error models;point cloud
data;point-to-plane ICP;point-to-point ICP;robot
localisation;time-of-flight camera data;vicon
system;Accuracy;Cameras;Estimation;Feature extraction;Iterative closest
point algorithm;Noise;Robot sensing systems},
doi={10.1109/ROBOMECH.2012.6558458},
ISSN={2329-6429},
month={Nov},}
@INPROCEEDINGS{6523879,
author={P. Vieira and R. Ventura},
booktitle={2012 IEEE International Symposium on Safety, Security, and
Rescue Robotics (SSRR)},
title={Interactive mapping in 3D using RGB-D data},
year={2012},
pages={1-6},
abstract={The task of 3D mapping indoor environments in Search and
Rescue missions can be very useful on providing detailed spacial
informarion to human teams. This can be accomplished using field robots,
equipped with sensors capable of obtaining depth and color data, such as
the one provided by the Kinect sensor. Several methods have been
proposed in the literature to address the problem of automatic 3D
reconstruction from depth data. Most methods rely on the minimization of
the matching error among individual depth frames. However, ambiguity in
sensor data often leads to erroneous matching (due to local minima),
hard to cope with in a purely automatic approach. This paper is targeted
to 3D reconstruction from RGB-D data, and proposes a semi-automatic
approach, denoted Interactive Mapping, involving a human operator in the
process of detecting and correcting erroneous matches. Instead of
allowing the operator complete freedom in correcting the matching in a
frame by frame basis, the proposed method constrains human intervention
along the degrees of freedom with most uncertainty. The user is able to
translate and rotate individual RGB-D point clouds, with the help of a
force field-like reaction to the movement of each point cloud. A dataset
was obtained and used using a kinect equipped on the tracked wheel robot
RAPOSA-NG, developed for Search and Rescue missions. Some preliminary
results are presented, illustrating the advantages of the method.},
keywords={error correction;error detection;image colour analysis;image
matching;image reconstruction;indoor environment;iterative
methods;mobile robots;path planning;robot vision;service
robots;wheels;3D interactive mapping;3D mapping indoor environments;3D
reconstruction;Kinect sensor;RAPOSA-NG;RGB-D data;RGB-D point cloud
rotation;RGB-D point cloud translation;automatic 3D
reconstruction;automatic approach;color data;erroneous match correction
process;erroneous match detection process;force field-like
reaction;human operator;human teams;matching error minimization;search
and rescue missions;semiautomatic approach;tracked wheel robot;3D
Mapping;Interactive Alignment;Interactive Closest Points;Iterative
Methods},
doi={10.1109/SSRR.2012.6523879},
month={Nov},}
@INPROCEEDINGS{6523892,
author={A. K. Krishnan and S. Saripalli and E. Nissen and R. Arrowsmith},
booktitle={2012 IEEE International Symposium on Safety, Security, and
Rescue Robotics (SSRR)},
title={3D change detection using low cost aerial imagery},
year={2012},
pages={1-6},
abstract={We present a method to register point clouds obtained from
aerial images through Structure from motion (SFM) techniques with data
from airborne LiDAR systems. The data was obtained by the United States
Geological Survey (USGS) over a 800 sq km stretch in California using
airborne LiDAR. The images were obtained by a downward looking camera on
an autonomous helicopter along the San Andreas fault [9]. A 3D point
cloud is built by fusing GPS information with the aerial images. Our
approach to detect changes is to compare the LiDAR data with 3D point
cloud derived from aerial images. This comparison necessitates the two
point clouds to be in the same co-ordinate frame. We adopt a
registration approach to bring the point clouds to the same co-ordinate
frame. We highlight the challenges involved in registering aerial point
clouds and propose a semi automated way for registration. We also
present a simulation of a change detection scenario by introducing
displacement fields in the source point cloud and obtaining a target
point cloud by additionally simulating the GPS offsets. We recover the
displacement vectors in two steps (1) globally registering the source
and target point clouds using the method described in this paper (2)
using our change detection module [5] for computing the displacement
fields. We present results for global registration and change detection.},
keywords={Global Positioning System;autonomous aerial
vehicles;cameras;geophysical image processing;helicopters;image
fusion;image motion analysis;image registration;object detection;optical
radar;radar imaging;3D change detection;3D point cloud;California;GPS
information fusion;GPS offsets;SFM techniques;San Andreas
fault;USGS;United States Geological Survey;airborne LiDAR
systems;autonomous helicopter;change detection scenario;coordinate
frame;downward looking camera;low cost aerial imagery;semiautomated
aerial point cloud registration approach;structure from motion
techniques;ICP;Registration;change detection},
doi={10.1109/SSRR.2012.6523892},
month={Nov},}
@INPROCEEDINGS{6491051,
author={H. Wang and W. Mou and H. Suratno and G. Seet and M. Li and M.
W. S. Lau and D. Wang},
booktitle={2012 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Visual odometry using RGB-D camera on ceiling vision},
year={2012},
pages={710-714},
abstract={In this paper, we present a novel algorithm for odometry
computation based on ceiling vision. The main contribution in this
algorithm is the introduction of principal direction detection that can
greatly reduce error accumulation problem present in most visual
odometry estimation approaches. The principal direction is defined based
on the fact that our ceiling is filled with artificial vertical and
horizontal lines and these lines can be used as reference for the
current robot's heading direction. The proposed approach can be operated
in realtime and it performs well even with camera's disturbance. A
moving low-cost RGB-D camera (Kinect), mounted on a robot, is used to
continuously acquire point clouds. Iterative Closest Point (ICP) is the
common way to estimate current camera position by calculating the
translation and rotation to the previous frame. However, its performance
suffers from data association problem or it requires pre-alignment
information. Unlike ICP, the performance of the proposed approach does
not rely on data association knowledge. Using this method, two point
clouds are pre-aligned. Hence, we can use ICP to fine-tune the
transformation parameters and to minimize registration error.
Experimental results demonstrate the performance and stability of the
proposed system under disturbance in real-time.},
keywords={cameras;ceilings;distance measurement;sensor fusion;ICP;RGB-D
camera;artificial horizontal lines;artificial vertical lines;camera
disturbance;ceiling vision;data association knowledge;error accumulation
problem;iterative closest point;principal direction
detection;registration error;visual odometry computation;Ceiling
vision;Visual Odometry;principal direction;real-time},
doi={10.1109/ROBIO.2012.6491051},
month={Dec},}
@INPROCEEDINGS{6490980,
author={J. Guan and M. Q. H. Meng},
booktitle={2012 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Study on distance measurement for NAO humanoid robot},
year={2012},
pages={283-286},
abstract={In many applications, there is the need to know the accurate
distance between NAO and a certain object. Distance information is
always of great importance for NAO to do extra tasks in many home
monitoring and research circumstance. However, NAO is not equiped with a
depth-sensing device which can offer the distance information directly.
In this paper, we use four different methods to measure the distance.
These ways and devices include the Asus Xtion Pro Live which is a
low-priced 3D point cloud sensor, sonar which is a technique that uses
sound propagation to position, its own cameras to achieve stereo vision,
and NDI which is used to get the 3D coordinate and also the ground
truth. We make a comparison among the four methods. The purpose of the
study is to find the most appropriate way to measure the depth
information.},
keywords={distance measurement;humanoid robots;image sensors;mobile
robots;robot vision;sonar;stereo image processing;3D coordinate;3D point
cloud sensor;Asus Xtion Pro Live;NAO humanoid robot;NDI;distance
information;distance measurement;ground truth;nondestructive
imaging;sonar;sound propagation;stereo vision;NAO humanoid
robot;depth-sensing device;distance measurement;sonar;stereo vision},
doi={10.1109/ROBIO.2012.6490980},
month={Dec},}
@INPROCEEDINGS{6491044,
author={W. Sukmanee and M. Ruchanurucks and P. Rakprayoon},
booktitle={2012 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Obstacle modeling for manipulator using iterative least square
(ILS) and iterative closest point (ICP) base on Kinect},
year={2012},
pages={672-676},
abstract={This paper presents a method to distinguish between a
manipulator and its surroundings using a depth sensor. The depth sensor
used is Kinect. First Kinect calibration is addressed. Then coordinate
calibration between Kinect and the manipulator are solved using
iterative least square (ILS) algorithm. At this point, to delete the
robot from the scene and keep only the surrounding surface, the accuracy
of homogeneous transformation acquired from ILS is inadequate. We
further focus on a matching method between the manipulator's model and
point cloud, to use iterative closest point (ICP) algorithm. ICP
enhances the accuracy for a great deal. Experiment shows that this
comprehensive method is practical and robust. It can be used in dynamic
environment as well.},
keywords={collision avoidance;iterative methods;least squares
approximations;manipulators;pattern matching;ICP;ILS;Kinect
calibration;coordinate calibration;depth sensor;homogeneous
transformation;iterative closest point;iterative least square
algorithm;manipulator;matching method;obstacle modeling},
doi={10.1109/ROBIO.2012.6491044},
month={Dec},}
@INPROCEEDINGS{6485492,
author={H. C. Roh and C. H. Sung and M. J. Chung},
booktitle={The 19th Korea-Japan Joint Workshop on Frontiers of Computer
Vision},
title={Rapid SLAM using simple map representation in indoor environment},
year={2013},
pages={225-229},
abstract={Simultaneous Localization and Mapping is the one of essential
techniques for mobile robot navigation. In this paper, we propose rapid
SLAM using simple map representation in indoor environment for mobile
robot. This approach offers a new way to look at the problem focusing on
the issues that have caused the use of 3D laser scanner which provide
lot of 3D point data. We have tried to create the simple segments of
line and range table for scan-matching in a way that allows a robust
solution to the problem. In this article, two important issues of this
work on 2D registrations are made. First, it is shown that the algorithm
performs very well on the transformations of segmented line map from lot
of 3D point cloud data. Second the mapping logic and sequence are
explained in this paper to a line component accumulation for a map
building. Experimental results from 3-D sensor Velodyne and
wheel-odometry data logger Racelogic Vbox are given. Experimental
results show that this approach is not only robust for line mapping but
it is also fast, requires significantly less memory.},
keywords={SLAM (robots);computer graphics;image matching;image
registration;image segmentation;indoor environment;mobile robots;path
planning;robot vision;2D registration;3D laser scanner;3D point cloud
data;3D point data;3D sensor Velodyne;Racelogic Vbox;indoor
environment;line component accumulation;map building;map
representation;mapping logic;mapping sequence;mobile robot
navigation;range table;rapid SLAM;scan-matching;segmented line
map;simultaneous localization and mapping;wheel-odometry data
logger;Buildings;Conferences;Indoor environments;Mobile
robots;Simultaneous localization and mapping;Vehicles;Data
Association;Line Map;Mobile Robot;Polar Scan Matching;SLAM},
doi={10.1109/FCV.2013.6485492},
month={Jan},}
@INPROCEEDINGS{6489772,
author={P. Vieira and R. Ventura},
booktitle={Proceedings of 2012 IEEE 17th International Conference on
Emerging Technologies Factory Automation (ETFA 2012)},
title={Interactive 3D scan-matching using RGB-D data},
year={2012},
pages={1-4},
abstract={Several methods have been proposed in the literature to
address the problem of automatic 3D reconstruction from depth data. Most
methods rely on the minimization of the matching error among individual
depth frames. However, ambiguity in sensor data often leads to erroneous
matching (due to local minima), hard to cope with in a purely automatic
approach. This paper proposes a semiautomatic approach, denoted
interactive mapping, involving a human operator in the process of
detecting and correcting erroneous matches. Instead of allowing the
operator complete freedom in correcting the matching in a frame by frame
basis, the proposed method constrains human intervention along the
degrees of freedom with most uncertainty. This paper is targeted to 3D
reconstruction from RGB-D data, such as the one provided by the Kinect
sensor. The user is able to translate and rotate individual RGB-D point
clouds, with the help of a force field-like reaction to the movement of
each point cloud. Some preliminary results are presented, illustrating
the advantages of the method.},
keywords={image colour analysis;image matching;image
reconstruction;image sensors;minimisation;Kinect sensor;RGB-D point
clouds;automatic 3D reconstruction;denoted interactive mapping;depth
data;force field-like reaction;human operator;interactive 3D
scan-matching;matching error minimization;sensor data},
doi={10.1109/ETFA.2012.6489772},
ISSN={1946-0740},
month={Sept},}
@INPROCEEDINGS{6485455,
author={B. G. Seo and M. J. Chung},
booktitle={The 19th Korea-Japan Joint Workshop on Frontiers of Computer
Vision},
title={Traversable ground detection based on geometric-featured voxel map},
year={2013},
pages={31-35},
abstract={The process of finding traversable ground is important to
autonomous vehicles. To achieve this, 3D maps that contain the
information about vehicle's surroundings are generally used. One of the
simple methods to represent 3D maps is to use 3D point clouds collected
by range sensors. But, it requires lots of memory and computation time
to deal with point clouds. To solve these problems, we propose a novel
traversable ground detection method using a 3D mapping algorithm. The
proposed mapping algorithm includes plentiful geometric information
based on voxels, which is called Geometric-Featured Voxel (GFV) maps. In
our experiments, point clouds collected in urban environments are tested
to evaluate performance of the proposed algorithm.},
keywords={distance measurement;geometry;image sensors;object
detection;robot vision;terrain mapping;vehicles;3D map representation;3D
mapping algorithm;3D point cloud;GFV map;autonomous vehicle;geometric
featured voxel;geometric information;range sensor;traversable ground
detection;traversable ground detection method;urban environment;vehicle
surrounding;Conferences;Feature extraction;Joints;Mobile
robots;Sensors;Urban areas;Vehicles;3D mapping;Geometric Feature;Ground
Detection;Range Sensor;Voxel},
doi={10.1109/FCV.2013.6485455},
month={Jan},}
@INPROCEEDINGS{6485409,
author={Z. Yang and D. Xiao},
booktitle={2012 12th International Conference on Control Automation
Robotics Vision (ICARCV)},
title={A systemic point-cloud de-noising and smoothing method for 3D
shape reuse},
year={2012},
pages={1722-1727},
abstract={3D shape reuse, as an effective way to carry out innovative
design, requires a digital model database where the entities are
accurate and sufficient representations of objects in the real world. 3D
scanning is a prevailing tool to quickly convert physical models into
virtual ones. However, the scanned models without post-processing could
not be used directly due to environment noise and accuracy limitation in
terms of discrete sampling property in scanning. This paper introduces a
systemic point-cloud de-noising and mesh smoothing method to handle this
issue. The model de-noising and regularity is based on k-means
clustering, and mesh smoothing module is an improved mean approach which
processes the discrete data in the regular order. Case study will be
given to verify the smoothing effectiveness. The proposed method could
facilitate the construction of model database for design reuse, and
could be output to downstream applications such as shape adaptive
deformation, and shape searching.},
keywords={CAD;design engineering;image denoising;image
representation;image sampling;pattern clustering;shape
recognition;smoothing methods;solid modelling;stereo image
processing;visual databases;3D scanning;3D shape reuse;accuracy
limitation;design reuse;digital model database;discrete data
processing;discrete sampling property;environment noise;innovative
design;k-means clustering;mesh smoothing method;mesh smoothing
module;model database construction;model denoising;model
regularity;object representation;physical model;shape adaptive
deformation;shape searching;smoothing effectiveness;systemic point-cloud
denoising method;virtual model;Databases;Deformable models;Mathematical
model;Noise reduction;Shape;Smoothing methods;Solid modeling;3D
model;de-noising;design reuse;meshes smoothing},
doi={10.1109/ICARCV.2012.6485409},
month={Dec},}
@INPROCEEDINGS{6475013,
author={C. L. Baker and W. Hoff},
booktitle={2013 IEEE Workshop on Applications of Computer Vision (WACV)},
title={DIRSAC: A directed sampling and consensus approach to
quasi-degenerate data fitting},
year={2013},
pages={154-159},
abstract={In this paper we propose a new data fitting method which,
similar to RANSAC, fits data to a model using sample and consensus. The
application of interest is fitting 3D point clouds to a prior geometric
model. Where the RANSAC process uses random samples of points in the
fitting trials, we propose a novel method which directs the sampling by
ordering the points according to their contribution to the solution's
constraints. This is particularly important when the data is
quasi-degenerate. In this case, the standard RANSAC algorithm often
fails to find the correct solution. Our approach selects points based on
a Mutual Information criterion, which allows us to avoid redundant
points that result in degenerate sample sets. We demonstrate our
approach on simulated and real data and show that in the case of
quasi-degenerate data, the proposed algorithm significantly outperforms
RANSAC.},
keywords={computational geometry;iterative methods;solid modelling;3D
point clouds;DIRSAC;RANSAC;directed sampling and consensus
approach;mutual information criterion;prior geometric
model;quasidegenerate data fitting;Computational
modeling;Equations;Jacobian matrices;Mutual
information;Robots;Sensors;Vectors},
doi={10.1109/WACV.2013.6475013},
ISSN={1550-5790},
month={Jan},}
@INPROCEEDINGS{6462926,
author={Hyun Chul Roh and Chang Hun Sung and Myung Jin Chung},
booktitle={2012 9th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI)},
title={Fast vehicle detection using orientation histogram and segmented
line projection},
year={2012},
pages={44-45},
abstract={Fast detection of moving vehicle is essential for autonomous
driving. In this paper we propose fast vehicle classification algorithm
for autonomous vehicle using orientation histogram and segmented line
projection, and also works well with in urban environment. The rapidness
of our approach stems from the sequential laser data processing and
orientation histogram techniques. Whereas accurate classification of
point cloud into vehicle object is done in a 2D grid with segmented line
projection. Experimental results show verification on data from outdoor
scenes acquired from our electrical vehicle equipped with 2D laser
scanners.},
keywords={electric vehicles;image classification;image enhancement;image
segmentation;object detection;2D grid;2D laser scanners;autonomous
vehicle;electrical vehicle;fast vehicle classification algorithm;fast
vehicle detection;moving vehicle;orientation histogram techniques;point
cloud;segmented line projection;sequential laser data processing;urban
environment;vehicle object;Histograms;Image
segmentation;Lasers;Robots;Urban areas;Vehicle
detection;Vehicles;autonomous vehicle;laser scanner;orientation
histogram;vehicle detection},
doi={10.1109/URAI.2012.6462926},
month={Nov},}
@INPROCEEDINGS{6462925,
author={Yungeun Choe and Seunguk Ahn and Myung Jin Chung},
booktitle={2012 9th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI)},
title={Fast point cloud segmentation for an intelligent vehicle using
sweeping 2D laser scanners},
year={2012},
pages={38-43},
abstract={The previously developed radially bounded nearest neighbor
(RBNN) algorithm have been shown a good performance for 3D point cloud
segmentation in indoor scenarios. In outdoor scenarios however it is
hard to adapt the original RBNN to an intelligent vehicle directly due
to several drawbacks. In this paper, drawbacks of RBNN are addressed and
we propose an enhanced RBNN for an intelligent vehicle operating in
urban environments by proposing the ground elimination and the
distance-varying radius. After the ground removal, objects can be
remained to segment without merging the ground and objects, whereas the
original RBNN with the fixed radius induced over-segmentation or
under-segmentation. We design the distance-varying radius which is
varied properly from the distance between a laser scanner and scanning
objects. The proposed distance-varying radius is successfully induced to
segment objects without over or under segmentation. In the experimental
results, we have shown that the enhance RBNN is preferable to segment
urban structures in terms of time consumption, and even segmentation
rates.},
keywords={image segmentation;optical scanners;vehicles;2D laser
scanners;3D point cloud segmentation;RBNN algorithm;distance-varying
radius;fast point cloud segmentation;ground elimination;ground
removal;indoor scenarios;intelligent vehicle;outdoor
scenarios;over-segmentation;radially bounded nearest neighbor
algorithm;scanning objects;under-segmentation;Buildings;Clustering
algorithms;Intelligent vehicles;Laser radar;Lasers;Robots;Urban
areas;Intelligent Vehicle;Laser Scanner;Point Clouds;Segmentation},
doi={10.1109/URAI.2012.6462925},
month={Nov},}
@INPROCEEDINGS{6463006,
author={A. Makhal and M. Raj and K. Singh and P. Chakraborty and G. C.
Nandi},
booktitle={2012 9th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI)},
title={Path planning through maze routing for a mobile robot with
nonholonomic constraints},
year={2012},
pages={325-331},
abstract={A comprehensive technique to plan path for a mobile robot with
nonholonomic constraints through maze routing technique has been
presented. Our robot uses a stereo vision based approach to detect the
obstacles by creating dense 3D point clouds from the stereo images. ROS
packages have been implemented on the robot for specific tasks of
providing: i) Linear and angular velocity commands, ii) Calibration and
rectification of the stereo images for generating point clouds, iii)
Simulating the URDF (Unified Robot Description Format) module in real
time, with respect to the real robot and iv) For visualizing the sensor
data. For efficient path planning a hybrid technique using Lee's
algorithm, modified by Hadlock and Soukup's algorithm has been
implemented. Different path planning results have been shown using the
maze routing algorithms. Preliminary results shows that Lee's algorithm
is more time consuming in comparison with other algorithms. A hybrid of
Lee's with Soukup's algorithm is more efficient but unpredictable for
minimal path. A hybrid of Lee's with Hadlock's algorithm is the most
efficient and least time consuming.},
keywords={data visualisation;mobile robots;object detection;path
planning;robot vision;stereo image processing;velocity
control;Hadlock-Soukup algorithm;Lee algorithm;ROS package;URDF
module;Unified Robot Description Format;angular velocity
command;calibration;dense 3D point cloud;hybrid technique;linear
velocity command;maze routing technique;mobile robot;nonholonomic
constraint;obstacle detection;path planning;point cloud
generation;sensor data visualization;stereo image rectification;stereo
vision;Algorithm design and analysis;Cameras;Mobile
robots;Navigation;Robot kinematics;Robot sensing systems;Lee's
Algorithm;Maze Routing;Nonholonomic Mobile Robot;Path Planning;ROS},
doi={10.1109/URAI.2012.6463006},
month={Nov},}
@INPROCEEDINGS{6413384,
author={K. Hafeez and S. Khan},
booktitle={2012 International Conference of Robotics and Artificial
Intelligence},
title={Risk management analysis with the help of lightning strike
mapping around 500 k-v grid station using artificial intelligence
technique},
year={2012},
pages={165-168},
abstract={Lightning has been one of the major problems for insulation
design of power systems and it is still one of the main causes of forced
outages in transmission and distribution lines. It is necessary to
protect the power apparatus from over volts in electric system caused by
lightning over voltages. Artificial intelligence (AI) techniques, such
as expert system (ES), fuzzy logic (FL), and artificial neural network
(ANN), and genetic algorithm (GA), techniques can solve complex
engineering problems which are difficult to solve by traditional
methods. This research focuses on (AI) technique i.e. fuzzy logic on
mapping the lightning strike area around 500 k-v Grid station based on;
level of strike (high, medium, low) and category of lightning (positive
cloud-to-ground, negative cloud-to-ground, flash).Predefined areas
around 500k-v Sheikh muhammadi Grid station Peshawar is chosen as a case
study. This research will be helpful in risk management and lightning
protection analysis by determining the area and characterizing the
lightning strikes around the Grid station.},
keywords={artificial intelligence;fuzzy logic;lightning
protection;overvoltage protection;power distribution lines;power
engineering computing;power grids;power transmission lines;risk
analysis;AI techniques;Peshawar;Sheikh Muhammadi Grid station;artificial
intelligence technique;distribution lines;electric system;forced
outages;fuzzy logic;lightning overvoltages;lightning protection
analysis;lightning strike mapping;overvoltage protection;power apparatus
protection;power systems insulation design;risk management
analysis;transmission lines;voltage 500 kV;Artificial
intelligence;Clouds;Conductors;Fuzzy logic;Lighting;Lightning;Voltage
control;Artificial Intelligence;Fuzzy Logic;Grid Station;LightningStrike},
doi={10.1109/ICRAI.2012.6413384},
month={Oct},}
@INPROCEEDINGS{6402621,
author={E. W. Y. So and S. Michieletto and E. Menegatti},
booktitle={2012 IEEE International Symposium on Robotic and Sensors
Environments Proceedings},
title={Calibration of a dual-laser triangulation system for assembly
line completeness inspection},
year={2012},
pages={138-143},
abstract={In controlled industrial environments, laser triangulation is
an effective technique for 3D reconstruction, which is increasingly
being used for quality inspection and metrology. In this paper, we
propose a method for calibrating a dual laser triangulation system -
consisting of a camera, two line lasers, and a linear motion platform
designed to perform completeness inspection tasks on an assembly line.
Calibration of such a system involves the recovery of two sets of
parameters - the plane parameters of the two line lasers, and the
translational direction of the linear motion platform. First, we address
these two calibration problems separately. While many solutions have
been given for the former problem, the latter problem has been largely
ignored. Next, we highlight an issue specific to the use of multiple
lasers - that small errors in the calibration parameters can lead to
misalignment between the reconstructed point clouds of the different
lasers. We present two different procedures that can eliminate such
misalignments by imposing constraints between the two sets of
calibration parameters. Our calibration method requires only the use of
planar checkerboard patterns, which can be produced easily and
inexpensively.},
keywords={assembling;automatic optical inspection;calibration;image
motion analysis;image reconstruction;measurement by laser
beam;production engineering computing;quality control;3D measurement
technology;assembly line completeness inspection;calibration;controlled
industrial environment;laser triangulation system;line laser;linear
motion platform;metrology;planar checkerboard pattern;plane
parameter;point cloud reconstruction;quality inspection;translational
motion direction;Belts;Calibration;Cameras;Estimation;Image
reconstruction;Laser noise;Lasers;conveyor calibration;dual lasers;laser
calibration;laser triangulation;misalignment},
doi={10.1109/ROSE.2012.6402621},
month={Nov},}
@INPROCEEDINGS{6402610,
author={M. F. Alhamid and M. Eid and A. Alshareef and A. El Saddik},
booktitle={2012 IEEE International Symposium on Robotic and Sensors
Environments Proceedings},
title={MMBIP: Biofeedback system design on Cloud-Oriented Architecture},
year={2012},
pages={79-84},
abstract={in this paper, we propose a biofeedback system that employs a
Cloud-Oriented Architecture (COA) for the dissemination of biofeedback
information and services. The architecture provides the software
infrastructure to build biofeedback applications that maintain the
user's well-being by monitoring a number of physiological parameters and
generate the appropriate feedback. Consequently, the architecture
combines the collection of various sensory physiological data and
utilizes the existing cloud of resources to provide processing, storage,
and responses for biofeedback applications. The performance evaluation
has shown three distinguished features of the proposed architecture,
namely adaptability for various sensory streams, soft real-timeliness,
and scalability.},
keywords={Web services;cloud computing;information dissemination;medical
computing;physiology;psychology;resource allocation;service-oriented
architecture;software performance evaluation;COA;MMBIP;Web
services;biofeedback applications;biofeedback information
dissemination;biofeedback services;biofeedback system design;cloud
computing;cloud-oriented architecture;performance
evaluation;physiological parameters;resource cloud;sensory physiological
data;software infrastructure;Biological control
systems;Biosensors;Clouds;Computer architecture;Databases;Servers;Web
services;biofeedback;cloud computing;web services},
doi={10.1109/ROSE.2012.6402610},
month={Nov},}
@INPROCEEDINGS{6385920,
author={H. Hwang and S. Hyung and S. Yoon and K. Roh},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Robust descriptors for 3D point clouds using Geometric and
Photometric Local Feature},
year={2012},
pages={4027-4033},
abstract={The robust perception of robots is strongly needed to handle
various objects skillfully. In this paper, we propose a novel approach
to recognize objects and estimate their 6-DOF pose using 3D feature
descriptors, called Geometric and Photometric Local Feature (GPLF). The
proposed descriptors use both the geometric and photometric information
of 3D point clouds from RGB-D camera and integrate those information
into efficient descriptors. GPLF shows robust discriminative performance
regardless of characteristics such as shapes or appearances of objects
in cluttered scenes. The experimental results show how well the proposed
approach classifies and identify objects. The performance of pose
estimation is robust and stable enough for the robot to manipulate
objects. We also compare the proposed approach with previous approaches
that use partial information of objects with a representative
large-scale RGB-D object dataset.},
keywords={computational geometry;feature extraction;humanoid
robots;image colour analysis;image sensors;manipulators;object
recognition;pose estimation;robot vision;3D feature descriptors;3D point
clouds;6-DOF pose estimation;GPLF;RGB-D camera;RGB-D object
dataset;geometric local feature;humanoid robot platform;object
recognition;photometric local feature;robotic manipulation;robust
descriptors;Databases;Estimation;Object
recognition;Robots;Robustness;Shape;Vectors},
doi={10.1109/IROS.2012.6385920},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{6385638,
author={G. Freitas and B. Hamner and M. Bergerman and S. Singh},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={A practical obstacle detection system for autonomous orchard
vehicles},
year={2012},
pages={3391-3398},
abstract={Safe robot navigation in tree fruit orchards requires that the
vehicle be capable of robustly navigating between rows of trees and
turning from one aisle to another; that the vehicle be dynamically
stable, especially when carrying workers; and that the vehicle be able
to detect obstacles on its way and adjust its speed accordingly. In this
paper we address the latter, in particular the problem of detecting
people and apple bins in the aisles between rows. One of our
requirements is that the obstacle avoidance subsystem shouldn't add to
the robot's hardware cost, so as to keep the acquisition cost to growers
as low as possible. Therefore, we confine ourselves to solutions that
use only the sensor suite already installed on the robot for
navigation-in our case, a laser scanner, low-cost inertial measurement
unit, and steering and wheel encoders. Our methodology is based on the
classification and clustering of registered 3D points as obstacles. In
the current implementation, obstacle avoidance takes in 3D point clouds
collected in apple orchards and generates an off-line assessment of
obstacle position. Tests conducted at our experimental orchard-like
environment in Pittsburgh and an actual apple orchard in Washington
state indicate that the method is able to detect people and bins located
along the vehicle path. Stretch tests indicate that it is also capable
of dealing with objects as small as 15 cm tall as long as they aren't
covered by grass, and to detect people crossing the aisles at walking
speed.},
keywords={agriculture;collision avoidance;image classification;image
registration;inertial navigation;mobile robots;object detection;optical
scanners;pattern clustering;robot vision;stability;Pittsburgh;Washington
state;apple bins;apple orchards;autonomous orchard vehicles;dynamic
stability;inertial measurement unit;laser scanner;obstacle avoidance
subsystem;people detection;practical obstacle detection
system;registered 3D points classification;registered 3D points
clustering;safe robot navigation;steering encoders;tree fruit
orchards;wheel encoders;Measurement by laser beam;Navigation;Robot
sensing systems;Vegetation;Vehicles;Wheels},
doi={10.1109/IROS.2012.6385638},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{6385841,
author={J. Biswas and M. Veloso},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Planar polygon extraction and merging from depth images},
year={2012},
pages={3859-3864},
abstract={There has been considerable interest recently in building 3D
maps of environments using inexpensive depth cameras like the Microsoft
Kinect sensor. We exploit the fact that typical indoor scenes have an
abundance of planar features by modeling environments as sets of plane
polygons. To this end, we build upon the Fast Sampling Plane Filtering
(FSPF) algorithm that extracts points belonging to local neighborhoods
of planes from depth images, even in the presence of clutter. We
introduce an algorithm that uses the FSPF-generated plane filtered point
clouds to generate convex polygons from individual observed depth
images. We then contribute an approach of merging these detected
polygons across successive frames while accounting for a complete
history of observed plane filtered points without explicitly maintaining
a list of all observed points. The FSPF and polygon merging algorithms
run in real time at full camera frame rates with low CPU requirements:
in a real world indoor environment scene, the FSPF and polygon merging
algorithms take 2.5 ms on average to process a single 640 × 480 depth
image. We provide experimental results demonstrating the computational
efficiency of the algorithm and the accuracy of the detected plane
polygons by comparing with ground truth.},
keywords={cameras;feature extraction;filtering theory;image
sampling;sensors;3D map;Microsoft Kinect sensor;convex polygon
generation;depth camera;depth image;fast sampling plane filtering
algorithm;indoor scene;planar polygon extraction;planar polygon
merging;plane polygon;Cameras;Clutter;Feature extraction;Indoor
environments;Merging;Simultaneous localization and mapping;Solid modeling},
doi={10.1109/IROS.2012.6385841},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{6386073,
author={R. Paul and R. Triebel and D. Rus and P. Newman},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Semantic categorization of outdoor scenes with uncertainty
estimates using multi-class gaussian process classification},
year={2012},
pages={2404-2410},
abstract={This paper presents a novel semantic categorization method for
3D point cloud data using supervised, multiclass Gaussian Process (GP)
classification. In contrast to other approaches, and particularly
Support Vector Machines, which probably are the most used method for
this task to date, GPs have the major advantage of providing informative
uncertainty estimates about the resulting class labels. As we show in
experiments, these uncertainty estimates can either be used to improve
the classification by neglecting uncertain class labels or - more
importantly - they can serve as an indication of the
under-representation of certain classes in the training data. This means
that GP classifiers are much better suited in a lifelong learning
framework, where not all classes are represented initially, but instead
new training data arrives during the operation of the robot.},
keywords={Gaussian processes;continuing professional development;image
classification;mobile robots;path planning;robot vision;support vector
machines;uncertainty handling;3D point cloud data;GP
classifiers;life-long learning framework;semantic outdoor scene
categorization method;supervised multiclass Gaussian process
classification;support vector machines;training data;uncertain class
labels;uncertainty estimates;Buildings;Entropy;Robot sensing
systems;Support vector machines;Training;Uncertainty},
doi={10.1109/IROS.2012.6386073},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{6385686,
author={F. Schnitzer and K. Janschek and G. Willich},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Experimental results for image-based geometrical reconstruction
for spacecraft Rendezvous navigation with unknown and uncooperative
target spacecraft},
year={2012},
pages={5040-5045},
abstract={For both manned and autonomous space on-orbit servicing
missions, the collision avoidance and motion prediction of the target
objects are essential for a safe mission operation. Our approach assumes
unknown and uncooperative target objects (spacecraft, space debris) and
a camera only vision system. The target's 3D structure is reconstructed
from a sparse point cloud extracted from a rendezvous-SLAM algorithm and
processed by a RANSAC algorithm. Afterwards the 3D model can be used in
a feedback manner for enhancing visual navigation processing tasks. The
paper details the general concept and presents experimental results with
camera image data from a spacecraft rendezvous simulator testbed.},
keywords={SLAM (robots);aerospace simulation;cameras;collision
avoidance;control engineering computing;image reconstruction;motion
estimation;navigation;solid modelling;space vehicles;3D model;3D
structure;RANSAC algorithm;autonomous space on-orbit servicing
missions;camera image data;camera only vision system;collision
avoidance;feedback manner;image-based geometrical reconstruction;manned
on-orbit servicing missions;motion prediction;rendezvous-SLAM
algorithm;safe mission operation;space debris;spacecraft rendezvous
navigation;spacecraft rendezvous simulator testbed;sparse point cloud
extraction;target objects;target spacecraft;visual navigation processing
tasks;Cameras;Image reconstruction;Mathematical model;Satellites;Space
vehicles;Surface reconstruction;Surface
treatment;Autonomous;Navigation;On-Orbit
Servicing;RANSAC;Reconstruction;Rendezvous;Space Robotics;Sparse Point
Cloud},
doi={10.1109/IROS.2012.6385686},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{6365925,
author={D. Mazzanti and V. Zappi and A. Brogni and D. Caldwell},
booktitle={2012 18th International Conference on Virtual Systems and
Multimedia},
title={Point clouds indexing in real time motion capture},
year={2012},
pages={195-202},
abstract={Today's human-computer interaction techniques are often
gesture-inspired and thus pushed towards naturalness and immediateness.
Their implementation requires non-invasive tracking systems which can
work with little or no body attached devices, like wireless optical
motion capture. These technologies present a recurrent problem, which is
to keep a coherent indexing for the different captured points during
real time tracking. The inability to constantly distinguish tracked
points limits interaction naturalness and design possibilities. In this
paper we present a real time algorithm capable of dealing with points
indexing matter. Compared to other solutions, the presented research
adds a computed indexing correction to keep coherent indexing throughout
the tracking session. The correction is applied automatically by the
system, whenever a specific configuration is detected. Our solution
works with an arbitrary number of points and it was primarily designed
for fingertips tracking. A Virtual Reality application was developed in
order to exploit the algorithm functionalities while testing its
behavior and effectiveness. The application provides a virtual
stereoscopic, user-centric environment in which the user can trigger
simple interactions by reaching virtual objects with his/her fingertips.},
keywords={human computer interaction;image motion analysis;object
tracking;optical tracking;real-time systems;virtual reality;automatic
correction;coherent indexing;computed indexing correction;design
possibilities;fingertips tracking;gesture-inspired human computer
interaction technique;interaction naturalness;noninvasive tracking
systems;point clouds indexing;real time algorithm;real time motion
capture;real time tracking;user-centric environment;virtual
objects;virtual reality application;virtual stereoscopic
environment;wireless optical motion capture;Adaptive optics;Algorithm
design and analysis;Calibration;Indexing;Real-time
systems;Tracking;Virtual reality},
doi={10.1109/VSMM.2012.6365925},
month={Sept},}
@INPROCEEDINGS{6347906,
author={K. Okarma and M. Grudziński},
booktitle={2012 17th International Conference on Methods Models in
Automation Robotics (MMAR)},
title={The 3D scanning system for the machine vision based positioning
of workpieces on the CNC machine tools},
year={2012},
pages={85-90},
abstract={In the paper some algorithms applied for the calibration of
the 3D scanning system and image analysis in the experimental system for
positioning the workpieces on the CNC machines are discussed. The idea
of the scanning is based on the application of photogrammetric
algorithms using the fringe patterns approach. An experimental system
consisting of three cameras and three structural light projectors has
been built in order to acquire the images representing the scanned
object with projected light patterns. These images are then analyzed in
order to obtain the depth information for each point representing the
workpiece or the background. Nevertheless, a good accuracy requires a
proper calibration of the system considering the distortions introduced
by the optics of the cameras and projectors. After the calibration, the
acquisition of image series and their analysis, the 3D model of the
object, represented as a point cloud, is obtained as the result, which
should be filtered and can be fitted into the known model. The results
obtained in the conducted experiments have also been compared to the
effects of the application of an available commercial system with
similar cameras.},
keywords={calibration;cameras;computer vision;computerised numerical
control;image representation;machine tools;production engineering
computing;solid modelling;3D model;3D scanning system;CNC machine
tool;calibration;cameras;computerised numerical control;fringe pattern
approach;image analysis;image representation;image series
acquisition;machine vision;photogrammetric algorithm;structural light
projector;workpiece positioning;Accuracy;Calibration;Cameras;Machine
vision;Optical distortion;Robots;Shape},
doi={10.1109/MMAR.2012.6347906},
month={Aug},}
@INPROCEEDINGS{6343023,
author={J. Lobo and J. F. Ferreira and P. Trindade and J. Dias},
booktitle={2012 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Bayesian 3D independent motion segmentation with IMU-aided RBG-D
sensor},
year={2012},
pages={445-450},
abstract={In this paper we propose a two-tiered hierarchical Bayesian
model to estimate the location of objects moving independently from the
observer. Biological vision systems are very successful in motion
segmentation, since they efficiently resort to flow analysis and
accumulated prior knowledge of the 3D structure of the scene. Artificial
perception systems may also build 3D structure maps and use optical flow
to provide cues for ego- and independent motion segmentation. Using
inertial and magnetic sensors and an image and depth sensor (RGB-D) we
propose a method to obtain registered 3D maps, which are subsequently
used in a probabilistic model (the bottom tier of the hierarchy) that
performs background subtraction across several frames to provide a prior
on moving objects. The egomotion of the RGB-D sensor is estimated
starting with the angular pose obtained from the filtered accelerometers
and magnetic data. The translation is derived from matched points across
the images and corresponding 3D points in the rotation-compensated depth
maps. A gyro-aided Lucas Kanade tracker is used to obtain matched points
across the images. The tracked points can also used to refine the
initial sensor based rotation estimation. Having determined the camera
egomotion, the estimated optical flow assuming a static scene can be
compared with the observed optical flow via a probabilistic model (the
top tier of the hierarchy), using the results of the background
subtraction process as a prior, in order to identify volumes with
independent motion in the corresponding 3D point cloud. To deal with the
computational load CUDA-based solutions on GPUs were used. Experimental
results are presented showing the validity of the proposed approach.},
keywords={Bayes methods;accelerometers;graphics processing units;image
motion analysis;image segmentation;image sensors;image
sequences;inertial systems;parallel architectures;pose estimation;3D
point cloud;3D scene structure;3D structure maps;Bayesian 3D independent
motion segmentation;CUDA-based solutions;GPU;IMU-aided RBG-D
sensor;accelerometers;angular pose estimation;artificial perception
systems;background subtraction;biological vision systems;depth
sensor;ego-motion segmentation;flow analysis;gyro-aided Lucas Kanade
tracker;image sensor;inertial sensors;initial sensor based rotation
estimation;magnetic data;magnetic sensors;object location
estimation;optical flow;probabilistic model;rotation-compensated depth
maps;two-tiered hierarchical Bayesian model;Cameras;Computer
vision;Motion segmentation;Observers;Optical imaging;Optical
sensors;Robot sensing systems},
doi={10.1109/MFI.2012.6343023},
month={Sept},}
@INPROCEEDINGS{6343032,
author={P. Trindade and J. Lobo and J. P. Barreto},
booktitle={2012 IEEE International Conference on Multisensor Fusion and
Integration for Intelligent Systems (MFI)},
title={Hand gesture recognition using color and depth images enhanced
with hand angular pose data},
year={2012},
pages={71-76},
abstract={In this paper we propose a hand gesture recognition system
that relies on color and depth images, and on a small pose sensor on the
human palm. Monocular and stereo vision systems have been used for human
pose and gesture recognition, but with limited scope due to limitations
on texture, illumination, etc. New RGB-Depth sensors, that reply on
projected light such as the Microsoft Kinect, have overcome many of
those limitations. However, the point clouds for hand gestures are still
in many cases noisy and partially occluded, and hand gesture recognition
is not trivial. Hand gesture recognition is much more complex than full
body motion, since we can have the hands in any orientation and can not
assume a standing body on a ground plane. In this work we propose to add
a tiny pose sensor to the human palm, with a minute accelerometer and
magnetometer that combined provide 3D angular pose, to reduce the search
space and have a robust and computationally light recognition method.
Starting with the full depth image point cloud, segmentation can be
performed by taking into account the relative depth and hand
orientation, as well as skin color. Identification is then performed by
matching 3D voxel occupancy against a gesture template database.
Preliminary results are presented for the recognition of Portuguese Sign
Language alphabet, showing the validity of the approach.},
keywords={accelerometers;gesture recognition;image colour analysis;image
matching;image segmentation;magnetometers;pose estimation;skin;stereo
image processing;visual databases;3D angular pose;3D voxel occupancy
matching;Microsoft Kinect;Portuguese sign language alphabet
recognition;RGB-depth sensors;color images;depth images;depth
orientation;full depth image point cloud;gesture template database;hand
angular pose data;hand gesture recognition system;hand orientation;image
segmentation;light recognition method;magnetometer;minute
accelerometer;monocular vision systems;skin color;stereo vision
systems;Gesture recognition;Image color analysis;Iterative closest point
algorithm;Libraries;Robot sensing systems;Skin},
doi={10.1109/MFI.2012.6343032},
month={Sept},}
@INPROCEEDINGS{6334272,
author={M. Hernandez and J. Choi and G. Medioni},
booktitle={2012 Proceedings of the 20th European Signal Processing
Conference (EUSIPCO)},
title={Laser scan quality 3-D face modeling using a low-cost depth camera},
year={2012},
pages={1995-1999},
abstract={We propose a method to produce laser scan quality 3-D face
models from a freely moving user with a low-cost, low resolution depth
camera. Our approach does not rely on any prior face model and can
produce faithful geometric models of star-shaped objects. We represent
the object in cylindrical coordinates, which enables us to perform
filtering operations very efficiently. We initialize the model with the
first depth image, and then register each subsequent cloud of 3-D points
to the reference using a GPU (Graphics Processing Unit) implementation
of the ICP (Iterative Closest Point) algorithm. This registration is
robust in that it rejects poor alignment due to facial expressions,
occlusions, or a poor estimation of the transformation. We perform both
temporal and spatial smoothing of the successively incremented model. To
validate our approach, we quantitatively compare our model to one
produced by laser scanning, and show comparable accuracy.},
keywords={cameras;face recognition;graphics processing units;image
registration;image representation;iterative methods;object
detection;optical scanners;smoothing methods;spatiotemporal phenomena;3D
face model;3D points cloud;GPU;ICP;ICP algorithm;cylindrical
coordinate;depth camera;depth image;facial expression;filtering
operation;geometric model;graphics processing unit;image
registration;iterative closest point;laser scan quality;object
representation;occlusion;spatial smoothing;star shaped object;temporal
smoothing;Cameras;Computational modeling;Estimation;Face;Image
reconstruction;Laser modes;Solid modeling;Kinect;face modeling;graphics},
ISSN={2219-5491},
month={Aug},}
@INPROCEEDINGS{6309554,
author={S. Escaida Navarro and T. Klock and D. Braun and H. Woern},
booktitle={ROBOTIK 2012; 7th German Conference on Robotics},
title={Extraction of Solids of Revolution from Point Cloud Scenes for
Grasp Planning Tasks},
year={2012},
pages={1-6},
abstract={This paper presents a method for constructing high fidelity
geometrical models of objects in a point cloud scene with the goal of
grasp planning and evaluation. We consider objects whose volume can be
modeled as a solid of revolution which is extracted from the scene in a
sample consensus framework. The approach also provides a natural
strategy for segmenting object features such has cup-handles that are an
exception to this shape assumption.},
keywords={Approximation
methods;Estimation;Geometry;Grasping;Noise;Robots;Shape},
month={May},}
@INPROCEEDINGS{6309553,
author={M. Nieuwenhuisen and J. Stueckler and A. Berner and R. Klein and
S. Behnke},
booktitle={ROBOTIK 2012; 7th German Conference on Robotics},
title={Shape-Primitive Based Object Recognition and Grasping},
year={2012},
pages={1-5},
abstract={Grasping objects from unstructured piles is an important, but
difficult task. We present a new framework to grasp objects composed of
shape primitives like cylinders and spheres. For object recognition, we
employ efficient shape primitive detection methods in 3D point clouds.
Object models composed of such primitives are then found in the detected
shapes with a probabilistic graph-matching technique. We implement
object grasping based on the shape primitives in an efficient
multi-stage process that successively prunes infeasible grasps in tests
of increasing complexity. The final step is to plan collision-free
reaching motions to execute the grasps. With our approach, our service
robot can grasp object compounds from piles of objects, e. g., in
transport boxes.},
keywords={Design automation;Grasping;Object
recognition;Planning;Robots;Shape;Solid modeling},
month={May},}
@INPROCEEDINGS{6309508,
author={F. Spenrath and A. Spiller and A. Verl},
booktitle={ROBOTIK 2012; 7th German Conference on Robotics},
title={Gripping Point Determination and Collision Prevention in a Bin-
Picking application},
year={2012},
pages={1-6},
abstract={Bin-Picking is a complex subject. Attempts of industrial
realizations are often too slow or too unreliable. Many approaches
strongly focus on detecting the pose of objects inside the bin. While
this is an essential part of bin-picking, industrial realizations also
need to have a robust strategy to pick the detected objects out of the
bin, even in complex situations. We address this issue by separating the
object pose detection from the task of finding an appropriate gripping
position. Our goal is to find a suitable position for the gripper and
avoid collisions with the bin or other objects. This collision-free and
fast gripping point determination will be presented in this paper. The
basic idea is to generate several potential gripping configurations and
to rate them, primarily based on the probability of collisions with any
obstacles inside the sensor point cloud. The implemented approach has
produced good results in experiments and industrial applications.},
keywords={Design automation;Flanges;Grippers;Robot sensing systems;Solid
modeling;Testing},
month={May},}
@INBOOK{6280132,
author={Wolfram Burgard and Oliver Brock and Cyrill Stachniss},
booktitle={Robotics:Science and Systems III},
title={Context and Feature Sensitive Re-sampling from Discrete Surface
Measurements},
year={2008},
pages={352-},
abstract={This paper concerns context and feature-sensitive re-sampling
of workspace surfaces represented by 3D point clouds. We interpret a
point cloud as the outcome of repetitive and non-uniform sampling of the
surfaces in the workspace. The nature of this sampling may not be ideal
for all applications, representations and downstream processing. For
example it might be preferable to have a high point density around sharp
edges or near marked changes in texture. Additionally such preferences
might be dependent on the semantic classification of the surface in
question. This paper addresses this issue and provides a framework which
given a raw point cloud as input, produces a new point cloud by
re-sampling from the underlying workspace surfaces. Moreover it does
this in a manner which can be biased by local low-level geometric or
appearance properties and higher level (semantic) classification of the
surface. We are in no way prescriptive about what justifies a biasing in
the re-sampling scheme — this is left up to the user who may encapsulate
what constitutes “interesting” into one or more “policies” which are
used to modulate the default re-sampling behavior.},
publisher={MIT Press},
isbn={9780262255868},
url={http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6280132},}
@ARTICLE{6299166,
author={A. Aldoma and Z. C. Marton and F. Tombari and W. Wohlkinger and
C. Potthast and B. Zeisl and R. B. Rusu and S. Gedikli and M. Vincze},
journal={IEEE Robotics Automation Magazine},
title={Tutorial: Point Cloud Library: Three-Dimensional Object
Recognition and 6 DOF Pose Estimation},
year={2012},
volume={19},
number={3},
pages={80-91},
abstract={With the advent of new-generation depth sensors, the use of
three-dimensional (3-D) data is becoming increasingly popular. As these
sensors are commodity hardware and sold at low cost, a rapidly growing
group of people can acquire 3- D data cheaply and in real time.},
keywords={object recognition;pose estimation;DoF pose estimation;depth
sensors;object recognition;point cloud library},
doi={10.1109/MRA.2012.2206675},
ISSN={1070-9932},
month={Sept},}
@INPROCEEDINGS{6292489,
author={H. Soltani and H. D. Taghirad and A. R. Norouzzadeh Ravari},
booktitle={20th Iranian Conference on Electrical Engineering (ICEE2012)},
title={Stereo-based visual navigation of mobile robots in unknown
environments},
year={2012},
pages={946-951},
abstract={In this paper a stereo vision-based algorithm for mobile
robots navigation and exploration in unknown outdoor environments is
proposed. The algorithm is solely based on stereo images and implemented
on a nonholonomic mobile robot. The first step for exploration in
unknown environments is construction of the map of circumference in
real-time. By getting disparity image from rectified stereo images and
translating its data to 3D-space, point cloud model of environments is
constructed. Then by projecting points to XZ plane and put local maps
together based on visual odometry, global map of environment is
constructed in real-time. A* algorithm is used for investigating optimal
path and nonlinear back-stepping controller guides the robot to follow
the identified path. Finally, the mobile robot explores for a desired
object in an unknown environment through these steps. Experimental
results verify the effectiveness of the proposed algorithm in real-time
implementations.},
keywords={mobile robots;nonlinear control systems;robot vision;stereo
image processing;visual perception;3D-space;A* algorithm;XZ
plane;disparity image;global map;local maps;mobile robots
exploration;mobile robots navigation;nonholonomic mobile robot;nonlinear
backstepping controller;optimal path;point cloud model;realtime
implementations;rectified stereo images;stereo vision-based
algorithm;unknown outdoor environments;visual odometry;Q
measurement;Robot sensing systems;Mobile
robot;exploration;navigation;stereo vision;unknown outdoor environments},
doi={10.1109/IranianCEE.2012.6292489},
ISSN={2164-7054},
month={May},}
@INPROCEEDINGS{6265627,
author={L. Tamas and L. C. Goron},
booktitle={2012 20th Mediterranean Conference on Control Automation (MED)},
title={3D map building with mobile robots},
year={2012},
pages={134-139},
abstract={This paper presents a feature-based registration for 3D
environments using mobile robots. The developed 3D laser scanner with
custom hardware setup is able to scan both indoor and outdoor. For the
map registration a nonlinear variant of the Iterative Closest Point
(ICP) algorithm was used with initial alignment from the correspondences
given by the features of the scenes. The initial alignment was
determined using a set of key-points and the features of the keypoints
in order to reduce the computational time and to ensure a robust
estimation. Considering the increasing interest in 3D navigation for
mobile robots, our aim was to use the created maps for both indoor and
outdoor navigation purposes. Several maps were built by merging point
clouds while our method was tested for a wide range of datasets
including urban and office environments.},
keywords={SLAM (robots);estimation theory;iterative methods;mobile
robots;optical scanners;path planning;3D laser scanner;3D map
building;3D navigation;ICP algorithm;feature-based registration;indoor
navigation;iterative closest point algorithm;keypoint features;map
registration;mobile robots;office environments;outdoor navigation;point
clouds;robust estimation;urban environments;Estimation;Feature
extraction;Iterative closest point algorithm;Lasers;Mobile
robots;Robustness},
doi={10.1109/MED.2012.6265627},
month={July},}
@INPROCEEDINGS{6263625,
author={A. Sehgal and D. Cernea and M. Makaveeva},
booktitle={2012 Oceans - Yeosu},
title={Pose estimation and trajectory derivation from underwater imagery},
year={2012},
pages={1-5},
abstract={Obtaining underwater imagery is normally a costly affair since
expensive equipment such as multi-beam sonar scanners need to be
utilized. Even though such scanners provide imagery in form of 3D point
clouds, the tasks of locating accurate and dependable correspondences
between point clouds and registration can be quite slow. Registered 3D
point clouds can provide pose estimation and trajectory information
vital to the navigation of a robot, however, the slow speed of point
cloud registration normally means that maps are generated offline for
later use. Furthermore, any algorithm must be robust against artifacts
in 3D range data as sensor motion, reflection and refraction are
commonplace. In our work we describe the use of the SIFT feature
detector on scaled images based on point clouds captured by sonar in
order to register them in real-time. This online registration approach
is used to derive navigational information vital to underwater vehicles.
The algorithm utilizes the known point correspondence registration
algorithm in order to achieve real-time registration of point clouds,
thereby generating 3D maps in real-time and providing 3D pose estimation
and trajectory information.},
keywords={image registration;marine engineering;mobile robots;path
planning;pose estimation;sonar imaging;3D point clouds;3D range
data;multibeam sonar scanners;navigational information;online
registration approach;pose estimation;robot navigation;scaled
images;trajectory derivation;trajectory information;underwater
imagery;underwater vehicles;Detectors;Feature extraction;Robot sensing
systems;Sonar;Sonar navigation;Trajectory},
doi={10.1109/OCEANS-Yeosu.2012.6263625},
month={May},}
@INPROCEEDINGS{6224680,
author={Meng Song and Fengchi Sun and K. Iagnemma},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Natural landmark extraction in cluttered forested environments},
year={2012},
pages={4836-4843},
abstract={In this paper, a new systematical method for extracting tree
trunk landmarks from 3D point clouds of cluttered forested environments
is proposed. This purely geometric method is established on scene
understanding and automatic analysis of trees. The pipeline of our
method includes three steps. First, the raw point clouds are segmented
by utilizing the circular shape of trees, and segments are grouped into
tree sections based on the principle of spatial proximity. Second,
circles and axes are extracted from tree sections which are subject to
loss of shape information. Third, by clustering and integrating the tree
sections resulted from various space inconsistencies, straight tree
trunk landmarks are finally formed for future localization. The
experimental results from real forested environments are presented.},
keywords={feature extraction;forestry;geometry;pattern
clustering;vegetation;3D point clouds;circular shape;cluttered forested
environments;geometric method;natural landmark extraction;new
systematical method;scene understanding;shape information;spatial
proximity;tree sections;tree trunk landmarks;Feature
extraction;Fitting;Measurement by laser beam;Robot sensing
systems;Shape;Vegetation},
doi={10.1109/ICRA.2012.6224680},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225224,
author={A. Tatoglu and K. Pochiraju},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Point cloud segmentation with LIDAR reflection intensity behavior},
year={2012},
pages={786-790},
abstract={Light Detection and Ranging (LIDAR) scans are increasingly
being used for 3D map construction and reverse engineering. The utility
and benefit of the processed data maybe enhanced if the objects and
geometry of the area scanned can be segmented and labeled. In this
paper, we present techniques to model the intensity of the laser
reflection return from a point during LIDAR scanning to determine
diffuse and specular reflection properties of the scanned surface. Using
several illumination models, the reflection properties of the surface
are characterized by Lambertian diffuse reflection model and
Blinn-Phong, Gaussian and Beckmann specular models. Experimental set up
with eight different surfaces with varied textures and glossiness
enabled measurement of algorithm performance. Examples of point cloud
segmentation with the presented approach are presented.},
keywords={Gaussian processes;cartography;geophysical techniques;image
segmentation;optical radar;optical scanners;reverse engineering;3D map
construction;Beckmann specular models;Blinn-Phong models;Gaussian
models;LIDAR reflection intensity behavior;LIDAR scanning;Lambertian
diffuse reflection model;algorithm performance;glossiness enabled
measurement;illumination models;laser reflection return;light detection
and ranging scans;point cloud segmentation;reverse engineering;scanned
surface;specular reflection properties;Artificial intelligence;Laser
radar;Materials;Navigation},
doi={10.1109/ICRA.2012.6225224},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225189,
author={G. Basu and Y. Jiang and A. Saxena},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Learning to place objects: Organizing a room},
year={2012},
pages={3545-3546},
abstract={In this video, we consider the task of a personal robot
organizing a room by placing objects stably as well as in semantically
preferred locations. While this includes many sub-tasks such as grasping
an object, moving to a placing position, localizing itself and placing
the object in a proper location and orientation, it is the last problem
- how and where to place the objects - that is our focus in this work
and has not been widely studied yet. We formulate the placing task as a
learning problem. By computing appearance and shape features from the
input (point-clouds) that can capture the stability and semantics, our
algorithm can identify good placements for multiple objects. In this
video, we put together the placing algorithm with other sub-tasks to
enable a robot organize a room in several scenarios, such as loading a
bookshelf, a fridge, a waste bin and blackboard with various objects.},
keywords={mobile robots;blackboard;bookshelf;fridge;learning
problem;object placing;personal robot;placing algorithm;placing
task;subtasks;waste bin;Conferences;Grasping;Histograms;Organizing;Robot
sensing systems;Shape},
doi={10.1109/ICRA.2012.6225189},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224767,
author={Kuo-Chen Huang and Shih-Huan Tseng and Wei-Hao Mou and Li-Chen Fu},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Simultaneous localization and scene reconstruction with monocular
camera},
year={2012},
pages={2102-2107},
abstract={In this paper, we propose an online scene reconstruction
algorithm with monocular camera since there are many advantages on
modeling and visualization of an environment with physical scene
reconstruction instead of resorting to sparse 3D points. The goal of
this algorithm is to simultaneously track the camera position and map
the 3D environment, which is close to the spirit of visual SLAM.
There're plenty of visual SLAM algorithms in the current literature
which can provide a high accuracy performance, but many of them rely on
stereo cameras. It's true that we'll face many more challenges to
accomplish this task with monocular camera. However, the advantages of
cheaper and easier deployable hardware setting have made monocular
approach more attractive. Specifically, we apply a maximum a posteriori
Bayesian approach with optimization technique to simultaneously track
the camera and build a dense point cloud. We also propose a feature
expansion method to expand the density of points, and then online
reconstruct the scene with a delayed approach. Furthermore, we utilize
the reconstructed model to accomplish visual localization task without
extracting the features. Finally, a number of experiments have been
conducted to validate our proposed approach, and promising performance
can be observed.},
keywords={Bayes methods;SLAM (robots);feature extraction;image
sensors;optimisation;3D environment;Bayesian approach;camera
position;feature expansion method;monocular camera;optimization
technique;physical scene reconstruction;simultaneous localization and
scene reconstruction;sparse 3D points;stereo cameras;visual SLAM
algorithms;visual
localization;Buildings;Cameras;Face;Hardware;Optimization;Simultaneous
localization and mapping;Trajectory},
doi={10.1109/ICRA.2012.6224767},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225359,
author={J. T. C. Tan and T. Inamura},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={SIGVerse - A cloud computing architecture simulation platform for
social human-robot interaction},
year={2012},
pages={1310-1315},
abstract={The aim of this work is to propose a cloud computing
architecture simulation platform for social human-robot interaction.
This paper explains the design and development of this system named
SIGVerse in four main components, namely (1) SIGServer as the central
server, (2) Agent Controller for user applications, (3) Service
Provider, and (4) SIGViewer as the client terminal, and web based
development interface, in addressing the two main “human” issues in
human-robot interaction with cloud computing architecture, (a)
Distributed development platform, and (b) Large scale of human-robot
simulation. Three current applications are discussed for the validation
of the cloud computing architecture in social human-robot interaction
simulations.},
keywords={Internet;cloud computing;control engineering
computing;graphical user interfaces;human-robot interaction;humanoid
robots;mobile robots;service robots;software
architecture;GUI;SIGVerse;Web based development interface;agent
controller;central server;client terminal;cloud computing architecture
simulation platform;cloud computing architecture validation;distributed
development platform;graphical user interface;service provider;service
robots;social human-robot interaction;Cloud
computing;Collaboration;Computational modeling;Computer
architecture;Humans;Robot sensing systems},
doi={10.1109/ICRA.2012.6225359},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224939,
author={U. Klank and L. Mösenlechner and A. Maldonado and M. Beetz},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Robots that validate learned perceptual models},
year={2012},
pages={4456-4462},
abstract={Service robots that should operate autonomously need to
perform actions reliably, and be able to adapt to their changing
environment using learning mechanisms. Optimally, robots should learn
continuously but this approach often suffers from problems like
over-fitting, drifting or dealing with incomplete data. In this paper,
we propose a method to automatically validate autonomously acquired
perception models. These perception models are used to localize objects
in the environment with the intention of manipulating them with the
robot. Our approach verifies the learned perception models by moving the
robot, trying to re-detect an object and then to grasp it. From
observable failures of these actions and highlevel loop-closures to
validate the eventual success, we can derive certain qualities of our
models and our environment. We evaluate our approach by using two
different detection algorithms, one using 2D RGB data and one using 3D
point clouds. We show that our system is able to improve the perception
performance significantly by learning which of the models is better in a
certain situation and a specific context. We show how additional
validation allows for successful continuous learning. The strictest
precondition for learning such perceptual models is correct segmentation
of objects which is evaluated in a second experiment.},
keywords={dexterous manipulators;image colour analysis;image
segmentation;learning (artificial intelligence);object detection;robot
vision;service robots;visual perception;2D RGB;3D point cloud;learned
perceptual model;learning mechanism;object grasping;object
localization;object manipulation;object redetection;object
segmentation;perception performance;service robot;Context;Image
segmentation;Predictive models;Robots;Sensors;Shape;Solid modeling},
doi={10.1109/ICRA.2012.6224939},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224714,
author={M. Krainin and K. Konolige and D. Fox},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Exploiting segmentation for robust 3D object matching},
year={2012},
pages={4399-4405},
abstract={While Iterative Closest Point (ICP) algorithms have been
successful at aligning 3D point clouds, they do not take into account
constraints arising from sensor viewpoints. More recent beam-based
models take into account sensor noise and viewpoint, but problems still
remain. In particular, good optimization strategies are still lacking
for the beam-based model. In situations of occlusion and clutter, both
beam-based and ICP approaches can fail to find good solutions. In this
paper, we present both an optimization method for beambased models and a
novel framework for modeling observation dependencies in beam-based
models using over-segmentations. This technique enables reasoning about
object extents and works well in heavy clutter. We also make available a
ground-truth 3D dataset for testing algorithms in this area.},
keywords={image matching;image segmentation;inference
mechanisms;iterative methods;optimisation;solid modelling;3D point
clouds;ICP approach;beam-based models;ground-truth 3D dataset;iterative
closest point algorithms;object extent reasoning;optimization
method;over-segmentations;robust 3D object matching;segmentation
exploitation;sensor noise;sensor viewpoints;Clutter;Computational
modeling;Data models;Estimation;Iterative closest point
algorithm;Optimization;Robot sensing systems},
doi={10.1109/ICRA.2012.6224714},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225337,
author={J. Pan and S. Chitta and D. Manocha},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={FCL: A general purpose library for collision and proximity queries},
year={2012},
pages={3859-3866},
abstract={We present a new collision and proximity library that
integrates several techniques for fast and accurate collision checking
and proximity computation. Our library is based on hierarchical
representations and designed to perform multiple proximity queries on
different model representations. The set of queries includes discrete
collision detection, continuous collision detection, separation distance
computation and penetration depth estimation. The input models may
correspond to triangulated rigid or deformable models and articulated
models. Moreover, FCL can perform probabilistic collision checking
between noisy point clouds that are captured using cameras or LIDAR
sensors. The main benefit of FCL lies in the fact that it provides a
unified interface that can be used by various applications. Furthermore,
its flexible architecture makes it easier to implement new algorithms
within this framework. The runtime performance of the library is
comparable to state of the art collision and proximity algorithms. We
demonstrate its performance on synthetic datasets as well as motion
planning and grasping computations performed using a two-armed mobile
manipulation robot.},
keywords={cameras;collision avoidance;deformation;grippers;mobile
robots;optical radar;query processing;robot vision;FCL;LIDAR sensors;art
collision algorithms;art proximity algorithms;articulated
models;collision queries;continuous collision detection;deformable
models;discrete collision detection;flexible architecture;general
purpose library;grasping computations;library runtime performance;model
representations;motion planning;multiple proximity queries;noisy point
clouds;penetration depth estimation;probabilistic collision
checking;proximity computation;separation distance computation;synthetic
datasets;two-armed mobile manipulation robot;Charge coupled
devices;Collision avoidance;Computational modeling;Deformable
models;Libraries;Robots;Shape},
doi={10.1109/ICRA.2012.6225337},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224766,
author={J. Biswas and M. Veloso},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Depth camera based indoor mobile robot localization and navigation},
year={2012},
pages={1697-1702},
abstract={The sheer volume of data generated by depth cameras provides a
challenge to process in real time, in particular when used for indoor
mobile robot localization and navigation. We introduce the Fast Sampling
Plane Filtering (FSPF) algorithm to reduce the volume of the 3D point
cloud by sampling points from the depth image, and classifying local
grouped sets of points as belonging to planes in 3D (the “plane
filtered” points) or points that do not correspond to planes within a
specified error margin (the “outlier” points). We then introduce a
localization algorithm based on an observation model that down-projects
the plane filtered points on to 2D, and assigns correspondences for each
point to lines in the 2D map. The full sampled point cloud (consisting
of both plane filtered as well as outlier points) is processed for
obstacle avoidance for autonomous navigation. All our algorithms process
only the depth information, and do not require additional RGB data. The
FSPF, localization and obstacle avoidance algorithms run in real time at
full camera frame rates (30Hz) with low CPU requirements (16%). We
provide experimental results demonstrating the effectiveness of our
approach for indoor mobile robot localization and navigation. We further
compare the accuracy and robustness in localization using depth cameras
with FSPF vs. alternative approaches that simulate laser rangefinder
scans from the 3D data.},
keywords={edge detection;mobile robots;navigation;path planning;robot
vision;2D map;3D data;3D point cloud;FSPF algorithm;depth camera based
indoor mobile robot localization;depth camera based indoor mobile robot
navigation;fast sampling plane filtering algorithm;laser rangefinder
scans;outlier points;plane filtered
points;Cameras;Lasers;Navigation;Robot vision systems;Vectors},
doi={10.1109/ICRA.2012.6224766},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224781,
author={B. Kehoe and D. Berenson and K. Goldberg},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Toward cloud-based grasping with uncertainty in shape: Estimating
lower bounds on achieving force closure with zero-slip push grasps},
year={2012},
pages={576-583},
abstract={This paper explores how Cloud Computing can facilitate
grasping with shape uncertainty. We consider the most common robot
gripper: a pair of thin parallel jaws, and a class of objects that can
be modeled as extruded polygons. We model a conservative class of
push-grasps that can enhance object alignment. The grasp planning
algorithm takes as input an approximate object outline and Gaussian
uncertainty around each vertex and center of mass. We define a grasp
quality metric based on a lower bound on the probability of achieving
force closure. We present a highly-parallelizable algorithm to compute
this metric using Monte Carlo sampling. The algorithm uses Coulomb
frictional grasp mechanics and a fast geometric test for conservative
conditions for force closure. We run the algorithm on a set of sample
shapes and compare the grasps with those from a planner that does not
model shape uncertainty. We report computation times with single and
multi-core computers and sensitivity analysis on algorithm parameters.
We also describe physical grasp experiments using the Willow Garage PR2
robot.},
keywords={Gaussian processes;Monte Carlo methods;cloud computing;control
engineering computing;force
control;geometry;grippers;probability;sampling methods;Coulomb
frictional grasp mechanics;Gaussian uncertainty;Monte Carlo
sampling;Willow Garage PR2 robot;approximate object outline;cloud
computing;cloud-based grasping;extruded polygon;force closure;geometric
test;grasp planning algorithm;grasp quality metric;highly-parallelizable
algorithm;lower bound estimation;multicore computer;object
alignment;probability;robot gripper;sensitivity analysis;shape
uncertainty;thin parallel jaws;zero-slip push
grasp;Force;Grasping;Grippers;Robot sensing systems;Shape;Uncertainty},
doi={10.1109/ICRA.2012.6224781},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224647,
author={J. Kammerl and N. Blodow and R. B. Rusu and S. Gedikli and M.
Beetz and E. Steinbach},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Real-time compression of point cloud streams},
year={2012},
pages={778-785},
abstract={We present a novel lossy compression approach for point cloud
streams which exploits spatial and temporal redundancy within the point
data. Our proposed compression framework can handle general point cloud
streams of arbitrary and varying size, point order and point density.
Furthermore, it allows for controlling coding complexity and coding
precision. To compress the point clouds, we perform a spatial
decomposition based on octree data structures. Additionally, we present
a technique for comparing the octree data structures of consecutive
point clouds. By encoding their structural differences, we can
successively extend the point clouds at the decoder. In this way, we are
able to detect and remove temporal redundancy from the point cloud data
stream. Our experimental results show a strong compression performance
of a ratio of 14 at 1 mm coordinate precision and up to 40 at a
coordinate precision of 9 mm.},
keywords={cloud computing;data compression;tree data structures;coding
complexity;coding precision;novel lossy compression approach;octree data
structures;point cloud streams;real-time compression;spatial
decomposition;temporal redundancy;Decoding;Encoding;Entropy;Octrees;Real
time systems;Sensors},
doi={10.1109/ICRA.2012.6224647},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225278,
author={A. Leeper and S. Chan and K. Salisbury},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Point clouds can be represented as implicit surfaces for
constraint-based haptic rendering},
year={2012},
pages={5000-5005},
abstract={We present a constraint-based strategy for haptic rendering of
arbitrary point cloud data. With the recent proliferation of low-cost
range sensors, dense 3D point cloud data is readily available at high
update rates. Taking a cue from the graphics literature, we propose that
point data should be represented as an implicit surface, which can be
formulated to be mathematically smooth and efficient for computing
interaction forces, and for which haptic constraint algorithms are
already well-known. This method is resistant to sensor noise, makes no
assumptions about surface connectivity or orientation, and data
pre-processing is fast enough for use with streaming data. We compare
the performance of two different implicit representations and discuss
our strategy for handling time-varying point clouds from a depth camera.
Applications of haptic point cloud rendering to remote sensing, as in
robot telemanipulation, are also discussed.},
keywords={haptic interfaces;manipulators;rendering (computer
graphics);telerobotics;arbitrary point cloud data;constraint-based
haptic rendering;constraint-based strategy;dense 3D point cloud
data;haptic constraint algorithms;implicit surfaces;low-cost range
sensors;remote sensing;robot telemanipulation;time-varying point
clouds;Cameras;Force;Haptic interfaces;Noise;Rendering (computer
graphics);Sensors;Surface treatment},
doi={10.1109/ICRA.2012.6225278},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225098,
author={P. R. Osteen and J. L. Owens and C. C. Kessens},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Online egomotion estimation of RGB-D sensors using spherical
harmonics},
year={2012},
pages={1679-1684},
abstract={We present a technique to estimate the egomotion of an RGB-D
sensor based on rotations of functions defined on the unit sphere. In
contrast to traditional approaches, our technique is not based on image
features and does not require correspondences to be generated between
frames of data. Instead, consecutive functions are correlated using
spherical harmonic analysis. An Extended Gaussian Image (EGI), created
from the local normal estimates of a point cloud, defines each function.
Correlations are efficiently computed using Fourier transformations,
resulting in a 3 Degree of Freedom (3-DoF) rotation estimate. An
Iterative Closest Point (ICP) process then refines the initial rotation
estimate and adds a translational component, yielding a full 6-DoF
egomotion estimate. The focus of this work is to investigate the merits
of using spherical harmonic analysis for egomotion estimation by
comparison with alternative 6-DoF methods. We compare the performance of
the proposed technique with that of stand-alone ICP and image feature
based methods. As with other egomotion techniques, estimation errors
accumulate and degrade results, necessitating correction mechanisms for
robust localization. For this report, however, we use the raw estimates;
no filtering or smoothing processes are applied. In-house and external
benchmark data sets are analyzed for both runtime and accuracy. Results
show that the algorithm is competitive in terms of both accuracy and
runtime, and future work will aim to combine the various techniques into
a more robust egomotion estimation framework.},
keywords={feature extraction;image sensors;motion estimation;6-DoF
methods;RGB-D sensors;correction mechanisms;extended Gaussian
image;image features;iterative closest point process;online egomotion
estimation;robust localization;spherical
harmonics;Accuracy;Correlation;Estimation;Harmonic analysis;Iterative
closest point algorithm;Runtime;Sensors},
doi={10.1109/ICRA.2012.6225098},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224801,
author={W. Wan and R. Fukui and M. Shimosaka and T. Sato and Y. Kuniyoshi},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={On the caging region of a third finger with object boundary
clouds and two given contact positions},
year={2012},
pages={4154-4161},
abstract={This paper presents a caging approach which deals with planar
boundary clouds collected from a laser scanner. Given the boundary
clouds of a target object and two fixed finger positions, our aim is to
find potential third finger positions that can prevent target from
escaping into infinity. The major challenge in working with boundary
clouds lies in their uncertainty in geometric model fitting and the
failure of critical orientations. In this paper, we track canonical
motions according to the rotational intersection of Configuration space
fingers and rasterize Work space with grids to compute the third caging
positions. Our approach can generate the capture region with
max(O(np),O(h^2 )) ≤ O(n^2 ) cost where n denotes the resolution of grid
rasterization, p denotes the resolution of canonical rasterization and h
denotes the resolution of boundary rasterization or the number of
boundary cloud points. Moreover, we propose a rough approximation which
measures a subset of the possible positions by contracting rotations,
indicating computational complexity of max(O(n),O(h^2 )). In the
experimental part, our proposal is compared with state-of-the-art works
and applied to many other objects. The approach makes caging fast and
effective.},
keywords={approximation theory;computational complexity;curve
fitting;dexterous manipulators;object tracking;optical scanners;robot
vision;boundary rasterization;canonical motion tracking;canonical
rasterization resolution;computational complexity;configuration space
finger;contact position estimation;critical orientation failure;finger
convex caging problem;geometric model fitting;grid rasterization
resolution;laser scanner;object boundary cloud point;rotational
intersection;rough approximation;uncertainty handling;Approximation
methods;Grasping;Proposals;Robots;Rotation measurement;Target tracking},
doi={10.1109/ICRA.2012.6224801},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224940,
author={Y. Zhao and M. He and H. Zhao and F. Davoine and H. Zha},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Computing object-based saliency in urban scenes using laser
sensing},
year={2012},
pages={4436-4443},
abstract={It becomes a well-known technology that a low-level map of
complex environment containing 3D laser points can be generated using a
robot with laser scanners. Given a cloud of 3D laser points of an urban
scene, this paper proposes a method for locating the objects of
interest, e.g. traffic signs or road lamps, by computing object-based
saliency. Our major contributions are: 1) a method for extracting simple
geometric features from laser data is developed, where both range images
and 3D laser points are analyzed; 2) an object is modeled as a graph
used to describe the composition of geometric features; 3) a graph
matching based method is developed to locate the objects of interest on
laser data. Experimental results on real laser data depicting urban
scenes are presented; efficiency as well as limitations of the method
are discussed.},
keywords={computational geometry;feature extraction;graph theory;mobile
robots;object detection;optical scanners;robot vision;3D laser
points;geometric feature extraction;graph matching based method;laser
scanners;laser sensing;object detection;object location;object-based
saliency computation;road lamps;traffic signs;urban scenes;Feature
extraction;Laser modes;Measurement by laser beam;Merging;Sensors;Vectors},
doi={10.1109/ICRA.2012.6224940},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224726,
author={G. A. Hollinger and B. Englot and F. Hover and U. Mitra and G.
S. Sukhatme},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Uncertainty-driven view planning for underwater inspection},
year={2012},
pages={4884-4891},
abstract={We discuss the problem of inspecting an underwater structure,
such as a submerged ship hull, with an autonomous underwater vehicle
(AUV). In such scenarios, the goal is to construct an accurate 3D model
of the structure and to detect any anomalies (e.g., foreign objects or
deformations). We propose a method for constructing 3D meshes from
sonar-derived point clouds that provides watertight surfaces, and we
introduce uncertainty modeling through non-parametric Bayesian
regression. Uncertainty modeling provides novel cost functions for
planning the path of the AUV to minimize a metric of inspection
performance. We draw connections between the resulting cost functions
and submodular optimization, which provides insight into the formal
properties of active perception problems. In addition, we present
experimental trials that utilize profiling sonar data from ship hull
inspection.},
keywords={Bayes methods;autonomous underwater vehicles;control
engineering computing;inspection;mesh generation;mobile
robots;nonparametric statistics;optimisation;path planning;regression
analysis;ships;solid modelling;sonar;uncertainty handling;3D meshes;3D
model;AUV;active perception problems;anomaly detection;autonomous
underwater vehicle;cost functions;experimental trials;formal
property;inspection performance;nonparametric Bayesian regression;path
planning;profiling sonar data;ship hull inspection;sonar-derived point
clouds;submerged ship hull;submodular optimization;uncertainty
modeling;uncertainty-driven view planning;underwater
inspection;underwater structure inspection;watertight surfaces;Data
models;Inspection;Marine vehicles;Surface reconstruction;Surface
treatment;Uncertainty;Vectors},
doi={10.1109/ICRA.2012.6224726},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225216,
author={R. Anati and D. Scaramuzza and K. G. Derpanis and K. Daniilidis},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Robot localization using soft object detection},
year={2012},
pages={4992-4999},
abstract={In this paper, we give a new double twist to the robot
localization problem. We solve the problem for the case of prior maps
which are semantically annotated perhaps even sketched by hand. Data
association is achieved not through the detection of visual features but
the detection of object classes used in the annotation of the prior
maps. To avoid the caveats of general object recognition, we propose a
new representation of the query images that consists of a vector of the
detection scores for each object class. Given such soft object
detections we are able to create hypotheses about pose and to refine
them through particle filtering. As opposed to small confined office and
kitchen spaces, our experiment takes place in a large open urban rail
station with multiple semantically ambiguous places. The success of our
approach shows that our new representation is a robust way to exploit
the plethora of existing prior maps for GPS-denied environments avoiding
the data association problems when matching point clouds or visual
features.},
keywords={object recognition;path planning;robot vision;sensor
fusion;data association;object detection;particle filtering;robot
localization problem;soft object detection;urban rail
station;Cameras;Heating;Histograms;Image color analysis;Object
detection;Robot kinematics},
doi={10.1109/ICRA.2012.6225216},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225367,
author={Ö. Erkent and I. Bozma},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Place representation in topological maps based on bubble space},
year={2012},
pages={3497-3502},
abstract={Place representation is a key element in topological maps.
This paper presents bubble space - a novel representation for “places”
(nodes) in topological maps. The novelties of this model are two-fold:
First, a mathematical formalism that defines bubble space is presented.
This formalism extends previously proposed bubble memory to accommodate
two new variables - varying robot pose and multiple features. Each
bubble surface preserves the local S^2 -metric relations of the incoming
sensory data from the robot's viewpoint. Secondly, for learning and
recognition, bubble surfaces can be transformed into bubble descriptors
that are compact and rotationally invariant, while being computable in
an incremental manner. The proposed model is evaluated with support
vector machine based decision making in two different settings: first
with a mobile robot placed in a variety of locations and secondly using
benchmark visual data.},
keywords={decision making;mobile robots;path planning;support vector
machines;bubble descriptors;bubble space;mathematical formalism;mobile
robot;place representation;support vector machine;topological
maps;variables-varying robot pose;Clouds;Laboratories;Measurement;Robot
sensing systems;Support vector machines;Visualization},
doi={10.1109/ICRA.2012.6225367},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224740,
author={D. Fehr and A. Cherian and R. Sivalingam and S. Nickolay and V.
Morellas and and N. Papanikolopoulos},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Compact covariance descriptors in 3D point clouds for object
recognition},
year={2012},
pages={1793-1798},
abstract={One of the most important tasks for mobile robots is to sense
their environment. Further tasks might include the recognition of
objects in the surrounding environment. Three dimensional range finders
have become the sensors of choice for mapping the environment of a
robot. Recognizing objects in point clouds provided by such sensors is a
difficult task. The main contribution of this paper is the introduction
of a new covariance based point cloud descriptor for such object
recognition. Covariance based descriptors have been very successful in
image processing. One of the main advantages of these descriptors is
their relatively small size. The comparisons between different
covariance matrices can also be made very efficient. Experiments with
real world and synthetic data will show the superior performance of the
covariance descriptors on point clouds compared to state-of-the-art
methods.},
keywords={covariance matrices;feature extraction;mobile robots;object
recognition;robot vision;stereo image processing;3D point clouds;3D
range finders;compact covariance descriptors;covariance based point
cloud descriptor;covariance matrix;environment mapping;image
processing;mobile robots;object recognition;Covariance
matrix;Histograms;Measurement;Object recognition;Robots;Sensors;Vectors},
doi={10.1109/ICRA.2012.6224740},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6224607,
author={W. Maddern and A. Harrison and P. Newman},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Lost in translation (and rotation): Rapid extrinsic calibration
for 2D and 3D LIDARs},
year={2012},
pages={3096-3102},
abstract={This paper describes a novel method for determining the
extrinsic calibration parameters between 2D and 3D LIDAR sensors with
respect to a vehicle base frame. To recover the calibration parameters
we attempt to optimize the quality of a 3D point cloud produced by the
vehicle as it traverses an unknown, unmodified environment. The point
cloud quality metric is derived from Rényi Quadratic Entropy and
quantifies the compactness of the point distribution using only a single
tuning parameter. We also present a fast approximate method to reduce
the computational requirements of the entropy evaluation, allowing
unsupervised calibration in vast environments with millions of points.
The algorithm is analyzed using real world data gathered in many
locations, showing robust calibration performance and substantial speed
improvements from the approximations.},
keywords={calibration;entropy;mobile robots;optical radar;2D LIDAR;3D
LIDAR;3D point cloud;Renyi quadratic entropy;calibration
parameters;entropy evaluation;point cloud quality metric;point
distribution;rapid extrinsic calibration;single tuning
parameter;unsupervised calibration;vehicle base frame;Calibration;Cost
function;Entropy;Laser radar;Sensors;Vehicles},
doi={10.1109/ICRA.2012.6224607},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6225287,
author={A. J. B. Trevor and J. G. Rogers and H. I. Christensen},
booktitle={2012 IEEE International Conference on Robotics and Automation},
title={Planar surface SLAM with 3D and 2D sensors},
year={2012},
pages={3041-3048},
abstract={We present an extension to our feature based mapping technique
that allows for the use of planar surfaces such as walls, tables,
counters, or other planar surfaces as landmarks in our mapper. These
planar surfaces are measured both in 3D point clouds, as well as 2D
laser scans. These sensing modalities compliment each other well, as
they differ significantly in their measurable fields of view and maximum
ranges. We present experiments to evaluate the contributions of each
type of sensor.},
keywords={SLAM (robots);sensors;service robots;2D laser scans;2D
sensors;3D point clouds;3D sensors;SLAM;feature based mapping
technique;planar surface;service robots;Feature extraction;Measurement
by laser beam;Simultaneous localization and mapping;Trajectory},
doi={10.1109/ICRA.2012.6225287},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{6219256,
author={Y. Sun},
booktitle={2012 IEEE Symposium on Robotics and Applications (ISRA)},
title={The development of the library under the cloud computing era},
year={2012},
pages={585-587},
abstract={Cloud computing is a new IT resources model with the
distributed computing, grid computing, parallel computing and the
Internet, which can provide the users with dynamic, scalable IT
computing resources services. This article is based on the cloud
computing era, with the introduction of the trends in the cloud
computing library era. It can strive to objectively reflect the library
and promote the current progress and future trends of cloud computing
practical and theoretical aspects to provide a reference for future
research.},
keywords={cloud computing;libraries;IT resources model;Internet;cloud
computing era;distributed computing;grid computing;library
development;parallel computing;scalable IT computing resources
service;Cloud computing;Computational
modeling;Computers;Libraries;Servers;Cloud computing;application;library},
doi={10.1109/ISRA.2012.6219256},
month={June},}
@INPROCEEDINGS{6219174,
author={L. Zhang and Z. Wang and Z. Chen},
booktitle={2012 IEEE Symposium on Robotics and Applications (ISRA)},
title={P2PAV: Anti-virus through P2P},
year={2012},
pages={265-267},
abstract={Antivirus software is one of the most widely installed
software for protecting computer from malicious attacks. However,
traditional host-based antivirus needs to store all viruses' pattern or
sample, which not only needs large local storage, but also fails to
detect modern threats in time. The newly come cloud-based antivirus,
which needs no extra local storage and compares virus' pattern on
provider's cloud servers, has the disadvantages of being more likely to
be attacked when offline. This paper advocates P2PAV, a new model for
malware detection through P2P network, which takes advantages of
traditional ones and cloud antivirus, reduces local storage, and
meanwhile, improves the ability of detecting modern virus than
traditional ways, and combating virus when offline than cloud-based
antivirus.},
keywords={cloud computing;computer network security;computer
viruses;peer-to-peer computing;storage management;P2P
network;P2PAV;antivirus software;cloud-based antivirus;computer
protection;host-based antivirus;local storage reduction;malicious
attacks;malware detection;virus detection ability improvement;Cloud
computing;Databases;Engines;Malware;Anti-virus;Cloud-base
antivirus;Host-based antivirus;Malicious;P2P},
doi={10.1109/ISRA.2012.6219174},
month={June},}
@INPROCEEDINGS{6215724,
author={G. Song and S. Kim},
booktitle={2012 International Conference on Cloud Computing and Social
Networking (ICCCSN)},
title={Classifying behavior patterns of user nodes},
year={2012},
pages={1-4},
abstract={To increase the scalability of cloud computing, utilizing
resources of individual users has been widely adopted especially in
video streaming services. Accurately predicting behavior of user nodes
is critical to achieve a high efficiency in such a peer-assisted system.
Though there have been many measurement studies on peer-to-peer systems,
most of them have focused on the design and characterization of the
systems. Thus the behavior patterns of individual nodes have seldom been
studied. In this paper, we present new techniques for classifying
behavior of nodes in terms of availability and compare them with naive
manual classification. We apply a k-means clustering algorithm with
various classification criteria on real trace data of a peer-to-peer
system. Our analysis shows that there are three dominant time zones with
respect to the availability peak time. Our study will give a useful hint
to a system designer in handling churns more efficiently based on the
peer classification.},
keywords={cloud computing;pattern classification;pattern
clustering;peer-to-peer computing;resource allocation;video
streaming;availability;behavior pattern classification;cloud
computing;individual node behavior patterns;k-means clustering
algorithm;peer-to-peer systems;resource utilization;systems
characterization;systems design;user node behavior prediction;user
nodes;video streaming services;Availability;Clustering
algorithms;Computers;Peer to peer computing;Scalability;Shape;USA
Councils;Scalability;availability;behavior pattern;peer-to-peer system},
doi={10.1109/ICCCSN.2012.6215724},
month={April},}
@INPROCEEDINGS{6197123,
author={T. Yokoo and M. Yamada and S. Sakaino and S. Abe and T. Tsuji},
booktitle={2012 12th IEEE International Workshop on Advanced Motion
Control (AMC)},
title={Development of a physical therapy robot for rehabilitation
databases},
year={2012},
pages={1-6},
abstract={With increasing demands for rehabilitation, the need for
physical therapy robots is also increasing. This paper proposes the
construction of a rehabilitation database inspired with medical cloud
technologies. We discuss the possibility of establishing a new
validation methodology by generating a database based on the data
collected using rehabilitation equipment. In this research, data was
obtained by rehabilitation equipment and statistical processing was
applied to the data to investigate an example of a validation method.
The experimental results suggest that improvement of tracking property
of subjects is much larger than improvement of maneuverability.},
keywords={cloud computing;control engineering computing;database
management systems;medical computing;medical robotics;statistical
analysis;medical cloud technologies;physical therapy robot
development;rehabilitation database;rehabilitation
databases;rehabilitation equipment;statistical
processing;Databases;Force;Medical treatment;Muscles;Pneumatic
systems;Robot sensing systems;bi-articular muscle;movement
therapy;physical therapy robot;pneumatic artificial muscle},
doi={10.1109/AMC.2012.6197123},
ISSN={1943-6572},
month={March},}
@INPROCEEDINGS{6181760,
author={W. Wohlkinger and M. Vincze},
booktitle={2011 IEEE International Conference on Robotics and Biomimetics},
title={Ensemble of shape functions for 3D object classification},
year={2011},
pages={2987-2992},
abstract={This work addresses the problem of real-time 3D shape based
object class recognition, its scaling to many categories and the
reliable perception of categories. A novel shape descriptor for partial
point clouds based on shape functions is presented, capable of training
on synthetic data and classifying objects from a depth sensor in a
single partial view in a fast and robust manner. The classification task
is stated as a 3D retrieval task finding the nearest neighbors from
synthetically generated views of CAD-models to the sensed point cloud
with a Kinect-style depth sensor. The presented shape descriptor shows
that the combination of angle, point-distance and area shape functions
gives a significant boost in recognition rate against the baseline
descriptor and outperforms the state-of-the-art descriptors in our
experimental evaluation on a publicly available dataset of real-world
objects in table scene contexts with up to 200 categories.},
keywords={CAD;image classification;image retrieval;image sensors;object
recognition;shape recognition;3D object classification;3D retrieval
task;CAD-models;Kinect-style depth sensor;object class
recognition;partial point clouds;real-time 3D shape;shape
descriptor;shape functions;synthetic data;table scene
contexts;Databases;Histograms;Real time systems;Robot sensing
systems;Shape;Solid modeling;Three dimensional displays},
doi={10.1109/ROBIO.2011.6181760},
month={Dec},}
@INPROCEEDINGS{6181412,
author={J. C. Ramirez and D. Burschka},
booktitle={2011 IEEE International Conference on Robotics and Biomimetics},
title={Framework for consistent maintenance of geometric data and
abstract task-knowledge from range observations},
year={2011},
pages={963-969},
abstract={We present a framework for on-line exploration of object
attributes from range data designed to include the cognitive aspects for
surprise detection. In this framework we introduce a layered
representation of the environment that couples the pure geometric 3D
representation of the world to the abstract knowledge about the
structures in the scene. This knowledge in the higher layer represents
a-priori known, task-relevant information about structures in the world
like mass, handling properties and grasping points being examples in the
case of a manipulation task. The coupling of abstract knowledge to the
geometry in the dual layered structure of our map helps to ensure
consistency of the representation. The focus of the paper is on data
association of dense 3D points from range sensors. We introduce a
z-buffered re-projection method as a way to filter outlier information
in sensor readings and present our technique for fusion based on the
uncertainties in the map representation and the current observation. In
contrast to common registration methods, our approach does not store the
data as one rigid model but as a set of independent point clusters
(foreground) embedded in a 3D point cloud of the supporting structure
(background). This allows us to cope with dynamic changes in the world.
We register the incoming data not rigidly to the entire map but we
update independently the pose of single objects represented in our
hierarchical model. The fusion approach combines a local-heuristic with
a global-robust procedure and the correspondence search cost of O(nm) is
reduced to a set of m sub-searches with linear cost each. The benefit of
the re-projection is twofold: it helps speeding up the point matching
search by ordering the 3D data according to the manner they might have
been captured and making the matching process robust by filtering out
outliers and occluded object parts. We present the theoretical framework
and we validate our approach on range data from a binocul- r stereo
setup.},
keywords={cognition;computational complexity;image matching;image
representation;image sensors;object detection;robot vision;search
problems;sensor fusion;3D data;abstract task-knowledge;binocular stereo
setup;cognitive aspects;consistent maintenance framework;correspondence
search cost;data association;dense 3D points;dual layered
structure;filter outlier information;fusion approach;geometric 3D
representation;global-robust procedure;handling properties;layered
representation;map representation;online exploration;point matching
search;range observations;range sensors;registration methods;surprise
detection;task-relevant information;z-buffered reprojection
method;Cameras;Containers;Octrees;Sensor fusion;Three dimensional
displays;Uncertainty},
doi={10.1109/ROBIO.2011.6181412},
month={Dec},}
@INPROCEEDINGS{6181411,
author={D. Klimentjew and J. Zhang},
booktitle={2011 IEEE International Conference on Robotics and Biomimetics},
title={Adaptive sensor-fusion of depth and color information for
cognitive robotics},
year={2011},
pages={957-962},
abstract={The presented work goes one step further than only combining
data from different sensors. The corresponding points of an image and a
3D point cloud are determined through calibration. Color information is
thereby assigned to every voxel in the overlapping area of a stereo
camera system and a laser range finder. Then we analyze the image and
search for the locations, which are especially susceptible to errors by
both sensors. Depending on the ascertained situation, we try to correct
or minimize errors. By analyzing and interpreting the images as well as
removing errors we create an adaptive tool which improves multi-sensor
fusion. This allows us to correct the fused data and to perfect the
multi-modal sensor fusion or to predict the locations where the sensor
information is vague or defective. The presented results demonstrate a
clear improvement over standard procedures and show that other progress
based on our work is possible.},
keywords={calibration;cognitive systems;image colour analysis;image
fusion;intelligent robots;laser ranging;mobile robots;robot
vision;stereo image processing;3D point cloud;adaptive sensor
fusion;calibration;cognitive robotics;color information;data
fusion;depth information;error minimization;image analysis;laser range
finder;multimodal sensor fusion;stereo camera
system;Calibration;Cameras;Laser fusion;Sensor fusion;Three dimensional
displays},
doi={10.1109/ROBIO.2011.6181411},
month={Dec},}
@INPROCEEDINGS{6181575,
author={K. Ohno and K. Kensuke and E. Takeuchi and L. Zhong and M.
Tsubota and S. Tadokoro},
booktitle={2011 IEEE International Conference on Robotics and Biomimetics},
title={Unknown object modeling on the basis of vision and pushing
manipulation},
year={2011},
pages={1942-1948},
abstract={The authors aim at modeling unknown objects in human life
environment using robot technologies. In this paper, the authors propose
a method for modeling an unknown object on a desk using vision sensors,
a manipulator, and a tactile sensor. The created model of an unknown
object consists of the whole images and the 3-D point cloud with the
texture. For getting the whole images, the unknown object is rotated by
graspless manipulation. Pushing manipulation is used for the rotation.
Pushing manipulation enables one to create models for objects that
cannot be grasped by a robot hand. Tactile information is also used to
confirm the pushing manipulation. The authors propose a method that
decides a contact point and pushing direction for rotating the object
from the fragmentary visual information. During the pushing
manipulation, the object shape and the motion are measured by using a
3-D range camera and a stereo camera. The motion is estimated using 3-D
flow. The model is built by combining the estimated motion and the
measured 3-D shape. The method is useful for gathering objects'
information in human life environment.},
keywords={image texture;manipulators;motion estimation;motion
measurement;robot vision;shape measurement;stereo image
processing;tactile sensors;3D flow;3D point cloud;3D range camera;3D
shape measurement;graspless manipulation;manipulator;motion
estimation;motion measurement;object shape measurement;pushing
manipulation;robot hand;robot technology;stereo camera;tactile
information;tactile sensor;texture;unknown object modeling;vision
sensor;Cameras;Manipulators;Sensors;Shape;Solid modeling;Visualization},
doi={10.1109/ROBIO.2011.6181575},
month={Dec},}
@INPROCEEDINGS{6181419,
author={M. Matsumoto and S. Yuta},
booktitle={2011 IEEE International Conference on Robotics and Biomimetics},
title={Transportable laser range sensing system for estimation of three
dimensional motion distance and map making using in-building shape
feature},
year={2011},
pages={1004-1009},
abstract={This paper reports a transportable system for estimating a
three dimensional motion distance and making a map in the building
environment. This system takes three dimensional laser reflecting points
information, and integrates them to make a map in the point cloud format
after the estimation of sensing position using the three dimensional
motion estimation. It estimates the self-location using the information
of surfaces of walls, posts and ceilings as feature of the in-building
environment. We have developed the prototype system using a three
dimensional scanning unit with the roundly swinging mechanism, and an
posture sensor unit. The experimental results have shown the
effectiveness of the three dimensional estimation of motion distance
method designed for this system.},
keywords={laser ranging;motion estimation;dimensional laser reflecting
points information;in building shape feature;map making;motion distance
estimation;motion estimation;posture sensor;prototype system;roundly
swinging mechanism;sensing position;transportable laser range sensing
system;Buildings;Conferences;Histograms;Motion measurement;Position
measurement;Prototypes;Robot sensing systems},
doi={10.1109/ROBIO.2011.6181419},
month={Dec},}
@INPROCEEDINGS{6172974,
author={K. M. Varadarajan and I. Gupta and M. Vincze},
booktitle={2011 8th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI)},
title={Grasp hypothesis generation for parametric object 3D point cloud
models},
year={2011},
pages={1-5},
abstract={Grasping by Components (GBC) is a very important component of
any scalable and holistic grasping system that abstracts point cloud
object data to work with arbitrary shapes with no apriori data.
Superquadric representation of point cloud data is a suitable parametric
method for representing and manipulating point cloud data. Most
Superquadrics based grasp hypotheses generation methods perform the step
of classifying the parametric shapes into one of different simple shapes
with apriori established grasp hypotheses. Such a method is suitable for
simple scenarios. But for a holistic and scalable grasping system,
direct grasp hypothesis generation from Superquadric representation is
crucial. In this paper, we present an algorithm to directly estimate
grasp points and approach vectors from Superquadric parameters. We also
present results for a number of complex Superquadric shapes and show
that the results are in line with grasp hypotheses conventionally
generated by humans.},
keywords={image classification;shape recognition;solid
modelling;vectors;GBC;complex superquadric shape;grasp hypothesis
generation;grasp points;grasping by components;holistic grasping
system;parametric object 3D point cloud model;parametric shape
classification;point cloud data;point cloud object data;scalable
grasping system;superquadric parameter;superquadric
representation;vector;Equations;Fitting;Grasping;Shape;Solid
modeling;Three dimensional displays;Vectors;Approach Vectors;Dexterous
Manipulation;Grasp Hypotheses;GraspPoints;Superquadrics},
doi={10.1109/URAI.2011.6172974},
month={Nov},}
@INPROCEEDINGS{6163017,
author={D. Dey and L. Mummert and R. Sukthankar},
booktitle={2012 IEEE Workshop on the Applications of Computer Vision
(WACV)},
title={Classification of plant structures from uncalibrated image
sequences},
year={2012},
pages={329-336},
abstract={This paper demonstrates the feasibility of recovering
fine-scale plant structure in 3D point clouds by leveraging recent
advances in structure from motion and 3D point cloud segmentation
techniques. The proposed pipeline is designed to be applicable to a
broad variety of agricultural crops. A particular agricultural
application is described, motivated by the need to estimate crop yield
during the growing season. The structure of grapevines is classified
into leaves, branches, and fruit using a combination of shape and color
features, smoothed using a conditional random field (CRF). Our
experiments show a classification accuracy (AUC) of 0.98 for grapes
prior to ripening (while still green) and 0.96 for grapes during
ripening (changing color), significantly improving over the baseline
performance achieved using established methods.},
keywords={crops;feature extraction;image classification;image colour
analysis;image motion analysis;image segmentation;image sequences;3D
point cloud segmentation techniques;3D point clouds;color
features;conditional random field;crop yield;fine scale plant structure
recovery;grapevines;motion segmentation techniques;plant structure
classification;shape features;uncalibrated image
sequences;Agriculture;Feature extraction;Image color analysis;Image
reconstruction;Pipelines;Three dimensional displays;Vegetation},
doi={10.1109/WACV.2012.6163017},
ISSN={1550-5790},
month={Jan},}
@INPROCEEDINGS{6146954,
author={M. Kjaergaard and E. Bayramoglu and A. S. Massaro and K. Jensen},
booktitle={2011 10th International Conference on Machine Learning and
Applications and Workshops},
title={Terrain Mapping and Obstacle Detection Using Gaussian Processes},
year={2011},
volume={1},
pages={118-123},
abstract={In this paper we consider a probabilistic method for
extracting terrain maps from a scene and use the information to detect
potential navigation obstacles within it. The method uses Gaussian
process regression (GPR) to predict an estimate function and its
relative uncertainty. To test the new methods, we have arranged two
setups: an artificial flat surface with an object in front of the
sensors and an outdoor unstructured terrain. Two sensor types have been
used to determine the point cloud fed to the system: a 3D laser scanner
and a stereo camera pair. The results from both sensor systems show that
the estimated maps follow the terrain shape, while protrusions are
identified and may be isolated as potential obstacles. Representing the
data with a covariance function allows a dramatic reduction of the
amount of data to process, while maintaining the statistical properties
of the measured and interpolated features.},
keywords={Gaussian processes;collision avoidance;covariance
analysis;geophysical image processing;probability;stereo image
processing;terrain mapping;3D laser scanner;Gaussian process
regression;artificial flat surface;covariance function;navigation
obstacle;obstacle detection;probabilistic method;sensor
system;statistical properties;stereo camera pair;terrain mapping;terrain
shape;Cameras;Estimation;Gaussian processes;Lasers;Object
detection;Sensors;Three dimensional displays;Gaussian Processes;Obstacle
Detection;Robotics;Terrain Mapping},
doi={10.1109/ICMLA.2011.137},
month={Dec},}
@INPROCEEDINGS{6144884,
author={M. Agarwal and K. K. Biswas and M. Hanmandlu},
booktitle={The 5th International Conference on Automation, Robotics and
Applications},
title={Probabilistic intuitionistic fuzzy rule based controller},
year={2011},
pages={214-219},
abstract={This paper explores the connections between intuitionistic
fuzzy logic and probability to discover the potential of the combination
of these two forms of uncertainty in modeling of the real world events.
We have defined intuitionistic fuzzy set on probabilistic spaces and
discussed its links with evidence theory. The notion of probabilistic
intuitionistic fuzzy rule is also introduced. An approach is devised to
compute the net conditional possibility for such rules that can form the
basis to extend the existing fuzzy models. The proposed approach is
illustrated through a case-study.},
keywords={fuzzy control;fuzzy set theory;probabilistic
logic;probability;conditional possibility;evidence theory;intuitionistic
fuzzy set;probabilistic intuitionistic fuzzy rule based
controller;probabilistic spaces;probability;Clouds;Fuzzy logic;Fuzzy
sets;Probabilistic logic;Rain;Uncertainty;Valves;decision making;fuzzy
rules;inuitionisitc;possibility;probabilistic;probability},
doi={10.1109/ICARA.2011.6144884},
month={Dec},}
@INPROCEEDINGS{6138556,
author={S. Zhang and Z. Song and Z. Liang},
booktitle={2011 International Conference on Cloud and Service Computing},
title={Behavior statistics and social network analysis of online Go game
players},
year={2011},
pages={77-82},
abstract={The statistics and principle of human behavior are of great
importance in interpreting the origin and formation of human behavior in
many scopes including economics, sociology, biology and engineering
systems. As a basic step toward the understanding of Go Players'
behavior, we investigate several statistical properties, including the
distribution of total game number in different time scale, the match
intensity in one day and the lifetime distribution of typical online
users, based on large amount of Go game data from amateur online Go
platform. In addition, the social network analysis verifies that the
degree distribution of such network users also follows power-law
principle although its slope is much smaller than reported network data
set. Our findings and analysis reveals some fundamental features for
online game players, which should be vital for understanding the human
dynamics in other fields.},
keywords={Internet;behavioural sciences computing;computer games;social
networking (online);statistical analysis;Go game data;behavior
statistics;human behavior;online Go game players;social network
analysis;statistical properties;Adaptation models;Base
stations;Electronic mail;Games;Green products;Humans;Social network
services},
doi={10.1109/CSC.2011.6138556},
month={Dec},}
@INPROCEEDINGS{6130498,
author={T. Senlet and A. Elgammal},
booktitle={2011 IEEE International Conference on Computer Vision
Workshops (ICCV Workshops)},
title={A framework for global vehicle localization using stereo images
and satellite and road maps},
year={2011},
pages={2034-2041},
abstract={We present a framework for global vehicle localization and 3D
point cloud reconstruction that combines stereo visual odometry,
satellite images, and road maps under a particle-filtering architecture.
The framework focuses on the general vehicle localization scenario
without the use of global positioning system for urban and rural
environments and with the presence of moving objects. The main novelties
of our approach are using road maps and rendering accurate top views
using stereo reconstruction, and match these views with the satellite
images in order to eliminate drifts and obtain accurate global
localization. We show that our method is practicable by presenting
experimental results on a 2 km road where mostly specific road features
do not exist.},
keywords={Global Positioning System;SLAM (robots);cartography;image
reconstruction;mobile robots;particle filtering (numerical
methods);rendering (computer graphics);robot vision;stereo image
processing;3D point cloud reconstruction;autonomous driving
problem;drift elimination;global positioning system;global vehicle
localization;particle-filtering architecture;road maps;robotic
competitions;satellite images;stereo images;stereo reconstruction;stereo
visual odometry;top view rendering;Cameras;Estimation;Image
reconstruction;Roads;Satellites;Stereo vision;Vehicles},
doi={10.1109/ICCVW.2011.6130498},
month={Nov},}
@INPROCEEDINGS{6109035,
author={Xiaoping Zhang and Yufeng Xiao and Heng Wang and Hua Zhang and
Manlu Liu},
booktitle={2011 International Conference on Image Analysis and Signal
Processing},
title={Research of space target segmentation algorithm based on ToF
camera},
year={2011},
pages={227-231},
abstract={In order to make robots perceive and explain the scene, and
plan its object grabbing and processing behavior, 3D information becomes
increasingly important. For obtaining 3D expression of objects from the
ToF camera data, this paper has carried a three-step process: a) data
pre-processing, including the definition of neighborhood, the
establishment of point cloud topology, and outliers removal; b) using of
RANSAC algorithm to detect the background; c) separating the candidate
targets point cloud into different clusters. Finally, the experiment
verifies that the approximated neighborhood reduces the calculation
complexity of surface normal vector and curvature, RANSAC is a robust
plane fitting algorithm, which can get a better plane parameter
estimates steadily.},
keywords={cameras;image segmentation;object detection;robot vision;3D
information;RANSAC algorithm;ToF camera;object grabbing;robots;robust
plane fitting algorithm;space target segmentation;Cameras;Clustering
algorithms;Fitting;Robot vision systems;Three dimensional
displays;Vectors;Random Sample Consesus (RANSAC);ToF camera;plane
fitting;point cloud;segmentation},
doi={10.1109/IASP.2011.6109035},
ISSN={2156-0110},
month={Oct},}
@INPROCEEDINGS{6106765,
author={N. Muhammad and S. Lacroix},
booktitle={2011 IEEE International Symposium on Safety, Security, and
Rescue Robotics},
title={Loop closure detection using small-sized signatures from 3D LIDAR
data},
year={2011},
pages={333-338},
abstract={This article presents a technique for loop closure detection
using lidar data for autonomous mobile robots. The technique consists in
extracting, indexing and matching a set of small-sized signatures from
lidar data. These signatures are based on histograms of local surface
normals for 3D point clouds. As the robot moves, some histograms are
automatically selected as key-histograms and histograms of newly
acquired lidar scans are matched to the key-histograms in order to
detect loop-closures. Results with real 3D lidar data validate the
proposed technique.},
keywords={mobile robots;motion control;optical radar;3D LIDAR data;3D
point cloud;autonomous mobile robot;lidar scans;loop closure
detection;small-sized signature;Data mining;Histograms;Laser beams;Laser
radar;Robot sensing systems;Three dimensional displays;Histogram;Loop
closure;Velodyne;lidar},
doi={10.1109/SSRR.2011.6106765},
ISSN={2374-3247},
month={Nov},}
@INPROCEEDINGS{6106745,
author={T. Fujiwara and T. Kamegawa and A. Gofuku},
booktitle={2011 IEEE International Symposium on Safety, Security, and
Rescue Robotics},
title={Stereoscopic presentation of 3D scan data obtained by mobile robot},
year={2011},
pages={178-183},
abstract={It is necessary for rescue workers to understand the internal
environment of a damaged building. A laser scanner mounted on a rotating
platform on a mobile robot is commonly used for acquiring the 3D
information of the building. In this study, a stereoscopic presentation
system that displays 3D scan data is constructed in order to improve the
cognitive ability of an operator who teleoperates the robot. Two types
of experiments that an end user evaluates the constructed system are
conducted and the experimental results show a good tendency to use the
stereoscopic presentation.},
keywords={cognition;mobile robots;optical scanners;service robots;stereo
image processing;telerobotics;3D building information;cognitive
ability;damaged building;laser scanner;mobile robot;rotating
platform;stereoscopic 3D scan data
presentation;teleoperation;Lasers;Measurement by laser beam;Mobile
robots;Stereo image processing;Three dimensional displays;Wires;3D laser
scanner;3D point cloud;3D visualization;mobile robot;stereoscopic
presentation},
doi={10.1109/SSRR.2011.6106745},
ISSN={2374-3247},
month={Nov},}
@INPROCEEDINGS{6106760,
author={Yungeun Choe and Inwook Shim and Myung Jin Chung},
booktitle={2011 IEEE International Symposium on Safety, Security, and
Rescue Robotics},
title={Geometric-featured voxel maps for 3D mapping in urban environments},
year={2011},
pages={110-115},
abstract={3D maps including urban structures and terrain information
facilitate urban searches for robots operating in rescue scenarios.
However it suffers from large memory requirements and computation time
to deal with point clouds collected by range sensors. It is problematic
for operating rescue robots in real time. To cope with the difficulties
we propose a novel 3D mapping algorithm based on voxels. The proposed
geometric-featured voxel is designed to store geometric features
compactly. Geometric-featured voxel maps provide compact and plentiful
information about urban environments for understanding the situation of
robots' surroundings. To do so we improve geometric features to extract
the principal characteristics of urban structures by using local point
cloud statistics. For a human operator, we also introduce a
visualization method to represent the proposed voxel maps by coloring
voxel's faces and edges. In the experimental result, we analyze the
proposed feature performance, memory consumption and computation time in
order to evaluate an ability of the proposed map.},
keywords={computational geometry;data visualisation;image colour
analysis;service robots;statistical analysis;terrain mapping;3D mapping
algorithm;3D maps;computation time;feature performance;geometric
features;geometric-featured voxel maps;human operator;local point cloud
statistics;memory consumption;memory requirements;operating rescue
robots;point clouds;principal characteristics;range sensors;rescue
scenarios;robots surroundings;terrain information;urban
environments;urban structures;visualization method;voxel edges;voxel
faces;Computational modeling;Roads;Robot kinematics;Robot sensing
systems;Solid modeling;Three dimensional displays;3D Mapping;Geometric
Feature;Range Sensor;Rescue Robotics;Urban Environments;Voxel},
doi={10.1109/SSRR.2011.6106760},
ISSN={2374-3247},
month={Nov},}
@INPROCEEDINGS{6100836,
author={S. Oßwald and J. S. Gutmann and A. Hornung and M. Bennewitz},
booktitle={2011 11th IEEE-RAS International Conference on Humanoid Robots},
title={From 3D point clouds to climbing stairs: A comparison of plane
segmentation approaches for humanoids},
year={2011},
pages={93-98},
abstract={In this paper, we consider the problem of building 3D models
of complex staircases based on laser range data acquired with a
humanoid. These models have to be sufficiently accurate to enable the
robot to reliably climb up the staircase. We evaluate two
state-of-the-art approaches to plane segmentation for humanoid
navigation given 3D range data about the environment. The first approach
initially extracts line segments from neighboring 2D scan lines, which
are successively combined if they lie on the same plane. The second
approach estimates the main directions in the environment by randomly
sampling points and applying a clustering technique afterwards to find
planes orthogonal to the main directions. We propose extensions for this
basic approach to increase the robustness in complex environments which
may contain a large number of different planes and clutter. In practical
experiments, we thoroughly evaluate all methods using data acquired with
a laser-equipped Nao robot in a multi-level environment. As the
experimental results show, the reconstructed 3D models can be used to
autonomously climb up complex staircases.},
keywords={data acquisition;feature extraction;humanoid robots;image
sampling;image segmentation;laser ranging;mobile robots;pattern
clustering;random processes;robot vision;solid modelling;2D scan
lines;3D model building;3D model reconstruction;3D point clouds;climbing
robots;clustering technique;humanoid navigation;humanoid robots;laser
range data acquisition;laser-equipped Nao robot;line segment
extraction;plane segmentation approach;random sampling points;Image
reconstruction;Image segmentation;Noise;Robots;Solid modeling;Three
dimensional displays;Vectors},
doi={10.1109/Humanoids.2011.6100836},
ISSN={2164-0572},
month={Oct},}
@INPROCEEDINGS{6095074,
author={L. Spinello and K. O. Arras},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={People detection in RGB-D data},
year={2011},
pages={3838-3843},
abstract={People detection is a key issue for robots and intelligent
systems sharing a space with people. Previous works have used cameras
and 2D or 3D range finders for this task. In this paper, we present a
novel people detection approach for RGB-D data. We take inspiration from
the Histogram of Oriented Gradients (HOG) detector to design a robust
method to detect people in dense depth data, called Histogram of
Oriented Depths (HOD). HOD locally encodes the direction of depth
changes and relies on an depth-informed scale-space search that leads to
a 3-fold acceleration of the detection process. We then propose
Combo-HOD, a RGB-D detector that probabilistically combines HOD and HOG.
The experiments include a comprehensive comparison with several
alternative detection approaches including visual HOG, several variants
of HOD, a geometric person detector for 3D point clouds, and an
Haar-based AdaBoost detector. With an equal error rate of 85% in a range
up to 8m, the results demonstrate the robustness of HOD and Combo-HOD on
a real-world data set collected with a Kinect sensor in a populated
indoor environment.},
keywords={Cameras;Detectors;Histograms;Measurement;Support vector
machines;Three dimensional displays;Visualization},
doi={10.1109/IROS.2011.6095074},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6094625,
author={J. Biswas and B. Coltin and M. Veloso},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Corrective Gradient Refinement for mobile robot localization},
year={2011},
pages={73-78},
abstract={Particle filters for mobile robot localization must balance
computational requirements and accuracy of localization. Increasing the
number of particles in a particle filter improves accuracy, but also
increases the computational requirements. Hence, we investigate a
different paradigm to better utilize particles than to increase their
numbers. To this end, we introduce the Corrective Gradient Refinement
(CGR) algorithm that uses the state space gradients of the observation
model to improve accuracy while maintaining low computational
requirements. We develop an observation model for mobile robot
localization using point cloud sensors (LIDAR and depth cameras) with
vector maps. This observation model is then used to analytically compute
the state space gradients necessary for CGR. We show experimentally that
the resulting complete localization algorithm is more accurate than the
Sampling/Importance Resampling Monte Carlo Localization algorithm, while
requiring fewer particles.},
keywords={Monte Carlo methods;cameras;gradient methods;mobile
robots;optical radar;state-space methods;LIDAR;corrective gradient
refinement algorithm;depth cameras;importance resampling Monte Carlo
localization algorithm;low computational requirements;mobile robot
localization;observation model;particle filters;point cloud
sensors;state space gradients;Accuracy;Computational
modeling;Proposals;Robots;Sensors;Three dimensional displays;Vectors},
doi={10.1109/IROS.2011.6094625},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6094638,
author={B. Steder and M. Ruhnke and S. Grzonka and W. Burgard},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Place recognition in 3D scans using a combination of bag of words
and point feature based relative pose estimation},
year={2011},
pages={1249-1255},
abstract={Place recognition, i.e., the ability to recognize previously
seen parts of the environment, is one of the fundamental tasks in mobile
robotics. The wide range of applications of place recognition includes
localization (determine the initial pose), SLAM (detect loop closures),
and change detection in dynamic environments. In the past, only
relatively little work has been carried out to attack this problem using
3D range data and the majority of approaches focuses on detecting
similar structures without estimating relative poses. In this paper, we
present an algorithm based on 3D range data that is able to reliably
detect previously seen parts of the environment and at the same time
calculates an accurate transformation between the corresponding
scan-pairs. Our system uses the estimated transformation to evaluate a
candidate and in this way to more robustly reject false positives for
place recognition. We present an extensive set of experiments using
publicly available datasets in which we compare our system to other
state-of-the-art approaches.},
keywords={SLAM (robots);mobile robots;pose estimation;robot vision;3D
scans;SLAM;bag of words;change detection;localization;mobile
robotics;place recognition;point feature;relative pose
estimation;Databases;Feature extraction;Robustness;Simultaneous
localization and mapping;Three dimensional displays;Trajectory;Place
recognition;SLAM;loop closing;point clouds;range images;range sensing},
doi={10.1109/IROS.2011.6094638},
ISSN={2153-0858},
month={Sept},}
@INPROCEEDINGS{6088637,
author={N. Gorges and S. E. Navarro and H. Wörn},
booktitle={2011 15th International Conference on Advanced Robotics (ICAR)},
title={Haptic object recognition using statistical point cloud features},
year={2011},
pages={15-20},
abstract={This work presents a point cloud approach for haptic object
recognition with an anthropomorphic robot hand. It introduces several
statistical point cloud features to provide robust descriptions of
objects. It addresses the domain specific problems of sparsely populated
and distorted point clouds that result from the direct interaction with
the object. Also the contact normals registered during exploration - a
natural byproduct - are taken into account for computing some of these
features.},
keywords={computational geometry;haptic interfaces;manipulators;object
recognition;statistical analysis;anthropomorphic robot hand;contact
normals;distorted point clouds;haptic object recognition;natural
byproduct;point cloud approach;robust object descriptions;sparsely
populated point clouds;statistical point cloud features;Haptic
interfaces;Histograms;Shape;Tactile sensors;Vectors},
doi={10.1109/ICAR.2011.6088637},
month={June},}
@INPROCEEDINGS{6083105,
author={J. Larson and M. Trivedi},
booktitle={2011 14th International IEEE Conference on Intelligent
Transportation Systems (ITSC)},
title={Lidar based off-road negative obstacle detection and analysis},
year={2011},
pages={192-197},
abstract={In order for an autonomous unmanned ground vehicle (UGV) to
drive in off-road terrain at high speeds, it must analyze and understand
its surrounding terrain in realtime: it must know where it intends to
go, where are the hazards, and many details of the topography of the
terrain. Much research has been done in the way of obstacle avoidance,
terrain classification, and path planning, but still so few UGV systems
can accurately traverse off-road environments at high speeds
autonomously. One of the most dangerous hazards found off-road are
negative obstacles, mainly because they are so difficult to detect. We
present algorithms that analyze the terrain using a point cloud produced
by a 3D laser range finder, then attempt to classify the negative
obstacles using both a geometry-based method we call the Negative
Obstacle DetectoR (NODR) as well as a support vector machine (SVM)
algorithm. The terrain is analyzed with respect to a large UGV with the
sensor mounted up high as well as a small UGV with the sensor mounted
low to the ground.},
keywords={geometry;laser ranging;optical radar;remotely operated
vehicles;support vector machines;3D laser range finder;LIDAR based
off-road negative obstacle detection;NODR;SVM algorithm;UGV;autonomous
unmanned ground vehicle;geometry-based method;negative obstacle
detector;off-road environments;off-road terrain;point
cloud;sensor;support vector machine algorithm;topography;Equations;Laser
radar;Lasers;Robot sensing systems;Support vector machines;Three
dimensional displays;Vehicles},
doi={10.1109/ITSC.2011.6083105},
ISSN={2153-0009},
month={Oct},}
@INPROCEEDINGS{6003633,
author={R. Campos and R. Garcia and T. Nicosevici},
booktitle={OCEANS 2011 IEEE - Spain},
title={Surface reconstruction methods for the recovery of 3D models from
underwater interest areas},
year={2011},
pages={1-10},
abstract={3D models of seafloor interest areas having a strong 3D relief
are a rich source of information for scientists. In the recent years,
the computer vision community has been developing techniques to achieve
the discrete reconstruction of these interest areas from a set of
images. However, 3D reconstruction methods output a 3D point cloud as a
representation of the shape of the observed object/area. This
representation lacks connectivity information, and a surface that
describes the underlying object is needed for both performing
computations on the object and to achieve its correct visualization. In
this article we aim to survey the State-of-the-Art of the problem of
reconstructing the surface of an object represented by a cloud of
points. This set of points is assumed to be the result of a Computer
Vision based 3D reconstruction pipeline.},
keywords={computer vision;geophysical image processing;image
reconstruction;image representation;seafloor phenomena;3D model
recovery;3D point cloud;3D reconstruction methods;computer vision
community;discrete image reconstruction;object shape
representation;object surface reconstruction methods;seafloor interest
areas;underwater interest areas;Cameras;Computer vision;Image
reconstruction;Pipelines;Surface reconstruction;Surface treatment;Three
dimensional displays},
doi={10.1109/Oceans-Spain.2011.6003633},
month={June},}
@INPROCEEDINGS{5995724,
author={S. Ross and D. Munoz and M. Hebert and J. A. Bagnell},
booktitle={CVPR 2011},
title={Learning message-passing inference machines for structured
prediction},
year={2011},
pages={2737-2744},
abstract={Nearly every structured prediction problem in computer vision
requires approximate inference due to large and complex dependencies
among output labels. While graphical models provide a clean separation
between modeling and inference, learning these models with approximate
inference is not well understood. Furthermore, even if a good model is
learned, predictions are often inaccurate due to approximations. In this
work, instead of performing inference over a graphical model, we instead
consider the inference procedure as a composition of predictors.
Specifically, we focus on message-passing algorithms, such as Belief
Propagation, and show how they can be viewed as procedures that
sequentially predict label distributions at each node over a graph.
Given labeled graphs, we can then train the sequence of predictors to
output the correct labeling s. The result no longer corresponds to a
graphical model but simply defines an inference procedure, with strong
theoretical properties, that can be used to classify new graphs. We
demonstrate the scalability and efficacy of our approach on 3D point
cloud classification and 3D surface estimation from single images.},
keywords={belief networks;computer vision;image classification;inference
mechanisms;message passing;solid modelling;3D point cloud
classification;3D surface estimation;approximate inference;belief
propagation;computer vision;graphical model;message-passing inference
machines;predictor sequence;structured prediction;Computational
modeling;Graphical models;Inference algorithms;Prediction
algorithms;Probabilistic logic;Three dimensional displays;Training},
doi={10.1109/CVPR.2011.5995724},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{5980522,
author={Y. Kakiuchi and R. Ueda and K. Okada and M. Inaba},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Creating household environment map for environment manipulation
using color range sensors on environment and robot},
year={2011},
pages={305-310},
abstract={A humanoid robot working in a household environment with
people needs to localize and continuously update the locations of
obstacles and manipulable objects. Achieving such system, requires
strong perception method to efficiently update the frequently changing
environment. We propose a method for mapping a household environment
using multiple stereo and depth cameras located on the humanoid head and
the environment. The method relies on colored 3D point cloud data
computed from the sensors. We achieve robot localization by matching the
point clouds from the robot sensor data directly with the environment
sensor data. Object detection is performed using Iterative Closest Point
(ICP) with a database of known point cloud models. In order to guarantee
accurate object detection results, objects are only detected within the
robot sensor data. Furthermore, we utilize the environment sensor data
to map out of the obstacles as bounding convex hulls. We show
experimental results creating a household environment map with known
object labels and estimate the robot position in this map.},
keywords={SLAM (robots);cameras;collision avoidance;humanoid
robots;image colour analysis;image matching;iterative methods;object
detection;robot vision;sensors;stereo image processing;visual
perception;color range sensors;colored 3D point cloud data;depth
cameras;environment manipulation;household environment sensor
data;humanoid head;humanoid robot;iterative closest point;object
detection;point cloud matching;robot localization;robot position
estimation;robot sensor data;stereo cameras;Humanoid robots;Image color
analysis;Object detection;Robot sensing systems;Three dimensional
displays},
doi={10.1109/ICRA.2011.5980522},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5979580,
author={Jingwen Dai and R. Chung},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Head pose estimation by imperceptible structured light sensing},
year={2011},
pages={1646-1651},
abstract={We describe a method of estimating head pose in space by
imperceptible structured light sensing. Firstly, through elaborate
pattern projection strategy and camera projector synchronization,
pattern-illuminated images of the subject and the corresponding
scene-texture image are captured under imperceptible patterned
illumination. 3D positions of the key facial feature points are then
derived by a combined use of (1) the 2D facial feature points in the
scene-texture image that are localized by AAM, and (2) the point cloud
generated by structured light sensing. Eventually, the head orientation
and translation are estimated by SVD of a correlation matrix that is
generated from the 3D corresponding feature point pairs over the various
image frames. Extensive experiments show that the proposed method is
effective, accurate, and fast in 6-DOF head pose estimation, making it
suitable for use in real-time applications.},
keywords={correlation methods;image texture;matrix algebra;pose
estimation;singular value decomposition;6-DOF head pose
estimation;SVD;camera projector synchronization;correlation matrix;head
orientation estimation;head translation estimation;imperceptible
structured light sensing;pattern projection strategy;pattern-illuminated
images;point cloud generation;scene-texture image;Cameras;Face;Facial
features;Lighting;Sensors;Three dimensional displays},
doi={10.1109/ICRA.2011.5979580},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980525,
author={A. L. Rankin and L. H. Matthies and P. Bellutta},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Daytime water detection based on sky reflections},
year={2011},
pages={5329-5336},
abstract={Robust water detection is a critical perception requirement
for unmanned ground vehicle (UGV) autonomous navigation. This is
particularly true in wide-open areas where water can collect in
naturally occurring terrain depressions during periods of heavy
precipitation and form large water bodies. One of the properties of
water useful for detecting it is that its surface acts as a horizontal
mirror at large incidence angles. Water bodies can be indirectly
detected by detecting reflections of the sky below the horizon in color
imagery. The Jet Propulsion Laboratory (JPL) has implemented a water
detector based on sky reflections that geometrically locates the pixel
in the sky that is reflecting on a candidate water pixel on the ground
and predicts if the ground pixel is water based on color similarity and
local terrain features. This software detects water bodies in wide-open
areas on cross-country terrain at mid- to far-range using imagery
acquired from a forward-looking stereo pair of color cameras mounted on
a terrestrial UGV. In three test sequences approaching a pond under a
clear, overcast, and cloudy sky, the true positive detection rate was
100% when the UGV was beyond 7 meters of the water's leading edge and
the largest false positive detection rate was 0.58%. The sky reflection
based water detector has been integrated on an experimental unmanned
vehicle and field tested at Ft. Indiantown Gap, PA, USA.},
keywords={edge detection;image colour analysis;mobile
robots;reflection;remotely operated vehicles;stereo image
processing;water;Ft. Indiantown Gap;Jet Propulsion
Laboratory;PA;USA;color cameras;color similarity;daytime water
detection;edge detection;forward looking stereo pair;ground
pixel;horizontal mirror;local terrain features;perception
requirement;sky reflection;terrestrial UGV;unmanned ground vehicle
autonomous navigation;water detector;Cameras;Clouds;Image color
analysis;Image edge detection;Optical sensors;Reflection},
doi={10.1109/ICRA.2011.5980525},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980490,
author={P. Brook and M. Ciocarlie and K. Hsiao},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Collaborative grasp planning with multiple object representations},
year={2011},
pages={2851-2858},
abstract={Grasp planning based on perceived sensor data of an object can
be performed in different ways, depending on the chosen semantic
interpretation of the sensed data. For example, if the object can be
recognized and a complete 3D model is available, a different planning
tool can be selected compared to the situation in which only the raw
sensed data, such as a single point cloud, is available. Instead of
choosing between these options, we present a framework that combines
them, aiming to find consensus on how the object should be grasped by
using the information from each object representation according to their
confidence levels. We show that this method is robust to common errors
in perception, such as incorrect object recognition, while also taking
into account potential grasp execution errors due to imperfect robot
calibration. We illustrate this method on the PR2 robot by grasping
objects common in human environments.},
keywords={mobile robots;multi-robot systems;object recognition;path
planning;solid modelling;3D model;PR2 robot;collaborative grasp
planning;grasp execution error;human environment;multiple object
representation;object recognition;object representation;robot
calibration;semantic interpretation;sensor data;Clustering
algorithms;Computational modeling;Grasping;Object
recognition;Planning;Robot sensing systems},
doi={10.1109/ICRA.2011.5980490},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980407,
author={H. Men and B. Gebre and K. Pochiraju},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Color point cloud registration with 4D ICP algorithm},
year={2011},
pages={1511-1516},
abstract={This paper presents methodologies to accelerate the
registration of 3D point cloud segments by using hue data from the
associated imagery. The proposed variant of the Iterative Closest Point
(ICP) algorithm combines both normalized point range data and weighted
hue value calculated from RGB data of an image registered 3D point
cloud. A k-d tree based nearest neighbor search is used to associated
common points in {x, y, z, hue} 4D space. The unknown rigid translation
and rotation matrix required for registration is iteratively solved
using Singular Value Decomposition (SVD) method. A mobile robot mounted
scanner was used to generate color point cloud segments over a large
area. The 4D ICP registration has been compared with typical 3D ICP and
numerical results on the generated map segments shows that the 4D method
resolves ambiguity in registration and converges faster than the 3D ICP.},
keywords={SLAM (robots);image colour analysis;image
segmentation;iterative methods;mobile robots;pattern
classification;robot vision;singular value decomposition;3D point cloud
segment registration;4D ICP algorithm;RGB data;color point cloud
registration;iterative closest point algorithm;k-d tree based nearest
neighbor search;map segment generation;mobile robot mounted
scanner;rotation matrix;singular value decomposition method;translation
matrix;weighted hue value;Cameras;Computational modeling;Data
models;Image color analysis;Iterative closest point algorithm;Laser
radar;Three dimensional displays},
doi={10.1109/ICRA.2011.5980407},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5979636,
author={A. Teichman and J. Levinson and S. Thrun},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Towards 3D object recognition via classification of arbitrary
object tracks},
year={2011},
pages={4034-4041},
abstract={Object recognition is a critical next step for autonomous
robots, but a solution to the problem has remained elusive. Prior
3D-sensor-based work largely classifies individual point cloud segments
or uses class-specific trackers. In this paper, we take the approach of
classifying the tracks of all visible objects. Our new track
classification method, based on a mathematically principled method of
combining log odds estimators, is fast enough for real time use, is
non-specific to object class, and performs well (98.5% accuracy) on the
task of classifying correctly-tracked, well-segmented objects into car,
pedestrian, bicyclist, and background classes. We evaluate the
classifier's performance using the Stanford Track Collection, a new
dataset of about 1.3 million labeled point clouds in about 14,000 tracks
recorded from an autonomous vehicle research platform. This dataset,
which we make publicly available, contains tracks extracted from about
one hour of 360-degree, 10Hz depth information recorded both while
driving on busy campus streets and parked at busy intersections.},
keywords={estimation theory;image classification;image
segmentation;mobile robots;object recognition;3D object
recognition;3D-sensor-based work;autonomous robot;class-specific
tracker;log odds estimator;object track classification;point cloud
segment;Boosting;Image segmentation;Object recognition;Sensors;Three
dimensional displays;Training;Vehicle dynamics},
doi={10.1109/ICRA.2011.5979636},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5979766,
author={E. Coyle and E. G. Collins and R. G. Roberts},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Speed independent terrain classification using Singular Value
Decomposition Interpolation},
year={2011},
pages={4014-4019},
abstract={Terrain classification is key to using terrain dependent
control modes to improve performance of autonomous ground vehicles
(AGVs). One of the most viable forms of terrain classification,
reaction-based terrain classification, is subject to the problem of
speed and load dependency, which requires collecting large data sets for
algorithm training. The research presented here presents a method of
interpolating point clouds called Singular Value Decomposition
Interpolation or SVDI, which uses singular value decomposition, matrix
logarithms and Catmull-Rom splines. The estimated point clouds can then
substitute for empirical training data, thereby reducing the need to
collect large data sets for algorithm training. Here, SVDI is applied to
the problem of speed dependency using a mobile robot. Although it is
seen that interpolated point clouds are not as effective as real data,
interpolated point clouds are seen to be more effective than known point
clouds that do not correspond to the desired vehicle speed. Therefore it
is concluded that SVDI can effectively reduce the speed and load
dependence of reaction-based terrain classification.},
keywords={image classification;interpolation;mobile robots;path
planning;robot vision;singular value decomposition;splines
(mathematics);vehicles;Catmull-Rom splines;autonomous ground
vehicles;load dependence;matrix logarithms;mobile robot;point cloud
interpolation method;reaction-based terrain classification;singular
value decomposition interpolation;speed independent terrain
classification;terrain-dependent control
modes;Clouds;Interpolation;Mobile robots;Shape;Singular value
decomposition;Spline;Vehicles},
doi={10.1109/ICRA.2011.5979766},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980322,
author={G. Pandey and S. Savarese and J. R. McBride and R. M. Eustice},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Visually bootstrapped generalized ICP},
year={2011},
pages={2660-2667},
abstract={This paper reports a novel algorithm for boot strapping the
automatic registration of unstructured 3D point clouds collected using
co-registered 3D lidar and omnidirectional camera imagery. Here, we
exploit the co-registration of the 3D point cloud with the available
camera imagery to associate high dimensional feature descriptors such as
scale invariant feature transform (SIFT) or speeded up robust features
(SURF) to the 3D points. We first establish putative point
correspondence in the high dimensional feature space and then use these
correspondences in a random sample consensus (RANSAC) framework to
obtain an initial rigid body transformation that aligns the two scans.
This initial transformation is then refined in a generalized iterative
closest point (ICP) framework. The proposed method is completely data
driven and does not require any initial guess on the transformation. We
present results from a real world dataset collected by a vehicle
equipped with a 3D laser scanner and an omnidirectional camera.},
keywords={image registration;iterative methods;optical radar;optical
scanners;solid modelling;statistical analysis;transforms;3D laser
scanner;automatic registration;coregistered 3D lidar;high dimensional
feature space;iterative closest point;omnidirectional camera
imagery;putative point;random sample consensus framework;rigid body
transformation;scale invariant feature transform;speeded up robust
features;unstructured 3D point cloud;visually bootstrapped generalized
ICP;Cameras;Feature extraction;Image color analysis;Iterative closest
point algorithm;Laser radar;Robustness;Three dimensional displays},
doi={10.1109/ICRA.2011.5980322},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980187,
author={B. Steder and R. B. Rusu and K. Konolige and W. Burgard},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Point feature extraction on 3D range scans taking into account
object boundaries},
year={2011},
pages={2601-2608},
abstract={In this paper we address the topic of feature extraction in 3D
point cloud data for object recognition and pose identification. We
present a novel interest keypoint extraction method that operates on
range images generated from arbitrary 3D point clouds, which explicitly
considers the borders of the objects identified by transitions from
foreground to background. We furthermore present a feature descriptor
that takes the same information into account. We have implemented our
approach and present rigorous experiments in which we analyze the
individual components with respect to their repeatability and matching
capabilities and evaluate the usefulness for point feature based object
detection methods.},
keywords={feature extraction;object detection;pose estimation;3D point
cloud data;3D range scans;feature descriptor;matching
capabilities;object boundaries;object recognition;point feature based
object detection methods;point feature extraction;pose
identification;range images;repeatability capabilities;Data
mining;Estimation;Feature extraction;Noise;Object
recognition;Robustness;Three dimensional displays},
doi={10.1109/ICRA.2011.5980187},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980255,
author={R. Truax and R. Platt and J. Leonard},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Using prioritized relaxations to locate objects in points clouds
for manipulation},
year={2011},
pages={2091-2097},
abstract={This paper considers the problem of identifying objects of
interest in laser range point clouds for the purposes of manipulation.
One of the characteristics of perception for manipulation is that while
it is unnecessary to label all objects in the scene, it may be very
important to maximize the likelihood of correctly locating a desired
object. This paper leverages this and proposes an approach for locating
the most likely object configurations given an object parameterization
and a point cloud. While many other approaches to object localization
need to explicitly associate points with hypothesized objects, our
proposed method avoids this by optimizing relaxations of the likelihood
function rather than the exact likelihood. The result is a simple,
efficient, and robust method for locating objects that makes few
assumptions beyond the desired object parameterization and with few
parameters that require tuning.},
keywords={laser ranging;manipulators;laser range point clouds;likelihood
function;object location;object parameterization;prioritized
relaxations;robot manipulation;Data models;Equations;Mathematical
model;Object recognition;Robot sensing systems;Shape},
doi={10.1109/ICRA.2011.5980255},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5979711,
author={H. Lategahn and A. Geiger and B. Kitt},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Visual SLAM for autonomous ground vehicles},
year={2011},
pages={1732-1737},
abstract={Simultaneous Localization and Mapping (SLAM) and Visual SLAM
(V-SLAM) in particular have been an active area of research lately. In
V-SLAM the main focus is most often laid on the localization part of the
problem allowing for a drift free motion estimate. To this end, a sparse
set of landmarks is tracked and their position is estimated. However,
this set of landmarks (rendering the map) is often too sparse for tasks
in autonomous driving such as navigation, path planning, obstacle
avoidance etc. Some methods keep the raw measurements for past robot
poses to address the sparsity problem often resulting in a pose only
SLAM akin to laser scanner SLAM. For the stereo case, this is however
impractical due to the high noise of stereo reconstructed point clouds.
In this paper we propose a dense stereo V-SLAM algorithm that estimates
a dense 3D map representation which is more accurate than raw stereo
measurements. Thereto, we run a sparse V SLAM system, take the resulting
pose estimates to compute a locally dense representation from dense
stereo correspondences. This dense representation is expressed in local
coordinate systems which are tracked as part of the SLAM estimate. This
allows the dense part to be continuously updated. Our system is driven
by visual odometry priors to achieve high robustness when tracking
landmarks. Moreover, the sparse part of the SLAM system uses recently
published sub mapping techniques to achieve constant runtime complexity
most of the time. The improved accuracy over raw stereo measurements is
shown in a Monte Carlo simulation. Finally, we demonstrate the
feasibility of our method by presenting outdoor experiments of a car
like robot.},
keywords={Monte Carlo methods;SLAM (robots);collision avoidance;image
reconstruction;image representation;mobile robots;motion
estimation;navigation;object tracking;pose estimation;road
vehicles;robot vision;stereo image processing;Monte Carlo
simulation;autonomous driving;autonomous ground vehicles;car like
robot;dense 3D map representation;drift free motion estimate;landmark
tracking;local coordinate system;navigation;obstacle avoidance;path
planning;pose estimate;position estimation;robot pose;runtime
complexity;simultaneous localization and mapping;sparsity problem;stereo
reconstructed point clouds;submapping technique;visual SLAM;visual
odometry;Accuracy;Cameras;Image reconstruction;Kalman
filters;Simultaneous localization and mapping;Visualization},
doi={10.1109/ICRA.2011.5979711},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980504,
author={M. Liu and S. Huang and G. Dissanayake},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Feature based SLAM using laser sensor data with maximized
information usage},
year={2011},
pages={1811-1816},
abstract={This paper formulates the SLAM problem using 2D laser data as
an optimization problem. The environment is modeled as a set of curves
and the variables of the optimization problem are the robot poses as
well as the parameters describing the curves. There are two key
differences between this SLAM formulation and existing SLAM methods.
First, the environment is represented by continuous curves instead of
point clouds or occupancy grids. Second, all the laser readings,
including laser beams which returns its maximum range value, have been
included in the objective function. As the objective function to be
optimized contains discontinuities, it can not be solved by standard
gradient based approaches and thus a Genetic Algorithm (GA) based method
is applied. Matching of laser scans acquired from relatively far apart
robot poses is achieved by applying GA on top of the Iterative closest
point (ICP) algorithm. The new SLAM formulation and the use of a global
optimization algorithm successfully avoid the convergence to local
minimum for both the scan matching and the SLAM problem. Both simulated
and experimental data are used to demonstrate the effectiveness of the
proposed techniques.},
keywords={SLAM (robots);convergence of numerical methods;genetic
algorithms;grid computing;iterative methods;laser beam
applications;mobile robots;feature based SLAM;genetic algorithm based
method;gradient based approach;iterative closest point algorithm;laser
beam;laser reading;laser sensor data;occupancy grid;optimization
problem;Genetic algorithms;Iterative closest point
algorithm;Lasers;Optimization;Simultaneous localization and mapping},
doi={10.1109/ICRA.2011.5980504},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5979989,
author={H. Wu and L. Lou and C. C. Chen and S. Hirche and K. Kühnlenz},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Performance-oriented networked visual servo control with sending
rate scheduling},
year={2011},
pages={6180-6185},
abstract={In order to speed up image processing in visual servoing, the
distributed computational power across networks and appropriate data
transmission mechanisms are of particular interest. In this paper, a
high sampling rate of visual feedback is achieved by distributed
computation on a cloud image processing platform. For target tracking
with a networked visual servo control system, a switching control law
considering the varying feedback delay caused by image processing and
data transmission is applied to improve the control performance. A
sending rate scheduling strategy aiming at saving the network load is
proposed based on the tracking error. Experiments on a 7
degree-of-freedom (DoF) manipulator are carried out to validate the
proposed approach. The proposed approach shows a similar control
performance as a system without sending rate scheduling, however,
beneficially with largely reduced network load.},
keywords={data communication;distributed parameter
systems;feedback;manipulators;networked control systems;robot
vision;target tracking;telecommunication control;time-varying
systems;visual servoing;7 DoF manipulator;cloud image processing
platform;data transmission mechanisms;distributed computational
power;performance-oriented networked visual servo control;sending rate
scheduling strategy;switching control law;target tracking;varying
feedback delay;visual feedback;visual servoing;Control
systems;Delay;Image processing;Manipulators;Servers;Target
tracking;Visualization},
doi={10.1109/ICRA.2011.5979989},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980125,
author={X. Xiong and D. Munoz and J. A. Bagnell and M. Hebert},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={3-D scene analysis via sequenced predictions over points and
regions},
year={2011},
pages={2609-2616},
abstract={We address the problem of understanding scenes from 3-D laser
scans via per-point assignment of semantic labels. In order to mitigate
the difficulties of using a graphical model for modeling the contextual
relationships among the 3-D points, we instead propose a multi-stage
inference procedure to capture these relationships. More specifically,
we train this procedure to use point cloud statistics and learn
relational information (e.g., tree-trunks are below vegetation) over
fine (point-wise) and coarse (region-wise) scales. We evaluate our
approach on three different datasets, that were obtained from different
sensors, and demonstrate improved performance.},
keywords={solid modelling;statistics;3D laser scans;3D scene
analysis;graphical model;point cloud statistics;sequenced
predictions;Buildings;Context;Graphical models;Solid
modeling;Stacking;Training;Vegetation},
doi={10.1109/ICRA.2011.5980125},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980334,
author={M. A. Roa and R. Suárez},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Influence of contact types and uncertainties in the computation
of Independent Contact Regions},
year={2011},
pages={3317-3323},
abstract={Independent Contact Regions provide robustness in front of
finger positioning errors during an object grasping. However, different
sources of uncertainty may be present like, for instance, the friction
model used in grasp planning, indetermination of the friction
coefficients, and errors in the model of the object that affect the
positions of the boundary points as well as the direction normal to the
object surface. These sources have not been previously considered in the
computation of the Independent Contact Regions. This paper discusses how
to take into account these factors when computing the Independent
Contact Regions for discretized objects, i.e. objects described with a
cloud or a mesh of points. The considerations provided allow a more
robust result for application in grasp synthesis and regrasp planning.},
keywords={dexterous manipulators;friction;grippers;mechanical
contact;position control;boundary point position;finger positioning
error;friction coefficients;friction model;grasp planning;grasp
synthesis;independent contact region computation uncertainties;object
grasping;regrasp planning;Computational
modeling;Force;Friction;Grasping;Planning;Three dimensional
displays;Uncertainty},
doi={10.1109/ICRA.2011.5980334},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5979818,
author={B. Douillard and J. Underwood and N. Kuntz and V. Vlaskine and
A. Quadros and P. Morton and A. Frenkel},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={On the segmentation of 3D LIDAR point clouds},
year={2011},
pages={2798-2805},
abstract={This paper presents a set of segmentation methods for various
types of 3D point clouds. Segmentation of dense 3D data (e.g. Riegl
scans) is optimised via a simple yet efficient voxelisation of the
space. Prior ground extraction is empirically shown to significantly
improve segmentation performance. Segmentation of sparse 3D data (e.g.
Velodyne scans) is addressed using ground models of non-constant
resolution either providing a continuous probabilistic surface or a
terrain mesh built from the structure of a range image, both
representations providing close to real-time performance. All the
algorithms are tested on several hand labeled data sets using two novel
metrics for segmentation evaluation.},
keywords={image resolution;image segmentation;mesh generation;optical
radar;probability;radar imaging;3D LIDAR point cloud
segmentation;continuous probabilistic surface;ground
extraction;nonconstant resolution;segmentation evaluation;sparse 3D data
segmentation;terrain mesh;Data models;Gaussian processes;Image
segmentation;Measurement;Partitioning algorithms;Probabilistic
logic;Three dimensional displays},
doi={10.1109/ICRA.2011.5979818},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980424,
author={T. Suzuki and M. Kitamura and Y. Amano and T. Hashizume},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={High-accuracy GPS and GLONASS positioning by multipath mitigation
using omnidirectional infrared camera},
year={2011},
pages={311-316},
abstract={This paper describes a precision positioning technique that
can be applied to vehicles or mobile robots in urban or leafy
environments. Currently, the availability of satellite positioning is
anticipated to improve because of the presence of various positioning
satellites such as GPS of the U.S., GLONASS of Russia and GALILEO of
Europe. However, because of the serious impact of multipath on their
positioning accuracy in urban or leafy areas, such improvements in the
availability of satellite positioning do not necessarily also facilitate
high precision positioning. Our proposed technique mitigates GPS and
GLONASS multipath by means of an omnidirectional infrared (IR) camera
that can eliminate the need for invisible satellites by using IR images.
With an IR camera, the sky appears distinctively dark. This facilitates
the detection of the borderline between the sky and the surrounding
buildings, which are captured in white, because of the difference in the
atmospheric transmittance rates between visible light and IR rays. The
proposed technique can automatically and robustly mitigate GPS and
GLONASS multipath by excluding the invisible satellites. Positioning
evaluation was carried out only with visible satellites that have less
multipath errors and without using invisible satellites. The evaluation
results confirm the effectiveness of the proposed technique and the
feasibility of its highly accurate positioning.},
keywords={Global Positioning System;cameras;infrared imaging;position
control;GALILEO;GLONASS positioning;GPS;IR image;atmospheric
transmittance rate;leafy environment;mobile robot;multipath
mitigation;omnidirectional infrared camera;positioning
evaluation;positioning satellite;satellite positioning;urban
environment;visible satellite;Cameras;Clouds;Global Positioning
System;Receivers;Satellite broadcasting;Satellites;Vehicles},
doi={10.1109/ICRA.2011.5980424},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5980326,
author={C. J. Taylor and A. Cowley},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Fast scene analysis using image and range data},
year={2011},
pages={3562-3567},
abstract={This paper presents a scheme which takes as input a 3D point
cloud and an associated color image and parses the scene into a
collection of salient planar surfaces. The scheme makes use of a fast
color segmentation scheme to divide the color image into coherent
regions and the groupings suggested by this procedure are used to inform
and accelerate a RANSAC based interpretation process. Results on real
data sets are presented.},
keywords={image colour analysis;image motion analysis;image
segmentation;iterative methods;3D point cloud;RANSAC based
interpretation process;fast color segmentation scheme;fast scene
analysis;Cameras;Color;Image color analysis;Image
segmentation;Robots;Sensors;Three dimensional displays},
doi={10.1109/ICRA.2011.5980326},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5981571,
author={T. Wiemann and A. Nüchter and K. Lingemann and S. Stiene and J.
Hertzberg},
booktitle={2010 IEEE Safety Security and Rescue Robotics},
title={Automatic construction of polygonal maps from point cloud data},
year={2010},
pages={1-6},
abstract={This paper presents a novel approach to create polygonal maps
from 3D point cloud data. The gained map is augmented with an
interpretation of the scene. Our procedure produces accurate maps of
indoor environments fast and reliably. These maps are successfully used
by different robots with varying sensor configurations for reliable self
localization.},
keywords={mobile robots;path planning;3D point cloud data;indoor
environments;polygonal maps automatic construction;selflocalization},
doi={10.1109/SSRR.2010.5981571},
ISSN={2374-3247},
month={July},}
@INPROCEEDINGS{5961968,
author={K. S. Hwang and T. L. Kuo},
booktitle={Proceedings 2011 International Conference on System Science
and Engineering},
title={Authoring tool for robotic puppet play},
year={2011},
pages={574-577},
abstract={Due to rapid development of robotic technology, service robots
for daily life will be a picture at a modern home in near future;
therefore, how to make robotic devices more autonomous and secure, i.e.,
working restrictedly under commands becomes an urgent need. Humanoid
robots exiting in a digital home for daily chores or entertainment is a
good way of promoting robots. The objective of this work aims at
applying a modularized platform to embody intelligence capability for
robotic puppets.},
keywords={entertainment;home automation;humanoid robots;service
robots;software tools;authoring tool;digital home;entertainment;humanoid
robots;robotic devices;robotic puppet play;service robots;Cloud
computing;Computer architecture;Hardware;Operating
systems;Robots;XML-based;body language;cloud computing;digital
home;intelligent device},
doi={10.1109/ICSSE.2011.5961968},
ISSN={2325-0909},
month={June},}
@INPROCEEDINGS{5881377,
author={G. x. Cheng and L. l. Liu and G. y. Lou and R. s. Hu and T. Yu},
booktitle={2011 International Conference on E-Business and E-Government
(ICEE)},
title={Service-oriented collaborative business platform for LED lighting},
year={2011},
pages={1-5},
abstract={Through analyzing the problems in the development process of
LED (light-emitting diode) lighting industry, The necessity of
application of advanced information and communication technology to
solving problems has been explained, The architecture of the
service-oriented collaborative business platform for LED lighting has
been proposed, which consists of e-commerce systems and collaborative
work system, the key technologies have been indicated, finally, the
applied cases have been introduced.},
keywords={Business;Collaboration;Industries;Light emitting
diodes;Lighting;Manufacturing;Marketing and sales;Cloud
services;Collaboration business;E-commerce;LED light},
doi={10.1109/ICEBEG.2011.5881377},
month={May},}
@ARTICLE{5740987,
author={M. Meier and M. Schopfer and R. Haschke and H. Ritter},
journal={IEEE Transactions on Robotics},
title={A Probabilistic Approach to Tactile Shape Reconstruction},
year={2011},
volume={27},
number={3},
pages={630-635},
abstract={In this paper, we present a probabilistic spatial approach to
build compact 3-D representations of unknown objects probed by tactile
sensors. Our approach exploits the high frame rates provided by modern
tactile sensors and utilizes Kalman filters to build a probabilistic
model of the contact point cloud that is efficiently stored in a
kd-tree. The quality of generated shape representations is compared with
a naive averaging approach, and we show that our method provides
superior accuracy. We also evaluate the feasibility of object
classification combining the generated object representations, together
with the iterative closest point algorithm.},
keywords={Kalman filters;iterative methods;pattern
classification;probability;solid modelling;tactile sensors;tree data
structures;Kalman filters;compact 3D object representation;contact point
cloud;iterative closest point algorithm;kd-tree;naive averaging
approach;object classification;probabilistic spatial approach;tactile
sensors;tactile shape reconstruction;Iterative closest point
algorithm;Kalman filters;Manganese;Shape;Tactile sensors;3-D
reconstruction;Grasping;recognition;tactile sensing},
doi={10.1109/TRO.2011.2120830},
ISSN={1552-3098},
month={June},}
@INPROCEEDINGS{5756815,
author={M. Wopfner and J. Brich and S. Hochdorfer and C. Schlegel},
booktitle={ISR 2010 (41st International Symposium on Robotics) and
ROBOTIK 2010 (6th German Conference on Robotics)},
title={Mobile Manipulation in Service Robotics: Scene and Object
Recognition with Manipulator-Mounted Laser Ranger},
year={2010},
pages={1-7},
abstract={In this paper, we present a method for safely manipulating
simple everyday objects. The manipulation task is done in a daily
environment rather than an artificial structured lab environment. The
key feature of our method is the laser ranger mounted onto the
manipulator. This approach gives the flexibility which is needed to
interact in human environments. It also enables us to create a 3d point
cloud of the environment. This point cloud is then segmented by the
object recognition to find potential objects. The collision-free
movement of the manipulator is calculated out of the objects which are
provided by the object recognition. The robustness of our approach is
demonstrated by an everyday scenario.},
keywords={Lasers;Manipulators;Object recognition;Three dimensional
displays;Trajectory},
month={June},}
@INPROCEEDINGS{5756775,
author={M. Suzuki and E. Terada and T. Saitoh and Y. Kuroda},
booktitle={ISR 2010 (41st International Symposium on Robotics) and
ROBOTIK 2010 (6th German Conference on Robotics)},
title={Vision Based Far-Range Perception and Traversability Analysis
using Predictive Probability of Terrain Classification},
year={2010},
pages={1-6},
abstract={This paper describes a far-range visual map building and
traversability analysis using geometrical information and image
appearance, which are observed by a stereo camera, for autonomous mobile
robots. In this research, long-range polar map that has multiple
resolution of the radius direction is introduced, in order to use far
area of terrain classification data effectively that has an huge error
that grows quadratically with range. The traversability analyzing
approach that reduces the influence of observation noise and a
classification error is proposed. In this approach, appearance
information is classified into traversable or not by Support Vector
Machine to analyze from near to far area of traversability. Predictive
probability of classification is calculated to reduce failure of terrain
classification. Simultaneously, accurate analysis of traversability near
a robot is accomplished by estimating planes of each grid from point
cloud. Finaly, our system was experimented in outdoor environment.},
keywords={Cameras;Geometry;Navigation;Probability;Robot vision
systems;Support vector machines},
month={June},}
@INPROCEEDINGS{5723530,
author={B. Sun and L. Liu and C. Hu and M. Q. H. Meng},
booktitle={2010 IEEE International Conference on Robotics and Biomimetics},
title={Surgical instrument recognition and calibration for optical
tracking system},
year={2010},
pages={1376-1381},
abstract={Computer aided surgery (CAS) often involves the use of
instrument, implants and other interactive device whose pose is tracked
in real time so as to provide navigated positioning of the instrument to
guide the surgeon carrying out the operation. The infrared tracking
system has been proved to be a valuable alternative to tracking system
based on other technologies, such as magnetic, acoustic, gyroscopic and
mechanical due to its precision and robustness. Often, markers attached
to instruments for infrared tracking system are spherical and therefore
not distinguishable, making identification targets from the points cloud
necessary. Owing to the wear, tear or damage of the instrument in
service, the relative position between the work point and the markers on
instruments will change, which makes the instrument calibration before
the application obbligato. This paper presents the effective algorithms
for detecting the targets from the reconstructed 3D point cloud and
estimating the relative position of work point to markers placed on the
instrument. Both of them are essential in the procedure of pose
estimation of surgical instrument.},
keywords={calibration;interactive devices;medical computing;object
recognition;optical tracking;pose estimation;solid modelling;surgery;3D
point cloud reconstruction;computer aided surgery;infrared tracking
system;instrument calibration;interactive device;navigated positioning
system;optical tracking system;pose estimation;surgical instrument
recognition;Calibration;Data models;Instruments;Mathematical
model;Surgery;Target tracking;Three dimensional displays;Infrared
cameras;Model fitting;Model-based optical tracking;Rigid body;Surgical
instrument calibration},
doi={10.1109/ROBIO.2010.5723530},
month={Dec},}
@INPROCEEDINGS{5708341,
author={K. Koo and X. Jiang and A. Konno and M. Uchiyama},
booktitle={2010 IEEE/SICE International Symposium on System Integration},
title={Clamp grasping and insertion task automation for automobile
industry},
year={2010},
pages={293-298},
abstract={Robot is now expending their ability from simple repetitive
tasks to complex assembling tasks for supporting human life activities
and advanced manufacturing automations. Manufacturing automation needs
more narrow tolerance to assemble parts than human life support. In
manufacturing automation, insertion tasks are the most frequently used
primitive tasks. It is simple but impossible assembling tasks for robots
without calibrations by high precision sensory devices. Laser
displacement sensors are more fast, robust, and high precision one than
any other measuring devices. The high precision performance of laser
displacement sensor makes robots use it as a calibrations and feature
extractions device. This paper will address how to find out the hole
position and insertion direction vector from the acquired point clouds.
Experiments will show the automated precision insertion task of
manipulators.},
keywords={automobile industry;clamps;industrial manipulators;robotic
assembly;automobile industry automation;clamp grasping robots;insertion
task automation;laser displacement sensor;manipulators;precision
insertion task automation;robotic assembly;Clamps;Lasers;Machine
vision;Manipulators;Solid modeling;Wire},
doi={10.1109/SII.2010.5708341},
month={Dec},}
@INPROCEEDINGS{5707222,
author={W. S. Leoputra and T. Tan and S. Venkatesh},
booktitle={2010 11th International Conference on Control Automation
Robotics Vision},
title={A unified 2D-3D video scene change detection framework for mobile
camera platforms},
year={2010},
pages={2147-2153},
abstract={In this paper, we present a novel scene change detection
algorithm for mobile camera platforms. Our approach integrates sparse 3D
scene background modelling and dense 2D image background modelling into
a unified framework. The 3D scene background modelling identifies
inconsistent clusters over time in a set of 3D cloud points as the scene
changes. The 2D image background modelling further confirms the scene
changes by finding inconsistent appearances in a set of aligned images
using the classical MRF background subtraction technique. We evaluate
the performance of our proposed system on a number of challenging video
datasets obtained from a camera placed on a moving vehicle and the
experiments show that our proposed method outperforms previous works in
scene change detection, which suggested the feasibility of our approach.},
keywords={mobile handsets;video signal processing;2D image background
modeling;3D cloud points;MRF background subtraction technique;mobile
camera platforms;unified 2D-3D video scene change detection
framework;video datasets;Cameras;Clustering algorithms;Feature
extraction;Lighting;Noise;Pixel;Three dimensional displays;3D and 2D
background modelling;Scene change detection;moving images},
doi={10.1109/ICARCV.2010.5707222},
month={Dec},}
@INPROCEEDINGS{5707837,
author={H. Wu and L. Lou and C. C. Chen and S. Hirche and K. Kühnlenz},
booktitle={2010 11th International Conference on Control Automation
Robotics Vision},
title={A framework of networked visual servo control system with
distributed computation},
year={2010},
pages={1466-1471},
abstract={In this paper, a networked visual servo control system with
distributed computation is proposed to overcome the low sampling rate
problem in vision-based control systems. A real-time image data
transmission protocol based on Realtime Transport Protocol (RTP) is
developed. The captured images are sent to different processing nodes
connected over a communication network and processed in parallel. Thus,
a high sampling rate of the visual feedback is achieved under a cloud
image processing architecture. The varying image processing delay caused
by the varying number of extracted features and the random transmission
delay are modeled as a random process with Bernoulli distribution. By
using the input-delay approach, the resulted networked visual servo
control system is reformulated into a stochastic continuous-time system
with time-varying delay. Experiments on two 1-DoF linear motor modules
are carried out to validate the proposed approach. A visual servo
control system without parallel distributed computation is implemented
for comparison. The experimental results demonstrate significant
performance improvement by the proposed approach.},
keywords={computer networks;computer vision;continuous time
systems;delay systems;feature extraction;random processes;real-time
systems;sampling methods;stochastic systems;transport protocols;visual
servoing;Bernoulli distribution;cloud image processing
architecture;communication network;distributed computation;feature
extraction;image processing delay;input-delay approach;linear motor
module;networked visual servo control system;random process;random
transmission delay;real-time image data transmission protocol;realtime
transport protocol;sampling rate problem;stochastic continuous-time
system;time-varying delay;vision-based control system;visual
feedback;Cameras;Delay;Feature extraction;Real time
systems;Servosystems;Visualization},
doi={10.1109/ICARCV.2010.5707837},
month={Dec},}
@INPROCEEDINGS{5707901,
author={L. Alboul and G. Chliveros},
booktitle={2010 11th International Conference on Control Automation
Robotics Vision},
title={A system for reconstruction from point clouds in 3D:
Simplification and mesh representation},
year={2010},
pages={2301-2306},
abstract={In this paper we present a complete system for acquisition of
fused (textured) point clouds in 3D, from a Laser Range Finder (LRF) and
a CCD camera. Furthermore, we describe an approach to build and process
the resulting models, including their pre-processing and mesh
simplification. This approach allows manipulating the resulting data
structure into consistent geometric representations, which can be
further adapted based on user requirements. The advantage of our system
is that of low computational cost, ease of use and accuracy in the
representation of the environment, even without prior data smoothing.},
keywords={image reconstruction;mesh generation;solid modelling;3D
reconstruction;CCD camera;fused point clouds;geometric
representations;laser range finder;mesh representation;textured point
clouds;Calibration;Cameras;Image reconstruction;Robot vision
systems;Shape;Three dimensional displays;Point cloud
acquisition;curvature criteria;mesh reconstruction},
doi={10.1109/ICARCV.2010.5707901},
month={Dec},}
@INPROCEEDINGS{5707823,
author={M. F. Mysorewala and L. Cheded and M. S. Baig and D. O. Popa},
booktitle={2010 11th International Conference on Control Automation
Robotics Vision},
title={A distributed multi-robot adaptive sampling scheme for complex
field estimation},
year={2010},
pages={2466-2471},
abstract={Monitoring widespread environmental fields is a complex task
that is of great use in many areas, such as building models of natural
phenomenon: e.g. moisture in a crop field, oil reservoirs, etc. A
successful monitoring of such spatio-temporally distributed fields
hinges upon the use of wireless sensor networks which, through their
distributed nature, allow for an effective adaptive sampling procedure
to gather the statistical information necessary for field density
estimation. The adaptive nature of the sampling procedure used embodies
a strategy which selects the next sampling location based on the
gathered statistical information, and which evolves with past
measurements. This paper presents a novel distributed multi-robot
"Adaptive sampling algorithm", which is an extension of the algorithm
proposed earlier for complex field estimation using a single-robot only.
New formulations of sensor fusion in a centralized, decentralized,
federated-decentralized, and distributed sensor network are presented
for field density estimation, and not just cloud boundary determination.
A comparison of the various computational loads involved is included.
Simulation results show that adding an efficient partitioning of the
sampling area and parallel multi-robot sampling improves the field
reconstruction time. With N robots, more than an N-fold reduction in the
number of sampling times is observed. The federated and distributed
scheme also leads to an improved communication and computational
efficiency.},
keywords={environmental monitoring (geophysics);mobile
robots;multi-robot systems;sampling methods;sensor fusion;cloud boundary
determination;complex field estimation;distributed multirobot adaptive
sampling;distributed sensor network;environmental monitoring;field
density estimation;parallel multirobot sampling;sampling location;sensor
fusion;spatio-temporally distributed field;statistical
information;Computational complexity;Estimation;Filtering
algorithms;Information filters;Robot sensing systems;Adaptive
Sampling;Environmental Monitoring;Extended Kaiman Filter;Mobile
WSN;Sensor Fusion},
doi={10.1109/ICARCV.2010.5707823},
month={Dec},}
@INPROCEEDINGS{5675302,
author={P. Ben-Tzvi and S. Charifa and M. Shick},
booktitle={2010 IEEE International Workshop on Robotic and Sensors
Environments},
title={Extraction of 3D images using pitch-actuated 2D laser range
finder for robotic vision},
year={2010},
pages={1-6},
abstract={Volumetric mapping is essential for recognizing the 3D
environment and consequentially for the decision making process for
special-task mobile robots (such as stair climbing, inspection, or
search and rescue). In this paper, we propose a fast surface detection
algorithm based on point cloud data from a small pitch-actuated laser
range finder mounted on a mobile manipulator arm. The developed
algorithm is mainly composed of two steps: (i) gradient based
classification; and (ii) Recursive Connected Component Labeling (RCCL)
algorithm. The algorithm was experimentally validated. A complete
segmentation of different planes in the environment was also
successfully accomplished.},
keywords={actuators;decision making;dexterous manipulators;gradient
methods;image classification;laser ranging;mobile robots;path
planning;robot vision;3D environment;3D image extraction;decision
making;gradient based classification;mobile manipulator arm;mobile
robot;pitch actuated 2D laser range finder;point cloud data;recursive
connected component labeling algorithm;robotic vision;surface detection
algorithm;volumetric mapping;Classification algorithms;Lasers;Robot
kinematics;Robot sensing systems;Surface treatment;Three dimensional
displays;3D environment recognition;Robotic vision;laser range finder},
doi={10.1109/ROSE.2010.5675302},
month={Oct},}
@INPROCEEDINGS{5668084,
author={Minh-Duc Tran and Hee-Jun Kang and Young-Shick Ro},
booktitle={International Forum on Strategic Technology 2010},
title={Improved ICP control algorithm in robot surgery application},
year={2010},
pages={66-69},
abstract={The iterative closest point (ICP) algorithm has been widely
used for the registration of geometric data and wide field of activities
that range from 3D object modeling to object recognition. However, the
main practical difficulty of the ICP algorithm is to require complicated
computation steps. This paper proposes an improved solution to overcome
that problem. The proposed algorithm focuses on how to select the best
sample points so as to reduce computation time while still retain
accuracy. The obtained simulation results show that it is highly
potential to apply for medical robot surgery applications.},
keywords={iterative methods;medical robotics;mobile robots;surgery;3D
object modeling;geometric data registration;improved ICP control
algorithm;iterative closest point algorithm;medical robot surgery
application;object recognition;Robot sensing systems;ICP Algorithm;Point
clouds;Registration},
doi={10.1109/IFOST.2010.5668084},
month={Oct},}
@INPROCEEDINGS{5654401,
author={K. Irie and T. Yoshida and M. Tomono},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Mobile robot localization using stereo vision in outdoor
environments under various illumination conditions},
year={2010},
pages={5175-5181},
abstract={This paper proposes a new localization method for outdoor
navigation using a stereo camera only. Vision-based navigation in
outdoor environments is still challenging because of large illumination
changes. To cope with various illumination conditions, we use 2D
occupancy grid maps generated from 3D point clouds obtained by a stereo
camera. Furthermore, we incorporate salient line segments extracted from
the ground into the grid maps. This grid map building is not much
affected by illumination conditions. On the grid maps, the robot poses
are estimated using a particle filter that combines visual odometry and
map-matching. Experimental results showed the effectiveness and
robustness of the proposed method under various weather and illumination
conditions.},
keywords={robot vision;stereo image processing;2D occupancy grid maps;3D
point clouds;illumination conditions;mobile robot localization;outdoor
environments;outdoor navigation;stereo camera;stereo vision;vision based
navigation;visual map matching;visual odometry},
doi={10.1109/IROS.2010.5654401},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5649286,
author={D. R. Faria and R. Martins and J. Lobo and J. Dias},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Probabilistic representation of 3D object shape by in-hand
exploration},
year={2010},
pages={1560-1565},
abstract={This work presents a representation of 3D object shape using a
probabilistic volumetric map derived from in-hand exploration. The
exploratory procedure is based on contour following through the
fingertip movements on the object surface. We first consider the simple
case of having single hand exploration of a static object. The
cumulative pose data provides a 3D point cloud that is quantized to the
probabilistic volumetric map. For each voxel we have a probability
distribution for the occupancy percentage. This is then extended to
in-hand exploration of non-static objects. Since the object is moving
during the in-hand exploration, and we also consider the use of the
other hand for re-grasping, object pose has to be tracked. By keeping
track of object motion we can register data to the initial pose to build
a consistent object representation. An object centered representation is
implemented using the computed object center of mass to define its frame
of reference. Results are presented for in-hand exploration of both
static and non-static objects that show that valid models can be
obtained. The 3D object probabilistic representation can be used in
several applications related with grasp generation tasks.},
keywords={dexterous manipulators;image representation;object
tracking;shape recognition;solid modelling;statistical distributions;3D
object shape representation;in-hand exploration;probabilistic volumetric
map;probability distribution},
doi={10.1109/IROS.2010.5649286},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5650169,
author={S. J. Lee and Jae-Bok Song},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={A new sonar salient feature structure for EKF-based SLAM},
year={2010},
pages={5966-5971},
abstract={Not all line or point features capable of being extracted by
sonar sensors from cluttered home environments are useful for
simultaneous localization and mapping (SLAM) due to their ambiguity. We
present a new sonar feature structure suitable for a cluttered
environment and the extended Kalman filter (EKF)-based SLAM scheme. The
key concept is to extract circle feature clouds on salient convex
objects by sonar data association. The centroid of each circle cloud,
called a sonar salient feature, is used as a natural landmark for
EKF-based SLAM. After completing initial exploration in an unknown
environment, SLAM-able areas with sonar salient features can be defined,
and cylindrical objects are placed conveniently at weak SLAM-able areas
as a supplemental environmental saliency to enhance SLAM performance.
Experimental results demonstrate the validity and robustness of the
proposed sonar salient feature structure for EKF-based SLAM.},
keywords={Kalman filters;SLAM (robots);feature extraction;mobile
robots;path planning;sensor fusion;sonar imaging;EKF-based
SLAM;autonomous navigation;circle feature clouds extraction;extended
Kalman filter;mobile robot;simultaneous localization and mapping;sonar
data association;sonar salient feature structure},
doi={10.1109/IROS.2010.5650169},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5650541,
author={B. Douillard and J. Underwood and N. Melkumyan and S. Singh and
S. Vasudevan and C. Brunner and A. Quadros},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Hybrid elevation maps: 3D surface models for segmentation},
year={2010},
pages={1532-1538},
abstract={This paper presents an algorithm for segmenting 3D point
clouds. It extends terrain elevation models by incorporating two types
of representations: (1) ground representations based on averaging the
height in the point cloud, (2) object models based on a voxelisation of
the point cloud. The approach is deployed on Riegl data (dense 3D laser
data) acquired in a campus type of environment and compared against six
other terrain models. Amongst elevation models, it is shown to provide
the best fit to the data as well as being unique in the sense that it
jointly performs ground extraction, overhang representation and 3D
segmentation. We experimentally demonstrate that the resulting model is
also applicable to path planning.},
keywords={image segmentation;mobile robots;path planning;solid
modelling;terrain mapping;3D point cloud segmention;3D surface
model;hybrid elevation maps;path planning;terrain elevation model},
doi={10.1109/IROS.2010.5650541},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5636419,
author={G. Veiga and P. Malaca and J. N. Pires},
booktitle={2010 International Conference on Information, Networking and
Automation (ICINA)},
title={Comparative study on the use of network services in robotic
work-cells},
year={2010},
volume={1},
pages={V1-137-V1-141},
abstract={The vertical integration of the manufacturing equipments
within the information infrastructure of companies is a growing demand.
The use of service-oriented standard interfaces and the upcoming trend
of cloud computing technologies both deserve careful attention. This
paper makes a comparison of remote database access times made through
service-oriented interfaces, both for a cloud computing solution and for
a local server data base. The results are analyzed not only in terms of
the interface used (REST or SOAP) but also within the context of a
laboratorial industrial robot work-cell evaluating the effective impact
of these results when compared with other automation operations.},
keywords={Internet;factory automation;industrial robots;information
retrieval;software architecture;cloud computing technology;information
infrastructure;laboratorial industrial robot workcell;local server data
base;manufacturing equipment;network service;remote database
access;service oriented standard interface;vertical integration;IP
networks;Laboratories;Silicon;Simple object access protocol;Cloud
computing;Industrial robotics;Manufacturing automation software;Service
Oriented architectures},
doi={10.1109/ICINA.2010.5636419},
ISSN={2162-5476},
month={Oct},}
@INPROCEEDINGS{5575165,
author={N. J. Lavigne and J. A. Marshall and U. Artan},
booktitle={CCECE 2010},
title={Towards underground mine drift mapping with RFID},
year={2010},
pages={1-6},
abstract={Despite a proliferation of research in the area of mapping in
the mobile robotics community, the practical realization of a truly
useful and low-cost mapping technology for drift networks in underground
mining has yet to become available. To this end, this paper presents
recent industry-directed work in the development of a globally
consistent occupancy grid mapping tool that combines odometry, scanning
laser data, and sporadically placed RFID beacons (common in mining). In
contrast to other related works, the suggested approach takes the
philosophy that localization of the actual tag locations is not
necessary. Results from preliminary field trials conducted in an
underground network, using a modified service vehicle, demonstrate the
potential for reliable mapping by including only a few fixed beacons in
unknown locations.},
keywords={mining industry;mobile robots;optical scanners;radiofrequency
identification;RFID beacons;grid mapping tool;mobile robotics;modified
service vehicle;odometry;scanning laser data;underground mine drift
mapping;Clouds;Lasers;Measurement by laser beam;Radiofrequency
identification;Robots;Vehicles;Mining robots;RFID;robotic mapping},
doi={10.1109/CCECE.2010.5575165},
ISSN={0840-7789},
month={May},}
@INPROCEEDINGS{5553546,
author={W. Wang and Z. Ma and X. Wu and D. Wu},
booktitle={Proceedings of the 2010 International Conference on
Modelling, Identification and Control},
title={Damaged region positioning for flexible remanufacturing using
structured light},
year={2010},
pages={304-309},
abstract={An approach for damaged region inspection and positioning in a
flexible remanufacturing system is presented. The system is based on a
welding robot and a measuring robot. A structured light vision sensor is
mounted at the end of an industrial robot to serve as a measuring robot
to obtain the 3-D surface data of damaged workpiece. The damaged region
can be determined by the geometric alignment of a standard CAD model to
the obtained 3-D surface data. The aforementioned geometric alignment is
achieved by a registration algorithm. The registration process presented
in this study consists of initial and precise registration. In initial
registration stage, principal component analysis (PCA) is firstly
applied to get the main direction of point cloud. After that the
iterative closest point (ICP) algorithm is employed to achieve a precise
registration of point cloud and standard CAD model. Then the damaged
region data of workpiece is determined by calculating the errors between
point cloud and standard CAD model. Finally, some real experimental
results are given to illustrate the effectiveness of the proposed
methodology.},
keywords={CAD;flexible manufacturing systems;image sensors;industrial
robots;inspection;position control;principal component
analysis;recycling;robotic welding;CAD;damaged region
positioning;flexible remanufacturing;geometric alignment;industrial
robot;inspection;iterative closest point algorithm;measuring robot;point
cloud;principal component analysis;registration algorithm;structured
light vision sensor;welding robot;Clouds;Welding},
month={July},}
@INPROCEEDINGS{5548010,
author={M. Perrollaz and A. Spalanzani and D. Aubert},
booktitle={2010 IEEE Intelligent Vehicles Symposium},
title={Probabilistic representation of the uncertainty of stereo-vision
and application to obstacle detection},
year={2010},
pages={313-318},
abstract={Stereo-vision is extensively used for intelligent vehicles,
mainly for obstacle detection, as it provides a large amount of data.
Many authors use it as a classical 3D sensor which provides a large
tri-dimensional cloud of metric measurements, and apply methods usually
designed for other sensors, such as clustering based on a distance. For
stereo-vision, the measurement uncertainty is related to the range. For
medium to long range, often necessary in the field of intelligent
vehicles, this uncertainty has a significant impact, limiting the use of
this kind of approaches. On the other hand, some authors consider
stereo-vision more like a vision sensor and choose to directly work in
the disparity space. This provides the ability to exploit the
connectivity of the measurements, but roughly takes into consideration
the actual size of the objects. In this paper, we propose a
probabilistic representation of the specific uncertainty for
stereo-vision, which takes advantage of both aspects - distance and
disparity. The model is presented and then applied to obstacle
detection, using the occupancy grid framework. For this purpose, a
computationally-efficient implementation based on the u-disparity
approach is given.},
keywords={image sensors;mobile robots;probability;robot vision;stereo
image processing;classical 3D sensor;intelligent vehicles;measurement
uncertainty;mobile robotics;obstacle detection;probabilistic
representation;stereo-vision;u-disparity approach;vision
sensor;Cameras;Clouds;Clustering algorithms;Grid computing;Intelligent
sensors;Intelligent vehicles;Measurement uncertainty;Sensor phenomena
and characterization;Sensor systems;Vehicle detection},
doi={10.1109/IVS.2010.5548010},
ISSN={1931-0587},
month={June},}
@INPROCEEDINGS{5547630,
author={A. L. Majdik and L. Tamas and M. Popa and I. Szoke and G. Lazea},
booktitle={Control Automation (MED), 2010 18th Mediterranean Conference
on},
title={Visual odometer system to build feature based maps for mobile
robot navigation},
year={2010},
pages={1200-1205},
abstract={This paper presents a visual odometer system for mobile robot
position correction. The developed algorithm detects the same Speeded Up
Robust Features (SURF) on the stereo pair images to obtain three
dimensional point clouds at every robot location. The algorithm tracks
the displacement of the identical features viewed from different
positions to compute the robots positions. The displacements between the
point clouds are computed with the use of the Iterative Closest Point
(ICP) algorithm. The ICP is used also to register the landmarks in the
feature based map of the entire environment. The results of an indoor
office environment experiments are shown.},
keywords={distance measurement;iterative methods;mobile robots;path
planning;robot vision;stereo image processing;feature based
maps;iterative closest point algorithm;mobile robot navigation;mobile
robot position correction;robot location;speeded up robust
features;stereo pair images;visual odometer system;Clouds;Feature
extraction;Iterative closest point algorithm;Mobile robots;Robot
kinematics;Visualization},
doi={10.1109/MED.2010.5547630},
month={June},}
@INPROCEEDINGS{5540176,
author={A. Flint and C. Mei and I. Reid and D. Murray},
booktitle={2010 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition},
title={Growing semantically meaningful models for visual SLAM},
year={2010},
pages={467-474},
abstract={Though modern Visual Simultaneous Localisation and Mapping
(vSLAM) systems are capable of localising robustly and efficiently even
in the case of a monocular camera, the maps produced are typically
sparse point-clouds that are difficult to interpret and of little use
for higher-level reasoning tasks such as scene understanding or human-
machine interaction. In this paper we begin to address this deficiency,
presenting progress on expanding the competency of visual SLAM systems
to build richer maps. Specifically, we concentrate on modelling indoor
scenes using semantically meaningful surfaces and accompanying labels,
such as “floor”, “wall”, and “ceiling” - an important step towards a
representation that can support higher-level reasoning and planning. We
leverage the Manhattan world assumption and show how to extract
vanishing directions jointly across a video stream. We then propose a
guided line detector that utilises known vanishing points to extract
extremely subtle axis- aligned edges. We utilise recent advances in
single view structure recovery to building geometric scene models and
demonstrate our system operating on-line.},
keywords={SLAM (robots);computer vision;video streaming;geometric
scene;monocular camera;reasoning tasks;sparse point-clouds;video
stream;visual SLAM;visual simultaneous localisation and
mapping;Buildings;Cameras;Clouds;Floors;Image edge detection;Image
reconstruction;Layout;Photometry;Simultaneous localization and
mapping;Surface reconstruction},
doi={10.1109/CVPR.2010.5540176},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{5524557,
author={M. Richtsfeld and R. Schwarz and M. Vincze},
booktitle={19th International Workshop on Robotics in Alpe-Adria-Danube
Region (RAAD 2010)},
title={Detection of cylindrical objects in tabletop scenes},
year={2010},
pages={363-369},
abstract={This paper presents a system with a fixed robot arm and a
scanning unit on a table, which is able to detect and grasp given
cylindrical objects with cluttered adjacent objects in soft real-time.
In the fields of industrial and home robotics, the requirements of
complete 3D data, noiselessness, and obstacle-free situations are often
not provided. The contribution of this work is a fast and robust method
optimised for fitting cylinders in sparse and noisy range data under
difficult and changing light conditions recorded from a single view. The
improvements focus on the treatment of different objects on the table.
The system must distinguish between them, detect, and grasp the given
cylindrical object.},
keywords={manipulators;object detection;robot vision;cylinder
fitting;cylindrical object detection;fixed robot arm;home
robotics;industrial robotics;scanning unit;tabletop scenes;Clouds;Image
edge detection;Layout;Object detection;Path planning;Real time
systems;Robotics and automation;Robustness;Sections;Service robots},
doi={10.1109/RAAD.2010.5524557},
month={June},}
@INPROCEEDINGS{5520710,
author={R. Racasan and D. Popescu and C. Neamtu and M. Dragomir},
booktitle={2010 IEEE International Conference on Automation, Quality and
Testing, Robotics (AQTR)},
title={Integrating the concept of reverse engineering in medical
applications},
year={2010},
volume={2},
pages={1-5},
abstract={Reverse engineering is a widespread concept used in industrial
applications worldwide. It involves obtaining the geometric CAD model
from 3D point clouds acquired by scanning existing parts or products.
Applying the concept in medical applications requires specialized
competences both in the field of health and medicine as well as in CAE
(Computer Aided Engineering). Most uses of reverse engineering in
medicine have been focused on developing applications around in vitro
and in vivo case studies. This paper presents an algorithm for
implementing the reverse engineering concept for applications in the
medical field. The steps required for developing reverse engineering
applications in the medical field are presented alongside case studies
conducted by the authors. The studies are focused on showcasing various
applications as well as other possible developments in the field of
health and medicine regarding in vitro applications.},
keywords={Application software;Biomedical equipment;Clouds;Design
automation;In vitro;In vivo;Medical services;Physics computing;Reverse
engineering;Shape},
doi={10.1109/AQTR.2010.5520710},
month={May},}
@INPROCEEDINGS{5518577,
author={S. Effendi and R. Jarvis and Wai Ho Li},
booktitle={2010 IEEE Conference on Cybernetics and Intelligent Systems},
title={3D shape recovery by superquadrics model using object Silhouettes
and stereo disparity},
year={2010},
pages={82-89},
abstract={This paper describes a 3D shape reconstruction method using
vision sensors targeted at domestic robotics applications. We propose a
new method to fuse stereo disparity map and Shape from “Silhouette”
(SFS). What we mean by silhouette in this paper is different from the
existing silhouette definition. The silhouette here is not obtained from
back projecting the object contour to the image plane but rather
foreground/background stereo segmentation. Therefore, we impose the
restraint that objects must be placed on a richly textured background.
Furthermore, we use only three views to obtain the SFS. Preliminary
results on domestic objects suggests that our method can distinguish
objects such as cylinder, box and ball shapes.},
keywords={image reconstruction;image segmentation;image sensors;object
recognition;position control;robot vision;shape recognition;stereo image
processing;3D shape reconstruction method;3D shape recovery;object
silhouettes;stereo disparity map;stereo segmentation;superquadrics
model;vision sensors;Clouds;Fuses;Image reconstruction;Intelligent
robots;Intelligent sensors;Object recognition;Robot sensing
systems;Robot vision systems;Shape;Stereo image processing;3D
reconstruction;robotics;shape;silhouette;stereo;superquadrics},
doi={10.1109/ICCIS.2010.5518577},
ISSN={2326-8123},
month={June},}
@INPROCEEDINGS{5520858,
author={A. L. Majdik and I. Szoke and L. Tamas and M. Popa and G. Lazea},
booktitle={2010 IEEE International Conference on Automation, Quality and
Testing, Robotics (AQTR)},
title={Laser and vision based map building techniques for mobile robot
navigation},
year={2010},
volume={1},
pages={1-6},
abstract={We presents some preliminary results of the ongoing research
with the final goal of building an autonomous mobile robot. To achieve
this scope the mapping problem is an ineluctable one. This paper
presents a visual mapping system which detects the same Speeded Up
Robust Features (SURF) on the stereo pair images in order to obtain
three dimensional point clouds at every robot location. The algorithm
tracks the displacement of the identical features viewed from different
positions to get back the robots positions. The Iterative Closest Point
(ICP) algorithm is used to register the obtained landmarks in the
feature based map of the entire environment. Also a mapping algorithm
based on the laser system is presented which can detect the dynamic
objects that are present in the robots field. The results of an indoor
office environment experiments are shown.},
keywords={SLAM (robots);iterative methods;laser beam applications;mobile
robots;object detection;optical radar;path planning;position
control;robot vision;stereo image processing;Iterative Closest Point
algorithm;autonomous mobile robot;dynamic object detection;indoor office
environment experiments;laser based map building techniques;mapping
algorithm;mobile robot navigation;speeded up robust feature
detection;stereo pair images;vision based map building
techniques;Computer vision;Detectors;Iterative algorithms;Mobile
robots;Navigation;Object detection;Robot vision
systems;Robustness;Simultaneous localization and mapping;Stereo vision},
doi={10.1109/AQTR.2010.5520858},
month={May},}
@INPROCEEDINGS{5520872,
author={L. C. Goron and L. Tamas and I. Reti and G. Lazea},
booktitle={2010 IEEE International Conference on Automation, Quality and
Testing, Robotics (AQTR)},
title={3D Laser scanning system and 3D segmentation of urban scenes},
year={2010},
volume={1},
pages={1-5},
abstract={This paper dwells upon the promising 3D technology for mobile
robots and automation industry. The first part of the paper describes
the design details of our own 3D Time of Flight (TOF) scanning system
based on 2D laser range finder. The second part presents a specific
segmentation technique for 3D outdoor urban environments by the common
detection of plane models. In a few words, the technique separates the
raw data into sparse and dense points, followed by the segmentation of
the dense points into urban background and foreground objects. In the
end we present some experimental results of real-world data-sets taken
from the repository of the Leibniz University in Hannover, Germany.},
keywords={image segmentation;laser ranging;optical scanners;2D laser
range finder;3D laser scanning system;3D outdoor urban environments;3D
segmentation;3D time of flight scanning system;automation industry;dense
points;foreground objects;mobile robots;urban background;urban
scenes;Clouds;Humans;Large-scale systems;Laser modes;Layout;Mobile
robots;Optical design;Robot sensing systems;Robotics and
automation;Service robots;3D laser scanning system;3D point cloud;plane
extraction;urban segmentation},
doi={10.1109/AQTR.2010.5520872},
month={May},}
@INPROCEEDINGS{5520660,
author={F. Farcas and S. Albert and L. Olaru and L. Miclea and G.
Popeneciu and M. Raportaru},
booktitle={2010 IEEE International Conference on Automation, Quality and
Testing, Robotics (AQTR)},
title={Techniques and solution for monitoring the RO-14-ITIM Grid site},
year={2010},
volume={3},
pages={1-5},
abstract={Data center monitoring has evolved from a simple alert system,
into a critical element required for maximizing uptime and ensuring the
increasingly complex environment within running effectively and
efficiently. The paper presents alternatives for monitoring the location
of the RO-14-ITIM Grid site, which is located in the National Institute
for Research and Development of Isotopic and Molecular Technologies
Cluj-Napoca (INCDTIM), a key data center in the Nord-West part of
Romania. The systems are monitoring the temperature, humidity, power
failure, proximity, network, water leak and one of them is capable of
double communication, meaning that a back call into the datacenter could
activate a microphone for listen the activity in the location.},
keywords={computer centres;grid computing;monitoring;RO-14-ITIM grid
site;alert system;data center monitoring;Cloud computing;Condition
monitoring;Data engineering;Grid computing;Hardware;Network
servers;Physics;Research and development;Temperature
measurement;Temperature sensors},
doi={10.1109/AQTR.2010.5520660},
month={May},}
@INPROCEEDINGS{5512189,
author={X. Dai and X. Ning and Z. Yao and H. Shao},
booktitle={The 2010 IEEE International Conference on Information and
Automation},
title={Integrating cloud model in evolutionary algorithm for path
planning of mobile robots},
year={2010},
pages={2352-2356},
abstract={Evolutionary algorithms (EA) have been used to solve path
planning of mobile robots successfully. However, accuracy and
convergence are always topics. The cloud model which transforms
qualitative concept into quantitative description, and vice versa, is
adept in uncertainty modeling. It can be used in EA for evolving in a
uniform and natural way. An EA embedded with cloud model (EACM) for path
planning of mobile robots was proposed in this paper. The algorithm
adopted floating-point coding and realized fast converging. Simulation
results verified the efficiency of our algorithm.},
keywords={evolutionary computation;mobile robots;path planning;cloud
model;evolutionary algorithm;floating-point coding;mobile robot;path
planning;uncertainty modeling;Clouds;Control engineering
computing;Educational institutions;Evolutionary computation;Genetic
algorithms;Genetic mutations;Mobile robots;Path planning;Robotics and
automation;Uncertainty;Cloud model;Evolutionary algorithms;Mobile
robots;Path planning},
doi={10.1109/ICINFA.2010.5512189},
month={June},}
@INPROCEEDINGS{5509508,
author={Q. V. Le and D. Kamm and A. F. Kara and A. Y. Ng},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Learning to grasp objects with multiple contact points},
year={2010},
pages={5062-5069},
abstract={We consider the problem of grasping novel objects and its
application to cleaning a desk. A recent successful approach applies
machine learning to learn one grasp point in an image and a point cloud.
Although those methods are able to generalize to novel objects, they
yield suboptimal results because they rely on motion planner for finger
placements. In this paper, we extend their method to accommodate grasps
with multiple contacts. This approach works well for many human-made
objects because it models the way we grasp objects. To further improve
the grasping, we also use a method that learns the ranking between
candidates. The experiments show that our method is highly effective
compared to a state-of-the-art competitor.},
keywords={dexterous manipulators;image motion analysis;learning
(artificial intelligence);robot vision;finger placements;machine
learning;motion planner;multiple contact points;object grasping;point
cloud;Cleaning;Clouds;Fingers;Grasping;Machine learning;Motion
detection;Robotics and automation;Robots;Shape;USA Councils},
doi={10.1109/ROBOT.2010.5509508},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509306,
author={A. Nüchter and J. Elseberg and P. Schneider and D. Paulus},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Linearization of rotations for globally consistent n-scan matching},
year={2010},
pages={1373-1379},
abstract={The ICP (Iterative Closest Point) algorithm is the de facto
standard for geometric alignment of three-dimensional models when an
initial relative pose estimate is available. The basis of the algorithm
is the minimization of an error function that takes point
correspondences into account. While four closed-form solution methods
are known for minimizing this function, linearization seems necessary
for solving the global scan registration problem. This paper presents
such linear solutions for registering n-scans in a global and
simultaneous fashion. It studies parameterizations for the rigid body
transformations of the n-scan registration problem.},
keywords={geometry;image matching;image registration;linearisation
techniques;pose estimation;robot vision;ICP algorithm;closed-form
solution;error function minimization;geometric alignment;global scan
registration;globally consistent n-scan matching;iterative closest
point;linearization;n-scan registration;pose estimate;rigid body
transformation;three-dimensional model;Closed-form
solution;Clouds;Iterative algorithms;Iterative closest point
algorithm;Layout;Quaternions;Robotics and automation;Springs;Stress;USA
Councils},
doi={10.1109/ROBOT.2010.5509306},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509963,
author={M. W. McDaniel and T. Nishihata and C. A. Brooks and K. Iagnemma},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Ground plane identification using LIDAR in forested environments},
year={2010},
pages={3831-3836},
abstract={To operate autonomously in forested environments, unmanned
ground vehicles (UGVs) must be able to identify the load-bearing surface
of the terrain (i.e. the ground). This paper presents a novel two-stage
approach for identifying ground points from 3-D point clouds sensed
using LIDAR. The first stage, a local height-based filter, discards most
of the non-ground points. The second stage, based on a support vector
machine (SVM) classifier, operates on a set of geometrically defined
features to identify which of the remaining points belong to the ground.
Experimental results from two forested environments demonstrate the
effectiveness of this approach.},
keywords={computer vision;filtering theory;mobile robots;optical
radar;pattern classification;remotely operated vehicles;support vector
machines;3D point cloud;LIDAR;forested environment;ground plane
identification;load bearing surface;local height based filter;support
vector machine classifier;unmanned ground vehicle;Clouds;Digital
elevation models;Filters;Land vehicles;Laser radar;Mars;Robotics and
automation;Support vector machine classification;Support vector
machines;USA Councils},
doi={10.1109/ROBOT.2010.5509963},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509444,
author={J. F. Brethe},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Intrinsic repeatability: A new index for repeatability
characterisation},
year={2010},
pages={3849-3854},
abstract={The paper deals with the question of robot precision and how
to characterise repeatability. Hence ISO and ANSI repeatability indexes
advantages and drawbacks are analysed. A new intrinsic repeatability
index is proposed that can estimate the robot endpoint position
variability satisfying the non-bias and convergence conditions.
Computation of this index is performed using simulated straight and
drifting trajectories. Influence of load on repeatability is studied
using an experimental determination of an angular covariance matrix.
Therefrom intrinsic repeatability can be computed in every workspace
location using only this covariance matrix and the stochastic ellipsoid
theory.},
keywords={ANSI standards;ISO standards;covariance
matrices;robots;ANSI;ISO;angular covariance matrix;intrinsic
repeatability index;repeatability characterisation;robot
precision;stochastic ellipsoid theory;ANSI
standards;Clouds;Convergence;Covariance matrix;Ellipsoids;ISO
standards;Position measurement;Robotics and automation;Service
robots;Stochastic processes;Industrial Robot;Repeatability;Robot
Accuracy;Stochastic Ellipsoids},
doi={10.1109/ROBOT.2010.5509444},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509895,
author={D. Maier and A. Kleiner},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Improved GPS sensor model for mobile robots in urban terrain},
year={2010},
pages={4385-4390},
abstract={Autonomous robot navigation in out-door scenarios gains
increasing importance in various growing application areas. Whereas in
non-urban domains such as deserts the problem of successful GPS-based
navigation appears to be almost solved, navigation in urban domains
particularly in the close vicinity of buildings is still a challenging
problem. In such situations GPS accuracy significantly drops down due to
multiple signal reflections with larger objects causing the so-called
multipath error. In this paper we contribute a novel approach for
incorporating multipath errors into the conventional GPS sensor model by
analyzing environmental structures from online generated point clouds.
The approach has been validated by experimental results conducted with
an all-terrain robot operating in scenarios requiring close-to-building
navigation. Presented results show that positioning accuracy can
significantly be improved within urban domains.},
keywords={Global Positioning System;mobile robots;navigation;path
planning;GPS sensor model;GPS-based navigation;all-terrain
robot;autonomous robot navigation;close-to-building
navigation;environmental structure analysis;mobile robots;multipath
errors;urban terrain;Computer errors;Filters;Global Positioning
System;Mobile robots;Navigation;Reflection;Robot sensing
systems;Robotics and automation;Satellites;Time measurement},
doi={10.1109/ROBOT.2010.5509895},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509463,
author={N. Vaskevicius and K. Pathak and R. Pascanu and A. Birk},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Extraction of quadrics from noisy point-clouds using a sensor
noise model},
year={2010},
pages={3466-3471},
abstract={Fitting optimum quadrics on segmented noisy range-image is a
challenging task. The algebraic least-squares fit usually considered in
the literature is biased, although it is fast to compute. We extend our
previous work in planar patch extraction using a detailed sensor noise
model to the extraction of quadrics. First, the fast least-squares
method is modified to detect degeneracy automatically and it is used to
aid segmentation. The final segmented quadric patch is then refined
using a numerical maximum likelihood approach which employs the sensor
range error model. Experimental results for artificial data and two
different 3D sensors are provided to show the feasibility of our
approach.},
keywords={image denoising;image segmentation;image sensors;least squares
approximations;maximum likelihood estimation;3D sensors;algebraic
least-squares;noisy point-clouds;numerical maximum likelihood
approach;optimum quadrics;planar patch extraction;segmentation;segmented
noisy range-image;segmented quadric patch;sensor noise model;sensor
range error model;Clouds;Data mining;Ellipsoids;Image
segmentation;Layout;Maximum likelihood detection;Maximum likelihood
estimation;Robotics and automation;Surface fitting;Uncertainty},
doi={10.1109/ROBOT.2010.5509463},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509197,
author={S. Foix and G. Alenyà and J. Andrade-Cetto and C. Torras},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Object modeling using a ToF camera under an uncertainty reduction
approach},
year={2010},
pages={1306-1312},
abstract={Time-of-Flight (ToF) cameras deliver 3D images at 25 fps,
offering great potential for developing fast object modeling algorithms.
Surprisingly, this potential has not been extensively exploited up to
now. A reason for this is that, since the acquired depth images are
noisy, most of the available registration algorithms are hardly
applicable. A further difficulty is that the transformations between
views are in general not accurately known, a circumstance that
multi-view object modeling algorithms do not handle properly under noisy
conditions. In this work, we take into account both uncertainty sources
(in images and camera poses) to generate spatially consistent 3D object
models fusing multiple views with a probabilistic approach. We propose a
method to compute the covariance of the registration process, and apply
an iterative state estimation method to build object models under noisy
conditions.},
keywords={cameras;image fusion;image registration;iterative
methods;probability;state estimation;uncertain systems;3D images;3D
object models;ToF camera;fast object modeling algorithms;image
registration algorithms;iterative state estimation method;multiple view
fusion;multiview object modeling algorithms;probabilistic
approach;time-of-flight camera;uncertainty reduction
approach;Cameras;Cloud computing;Computer interfaces;Iterative
methods;Robot vision systems;Robotics and automation;Solid
modeling;Topology;USA Councils;Uncertainty},
doi={10.1109/ROBOT.2010.5509197},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509401,
author={B. Steder and G. Grisetti and W. Burgard},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Robust place recognition for 3D range data based on point features},
year={2010},
pages={1400-1405},
abstract={The problem of place recognition appears in different mobile
robot navigation problems including localization, SLAM, or change
detection in dynamic environments. Whereas this problem has been studied
intensively in the context of robot vision, relatively few approaches
are available for three-dimensional range data. In this paper, we
present a novel and robust method for place recognition based on range
images. Our algorithm matches a given 3D scan against a database using
point features and scores potential transformations by comparing
significant points in the scans. A further advantage of our approach is
that the features allow for a computation of the relative
transformations between scans which is relevant for registration
processes. Our approach has been implemented and tested on different 3D
data sets obtained outdoors. In several experiments we demonstrate the
advantages of our approach also in comparison to existing techniques.},
keywords={SLAM (robots);image recognition;mobile robots;navigation;robot
vision;3D range data;SLAM;change detection;localization;mobile
robot;navigation;point features;robot vision;robust place
recognition;Change detection algorithms;Image databases;Image
recognition;Mobile robots;Motion planning;Robot vision
systems;Robustness;Simultaneous localization and mapping;Spatial
databases;Testing;Place recognition;SLAM;loop closing;point clouds;range
images;range sensing},
doi={10.1109/ROBOT.2010.5509401},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509517,
author={P. Borges and R. Zlot and M. Bosse and S. Nuske and A. Tews},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Vision-based localization using an edge map extracted from 3D
laser range data},
year={2010},
pages={4902-4909},
abstract={Reliable real-time localization is a key component of
autonomous industrial vehicle systems. We consider the problem of using
on-board vision to determine a vehicle's pose in a known, but
non-static, environment. While feasible technologies exist for vehicle
localization, many are not suited for industrial settings where the
vehicle must operate dependably both indoors and outdoors and in a range
of lighting conditions. We extend the capabilities of an existing
vision-based localization system, in a continued effort to improve the
robustness, reliability and utility of an automated industrial vehicle
system. The vehicle pose is estimated by comparing an edge-filtered
version of a video stream to an available 3D edge map of the site. We
enhance the previous system by additionally filtering the camera input
for straight lines using a Hough transform, observing that the 3D
environment map contains only linear features. In addition, we present
an automated approach for generating 3D edge maps from laser point
clouds, removing the need for manual map surveying and also reducing the
time for map generation down from days to minutes. We present extensive
localization results in multiple lighting conditions comparing the
system with and without the proposed enhancements.},
keywords={Hough transforms;edge detection;industrial robots;laser
ranging;mobile robots;robot vision;3D laser range data;Hough
transform;autonomous industrial vehicle systems;edge map;on board
vision;vehicle localization;vision based
localization;Cameras;Clouds;Data mining;Filtering;Mobile
robots;Nonlinear filters;Real time systems;Remotely operated
vehicles;Robustness;Streaming media},
doi={10.1109/ROBOT.2010.5509517},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509914,
author={J. Shin and R. Triebel and R. Siegwart},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Unsupervised discovery of repetitive objects},
year={2010},
pages={5041-5046},
abstract={We present a novel approach for unsupervised discovery of
repetitive objects from 3D point clouds. Our method assumes that objects
are non-deformable and uses multiple occurrences of an object as the
evidence for its existence. We segment input range data by superpixel
segmentation and extract features for each segment. We search for a
group of segments where each segment matches a segment in another group
using a joint compatibility test. The discovered objects are then
verified by the Iterative Closest Point algorithm to remove false
matches. The presented method was tested on real data of complex
objects. The experiments demonstrate that the proposed approach is
capable of finding objects that occur multiple times in a scene and
distinguish apart those objects of different types.},
keywords={feature extraction;image segmentation;iterative methods;object
detection;robot vision;3D point clouds;feature extraction;iterative
closest point algorithm;multiple occurrences;repetitive
objects;superpixel segmentation;unsupervised discovery;Clouds;Data
mining;Feature extraction;Iterative closest point
algorithm;Layout;Robotics and automation;Robots;Testing;USA
Councils;Uncertainty},
doi={10.1109/ROBOT.2010.5509914},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509864,
author={Gian Diego Tipaldi and K. O. Arras},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={FLIRT - Interest regions for 2D range data},
year={2010},
pages={3616-3622},
abstract={Local image features are used for a wide range of applications
in computer vision and range imaging. While there is a great variety of
detector-descriptor combinations for image data and 3D point clouds,
there is no general method readily available for 2D range data. For this
reason, the paper first proposes a set of benchmark experiments on
detector repeatability and descriptor matching performance using known
indoor and outdoor data sets for robot navigation. Secondly, the paper
introduces FLIRT that stands for Fast Laser Interest Region Transform, a
multi-scale interest region operator for 2D range data. FLIRT combines
the best detector with the best descriptor, experimentally found in a
comprehensive analysis of alternative detector and descriptor
approaches. The analysis yields repeatability and matching performance
results similar to the values found for features in the computer vision
literature, encouraging a wide range of applications of FLIRT on 2D
range data. We finally show how FLIRT can be used in conjunction with
RANSAC to address the loop closing/global localization problem in SLAM
in indoor as well as outdoor environments. The results demonstrate that
FLIRT features have a great potential for robot navigation in terms of
precision-recall performance, efficiency and generality.},
keywords={SLAM (robots);computer vision;feature extraction;2D range
data;3D point clouds;FLIRT;Fast Laser Interest Region
Transform;RANSAC;SLAM;benchmark experiments;computer vision;descriptor
matching performance;detector-descriptor combinations;image
features;multi-scale interest region operator;range imaging;robot
navigation;Application software;Clouds;Computer vision;Detectors;Laplace
equations;Layout;Level measurement;Navigation;Robots;Simultaneous
localization and mapping},
doi={10.1109/ROBOT.2010.5509864},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5509337,
author={R. Shade and P. Newman},
booktitle={2010 IEEE International Conference on Robotics and Automation},
title={Discovering and mapping complete surfaces with stereo},
year={2010},
pages={3910-3915},
abstract={This paper is about the automated discovery and mapping of
surfaces using a stereo pair. We begin with the observation that for any
workspace which is topologically connected (i.e. does not contain free
flying islands) there exists a single surface that covers the entirety
of the workspace. We call this surface the covering surface. We assume
that while this surface is complex and self intersecting every point on
it can be imaged from a suitable camera pose and furthermore that it is
locally smooth at some finite scale - it is a manifold. We show how by
representing the covering surface as a non-planar graph of observed
pixels we are able to plan new views and importantly fuse disparity maps
from multiple views. The resulting graph can be lifted to 3D to yield a
full scene reconstruction.},
keywords={cameras;graph theory;image sensors;stereo image
processing;camera pose;disparity maps;finite scale;full scene
reconstruction;nonplanar graph;surface mapping;Cameras;Clouds;Euclidean
distance;Image edge detection;Image reconstruction;Layout;Mobile
robots;Pixel;Robotics and automation;Surface reconstruction},
doi={10.1109/ROBOT.2010.5509337},
ISSN={1050-4729},
month={May},}
@ARTICLE{5431057,
author={K. Pathak and A. Birk and N. Vaskevicius and J. Poppinga},
journal={IEEE Transactions on Robotics},
title={Fast Registration Based on Noisy Planes With Unknown
Correspondences for 3-D Mapping},
year={2010},
volume={26},
number={3},
pages={424-441},
abstract={We present a robot-pose-registration algorithm, which is
entirely based on large planar-surface patches extracted from point
clouds sampled from a three-dimensional (3-D) sensor. This approach
offers an alternative to the traditional point-to-point
iterative-closest-point (ICP) algorithm, its point-to-plane variant, as
well as newer grid-based algorithms, such as the 3-D normal distribution
transform (NDT). The simpler case of known plane correspondences is
tackled first by deriving expressions for least-squares pose estimation
considering plane-parameter uncertainty computed during plane
extraction. Closed-form expressions for covariances are also derived. To
round-off the solution, we present a new algorithm, which is called
minimally uncertain maximal consensus (MUMC), to determine the unknown
plane correspondences by maximizing geometric consistency by minimizing
the uncertainty volume in configuration space. Experimental results from
three 3-D sensors, viz., Swiss-Ranger, University of South Florida
Odetics Laser Detection and Ranging, and an actuated SICK S300, are
given. The first two have low fields of view (FOV) and moderate ranges,
while the third has a much bigger FOV and range. Experimental results
show that this approach is not only more robust than point- or
grid-based approaches in plane-rich environments, but it is also faster,
requires significantly less memory, and offers a less-cluttered
planar-patches-based visualization.},
keywords={image registration;least squares approximations;normal
distribution;pose estimation;robots;3D mapping;3D normal distribution
transform;3D sensor;fast registration;geometric consistency;large
planar-surface;least-squares pose estimation;minimally uncertain maximal
consensus;noisy planes;plane extraction;plane-parameter
uncertainty;point clouds;point-to-point iterative-closest-point
algorithm;robot-pose-registration;Localization;mapping;planes-based pose
registration;sensor fusion;simultaneous localization and mapping (SLAM)},
doi={10.1109/TRO.2010.2042989},
ISSN={1552-3098},
month={June},}
@INPROCEEDINGS{5464818,
author={D. Huber and B. Akinci and P. Tang and A. Adan and B. Okorn and
Xuehan Xiong},
booktitle={2010 44th Annual Conference on Information Sciences and
Systems (CISS)},
title={Using laser scanners for modeling and analysis in architecture,
engineering, and construction},
year={2010},
pages={1-6},
abstract={Laser scanners are rapidly gaining acceptance as a tool for
three dimensional (3D) modeling and analysis in the architecture,
engineering, and construction (AEC) domain. Since 2001, our
cross-disciplinary research team has been developing new methods for
analyzing and modeling laser scanner data, with an emphasis on
applications in the AEC domain. This paper provides an overview of our
group's recent research efforts. Our work includes improving our
understanding of the low-level aspects of laser scanner data, using
comparison methods to analyze laser scanner data and derived models, and
developing modeling and recognition algorithms to support the automatic
creation of building models from laser scan data.},
keywords={laser beam applications;optical scanners;3D
modeling;architecture domain;construction domain;engineering
domain;laser scanners;recognition algorithms;Algorithm design and
analysis;Buildings;Clouds;Data analysis;Laser modes;Laser
theory;Software tools;Solid modeling;Surface emitting lasers;Variable
speed drives;Building information models;laser scanning;reverse
engineering},
doi={10.1109/CISS.2010.5464818},
month={March},}
@INPROCEEDINGS{5459318,
author={M. Marques and M. Stošić and J. Costeira},
booktitle={2009 IEEE 12th International Conference on Computer Vision},
title={Subspace matching: Unique solution to point matching with
geometric constraints},
year={2009},
pages={1288-1294},
abstract={Finding correspondences between feature points is one of the
most relevant problems in the whole set of visual tasks. In this paper
we address the problem of matching a feature vector (or a matrix) to a
given subspace. Given any vector base of such a subspace, we observe a
linear combination of its elements with all entries swapped by an
unknown permutation. We prove that such a computationally hard integer
problem is uniquely solved in a convex set resulting from relaxing the
original problem. Also, if noise is present, based on this result, we
provide a robust estimate recurring to a linear programming-based
algorithm. We use structure-from-motion and object recognition as
motivating examples.},
keywords={convex programming;geometry;image matching;image motion
analysis;integer programming;linear programming;object
recognition;vectors;computationally hard integer problem;convex
set;feature point;feature vector matching;geometric constraint;linear
programming;noise;object recognition;permutation;point
matching;structure-from-motion;subspace matching;vector base;Acoustic
noise;Cameras;Clouds;Computer vision;Image recognition;Object
recognition;Robots;Shape;Subspace constraints;Vectors},
doi={10.1109/ICCV.2009.5459318},
ISSN={1550-5499},
month={Sept},}
@INPROCEEDINGS{5457718,
author={R. B. Rusu and A. Holzbach and M. Beetz and G. Bradski},
booktitle={2009 IEEE 12th International Conference on Computer Vision
Workshops, ICCV Workshops},
title={Detecting and segmenting objects for mobile manipulation},
year={2009},
pages={47-54},
abstract={This paper proposes a novel 3D scene interpretation approach
for robots in mobile manipulation scenarios using a set of 3D point
features (Fast Point Feature Histograms) and probabilistic graphical
methods (Conditional Random Fields). Our system uses real time stereo
with textured light to obtain dense depth maps in the robot's
manipulators working space. For the purposes of manipulation, we want to
interpret the planar supporting surfaces of the scene, recognize and
segment the object classes into their primitive parts in 6 degrees of
freedom (6DOF) so that the robot knows what it is attempting to use and
where it may be handled. The scene interpretation algorithm uses a
two-layer classification scheme: (i) we estimate Fast Point Feature
Histograms (FPFH) as local 3D point features to segment the objects of
interest into geometric primitives; and (ii) we learn and categorize
object classes using a novel Global Fast Point Feature Histogram (GFPFH)
scheme which uses the previously estimated primitives at each point. To
show the validity of our approach, we analyze the proposed system for
the problem of recognizing the object class of 20 objects in 500 table
settings scenarios. Our algorithm identifies the planar surfaces,
decomposes the scene and objects into geometric primitives with 98.27%
accuracy and uses the geometric primitives to identify the object's
class with an accuracy of 96.69%.},
keywords={image segmentation;manipulators;object
detection;probability;robot vision;3D scene interpretation
approach;conditional random fields;fast point feature
histograms;manipulators;mobile manipulation;objects detection;objects
segmention;probabilistic graphical
methods;robots;Clouds;Histograms;Image segmentation;Layout;Machine
vision;Mobile robots;Object detection;Orbital robotics;Robot vision
systems;Stereo vision},
doi={10.1109/ICCVW.2009.5457718},
month={Sept},}
@INPROCEEDINGS{5456840,
author={R. Wang and W. G. Wan and X. L. Ma and Y. P. Li},
booktitle={2010 2nd International Asia Conference on Informatics in
Control, Automation and Robotics (CAR 2010)},
title={Cloud model-based control strategy on cluster communication
coverage for wireless sensor networks},
year={2010},
volume={1},
pages={307-310},
abstract={For random deployment in sensor networks, there exists uneven
node density, seriously reducing the network performance after
clustering. The idea of the power control in CDMA is applied to handle
the communication coverage control for clusters in wireless sensor
networks. The cloud model-based uncertainty reasoning and control
mechanism are introduced to adjust the transmit power adaptively to keep
the node number within each cluster in an appropriate range in
accordance with the node distribution density. The tendentious rules of
cloud model ensure the convergence performance of the coverage control,
while random process concealled in cloud model ensures the best
iteration times in all. After clustering, there exists an appropriate
node number within each cluster, improving the network topology. The
experiment results validate its rationality and effectiveness.},
keywords={code division multiple access;model-based reasoning;power
control;telecommunication network topology;wireless sensor
networks;CDMA;cloud model-based control strategy;cluster communication
coverage;communication coverage control;network topology;node
distribution density;power control;uncertainty reasoning;wireless sensor
networks;Clouds;Communication system control;Convergence;Multiaccess
communication;Network topology;Power control;Process control;Random
processes;Uncertainty;Wireless sensor networks;cloud model;communication
coverage;control strategy;wireless sesnor networks},
doi={10.1109/CAR.2010.5456840},
ISSN={1948-3414},
month={March},}
@INPROCEEDINGS{5426067,
author={B. Turgut and R. P. Martin},
booktitle={GLOBECOM 2009 - 2009 IEEE Global Telecommunications Conference},
title={Restarting Particle Filters: An Approach to Improve the
Performance of Dynamic Indoor Localization},
year={2009},
pages={1-7},
abstract={Particle filters have been found to be effective in tracking
mobile targets in indoor environments. One frequently encountered
problem in these settings occurs when the target's movement pattern
changes unexpectedly; such as when the target turns around, enters a
room from a corridor or turns left or right at an intersection. If the
particle filter makes an incorrect prediction, it might not be able to
recover using the normal techniques of prediction, weight update and
resampling. We propose an approach to automatically restart the particle
filter by sampling the latest trusted observation when the particle
cloud diverges too much from the observations. The restart decision is
based on Kullback-Leibler divergence between the probability surfaces
associated with the current observation and the particle cloud. Through
an experimental study we show that the restart algorithm allows the
successful early recovery of stranded particle filters, in our scenarios
providing a 36% average improvement in localization accuracy.},
keywords={indoor communication;particle filtering (numerical
methods);radio tracking;Kullback-Leibler divergence;dynamic indoor
localization;mobile target tracking;restarting particle
filters;Clouds;Computer science;Indoor environments;Mobile
computing;Particle filters;Particle tracking;Robot sensing
systems;Robotics and automation;Sampling methods;Target tracking},
doi={10.1109/GLOCOM.2009.5426067},
ISSN={1930-529X},
month={Nov},}
@INPROCEEDINGS{5424150,
author={K. Ohno and S. Tadokoro and K. Nagatani and E. Koyanagi and T.
Yoshida},
booktitle={2009 IEEE International Workshop on Safety, Security Rescue
Robotics (SSRR 2009)},
title={3-D mapping of an underground mall using a tracked vehicle with
four sub-tracks},
year={2009},
pages={1-6},
abstract={The authors attempted to create a 3-D map of an underground
mall and subway station using a tracked vehicle. This paper is a field
report of the 3-D mapping of the Sendai subway station by the tracked
vehicle in Dec. 2007. From the ticket barriers to the platform, the
Sendai subway station consists of three floors. For the 3-D mapping, we
developed a tracked vehicle named Â¿KenafÂ¿, a small and light-weight
3-D laser scanner called TK scanner, and a robust 3-D scan matching
method. Kenaf can pass through ticket barriers and climb up and down
stairs, while TK scanner has a wide view angle and can measure dense 3-D
shapes. During the experiment, the robot stopped at different points and
collected 3-D scan data. The 3-D shapes were recorded by the TK scanner
as point clouds. These 3-D point clouds were integrated into a map on
the basis of odometry data on-line. The constructed map was not correct
because of the lack of robot position z and the odometry error. The 3-D
map was constructed by matching these 3-D point clouds off-line. To
increase the robustness of the matching, we used the iterative closest
point (ICP) matching method with a gravity constraint.},
keywords={geotechnical engineering;iterative methods;mobile
robots;vehicles;3D laser scanner;3D mapping;Sendai subway
station;gravity constraint;iterative closest point matching method;robot
position;tracked vehicle;underground mall;Clouds;Error
correction;Goniometers;Gravity;Iterative closest point
algorithm;Iterative methods;Robots;Robustness;Shape
measurement;Vehicles;3-D Mapping;3-D laser scanner;tracked
vehicle;underground mall},
doi={10.1109/SSRR.2009.5424150},
ISSN={2374-3247},
month={Nov},}
@INPROCEEDINGS{5423176,
author={T. Ridene and F. Goulette},
booktitle={2009 IEEE International Symposium on Computational
Intelligence in Robotics and Automation - (CIRA)},
title={Registration of fixed-and-mobile- based terrestrial Laser data
sets with DSM},
year={2009},
pages={375-380},
abstract={This study tackles the production of 3D realistic map
databases for outdoor environments. An approach based on the fusion of
heterogeneous 3D representations was studied. We propose a variant of
ICP (Iterative Closest Point) based on an adaptive dynamic threshold and
a RANSAC removing outliers process. An application of our approach was
tested on two scenarios: a registration of point clouds obtained by a
fixed terrestrial 3D Laser on a 2.5D data Digital Model of Surface
(DSM), and the registration of point clouds obtained by a Mobile Mapping
System (MMS) on DSM. The objective is to obtain coherence of a
homogeneous 3D representation, which will be the input of the following
processing step: 3D modeling. We treat various scenarios of registration
which were specified; the algorithm exploits specific pre-processing
dedicated to the studied use-cases.},
keywords={iterative methods;remote sensing by laser beam;sensor
fusion;solid modelling;terrain mapping;visual databases;2.5D data
digital model of surface;3D realistic map databases;DSM;ICP;MMS;RANSAC
removing outliers process;adaptive dynamic threshold;fixed terrestrial
3D Laser;fixed-and-mobile-based terrestrial laser data
sets;heterogeneous 3D representations fusion;iterative closest
point;mobile mapping system;Clouds;Coherence;Databases;Iterative closest
point algorithm;Laser fusion;Laser modes;Production;Surface emitting
lasers;Surface treatment;System testing;3D Registration;DSM;ICP;Mobile
Mapping Systems;Point Clouds;RANSAC},
doi={10.1109/CIRA.2009.5423176},
month={Dec},}
@INPROCEEDINGS{5417507,
author={V. Sivakumar and M. Tesfaye and D. Moema and A. Sharma and C.
Bollig},
booktitle={2009 IEEE International Geoscience and Remote Sensing
Symposium},
title={CSIR-NLC mobile LIDAR #x2014; first scientific result},
year={2009},
volume={4},
pages={IV-837-IV-840},
abstract={In this paper, we present the obtained first scientific
results from CSIR-NLC mobile LIDAR (LIght Detection And Ranging) and its
validation/comparison with other ground and space-borne measurements.
The LIDAR results are compared using aerosol measurements from the
Stratosphere Aerosol Gas Experiment (SAGE) and Optical depth derived
from sun-photometer employed under AErosol RObotic NETwork (AERONET).},
keywords={aerosols;air pollution;atmospheric measuring
apparatus;atmospheric optics;meteorology;optical radar;remote sensing by
radar;AERONET;AErosol RObotic NETwork;CSIR-NLC mobile lidar;Light
Detection And Ranging;Stratosphere Aerosol Gas Experiment;aerosol
measurements;air pollution;atmospheric measurements;ground measurement
comparison;meteorology;optical depth;remote sensing;space-borne
measurement comparison;sun-photometer;Aerosols;Africa;Atmospheric
measurements;Clouds;Laser radar;Meteorology;Mirrors;Optical
receivers;Optical transmitters;Pollution measurement;Aerosols;Air
pollution;Atmospheric measurements;Meteorology;Remote sensing},
doi={10.1109/IGARSS.2009.5417507},
ISSN={2153-6996},
month={July},}
@INPROCEEDINGS{5414238,
author={M. Marques and J. Costeira},
booktitle={2009 16th IEEE International Conference on Image Processing
(ICIP)},
title={Lamp: Linear approach for matching points},
year={2009},
pages={2113-2116},
abstract={The 3D object recognition from a single or multiple 2D images
is a very important problem in the computer vision field with a wide
range of real applications. Considering the affine camera model, the
main issue in solving this problem is the matching process between the
object's 3D points and their 2D projections. In this work, we tackle the
3D-2D matching problem. It is formulated as a finite set of independent
linear programs, solved efficiently. The 2D-2D and 3D-3D are also
discussed. To show the validity of the proposed method, synthetic and
real experiments are performed.},
keywords={computer graphics;image matching;3D object recognition;3D
point matching;3D-2D matching problem;camera model;computer
vision;independent linear programs;multiple 2D images;single 2D
images;Application software;Cameras;Clouds;Computer vision;Image
processing;Image recognition;Lamps;Object recognition;Robot vision
systems;Shape;Correspondence problem;matching;object recognition},
doi={10.1109/ICIP.2009.5414238},
ISSN={1522-4880},
month={Nov},}
@INPROCEEDINGS{5413458,
author={J. F. P. Crespo and J. F. P. Crespo and G. A. S. Lopes and P. M.
Q. Aguiar},
booktitle={2009 16th IEEE International Conference on Image Processing
(ICIP)},
title={Principal moments for efficient representation of 2D shape},
year={2009},
pages={1085-1088},
abstract={The analytic signature is a recently proposed 2D shape
representation scheme. It is tailored to the representation of shapes
described by arbitrary sets of unlabeled points, or landmarks, because
its most distinctive feature is the maximal invariance to a permutation
of those points. The shape similarity of two point clouds can then be
obtained from a direct comparison of their representations. However,
since the analytic signature is a continuous function, performing the
comparison of their densely sampled versions may result excessively
time-consuming, e.g., when dealing with large databases, even of simple
shapes. In this paper we address the problem of efficiently storing and
comparing such powerful representations. We start by showing that their
frequency spectrum is related to particular complex moments of the
shape. From this relation, we derive the bandwidth of the representation
in terms of the shape complexity. Using this result, we show that the
analytic signature can be described by a small set of complex moments.
We call this compact description the principal moments (PMs) of a shape
and show how to efficiently compare shapes using PMs. Our experiments
illustrate that the gain in efficiency comes at no cost in performance.},
keywords={computational complexity;frequency-domain analysis;image
representation;image sampling;method of moments;object recognition;2D
shape representation scheme;Image shape analysis;analytic
signature;frequency domain analysis;frequency spectrum;large
databases;moment methods;object recognition;point clouds;principal
moments;shape complexity;signal
sampling;Bandwidth;Clouds;Costs;Databases;Frequency;Object
recognition;Performance analysis;Performance gain;Robots;Shape;Frequency
domain analysis;Image shape analysis;Moment methods;Object
recognition;Signal sampling},
doi={10.1109/ICIP.2009.5413458},
ISSN={1522-4880},
month={Nov},}
@INPROCEEDINGS{5378392,
author={D. Gibbins and L. Swierkowski},
booktitle={2009 24th International Conference Image and Vision Computing
New Zealand},
title={A comparison of terrain classification using local feature
measurements of 3-dimensional colour point-cloud data},
year={2009},
pages={293-298},
abstract={This paper describes an examination of local feature measures
for the classification of 3D range data collected using low cost sensors
mounted on an unmanned air vehicle. Specifically this paper proposes and
assesses the use of local metrics such as curvature, Zernike moments,
colour and SPIN representations used in 3D object recognition for the
labelling of terrain structures. The results of applying these features
to sample data are presented. The classification scores achieved
demonstrate that structures including vegetation, ditches and similar
earthworks can be classified using such local feature estimates.},
keywords={aerospace robotics;feature extraction;image
classification;image colour analysis;object recognition;remotely
operated vehicles;sensors;3D colour point-cloud data;3D object
recognition;3D range data collection;SPIN representations;Zernike
moments;local feature measurements;terrain classification;terrain
structures;unmanned air vehicle;Computer vision;Decision support
systems;3D reconstruction;feature estimation;range data;terrain analysis
and classification},
doi={10.1109/IVCNZ.2009.5378392},
ISSN={2151-2191},
month={Nov},}
@INPROCEEDINGS{5355976,
author={A. Yogeswaran and P. Payeur},
booktitle={2009 IEEE International Workshop on Robotic and Sensors
Environments},
title={Features extraction from point clouds for automated detection of
deformations on automotive body parts},
year={2009},
pages={122-127},
abstract={This paper proposes an innovative solution to the problem of
extracting feature nodes from a 3D model and grouping nearby feature
nodes according to the likelihood that they belong to the same feature.
The technique is designed specifically with the problem of detecting
unwanted deformations on automotive body part in mind, where feature
line detection will not always give the best results. Using an octree
representation, the multiresolution method is able to analyze the model
for features of various scales. It also uses the octree data structure
for feature grouping, and provides an alternative to feature line
extraction for connecting similar feature nodes. An existing technique
is compared to the proposed approach for feature extraction, and results
are presented for the feature grouping method using a point cloud of a
miniature car model.},
keywords={automotive engineering;feature extraction;group theory;object
detection;solid modelling;tree data structures;automated
detection;automotive body parts;deformations;feature grouping;features
extraction;octree data structure;point clouds;Automotive
engineering;Clouds;Computer vision;Data mining;Deformable models;Feature
extraction;Information technology;Pattern recognition;Quality
control;Shape;automotive body parts;deformation detection;feature
extraction;pattern recognition;quality control;surface map analysis},
doi={10.1109/ROSE.2009.5355976},
month={Nov},}
@INPROCEEDINGS{5354712,
author={P. Núñez and P. Drews and R. Rocha and M. Campos and J. Dias},
booktitle={2009 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Novelty detection and 3D shape retrieval based on Gaussian
Mixture Models for autonomous surveillance robotics},
year={2009},
pages={4724-4730},
abstract={This paper describes an efficient method for retrieving the
3-dimensional shape associated to novelties in the environment of an
autonomous robot, which is equipped with a laser range finder. First,
changes are detected over the point clouds using a combination of the
/Gaussian mixture model/ (GMM) and the /earth mover's distance/ (EMD)
algorithms. Next, the shape retrieval is achieved using two different
algorithms. First, new samplings are generated from each Gaussian
function, followed by a /random sampling consensus/ (RANSAC) algorithm
to retrieve geometric primitives. Furthermore, a new algorithm is
developed to directly retrieve the shape according to the mathematical
space of Gaussian mixture. In this paper, the set of geometric
primitives has been limited to the set /C = {sphere, cylinder, plane}/.
The two shape retrieval methods are compared in terms of computational
cost and accuracy. Experimental results in various real and simulated
scenarios demonstrate the feasibility of the approach.},
keywords={Gaussian processes;image retrieval;mobile robots;object
detection;3D shape retrieval;Gaussian mixture models;autonomous
surveillance robotics;earth mover distance algorithms;geometric
primitive retrieval;random sampling consensus;Change detection
algorithms;Clouds;Intelligent robots;Mobile robots;Navigation;Robotic
assembly;Sampling methods;Shape;Simultaneous localization and
mapping;Surveillance},
doi={10.1109/IROS.2009.5354712},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5354193,
author={S. Park and H. Cheong and S. K. Park},
booktitle={2009 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Coarse-to-fine global localization for mobile robots with hybrid
maps of objects and spatial layouts},
year={2009},
pages={3993-4000},
abstract={This paper proposes a novel global localization approach that
uses hybrid maps of objects and spatial layouts. We model indoor
environments using the following visual cues from a stereo camera: local
invariant features for object recognition and their 3D positions for
object location representation. We also use a 2D laser range finder.
Therefore, we can build a hybrid local node for a topological map that
is composed of an object location map and a spatial layout map. Based on
this modeling, we suggest a coarse-to-fine strategy for the global
localization. The coarse pose is obtained by means of object recognition
and point cloud fitting, and then its fine pose is estimated with a
probabilistic scan matching algorithm. With real experiments, we show
that our proposed method can be an effective global localization
algorithm.},
keywords={image representation;mobile robots;object recognition;pose
estimation;probability;robot vision;stereo image processing;2D laser
range finder;3D position;coarse-to-fine global localization;hybrid
maps;local invariant feature;mobile robot;object location map;object
location representation;object recognition;point cloud
fitting;probabilistic scan matching algorithm;spatial layout map;stereo
camera;topological map;Cameras;Clouds;Image databases;Image
recognition;Infrared sensors;Intelligent robots;Laser modes;Mobile
robots;Object recognition;Robot sensing systems},
doi={10.1109/IROS.2009.5354193},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5353960,
author={Q. Shi and C. Zhang and N. Xi and J. Xu},
booktitle={2009 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Develop feedback robot planning method for 3D surface inspection},
year={2009},
pages={4381-4386},
abstract={The non-contact 3D sensing technology, though achieved many
success in a variety of applications, needs an automation system to
expand its applications to automotive industries for 3D shape
inspection. The reason is the difficulty for an operator to find an
optimal solution by the manual control of sensor viewpoints. The problem
of this industrial application is the capability for the sensing system
to simultaneously satisfy all requirements of competence, efficiency,
and cost. A robot-aided 3D sensing system can provide such a solution. A
CAD-guided robot view planner can automatically generate viewpoints.
Measurement accuracy can be satisfied in a certain range. However, the
unpredictable image noises still need to be compensated for better
measurement performance. In this paper, a feedback planning system is
designed and applied to the CAD-guided robot sensor planning system. The
feedback controller can automatically evaluate the accuracy of obtained
point clouds and generate new viewpoints. This feedback-based inspection
system had been successfully implemented in filling holes of a point
cloud, caused by shadows and light reflections. Such a system had been
implemented on an ABB industrial robot for a 3D measurement of an
automotive glass and a pillar. This paper introduces the developed
planning system and our current results.},
keywords={CAD;automobile industry;feedback;industrial
robots;inspection;path planning;robot vision;shape measurement;3D shape
inspection;3D surface inspection;ABB industrial robot;CAD-guided robot
sensor planning system;CAD-guided robot view planner;automotive
glass;automotive industry;feedback controller;feedback robot
planning;feedback-based inspection system;image noise;measurement
accuracy;noncontact 3D sensing;point cloud;robot-aided 3D sensing
system;Automotive engineering;Clouds;Electrical equipment
industry;Feedback;Human factors;Inspection;Robot sensing
systems;Robotics and automation;Service robots;Shape},
doi={10.1109/IROS.2009.5353960},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5354684,
author={S. May and D. Droeschel and S. Fuchs and D. Holz and A. Nüchter},
booktitle={2009 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Robust 3D-mapping with time-of-flight cameras},
year={2009},
pages={1673-1678},
abstract={Time-of-flight cameras constitute a smart and fast technology
for 3D perception but lack in measurement precision and robustness. The
authors present a comprehensive approach for 3D environment mapping
based on this technology. Imprecision of depth measurements are properly
handled by calibration and application of several filters. Robust
registration is performed by a novel extension to the Iterative Closest
Point algorithm. Remaining registration errors are reduced by global
relaxation after loop-closure and surface smoothing. A laboratory ground
truth evaluation is provided as well as 3D mapping experiments in a
larger indoor environment.},
keywords={SLAM (robots);iterative methods;mobile
robots;calibration;depth measurements;global relaxation;iterative
closest point algorithm;loop-closure;robust 3D-mapping;robust
registration;surface smoothing;time-of-flight
cameras;Calibration;Clouds;Filtering;Intelligent robots;Iterative
closest point algorithm;Layout;Robot vision
systems;Robustness;Simultaneous localization and mapping;Smart cameras},
doi={10.1109/IROS.2009.5354684},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{5284745,
author={B. Varghese and G. McKee},
booktitle={2009 IEEE 5th International Conference on Intelligent
Computer Communication and Processing},
title={Intelligent cores and intelligent agents: Two approaches to
achieve autonomy in Swarm-Array Computing},
year={2009},
pages={315-322},
abstract={How can a bridge be built between autonomic computing
approaches and parallel computing systems? How can autonomic computing
approaches be extended towards building reliable systems? How can
existing technologies be merged to provide a solution for self-managing
systems? The work reported in this paper aims to answer these questions
by proposing swarm-array computing, a novel technique inspired from
swarm robotics and built on the foundations of autonomic and parallel
computing paradigms. Two approaches based on intelligent cores and
intelligent agents are proposed to achieve autonomy in parallel
computing systems. The feasibility of the proposed approaches is
validated on a multi-agent simulator.},
keywords={digital simulation;fault tolerant computing;multi-agent
systems;parallel processing;autonomic computing;intelligent
agent;intelligent core;multiagent simulator;parallel computing
system;reliable system;self-managing system;swarm robotics;swarm-array
computing;Biology computing;Cloud computing;Computer vision;Concurrent
computing;Distributed computing;Field programmable gate
arrays;Hardware;Intelligent agent;Parallel processing;Space technology},
doi={10.1109/ICCP.2009.5284745},
month={Aug},}
@INPROCEEDINGS{5205044,
author={M. Strand and R. Dillmann},
booktitle={2009 International Conference on Information and Automation},
title={Segmentation and approximation of objects in pointclouds using
superquadrics},
year={2009},
pages={887-892},
abstract={The abstraction from raw pointclouds to high level
representations is a necessity in real world robotic applications. This
concerns the semantic description of household objects as well as the
handling of complex objects (like grasping). This paper describes a
method for the manual segmentation of objects from scenes and the
automated approximation of objects with high level descriptors. For both
tasks solutions based on superquadrics are presented.},
keywords={approximation theory;image segmentation;manipulators;robot
vision;complex objects handling;household objects semantic
description;objects approximation;objects automated
approximation;objects segmentation;superquadrics;Clouds;Engine
cylinders;Intrusion detection;Layout;Nearest neighbor searches;Pattern
recognition;Reverse engineering;Robotics and automation;Shape;Surface
fitting},
doi={10.1109/ICINFA.2009.5205044},
month={June},}
@INPROCEEDINGS{5206590,
author={D. Munoz and J. A. Bagnell and N. Vandapel and M. Hebert},
booktitle={2009 IEEE Conference on Computer Vision and Pattern
Recognition},
title={Contextual classification with functional Max-Margin Markov
Networks},
year={2009},
pages={975-982},
abstract={We address the problem of label assignment in computer vision:
given a novel 3D or 2D scene, we wish to assign a unique label to every
site (voxel, pixel, superpixel, etc.). To this end, the Markov Random
Field framework has proven to be a model of choice as it uses contextual
information to yield improved classification results over locally
independent classifiers. In this work we adapt a functional gradient
approach for learning high-dimensional parameters of random fields in
order to perform discrete, multi-label classification. With this
approach we can learn robust models involving high-order interactions
better than the previously used learning method. We validate the
approach in the context of point cloud classification and improve the
state of the art. In addition, we successfully demonstrate the
generality of the approach on the challenging vision problem of
recovering 3-D geometric surfaces from images.},
keywords={Markov processes;computer vision;functional equations;gradient
methods;image classification;learning (artificial
intelligence);optimisation;random processes;computer vision;contextual
classification;functional gradient approach;functional max-margin Markov
random field framework;high-dimensional parameter learning;independent
classifier;label assignment problem;Application
software;Boosting;Clouds;Computer vision;Context modeling;Learning
systems;Markov random fields;Path planning;Robot vision
systems;Robustness},
doi={10.1109/CVPR.2009.5206590},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{5152628,
author={Z. C. Marton and R. B. Rusu and M. Beetz},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={On fast surface reconstruction methods for large and noisy point
clouds},
year={2009},
pages={3218-3223},
abstract={In this paper we present a method for fast surface
reconstruction from large noisy datasets. Given an unorganized 3D point
cloud, our algorithm recreates the underlying surface's geometrical
properties using data resampling and a robust triangulation algorithm in
near realtime. For resulting smooth surfaces, the data is resampled with
variable densities according to previously estimated surface curvatures.
Incremental scans are easily incorporated into an existing surface mesh,
by determining the respective overlapping area and reconstructing only
the updated part of the surface mesh. The proposed framework is flexible
enough to be integrated with additional point label information, where
groups of points sharing the same label are clustered together and can
be reconstructed separately, thus allowing fast updates via triangular
mesh decoupling. To validate our approach, we present results obtained
from laser scans acquired in both indoor and outdoor environments.},
keywords={Clouds;Computer graphics;Intelligent robots;Intelligent
systems;Layout;Mobile robots;Navigation;Reconstruction
algorithms;Robotics and automation;Surface reconstruction},
doi={10.1109/ROBOT.2009.5152628},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152722,
author={M. Benallegue and A. Escande and S. Miossec and A. Kheddar},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Fast C^1 proximity queries using support mapping of
sphere-torus-patches bounding volumes},
year={2009},
pages={483-488},
abstract={STP-BV is a bounding volume made of patches of spheres and
toruses. These patches are assembled so that a convex polyhedral hull is
bulged, in a tunable way, into a strictly convex form. Strict convexity
ensures at least C^1 property of the distance function -and hence, its
gradient continuity. STP-BV were introduced in our previous work [1],
but proximity distance queries were limited to pairs of STP-BV covered
objects. In this work we present an alternative to achieve fast
proximity distance queries between a STP-BV object and any other convex
shape. This is simply made by proposing a support mapping for STP-BV to
be used with GJK algorithm [2] and its EPA extension to penetration
cases [3]. Implementation and experiments of the proposed method and its
performance are demonstrated with potential applications to robotics and
computer graphics.},
keywords={Application software;Assembly;Clouds;Computer
graphics;Robotics and automation;Robots;Shape;Target
tracking;Testing;Trajectory},
doi={10.1109/ROBOT.2009.5152722},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152851,
author={M. Bosse and R. Zlot},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Continuous 3D scan-matching with a spinning 2D laser},
year={2009},
pages={4312-4319},
abstract={Scan-matching is a technique that can be used for building
accurate maps and estimating vehicle motion by comparing a sequence of
point cloud measurements of the environment taken from a moving sensor.
One challenge that arises in mapping applications where the sensor
motion is fast relative to the measurement time is that scans become
locally distorted and difficult to align. This problem is common when
using 3D laser range sensors, which typically require more scanning time
than their 2D counterparts. Existing 3D mapping solutions either
eliminate sensor motion by taking a “stop-and-scan” approach, or attempt
to correct the motion in an open-loop fashion using odometric or
inertial sensors. We propose a solution to 3D scan-matching in which a
continuous 6DOF sensor trajectory is recovered to correct the point
cloud alignments, producing locally accurate maps and allowing for a
reliable estimate of the vehicle motion. Our method is applied to data
collected from a 3D spinning lidar sensor mounted on a skid-steer loader
vehicle to produce quality maps of outdoor scenes and estimates of the
vehicle trajectory during the mapping sequences.},
keywords={Clouds;Distortion measurement;Laser noise;Laser radar;Motion
estimation;Motion measurement;Spinning;Time
measurement;Trajectory;Vehicles;3D mapping;motion
estimation;scan-matching;spinning laser},
doi={10.1109/ROBOT.2009.5152851},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152473,
author={R. B. Rusu and N. Blodow and M. Beetz},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Fast Point Feature Histograms (FPFH) for 3D registration},
year={2009},
pages={3212-3217},
abstract={In our recent work [1], [2], we proposed Point Feature
Histograms (PFH) as robust multi-dimensional features which describe the
local geometry around a point p for 3D point cloud datasets. In this
paper, we modify their mathematical expressions and perform a rigorous
analysis on their robustness and complexity for the problem of 3D
registration for overlapping point cloud views. More concretely, we
present several optimizations that reduce their computation times
drastically by either caching previously computed values or by revising
their theoretical formulations. The latter results in a new type of
local features, called Fast Point Feature Histograms (FPFH), which
retain most of the discriminative power of the PFH. Moreover, we propose
an algorithm for the online computation of FPFH features for realtime
applications. To validate our results we demonstrate their efficiency
for 3D registration and propose a new sample consensus based method for
bringing two datasets into the convergence basin of a local non-linear
optimizer: SAC-IA (SAmple Consensus Initial Alignment).},
keywords={Clouds;Computational
complexity;Convergence;Histograms;Intelligent systems;Iterative closest
point algorithm;Optimization methods;Performance analysis;Robotics and
automation;Robustness},
doi={10.1109/ROBOT.2009.5152473},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5174672,
author={T. Stoyanov and A. J. Lilienthal},
booktitle={2009 International Conference on Advanced Robotics},
title={Maximum likelihood point cloud acquisition from a mobile platform},
year={2009},
pages={1-6},
abstract={This paper describes an approach to acquire locally consistent
range data scans from a moving sensor platform. Data from a vertically
mounted rotating laser scanner and odometry position estimates are fused
and used to estimate maximum likelihood point clouds. An estimation
algorithm is applied to reduce the accumulated error after a full
rotation of the range finder. A configuration consisting of a SICK laser
scanner mounted on a rotational actuator is described and used to
evaluate the proposed approach. The data sets analyzed suggest a
significant improvement in point cloud consistency, even over a short
travel distance.},
keywords={distance measurement;maximum likelihood estimation;mobile
robots;motion control;position control;robot vision;estimation
algorithm;laser range sensor;maximum likelihood point cloud
acquisition;mobile platform;moving sensor;odometry position
estimate;point cloud consistency;rotating laser scanner;rotational
actuator;Actuators;Cameras;Clouds;Data acquisition;Laser modes;Maximum
likelihood estimation;Robot sensing systems;Robot vision systems;Sensor
phenomena and characterization;Vehicles},
month={June},}
@INPROCEEDINGS{5174817,
author={S. Park and S. K. Park},
booktitle={2009 International Conference on Advanced Robotics},
title={Vision-based global localization for mobile robots using an
object and spatial layout-based hybrid map},
year={2009},
pages={1-6},
abstract={This paper proposes a novel vision-based global localization
approach that uses an object and spatial layout based hybrid map. For
environment modeling, we use the following visual cues with a stereo
camera; local invariant features for object recognition and their 3D
positions for object position representation. Also, we use the depth
information at the horizontal centerline in images where the optical
axis passes through. Therefore, we can build a hybrid local node for a
topological map that is composed of a metric map and an object location
map. Based on such modeling, we suggest a coarse-to-fine strategy for
the global localization. The coarse pose is obtained by means of object
recognition and point cloud fitting, and then its fine pose is estimated
with a probabilistic scan matching algorithm. With real experiments, we
show that our proposed method can be an effective vision-based global
localization algorithm.},
keywords={cartography;image matching;mobile robots;object
recognition;pose estimation;probability;robot vision;stereo image
processing;topology;coarse pose;coarse-to-fine strategy;depth
information;environment modeling;horizontal centerline;hybrid local
node;local invariant features;metric map;mobile robots;object location
map;object position representation;object recognition;optical axis;point
cloud fitting;pose estimation;probabilistic scan matching
algorithm;spatial layout-based hybrid map;stereo camera;topological
map;vision-based global localization;Cameras;Clouds;Cognitive
robotics;Image databases;Mobile robots;Object recognition;Orbital
robotics;Robot kinematics;Spatial databases;Voting},
month={June},}
@INPROCEEDINGS{5174815,
author={K. Pathak and M. Pfingsthorn and N. Vaskevicius and A. Birk},
booktitle={2009 International Conference on Advanced Robotics},
title={Relaxing loop-closing errors in 3D maps based on planar surface
patches},
year={2009},
pages={1-6},
abstract={3D mapping using large planar patches results in a compact and
easily understandable representation of the environment as compared to
point-cloud or voxel based representations. In this work, relaxation of
loop-closing errors in a 3D map is presented. It is based on a
plane-matching algorithm for correspondence-finding and least-squares
registration of large planar patches fitted on 3D range-images. This
method also provides covariance matrices for the pose-registration
result. Based on this registration algorithm, a relaxation method
utilizing these covariances is formulated. In particular, we exploit the
plane-matcher properties that it provides an accurate rotation estimate,
and that it can easily identify principal translational directions of
uncertainty, to relax only the translation errors. This results in a
closed-form solution that can be computed in a very fast manner, namely
within a few milliseconds, as demonstrated by experimental results in an
indoor locomotion test arena in form of a high bay rack.},
keywords={covariance matrices;image matching;image registration;image
sensors;least squares approximations;robot vision;3D maps;3D
range-images;covariance matrices;large planar patches;least-squares
registration;loop-closing errors;loop-closing errors relaxation;planar
surface patches;plane-matching algorithm;pose-registration;principal
translational directions;registration algorithm;relaxation
method;Closed-form solution;Covariance matrix;Data
visualization;Iterative closest point algorithm;Jacobian
matrices;Relaxation methods;Robot sensing systems;Sampling
methods;Testing;Uncertainty},
month={June},}
@INPROCEEDINGS{5174717,
author={R. B. Rusu and W. Meeussen and S. Chitta and M. Beetz},
booktitle={2009 International Conference on Advanced Robotics},
title={Laser-based perception for door and handle identification},
year={2009},
pages={1-8},
abstract={In this paper, we present a laser-based approach for door and
handle identification. The approach builds on a 3D perception pipeline
to annotate doors and their handles solely from sensed laser data,
without any a priori model learning. In particular, we segment the parts
of interest using robust geometric estimators and statistical methods
applied on geometric and intensity distribution variations in the scan.
We present experimental results on a mobile manipulation platform (PR2)
intended for indoor manipulation tasks. We validate the approach by
generating trajectories that position the robot end-effector in front of
door handles and grasp the handle. The robustness of our approach is
demonstrated by real world experiments conducted on a large set of doors.},
keywords={end effectors;mobile robots;position control;robust
control;statistical analysis;3D perception pipeline;door
identification;end-effector;handle identification;indoor manipulation
task;laser-based perception;mobile manipulation platform;robot
position;robust geometric estimator;robustness;statistical
method;Clouds;Fixtures;Indoor environments;Intelligent robots;Laser
modes;Laser theory;Orbital robotics;Robot sensing
systems;Robustness;Tactile sensors},
month={June},}
@ARTICLE{5172883,
author={M. Ingebretsen},
journal={IEEE Intelligent Systems},
title={In the News},
year={2009},
volume={24},
number={4},
pages={5-9},
abstract={GPU-enabled AI is a subset of so- called general-purpose GPU
computing (GPGPU). But it promises to be one of the fastest-growing
subsets. The rise of cloud computing, recent high-powered graphics-chip
releases by AMD's competitor Nvidia, and the growing acceptance of the
OpenCL programming platform have all converged to allow GPU-enabled AI
to take off in the months ahead. AMD continues to work with partners
such as OTOY, a developer of high-speed rendering technologies, on cloud
computing initiatives such as its so-called Fusion Render Cloud.},
keywords={artificial intelligence;computer graphic equipment;rendering
(computer graphics);AMD;Fusion Render Cloud;GPU computing;GPU-enabled
AI;Nvidia;OTOY;OpenCL programming platform;cloud
computing;rendering;Artificial intelligence;Bandwidth;Cloud
computing;Graphics;High definition video;Intelligent systems;Rendering
(computer graphics);Space technology;Streaming media;User
interfaces;AI;AMD;Dennis Hong;GPU;HyDRAS;Hyper-Redundant Discrete
Robotic Articulated Serpentine;Virginia Tech;artificial
intelligence;froblins;graphics processing units;nvidia},
doi={10.1109/MIS.2009.77},
ISSN={1541-1672},
month={July},}
@INPROCEEDINGS{5152712,
author={M. Magnusson and H. Andreasson and A. Nuchter and A. J.
Lilienthal},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Appearance-based loop detection from 3D laser data using the
normal distributions transform},
year={2009},
pages={23-28},
abstract={We propose a new approach to appearance based loop detection
from metric 3D maps, exploiting the NDT surface representation.
Locations are described with feature histograms based on surface
orientation and smoothness, and loop closure can be detected by matching
feature histograms. We also present a quantitative performance
evaluation using two real-world data sets, showing that the proposed
method works well in different environments.},
keywords={SLAM (robots);laser ranging;normal distribution;robot
vision;3D laser data;appearance based loop detection;appearance-based
loop detection;feature histogram matching;feature histograms;loop
closure;metric 3D maps;normal distributions transform;quantitative
performance evaluation;surface orientation;surface
representation;Clouds;Covariance matrix;Eigenvalues and
eigenfunctions;Face detection;Gaussian
distribution;Histograms;Relaxation methods;Shape;Simultaneous
localization and mapping;Tellurium},
doi={10.1109/ROBOT.2009.5152712},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152688,
author={N. Fairfield and D. Wettergreen},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Evidence grid-based methods for 3D map matching},
year={2009},
pages={1637-1642},
abstract={Registering multiple sets of 3D range data is a crucial
capability for robots. The standard method for matching two sets of
range data is to convert the ranges to a point cloud representation, and
then use on of the many variants of iterative closest point (ICP). We
present a set of alternative methods for matching 3D range scans based
on a different data representation: evidence grid maps. Evidence grids
are robust to noise and variations in point density, can incorporate an
indefinite number of ranges, and explicitly encode empty as well as
occupied space. While 3D evidence grids can be huge when naively
implemented, we use an optimized octree data structure to efficiently
store sparse volumetric maps. To register a series of range scans, we
build an evidence grid map for each scan, and then register them
together using a several different methods. The first two methods are
based on a 3D extension of the classic 2D Lucas-Kanade template matching
method, and differ only in whether we match a single large region, or
multiple small regions that are selected heuristically. Our third method
involves extracting surfaces from the evidence grids, and then running
ICP to register the surfaces. We demonstrate our methods and compare
them to ICP using two datasets collected by two different subterranean
robots.},
keywords={image matching;image registration;image representation;mobile
robots;robot vision;2D Lucas-Kanade template matching method;3D map
matching;cloud representation;data representation;evidence grid-based
methods;evidence grids;explicitly encode empty;iterative closest
point;multiple sets registration;optimized octree data structure;sparse
volumetric maps;subterranean robots;Clouds;Crawlers;Data
structures;Iterative closest point algorithm;Iterative methods;Mobile
robots;Noise robustness;Robotics and automation;Simultaneous
localization and mapping;Wheels},
doi={10.1109/ROBOT.2009.5152688},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152502,
author={K. Pathak and N. Vaskevicius and A. Birk},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Revisiting uncertainty analysis for optimum planes extracted from
3D range sensor point-clouds},
year={2009},
pages={1631-1636},
abstract={In this work, we utilize a recently studied more accurate
range noise model for 3D sensors to derive from scratch the expressions
for the optimum plane which best fits a point-cloud and for the combined
covariance matrix of the plane's parameters. The parameters in question
are the plane's normal and its distance to the origin. The range
standard-deviation model used by us is a quadratic function of the true
range and is a function of the incidence angle as well. We show that for
this model, the maximum-likelihood plane is biased, whereas the
least-squares plane is not. The plane-parameters' covariance matrix for
the least-squares plane is shown to possess a number of desirable
properties, e.g., the optimal solution forms its null-space and its
components are functions of easily understood terms like the
planar-patch's center and scatter. We verify our covariance expression
with that obtained by the eigenvector perturbation method. We further
compare our method to that of renormalization with respect to the
theoretically best covariance matrix in simulation. The application of
our approach to real-time range-image registration and plane fusion is
shown by an example using a commercially available 3D range sensor.
Results show that our method has good accuracy, is fast to compute, and
is easy to interpret intuitively.},
keywords={covariance matrices;eigenvalues and eigenfunctions;image
fusion;image registration;image sensors;least squares
approximations;maximum likelihood estimation;3D range sensor
point-clouds;covariance matrix;eigenvector perturbation
method;least-squares plane;maximum-likelihood plane;plane
fusion;quadratic function;range noise model;range standard-deviation
model;real-time range-image registration;revisiting uncertainty
analysis;standard-deviation model;Computational modeling;Computer
science;Covariance matrix;Jacobian matrices;Maximum likelihood
estimation;Perturbation methods;Robotics and
automation;Scattering;Sensor fusion;Uncertainty;3D Mapping;Plane
Fusion;Plane uncertainty estimation;Plane-fitting},
doi={10.1109/ROBOT.2009.5152502},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152684,
author={J. Binney and G. S. Sukhatme},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={3D tree reconstruction from laser range data},
year={2009},
pages={1321-1326},
abstract={We present a method for reconstructing 3D models of tree
branch structure from laser range data. Our approach is probabilistic,
and uses general knowledge of tree structure to guide an iterative
reconstruction process. Our goal is to recover parameters such as branch
locations, angles, radii, and lengths, as well as connectivity
information between branches. These parameters can then be fed into
functional-structural plant models to study the relationships between
the structure of a plant, its environment, and its internal biology. In
this paper we present an algorithm for finding these parameters, and
results on both simulated and real datasets.},
keywords={botany;image reconstruction;iterative methods;3D tree
reconstruction;functional-structural plant models;iterative
reconstruction process;laser range data;tree structure;Biological system
modeling;Biosensors;Clouds;Computer architecture;Image
reconstruction;Laser modes;Plants (biology);Robot sensing systems;Tree
data structures;Tree graphs},
doi={10.1109/ROBOT.2009.5152684},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152431,
author={A. Censi and S. Carpin},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={HSM3D: Feature-less global 6DOF scan-matching in the Hough/Radon
domain},
year={2009},
pages={3899-3906},
abstract={This paper presents HSM3D, an algorithm for global rigid 6DOF
alignment of 3D point clouds. The algorithm works by projecting the two
input sets into the Radon/Hough domain, whose properties allow to
decompose the 6DOF search into a series of fast one-dimensional
cross-correlations. No planes or other particular features must be
present in the input data, and the algorithm is provably complete in the
case of noise-free input. The algorithm has been experimentally
validated on publicly available data sets.},
keywords={Hough transforms;Radon transforms;image matching;3D point
clouds;6DOF search;HSM3D;Radon-Hough domain;fast one-dimensional
cross-correlations;feature-less global 6DOF
scan-matching;Clouds;Iterative algorithms;Iterative closest point
algorithm;Iterative methods;Parameter estimation;Robot sensing
systems;Robot vision systems;Robotics and automation;Robustness;Voting},
doi={10.1109/ROBOT.2009.5152431},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152499,
author={B. D. Null and E. D. Sinzinger},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Autonomous map construction using three-dimensional feature
descriptors},
year={2009},
pages={2024-2029},
abstract={Autonomous robotic mapping has been an open research topic for
more than twenty years. The primary objective of the robotic mapping
problem is to design methods that can guide a robot around an
environment and allow it to create a map of what has been sensed. Most
automatic mapping algorithms rely on robot pose estimation to fuse map
data together. This paper demonstrates that through feature extraction
using spin-histograms, the pose of the robot can be estimated accurately
enough for an iterative closest point (ICP) algorithm to register
overlapping data sets. By eliminating consideration for points according
to curvature and saliency, the spin-histogram matching process can
improve in both accuracy and computation time. In combination with a
global registration algorithm known as simultaneous matching, this
process can obtain a fully autonomous registration process.},
keywords={distance measurement;feature extraction;image fusion;image
matching;image registration;iterative methods;mobile robots;pose
estimation;robot vision;autonomous robotic mapping algorithm;feature
extraction;global autonomous registration algorithm;iterative closest
point algorithm;map data fusion;odometer estimates;robot pose
estimation;spin-histogram matching process;three-dimensional feature
descriptor;Clouds;Feature extraction;Fuses;Iterative
algorithms;Iterative closest point algorithm;Libraries;Mobile
robots;Robot sensing systems;Robotics and automation;Shape},
doi={10.1109/ROBOT.2009.5152499},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152856,
author={D. Munoz and N. Vandapel and M. Hebert},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Onboard contextual classification of 3-D point clouds with
learned high-order Markov Random Fields},
year={2009},
pages={2009-2016},
abstract={Contextual reasoning through graphical models such as Markov
random fields often show superior performance against local classifiers
in many domains. Unfortunately, this performance increase is often at
the cost of time consuming, memory intensive learning and slow inference
at testing time. Structured prediction for 3-D point cloud
classification is one example of such an application. In this paper we
present two contributions. First we show how efficient learning of a
random field with higher-order cliques can be achieved using subgradient
optimization. Second, we present a context approximation using random
fields with high-order cliques designed to make this model usable
online, onboard a mobile vehicle for environment modeling. We obtained
results with the mobile vehicle on a variety of terrains, at 1/3 Hz for
a map 25 times 50 meters and a vehicle speed of 1-2 m/s.},
keywords={Markov processes;computational geometry;gradient methods;graph
theory;image classification;learning (artificial intelligence);mobile
robots;random processes;robot vision;3D point cloud
classification;autonomous ground vehicle;contextual
reasoning;environment modeling;graphical model;high-order Markov random
field learning;higher-order graph clique;memory-intensive
learning;mobile robot;subgradient optimization;supervised
learning;Classification tree analysis;Clouds;Context
modeling;Costs;Layout;Markov random fields;Random variables;Remotely
operated vehicles;Robotics and automation;Vegetation},
doi={10.1109/ROBOT.2009.5152856},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5152524,
author={M. Ruhnke and B. Steder and G. Grisetti and W. Burgard},
booktitle={2009 IEEE International Conference on Robotics and Automation},
title={Unsupervised learning of 3D object models from partial views},
year={2009},
pages={801-806},
abstract={We present an algorithm for learning 3D object models from
partial object observations. The input to our algorithm is a sequence of
3D laser range scans. Models learned from the objects are represented as
point clouds. Our approach can deal with partial views and it can
robustly learn accurate models from complex scenes. It is based on an
iterative matching procedure which attempts to recursively merge similar
models. The alignment between models is determined using a novel scan
registration procedure based on range images. The decision about which
models to merge is performed by spectral clustering of a similarity
matrix whose entries represent the consistency between different models.},
keywords={image registration;image sequences;laser ranging;unsupervised
learning;3D laser range scans;3D object models;iterative matching
procedure;partial object observations;scan registration
procedure;unsupervised learning;Clouds;Iterative algorithms;Laser
modes;Layout;Merging;Object detection;Robotics and
automation;Robots;Robustness;Unsupervised learning;model learning;object
detection;range images},
doi={10.1109/ROBOT.2009.5152524},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{5072953,
author={C. Wu and X. Mao and X. Shi and L. Zhang},
booktitle={2009 International Workshop on Intelligent Systems and
Applications},
title={Methods of Generating Robot Spraying Trajectory Based on Shoe
Sole Information},
year={2009},
pages={1-4},
abstract={Robots and automation technology has been increasingly used in
the production of leather shoes. How to get the work trajectory is one
of the key problems. Methods that can generate robot spraying trajectory
by extracting shoe sole information were presented. The information
including shoe sole surface and contour line could be obtained by
off-line program with CAD technology and on-line program with scanning
shoe lasted upper. In on-line program system, a method based on computer
vision with structured light is described. After scanning, basic
information rendered by three-dimensional point-cloud can be extracted.
The 3D scan curves and closed contour line could be interpolated as
B-spline curves. The spraying trajectory is an offset curve of contour
line on shoe sole surface. To pick up robot working points, trajectory
is discretized. Finally, robot working points are successfully applied
in a shoe upper spraying robot. Thus, presented methods are validated
effectively and feasibly.},
keywords={computer vision;footwear industry;industrial robots;production
engineering computing;splines (mathematics);spraying;3D scan
curves;B-spline curves;CAD technology;computer vision;leather shoe
production;online program system;robot spraying trajectory;robot working
points;shoe sole information;three-dimensional point-cloud;Computer
vision;Data mining;Design automation;Educational
institutions;Footwear;Mechanical engineering;Robotics and
automation;Robots;Spline;Spraying},
doi={10.1109/IWISA.2009.5072953},
month={May},}
@INPROCEEDINGS{4913337,
author={K. Ohno and T. Kawahara and S. Tadokoro},
booktitle={2008 IEEE International Conference on Robotics and Biomimetics},
title={Development of 3D laser scanner for measuring uniform and dense
3D shapes of static objects in dynamic environment.},
year={2009},
pages={2161-2167},
abstract={The authors aim at the development of a 3D laser scanner that
can measure uniform and dense 3D shape of static objects in dynamic
environment. The 3D scanner was composed of a 2D Laser Range Finder
(LRF) and Pan-Tilt base. 3D shape is measured by rotating 2D LRF that is
tilted around the two axes. The laser point trajectory shows cross scan.
Use of cross scan achieved the wide view angular and the uniform 3D
scan. The proposed 3D scanner can adjust the measurement area and the
density of 3D point clouds by changing the angle and angular velocity of
the pan-tilt mechanism. The 3D scanner can decrease 3D measurement time.
In addition, this 3D scanner measures same area twice during one 3D
scan. Comparing these two measurement distances, the moving object can
be detected.},
keywords={Area measurement;Cameras;Density measurement;Robot sensing
systems;Seismic measurements;Shape measurement;Simultaneous localization
and mapping;Time measurement;Trajectory;Velocity measurement},
doi={10.1109/ROBIO.2009.4913337},
month={Feb},}
@INPROCEEDINGS{4813878,
author={J. Zamora-Esquivel and E. Bayro-Corrochan},
booktitle={2007 7th IEEE-RAS International Conference on Humanoid Robots},
title={Humanoid egomotion using planes},
year={2007},
pages={264-269},
abstract={When dealing with humanoid vision, it is very important to
take into account the velocity and complexity of the algorithms. This
paper presents an efficient combination of basic algorithm with
quadratic complexity. Our real time procedure gives an excellent
representation of the 3D scene. This 3D reconstruction allows the
humanoid to maneuver, as well as the robotic manipulation of objects.},
keywords={humanoid robots;image reconstruction;robot vision;3D
reconstruction;humanoid egomotion;humanoid vision;quadratic
complexity;Cameras;Cloud computing;Clustering algorithms;Computer
vision;Humans;Image reconstruction;Image
segmentation;Layout;Navigation;Robot vision systems},
doi={10.1109/ICHR.2007.4813878},
ISSN={2164-0572},
month={Nov},}
@INPROCEEDINGS{4791854,
author={P. Dauchez and X. Delebarre and Y. Bouffard and E. Degoulange},
booktitle={1991 American Control Conference},
title={Task Description for Two Cooperative Manipulators},
year={1991},
pages={2503-2508},
abstract={The main goal of this paper is to present some solutions we
have developed to describe complex tasks that can be achieved by two
coordinated manipulators. Task description means definition of
controlled vectors. Therefore, it is a necessary step before actually
controlling the manipulators. The tasks we present involve either an
open kinematic chain or a closed kinematic chain. In both cases, the two
manipulators are seen as a single kinematic system. For the open chain
case, this approach allows us to fully use the possible redundancy of
the system to impose some particular constraint or criterion. For the
closed chain case, this concept of a single two-arm robot is used for
describing the tasks in the object space, rather than in the
manipulators space. This solution appears to be very convenient, from a
user point of view. In addition we present some experiments we have done
with two real six-axis manipulators. This of course requires
implementing some control scheme, on which we haven't done any
particular research, so far. Therefore, we only describe the solutions
we are currently utilizing and present some results which are aimed at
showing the effectiveness of our task descriptions more than the quality
of our control algorithms.},
keywords={Arm;Clouds;Force control;Force sensors;Hardware;IEEE
members;Manipulators;Robot kinematics;Robot sensing systems;Robotic
assembly},
month={June},}
@INPROCEEDINGS{4795697,
author={R. Kaushik and Y. Feng and W. Morris and J. Xiao and Z. Zhu},
booktitle={2008 10th International Conference on Control, Automation,
Robotics and Vision},
title={3D map construction using heterogeneous robots},
year={2008},
pages={1230-1235},
abstract={This paper presents a novel method to construct a complete 3D
map that includes all surfaces (ceiling, wall, and furniture tops, etc.)
in indoor environments. A team of four robots, including three ground
robots and one wall-climbing robot is deployed in a tetrahedron
configuration that satisfies the perspective three point (P3P) problem.
P3P problem is to estimate the pose of a perspective camera on the
wall-climbing robot viewing three ground robots, which will produce up
to four solutions using Grunert's algorithm while only one of them is
genuine. We propose a probabilistic Bayesian algorithm that identifies
the unique solution of the P3P problem using the mobility of the camera.
Based on this technique, we introduce an intra-robot localization method
to determine the geometric relationship among four robots. Each ground
robot is equipped with a rotary laser range finder (LRF), a
pan-tilt-zoom camera, and a LED cluster. The wall-climbing robot is
fitted with a LRF, a perspective camera, and a motion sensor. Through
the vision sensors, the robots obtain their relative poses by solving
the P3P problem. Through the LRF on each robot, 4 laser point cloud maps
are produced from each robot's point of view. With the information of
relative poses of the multiple robots and the calibration data of each
LRF and camera pair, the 4 partial maps are fused to acquire a complete
3D map that is rich with information of all surfaces. Our approach
outperforms the traditional range image fusion algorithms in terms of
time complexity and is suitable for real-time implementation. Real
experiments verified the effectiveness of the method.},
keywords={Bayes methods;computational complexity;image fusion;laser
ranging;mobile robots;multi-robot systems;path planning;pose
estimation;probability;robot vision;3D map construction;Grunert
algorithm;LED cluster;geometric relationship;ground robot;heterogeneous
robot;indoor environment;intra-robot localization method;laser point
cloud map;motion sensor;multirobot system;pan-tilt-zoom
camera;perspective camera;perspective three point problem;pose
estimation;probabilistic Bayesian algorithm;range image fusion;rotary
laser range finder;tetrahedron configuration;time complexity;vision
sensor;wall-climbing robot;Automatic control;Cameras;Clouds;Image
fusion;Indoor environments;Light emitting diodes;Robot kinematics;Robot
sensing systems;Robot vision systems;Robotics and automation;3D
mapping;image fusion;multirobot system;perspective three point (P3P)
problem},
doi={10.1109/ICARCV.2008.4795697},
month={Dec},}
@INPROCEEDINGS{4795593,
author={R. B. Rusu and Z. C. Marton and N. Blodow and M. Beetz},
booktitle={2008 10th International Conference on Control, Automation,
Robotics and Vision},
title={Learning informative point classes for the acquisition of object
model maps},
year={2008},
pages={643-650},
abstract={This paper proposes a set of methods for building informative
and robust feature point representations, used for accurately labeling
points in a 3D point cloud, based on the type of surface the point is
lying on. The feature space comprises a multi-value histogram which
characterizes the local geometry around a query point, is pose and
sampling density invariant, and can cope well with noisy sensor data. We
characterize 3D geometric primitives of interest and describe methods
for obtaining discriminating features used in a machine learning
algorithm. To validate our approach, we perform an in-depth analysis
using different classifiers and show results with both synthetically
generated datasets and real-world scans.},
keywords={data acquisition;image classification;learning (artificial
intelligence);robot vision;3D point cloud;dataset generation;geometric
primitives;machine learning algorithm;multivalue histogram;object model
map acquisition;robust feature point representations;sampling density
invariance;Clouds;Histograms;Intelligent robots;Labeling;Layout;Robot
sensing systems;Robotics and automation;Sensor phenomena and
characterization;Support vector machine classification;Support vector
machines},
doi={10.1109/ICARCV.2008.4795593},
month={Dec},}
@INPROCEEDINGS{4795808,
author={Q. Muhlbauer and K. Kuhnlenz and M. Buss},
booktitle={2008 10th International Conference on Control, Automation,
Robotics and Vision},
title={Fusing laser and vision data with a genetic ICP algorithm},
year={2008},
pages={1844-1849},
abstract={Knowledge about the environment is essential for humanoid and
mobile robots to move and act safely. The most intuitive way to perceive
information about the environment is through the vision system. However,
the accuracy provided by stereo vision is insufficient for many tasks. A
more accurate representation is created by a laser range-finder, which
delivers no color information. This paper describes a novel approach to
merge data obtained by stereo vision and laser range-finders by using a
genetic ICP algorithm, which is able to register noisy point clouds with
different resolutions and a small overlap. Furthermore, it describes an
easy-to-use and robust method to calibrate the extrinsic parameters of
two or more laser range-finders.},
keywords={genetic algorithms;humanoid robots;laser ranging;mobile
robots;robot vision;stereo image processing;fusing laser;genetic ICP
algorithm;humanoid robots;laser range-finder;laser range-finders;mobile
robots;stereo vision;vision system;Automatic
control;Calibration;Cameras;Clouds;Genetics;Iterative closest point
algorithm;Laser fusion;Machine vision;Mobile robots;Robotics and
automation},
doi={10.1109/ICARCV.2008.4795808},
month={Dec},}
@INPROCEEDINGS{4777238,
author={Y. K. Chen and T. Y. Cheng and S. T. Chiu},
booktitle={2009 International Asia Conference on Informatics in Control,
Automation and Robotics},
title={Motion Detection with Entropy in Dynamic Background},
year={2009},
pages={263-266},
abstract={The traditional automatic smart image surveillance system can
usually be used in the environment with still background. That is, the
background image must not contain the moving objects. If there is waving
ocean, waving tree, floating cloud, or raining in the background image,
the traditional methods do not work well. In order to improve this
problem, a new motion detection method based on the theory of entropy
and combined a multi-periods sigma-delta background estimation algorithm
is developed in this paper. Based on the theory of moving average, a
moving thresholding method is designed in this paper to obtain a
sequence of alarm announcements. Experiments are carried out for some
samples with dynamic backgrounds to demonstrate the computational
advantage of the proposed method.},
keywords={entropy;image motion analysis;moving average processes;video
surveillance;automatic smart image surveillance system;background
image;dynamic background;entropy theory;motion detection;moving average
theory;moving thresholding method;multi-periods sigma-delta background
estimation algorithm;Asia;Automatic control;Change detection
algorithms;Entropy;IEEE news;Informatics;Intelligent systems;Motion
control;Motion detection;Video surveillance;alarm announcement;computer
vision;entropy;image processing;motion detection;surveillance system},
doi={10.1109/CAR.2009.88},
ISSN={1948-3414},
month={Feb},}
@INPROCEEDINGS{4728303,
author={D. J. Thornley},
booktitle={2007 IEEE International Conference on Signal Processing and
Communications},
title={Novel Anisotropic Multidimensional Convolutional Filters for
Derivative Estimation and Reconstruction},
year={2007},
pages={253-256},
abstract={The Savitzky-Golay convolutional filter matches a polynomial
to even-spaced, one dimensional data and uses this to measure smoothed
derivatives. We re-examine the fundamental concept behind this filter,
and generate a formulation approach with multidimensional,
heterogeneous, anisotropic basis functions to provide a general
smoothing, derivative measurement and reconstruction filter for
arbitrary point clouds using a linear operator in the form of a
convolution kernel. This novel approach yields filters for a wide range
of applications such as robot vision, medical volumetric time series
analysis and numerical differential equation solution on arbitrary
meshes or point clouds without resampling. The urge to extend polynomial
filters to higher dimensions is obvious yet previously unfulfilled. We
provide a novel complete, arbitrary-dimensional approach to their
construction, and introduce anisotropy and irregularity.},
keywords={adaptive filters;convolution;digital filters;polynomial
approximation;signal reconstruction;Savitzky-Golay convolutional
filter;adaptive filter;anisotropic basis functions;anisotropic
multidimensional convolutional filters;arbitrary-dimensional
approach;convolution kernel;digital filters;polynomial
approximation;signal estimation;signal reconstruction;Anisotropic
magnetoresistance;Clouds;Convolution;Kernel;Matched filters;Medical
robotics;Multidimensional systems;Nonlinear
filters;Polynomials;Smoothing methods;Adaptive filters;Multidimensional
digital filters;differentiation;polynomial approximation;signal
reconstruction},
doi={10.1109/ICSPC.2007.4728303},
month={Nov},}
@INPROCEEDINGS{4712339,
author={G. Monteiro and J. Marcos and M. Ribeiro and J. Batista},
booktitle={2008 15th IEEE International Conference on Image Processing},
title={Robust segmentation for outdoor traffic surveillance},
year={2008},
pages={2652-2655},
abstract={In this paper it is presented a robust segmentation process
for detecting incidents on highways. This segmentation process is based
on background subtraction and uses an efficient background model
initialization and update to work 24/7. A cross- correlation based
shadow detection is also used for minimizing ghosts. It is also proposed
a stopped vehicle detection system based on the pixel history cache.
This method has proved to be quite robust in terms of different weather
conditions, lighting and image quality. Some experiments carried out on
some highway scenarios demonstrate the robustness of the proposed
solution.},
keywords={image segmentation;road traffic;video surveillance;background
subtraction;image quality;outdoor traffic surveillance;pixel history
cache;robust segmentation;stopped vehicle detection
system;Cameras;Clouds;History;Image segmentation;Layout;Noise
robustness;Road transportation;Surveillance;Traffic control;Vehicle
detection;Shadow Detection;Stopped Vehicles Detection;Traffic
Surveillance;Vehicles Segmentation},
doi={10.1109/ICIP.2008.4712339},
ISSN={1522-4880},
month={Oct},}
@INPROCEEDINGS{4690878,
author={S. Yu and L. Rong and W. Chen and X. Wu},
booktitle={2008 IEEE Conference on Robotics, Automation and Mechatronics},
title={An improved BP neural network based on GA for 3D laser data
repairing},
year={2008},
pages={571-576},
abstract={Affected by scanning object, environment, scanning speed and
user's operation .etc, some information of the object's surface can't be
detected by the laser scanner. Aiming at the data loss in laser
detecting , the paper presents an improved BP neural network based on GA
for 3D laser data repairing, the novelty of this method is adopting
Genetic Algorithm(GA) to optimize the configure and weight of network,
and at the same time combining Back Propagation(BP) Algorithm to find
optimal approximation. The simulation shows the improved BP neural
network based on GA has a faster constringency speed and better
repairing precision than traditional BP neural network and GA algorithm.
Lastly, the paper gives the result of repairing the point cloud
collected by 3D information reconstruction system using this network},
keywords={Approximation algorithms;Artificial
intelligence;Clouds;Genetics;Machine vision;Neural networks;Object
detection;Optimization methods;Surface emitting lasers;Turning;Data
repairing, GA, BP network, Laser scanner},
doi={10.1109/RAMECH.2008.4690878},
ISSN={2158-2181},
month={Sept},}
@INPROCEEDINGS{4694455,
author={Hyung-O Kim and Soohwan Kim and S. K. Park},
booktitle={2008 International Conference on Control, Automation and
Systems},
title={Pointing gesture-based unknown object extraction for learning
objects with robot},
year={2008},
pages={2156-2161},
abstract={This paper describes how to extract a unknown object pointed
by a person while interacting with a robot. Our proposed method consists
of two stages: the detection of the operatorspsila face and the
estimation of the pointing direction, extraction of the pointed object.
The operatorpsilas face is recognized by using the Haar-like features.
Then, from the shoulder-to-hand line we estimate the pointing direction.
Finally, we segment an unknown object from 3D point clouds in region of
interest. We implemented a object registration system with our mobile
robot and obtained reliable experimental results.},
keywords={face recognition;feature extraction;gesture
recognition;human-robot interaction;image registration;mobile
robots;Haar-like features;face detection;learning objects;mobile
robot;object registration system;pointing gesture estimation;unknown
object extraction;Automatic control;Cameras;Cognitive robotics;Face
detection;Face recognition;Mobile robots;Robot kinematics;Robot vision
systems;Robotics and automation;Robustness;Arm-pointing Gestures;Object
Extraction;Object Recognition;Stereo vision},
doi={10.1109/ICCAS.2008.4694455},
month={Oct},}
@INPROCEEDINGS{4681439,
author={Z. Lu and S. Baek and S. Lee},
booktitle={2008 IEEE Conference on Robotics, Automation and Mechatronics},
title={Robust 3D Line Extraction from Stereo Point Clouds},
year={2008},
pages={1-5},
abstract={The paper describes a robust method to extract 3D lines from
stereo point clouds. This method combines 2D image information with 3D
point clouds from a stereo camera. 2D lines are first extracted from the
image in the stereo pair, followed by 3D line regression from the
back-projected 3D point set of the images points in the detected 2D
lines. In this paper, random sample consensus (RANSAC) is used to
estimate 3D line from the 3D point set, the Mahalanobis distance from
each 3D point to the 3D line is derived, and the statistically motivated
distance measure is used to compute the support for the detected 3D
line. Experimental results on real environment with high level of
clutter, occlusion, and noise demonstrate the robustness of the
algorithm.},
keywords={computer vision;feature extraction;regression analysis;stereo
image processing;2D image information;3D line regression;Mahalanobis
distance;clutter;occlusion;random sample consensus;robust 3D line
extraction;stereo camera;stereo point clouds;Application
software;Cameras;Clouds;Computer vision;Data mining;Layout;Noise
robustness;Object recognition;Robot vision systems;Stereo vision;3D line
extraction;robust regression;stereo image},
doi={10.1109/RAMECH.2008.4681439},
ISSN={2158-2181},
month={Sept},}
@INPROCEEDINGS{4626412,
author={M. Richtsfeld and M. Zillich},
booktitle={2008 IEEE International Conference on Automation Science and
Engineering},
title={Grasping unknown objects based on 2 #x00BD;D range data},
year={2008},
pages={691-696},
abstract={The problem of grasping novel objects in a fully automatic way
has gained increasing importance. In this work we consider the problem
of grasping novel objects with the help of a laser range scanner. This
includes autonomous object detection and grasp motion planning. The used
system consists of a fixed working station equipped with a laser range
scanner, a seven degrees of freedom manipulator and a hand prosthesis as
gripper. We present a method for segmentation of a 2frac12D point cloud
into parts, assembly of parts into objects and calculation of grasping
points, which works for cylindrical objects and arbitrary objects. We
successfully demonstrate this approach by grasping a variety of
different shapes and present a step towards full automation.},
keywords={grippers;image segmentation;laser ranging;manipulators;medical
robotics;mobile robots;object detection;path planning;prosthetics;robot
vision;2frac12D point cloud segmentation;autonomous object
detection;grasp motion planning;gripper;hand prosthesis;laser range
scanner;manipulator;mobile robot;unknown object
grasping;Automation;Clouds;Grasping;Grippers;Image edge detection;Mobile
robots;Noise robustness;Object detection;Prosthetics;Shape},
doi={10.1109/COASE.2008.4626412},
ISSN={2161-8070},
month={Aug},}
@INPROCEEDINGS{4605516,
author={Niu Xuejuan and Liu Jingtai and Sun Lei},
booktitle={2008 27th Chinese Control Conference},
title={Multi-resolution analysis of grid point clouds based on wavelet
transform},
year={2008},
pages={341-345},
abstract={An application of classical wavelet in the realization of the
multi-resolution analysis of point cloud is presented in this paper.
Firstly, a proper wavelet function and the expending method are
determined by analyzing the character of grid point cloud. With the
theory of discrete wavelet transform (DWT), the multi-level wavelet
decomposition is realized. Based on the band-pass filter property of the
wavelet function, the high frequency information in the point cloud is
removed, and the low frequency part is reserved. And then the
multi-level of detail and multi-resolution models are generated, which
are the simplifications of the original point cloud. Finally, the
feasibility and the efficiency are proved by a study case.},
keywords={band-pass filters;image resolution;wavelet
transforms;band-pass filter;discrete wavelet transform;grid point
clouds;multilevel wavelet decomposition;multiresolution
analysis;Clouds;Control systems;Discrete wavelet
transforms;Frequency;Information analysis;Multiresolution
analysis;Robotics and automation;Sun;Wavelet analysis;Wavelet
transforms;Grid point clouds;Multi-resolution analysis (MRA);Wavelet
transform},
doi={10.1109/CHICC.2008.4605516},
ISSN={1934-1768},
month={July},}
@INPROCEEDINGS{4599427,
author={S. Schwertfeger and J. Poppinga and M. Pfingsthorn and A. Birk},
booktitle={2008 ECSIS Symposium on Learning and Adaptive Behaviors for
Robotic Systems (LAB-RS)},
title={Towards Object Classification Using 3D Sensor Data},
year={2008},
pages={53-58},
abstract={This paper presents an approach to classify objects using 3D
sensor data and an evolutionary algorithm. An important by-product of
this classification is, that additionally certain properties and the
pose in space of this object are determined. The reproductive perception
paradigm is used utilizing an evolutionary strategy. Two sub-approaches
are discussed using different representations of the 3D data. The first
one uses depth images while the second one uses point clouds stored in a
special octree. The approaches will be demonstrated in experiments with
simulated and real data.},
keywords={evolutionary computation;image recognition;image
sensors;object recognition;octrees;3D sensor data;evolutionary
algorithm;object classification;octree;reproductive perception
paradigm;Cameras;Clouds;Computer science;Infrared image sensors;Phase
measurement;Phased arrays;Robot sensing systems;Sensor arrays;Sensor
phenomena and characterization;Sensor systems;evolutionary
algorithm;object classification;octree;reproductive perception paradigm},
doi={10.1109/LAB-RS.2008.28},
month={Aug},}
@INPROCEEDINGS{4547087,
author={S. Subchan and B. A. White and A. Tsourdos and M. Shanmugavel
and R. Zbikowski},
booktitle={2008 IEEE Instrumentation and Measurement Technology
Conference},
title={Pythagorean Hodograph (PH) Path Planning for Tracking Airborne
Contaminant using Sensor Swarm},
year={2008},
pages={501-506},
abstract={This paper presents results on the path planning of
cooperating unmanned aerial vehicles (UAVs) to detect, model and track
the shape of airborne contaminants boundary using Pythagorean hodograph
(PH). The model of the contaminant boundary is based on SCIPUFF and used
it as reference for the path planning to track the airborne contaminant.
The UAVs sensor swarm has to take measurements of the air borne
contaminant clouds. The UAVs are assumed to just have a sensor package
which can sense nuclear, biological and chemical (NBC) contaminants.
Therefore as a UAVs flies through the contaminant the NBC sensors will
recognise the entry and exit points of the UAVs from the contaminant
boundary and give these two points as measurements. Based on the
measurements the splinegon approach uses to predict the contaminant
boundary and produces a segment for the next UAVs path.},
keywords={aerospace robotics;chemical sensors;path planning;remotely
operated vehicles;Pythagorean hodograph path planning;SCIPUFF;UAV sensor
swarm;airborne contaminants boundary;biological contaminants;chemical
contaminants;cooperating unmanned aerial vehicles;nuclear
contaminants;splinegon approach;Biosensors;Chemical and biological
sensors;Clouds;Niobium compounds;Packaging;Path planning;Pollution
measurement;Shape;Unmanned aerial vehicles;Vehicle detection},
doi={10.1109/IMTC.2008.4547087},
ISSN={1091-5281},
month={May},}
@INPROCEEDINGS{4544014,
author={G. Kim and D. Huber and M. Hebert},
booktitle={2008 IEEE Workshop on Applications of Computer Vision},
title={Segmentation of Salient Regions in Outdoor Scenes Using Imagery
and 3-D Data},
year={2008},
pages={1-8},
abstract={This paper describes a segmentation method for extracting
salient regions in outdoor scenes using both 3-D laser scans and imagery
information. Our approach is a bottom- up attentive process without any
high-level priors, models, or learning. As a mid-level vision task, it
is not only robust against noise and outliers but it also provides
valuable information for other high-level tasks in the form of optimal
segments and their ranked saliency. In this paper, we propose a new
saliency definition for 3-D point clouds and we incorporate it with
saliency features from color information.},
keywords={feature extraction;image colour analysis;image scanners;image
segmentation;color information;imagery information;laser scans;optimal
segments;outdoor scenes;salient regions extraction;salient regions
segmentation;segmentation method;Clouds;Clustering algorithms;Color;Data
mining;Image segmentation;Land vehicles;Laser modes;Laser
noise;Layout;Robots},
doi={10.1109/WACV.2008.4544014},
ISSN={1550-5790},
month={Jan},}
@INPROCEEDINGS{4543664,
author={J. A. Stipes and J. G. P. Cole and J. Humphreys},
booktitle={2008 IEEE International Conference on Robotics and Automation},
title={4D scan registration with the SR-3000 LIDAR},
year={2008},
pages={2988-2993},
abstract={An algorithm and data management scheme is presented to
utilize the data output of the SR-3000, a 3D LIDAR sensor. The SR-3000
generates a 4D point cloud at video frame rates where each point is
described by its 3D coordinates and its optical intensity. The
implementation of each major component of the algorithm is described,
including a projective ICP algorithm, reverse calibration equations for
the SR-3000 sensor, the processing of the intensity data, and a point
selection algorithm which includes the use of a 3D volume feature
extraction algorithm. This ICP based alignment algorithm is used in the
context of a data management scheme that stores and retrieves scan data
and intermediate data products to realize pseudo-global scan alignment,
data compression, and real time data display.},
keywords={data compression;feature extraction;image coding;image
registration;optical radar;radar imaging;3D LIDAR sensor;3D volume
feature extraction algorithm;4D scan registration;SR-3000 LIDAR;data
compression;data management scheme;optical intensity;point selection
algorithm;projective ICP algorithm;pseudo-global scan alignment;real
time data display;reverse calibration equations;video frame
rates;Calibration;Clouds;Data compression;Equations;Feature
extraction;Information retrieval;Iterative closest point algorithm;Laser
radar;Optical sensors;Sensor phenomena and characterization},
doi={10.1109/ROBOT.2008.4543664},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{4543179,
author={A. Harrison and P. Newman},
booktitle={2008 IEEE International Conference on Robotics and Automation},
title={High quality 3D laser ranging under general vehicle motion},
year={2008},
pages={7-12},
abstract={This paper describes an end-to-end system capable of
generating high-quality 3D point clouds from the popular LMS200 laser on
a continuously moving platform. We describe the hardware, data capture,
calibration and data stream processing we have developed which yields
remarkable detail in the generated point clouds of urban scenes. Given
the increasing interest in outdoor 3D navigation and scene
reconstruction by mobile platforms, our aim is to provide a level of
hardware and algorithmic detail suitable for replication of our system
by interested parties who do not wish to invest in dedicated 3D laser
rangers.},
keywords={calibration;laser ranging;LMS200
laser;calibration;continuously moving platform;data capture;data stream
processing;general vehicle motion;high quality 3D laser
ranging;high-quality 3D point clouds;mobile platform;outdoor 3D
navigation;scene
reconstruction;Calibration;Clouds;Costs;Hardware;Layout;Mobile
robots;Robot sensing systems;Simultaneous localization and
mapping;Timing;Vehicles},
doi={10.1109/ROBOT.2008.4543179},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{4543434,
author={K. Huebner and S. Ruthotto and D. Kragic},
booktitle={2008 IEEE International Conference on Robotics and Automation},
title={Minimum volume bounding box decomposition for shape approximation
in robot grasping},
year={2008},
pages={1628-1633},
abstract={Thinking about intelligent robots involves consideration of
how such systems can be enabled to perceive, interpret and act in
arbitrary and dynamic environments. While sensor perception and model
interpretation focus on the robot's internal representation of the world
rather passively, robot grasping capabilities are needed to actively
execute tasks, modify scenarios and thereby reach versatile goals. These
capabilities should also include the generation of stable grasps to
safely handle even objects unknown to the robot. We believe that the key
to this ability is not to select a good grasp depending on the
identification of an object (e.g. as a cup), but on its shape (e.g. as a
composition of shape primitives). In this paper, we envelop given 3D
data points into primitive box shapes by a fit-and-split algorithm that
is based on an efficient Minimum Volume Bounding Box implementation.
Though box shapes are not able to approximate arbitrary data in a
precise manner, they give efficient clues for planning grasps on
arbitrary objects. We present the algorithm and experiments using the 3D
grasping simulator Grasplt!.},
keywords={intelligent robots;sensors;Grasplt!;bounding box
decomposition;intelligent robots;model interpretation;robot
grasping;sensor perception;shape
approximation;Clouds;Grasping;Intelligent robots;Intelligent
sensors;Kinematics;Robot sensing systems;Robotics and automation;Service
robots;Shape;USA Councils},
doi={10.1109/ROBOT.2008.4543434},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{4543829,
author={B. Huhle and M. Magnusson and W. Strasser and A. J. Lilienthal},
booktitle={2008 IEEE International Conference on Robotics and Automation},
title={Registration of colored 3D point clouds with a Kernel-based
extension to the normal distributions transform},
year={2008},
pages={4025-4030},
abstract={We present a new algorithm for scan registration of colored 3D
point data which is an extension to the normal distributions transform
(NDT). The probabilistic approach of NDT is extended to a color-aware
registration algorithm by modeling the point distributions as Gaussian
mixture-models in color space. We discuss different point cloud
registration techniques, as well as alternative variants of the proposed
algorithm. Results showing improved robustness of the proposed method
using real-world data acquired with a mobile robot and a time-of-flight
camera are presented.},
keywords={Gaussian processes;SLAM (robots);image colour analysis;image
registration;mobile robots;probability;robot vision;video
cameras;Gaussian mixture-model;SLAM robot;color-aware registration
algorithm;colored 3D point cloud registration technique;kernel-based
extension;mobile robot;normal distribution transform;probabilistic
approach;time-of-flight camera;Cameras;Clouds;Gaussian
distribution;Iterative closest point algorithm;Mobile robots;Robot
vision systems;Robotics and automation;Robustness;Simultaneous
localization and mapping;USA Councils},
doi={10.1109/ROBOT.2008.4543829},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{4543323,
author={H. Schafer and A. Hach and M. Proetzsch and K. Berns},
booktitle={2008 IEEE International Conference on Robotics and Automation},
title={3D obstacle detection and avoidance in vegetated off-road terrain},
year={2008},
pages={923-928},
abstract={This paper presents a laser-based obstacle detection facility
for off-road robotics in vegetated terrain. In the context of this work
the mobile off-road platform RAVON was equipped with a 3D laser scanner
and accompanying evaluation routines working on individual vertical
scans. Identified terrain characteristics are used to build up a local
representation of the environment. Introducing the abstraction concept
of virtual sensors the transparent integration of additional terrain
information on the basis of standardized behavior modules can be
achieved.},
keywords={collision avoidance;laser ranging;mobile robots;object
detection;optical scanners;robot vision;terrain mapping;vegetation
mapping;3D obstacle avoidance;3D obstacle detection;RAVON mobile
off-road platform;vegetated off-road terrain;Clouds;Laser fusion;Mobile
robots;Navigation;Robot sensing systems;Robotics and automation;Sensor
phenomena and characterization;Sensor systems;Vegetation mapping;Vehicle
detection},
doi={10.1109/ROBOT.2008.4543323},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{4543816,
author={Q. Shi and N. Xi},
booktitle={2008 IEEE International Conference on Robotics and Automation},
title={Automated data processing for a rapid 3D surface inspection system},
year={2008},
pages={3939-3944},
abstract={For 3D dimensional inspection systems, point clouds measured
on each viewpoint need to be processed for quality evaluation. Three
steps are usually included in this process: filtering, registration, and
error map generation. For quality control, small defects like dints and
dents have to be kept in the point cloud. Therefore, a filtering
algorithm is required to automatically remove outliers and keep
dints/dents. Many filtering algorithm smooth the point cloud for better
display, however, since the measured point cloud is used to represent
the shape of the part, modification of any point's coordinates is not
allowed because that will modify the error map. A point cloud filtering
algorithm is developed using a link clustering algorithm to identify and
remove outliers. Point cloud filtering is especially important in an
iterative closest point (ICP)-based robot hand- eye calibration method
because outliers will bring calibration errors into the calculated
transformation matrix. With this technique, the cleaned point clouds can
be directly transformed to a world frame for registration. This
registration method has two advantages compared to feature-based
registration methods: 1) the entire inspection process can be
automatically executed, 2) avoid holes in point clouds caused by
artificial markers. For error map generation, a point-to-plane distance
is used in this paper which calculates the distance of a point to its
closest triangle. The introduced automated inspection system had been
implemented on a PUMA robot system. Experimental results are described
in this paper.},
keywords={filtering theory;industrial robots;inspection;robot
vision;automated data processing;error map generation;industrial
robot;iterative closest point;link clustering algorithm;point cloud
filtering algorithm;rapid 3D surface inspection system;robot hand- eye
calibration method;robot kinematics;Calibration;Clouds;Coordinate
measuring machines;Data processing;Displays;Filtering
algorithms;Inspection;Quality control;Robot kinematics;Shape
measurement;Point cloud registration;automatic dimensional inspection
system;link clustering},
doi={10.1109/ROBOT.2008.4543816},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{4522161,
author={Qizhi Xiao and Kun Qin and Zequn Guan and Tao Wu},
booktitle={2007 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Image mining for robot vision based on concept analysis},
year={2007},
pages={207-212},
abstract={- In the process of image mining for robot vision, concept
analysis is an important technique. The paper proposes a novel framework
of image mining for robot vision based on concept lattice theory and
cloud model theory. Concept lattice reflects the process of human's
concept formation with mathematical formal language. Cloud model is a
transformation model between qualitative concepts and quantitative
numerical values. Image mining for robot vision is considered as a
process of concept extraction from different granularities (image
pixels, image pixel groups, image features, image objects, image files
and image databases). The methods of image mining from image
features(texture features, color features, shape features, spatial
relationship features) are introduced in the paper, which include the
following basic steps: firstly pre-process images, secondly use cloud
model to extract concepts, lastly use concept lattice to extract a
series of image knowledge(association rules, clustering rules and
classification rules). At last, a software prototype is designed and
developed, and some experiments confirm the validity of the proposed
framework.},
keywords={data mining;feature extraction;image colour analysis;image
texture;robot vision;cloud model theory;color feature;concept lattice
theory;image mining;robot vision;shape feature;spatial relationship
feature;texture feature;Clouds;Formal languages;Image analysis;Image
databases;Lattices;Pixel;Robot vision systems;Shape;Software
design;Software prototyping;cloud model;concept analysis;concept
lattice;image mining;robot vision},
doi={10.1109/ROBIO.2007.4522161},
month={Dec},}
@INPROCEEDINGS{4522281,
author={Wenming Cao and Rui Wang and Yanshan Li and Weixin Xie},
booktitle={2007 IEEE International Conference on Robotics and
Biomimetics (ROBIO)},
title={Cloud model-based coverage for sensor networks},
year={2007},
pages={895-898},
abstract={The sensing abilities of sensors are influenced by the
uncertainty of environment and physical devices in real deployment,
especially the randomness andfuzziness. It's necessary to have practical
considerations in the design stage to handle the uncertainty. Cloud
theory provides a simple and effective way to carry out the randomness
andfuzziness. Hence we propose a cloud model-based coverage for sensor
networks, and analyze its properties for deterministic senor deployment
The analysis and simulation results show that the proposed model has a
good performance in solving coverage problem with uncertainty.},
keywords={distributed sensors;fuzzy set theory;cloud model-based
coverage;deterministic senor deployment;sensing abilities;sensor
networks;Analytical
models;Biomimetics;Biosensors;Clouds;Entropy;Helium;Intelligent
networks;Performance analysis;Robot sensing systems;Uncertainty;Sensor
network;cloud model;coverage;estimation;uncertainty},
doi={10.1109/ROBIO.2007.4522281},
month={Dec},}
@INPROCEEDINGS{4423175,
author={R. Liu and D. Burschka and G. Hirzinger},
booktitle={2007 IEEE International Geoscience and Remote Sensing
Symposium},
title={Real time landscape modelling and visualization},
year={2007},
pages={1820-1823},
abstract={With the rapid development of information and communication
technology and widely spreading of internet, digital landscape becomes a
high topic recently. This paper presents a landscape modeling- and
visualizationssystem. The first part of this paper focuses on the real
time 3D-model- lingsprocess out of large point-clouds. Diverse
experiments have been done on the reconstruction of various famous
regions of Bavaria with tourist features. And the second part
concentrates on the interactive online visualizationssystem for massive
meshes. To enable an efficient interactive online visualization of these
large meshes, we convert them firstly with a multi-resolution hierarchy,
and then display them progressively. As a preprocessing for the
inter-active rendering, we generate a hierarchical tree of bounding
spheres from triangle meshes and write it to disk. It will be used for
view frustum culling, backface culling and level-of-detail control. We
use a recursive algorithm for display. And the traverse-depth in the
tree is decided by the projected size of the current node on the screen.
By an interactive rendering, once the user stops moving the mouse, the
scene will be redrawn with successively smaller thresholds until a size
of one pixel is reached.},
keywords={data mining;data visualisation;geophysical signal
processing;geophysical techniques;image processing;information
services;interactive systems;real-time systems;rendering (computer
graphics);Bavarian regions;backface culling;detail level control;digital
landscape;interactive online visualization system;interactive rendering
preprocessing;real time 3D modelling;real time landscape modelling;real
time landscape visualization;recursive algorithm;view frustum
culling;Airplanes;Cameras;Communications technology;Computer
graphics;Data
visualization;Displays;Internet;Mechatronics;Mice;Robots;achieving and
retrieval;data mining;environmental modelling},
doi={10.1109/IGARSS.2007.4423175},
ISSN={2153-6996},
month={July},}
@INPROCEEDINGS{4422815,
author={R. Liu and D. Burschka and G. Hirzinger},
booktitle={2007 IEEE International Geoscience and Remote Sensing
Symposium},
title={A novel approach to automatic registration of point clouds},
year={2007},
pages={401-404},
abstract={For the 3D reconstruction inside historic buildings, we need a
marker-free automatic registration approach to align different views
together, because GPS does not work indoors and markers are not allowed
to paste on the walls. This paper presents an automatic matching
process, which employs a novel algorithm, Dynamic Matching Tree
technique, for a fast and stable coarse-matching to achieve the
automatic pre-alignment of two point clouds and uses modified ICP to do
a fine matching efficiently. The whole process can be divided in the
following stages: preprocessing, 2-View matching and N-View matching. To
validate our method, various experiments has been done on reconstruction
of historic sites and industrial objects.},
keywords={geophysical signal processing;geophysical techniques;image
matching;image reconstruction;image registration;image
segmentation;stereo image processing;3D reconstruction;dynamic matching
tree;historic buildings;historic site reconstruction;image
segmentation;independent component analysis;industrial
objects;marker-free automatic registration;n-view matching;point cloud
alignment;point cloud registration;Aerodynamics;Clouds;Global
Positioning System;Heuristic algorithms;Image reconstruction;Image
registration;Iterative closest point algorithm;Mechatronics;Robotics and
automation;Skeleton;range image registration;segmentation and analysis},
doi={10.1109/IGARSS.2007.4422815},
ISSN={2153-6996},
month={July},}
@INPROCEEDINGS{4406911,
author={Howon Cheong and Soonyong Park and Mignon Park and Sung-Kee Park},
booktitle={2007 International Conference on Control, Automation and
Systems},
title={Vision-based global localization in indoor environment with an
object entity-based hybrid map},
year={2007},
pages={218-223},
abstract={This paper presents a new object entity based global
localization approach with stereo camera. A local invariant feature and
stereo depth information are used as visual features. The map we use
here is a hybrid of global topological map and local object location
map. The topological map includes some semantic information about the
representing space and the object entities in the space. The object
location map has the pose information of each object entity and visual
features for object recognition. The localization process consists of
two stages: coarse pose estimation and refined pose estimation. The
coarse pose is computed by using the object recognition and point cloud
fitting method. And the refined pose is estimated with particle
filtering algorithm. An experiment shows that our approach can be an
effective vision-based global localization method.},
keywords={feature extraction;object recognition;particle filtering
(numerical methods);pose estimation;robot vision;stereo image
processing;coarse pose estimation;global topological map;indoor
environment;local invariant feature;local object location map;object
entity-based hybrid map;object recognition;particle filtering
algorithm;point cloud fitting method;refined pose
estimation;robot;stereo camera;stereo depth information;vision-based
global localization;visual features;Automatic
control;Cameras;Clouds;Indoor environments;Infrared
sensors;Lighting;Object recognition;Refrigeration;Robot kinematics;Robot
vision systems;global localization;hybrid map;object
recognition;particle filtering;point cloud fitting},
doi={10.1109/ICCAS.2007.4406911},
month={Oct},}
@INPROCEEDINGS{4399309,
author={Radu Bogdan Rusu and N. Blodow and Z. Marton and A. Soos and M.
Beetz},
booktitle={2007 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Towards 3D object maps for autonomous household robots},
year={2007},
pages={3191-3198},
abstract={This paper describes a mapping system that acquires 3D object
models of man-made indoor environments such as kitchens. The system
segments and geometrically reconstructs cabinets with doors, tables,
drawers, and shelves, objects that are important for robots retrieving
and manipulating objects in these environments. The system also acquires
models of objects of daily use such glasses, plates, and ingredients.
The models enable the recognition of the objects in cluttered scenes and
the classification of newly encountered objects. Key technical
contributions include (1) a robust, accurate, and efficient algorithm
for constructing complete object models from 3D point clouds
constituting partial object views, (2) feature-based recognition
procedures for cabinets, tables, and other task-relevant furniture
objects, and (3) automatic inference of object instance and class
signatures for objects of daily use that enable robots to reliably
recognize the objects in cluttered and real task contexts. We present
results from the sensor-based mapping of a real kitchen.},
keywords={computational geometry;image reconstruction;image
segmentation;mobile robots;object detection;robot vision;3D object
maps;3D point clouds;autonomous household robots;feature-based
recognition;geometric object reconstruction;man-made indoor
environments;object recognition;object segmentation;Clouds;Cognitive
robotics;Context modeling;Inference algorithms;Intelligent
robots;Libraries;Orbital robotics;Robot sensing systems;Robotics and
automation;Robustness},
doi={10.1109/IROS.2007.4399309},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{4399381,
author={H. Andreasson and M. Magnusson and A. Lilienthal},
booktitle={2007 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Has somethong changed here? Autonomous difference detection for
security patrol robots},
year={2007},
pages={3429-3435},
abstract={This paper presents a system for autonomous change detection
with a security patrol robot. In an initial step a reference model of
the environment is created and changes are then detected with respect to
the reference model as differences in coloured 3D point clouds, which
are obtained from a 3D laser range scanner and a CCD camera. The
suggested approach introduces several novel aspects, including a
registration method that utilizes local visual features to determine
point correspondences (thus essentially working without an initial pose
estimate) and the 3D-NDT representation with adaptive cell size to
efficiently represent both the spatial and colour aspects of the
reference model. Apart from a detailed description of the individual
parts of the difference detection system, a qualitative experimental
evaluation in an indoor lab environment is presented, which demonstrates
that the suggested system is able register and detect changes in spatial
3D data and also to detect changes that occur in colour space and are
not observable using range values only.},
keywords={image colour analysis;image registration;object
detection;robots;security;3D-NDT representation;autonomous change
detection;coloured 3D point cloud;registration method;security patrol
robot;Cameras;Cognitive robotics;Data security;Humans;Intelligent
robots;Mobile robots;Robot sensing systems;Sensor systems;USA Councils},
doi={10.1109/IROS.2007.4399381},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{4379405,
author={N. Ho and R. Jarvis},
booktitle={2007 3DTV Conference},
title={Large Scale 3D Environmental Modelling for Stereoscopic
Walk-Through Visualisation},
year={2007},
pages={1-4},
abstract={The availability of high resolution and long range laser range
finder with colour image registration facilities opens up the
possibility of large scale, accurate and dense 3D environment modelling.
This paper addresses the problem of integration and analysis of multiple
scans collected over extended regions for stereoscopic walk-through
visualisation. Video clip examples will be shown at the time of paper
presentation.},
keywords={image colour analysis;image registration;image
resolution;laser ranging;remote sensing by laser beam;stereo image
processing;colour image registration facilities;high resolution laser
range finder;large scale 3D environmental modelling;long range laser
range finder;stereoscopic walk-through visualisation;video
clip;Availability;Clouds;Data acquisition;Data visualization;Intelligent
robots;Iterative algorithms;Iterative closest point
algorithm;Large-scale systems;Laser modes;Legged locomotion;plane
extraction;point cloud registration;point cloud
visualisation;stereoscopic walk-through},
doi={10.1109/3DTV.2007.4379405},
ISSN={2161-2021},
month={May},}
@INPROCEEDINGS{4381276,
author={V. Sakenas and O. Kosuchinas and M. Pfingsthorn and A. Birk},
booktitle={2007 IEEE International Workshop on Safety, Security and
Rescue Robotics},
title={Extraction of Semantic Floor Plans from 3D Point Cloud Maps},
year={2007},
pages={1-6},
abstract={3D mapping is increasingly important for mobile robotics in
general and for safety, security and rescue robotics (SSRR) in
particular as complex environments must but captured in this domain. But
it is hard to visualize 3D data in a simple way, e.g. to print maps for
first responders, or to use it in standard robotics algorithms, e.g.,
for path planning. This paper describes a new approach to extract
standard planar maps from large scale 3D maps in a very fast manner. In
doing so, the approach can detect multiple floors, e.g., in a
multi-story building or in a pancake collapse, and segment the 3D map
accordingly. To each floor or level, a planar map is assigned, which is
augmented by semantic information, especially with respect to
traversability. Experiments are presented that are based on 3D maps
generated in the large scale environments of USARsim, a high fidelity
robot simulator. It is shown that the approach is very fast. The total
processing of a complete 3D map takes just a few hundred milliseconds,
leading to a proper extraction of floor plans to each of which semantic
maps are assigned.},
keywords={control engineering computing;data visualisation;digital
simulation;mobile robots;path planning;service robots;3D data
visualization;3D mapping;3D point cloud map;USARsim robot
simulator;mobile rescue robot;path planning;semantic floor plan
extraction;Cameras;Clouds;Data mining;Data security;Floors;Jacobian
matrices;Large-scale systems;Mobile robots;Robot sensing systems;Robot
vision systems;3D Map;Autonomous System;Navigation;Point Cloud;Safety,
Security, and Rescue Robotics (SSRR);Semantic Map},
doi={10.1109/SSRR.2007.4381276},
ISSN={2374-3247},
month={Sept},}
@INPROCEEDINGS{4303792,
author={M. Strand and F. Erb and R. Dillmann},
booktitle={2007 International Conference on Mechatronics and Automation},
title={Range Image Registration Using an Octree based Matching Strategy},
year={2007},
pages={1622-1627},
abstract={Autonomous world modeling is one of the major topics in
current robot research. A basic concept hereby is the registration of
consecutive range images. Consistent models can only be built with
robust registration methods. Already one incorrect registered range
image will affect the following registrations and lead to an
inconsistent model. Therefore we developed a robust ICP registration
method based on an octree matching strategy. This matching strategy
could cope with large odometry errors and achieved the generation of
consistent 3D models.},
keywords={distance measurement;error analysis;image matching;image
registration;iterative methods;octrees;autonomous world
modeling;iterative closest point;octree based matching strategy;odometry
errors;range image registration;Cameras;Clouds;Image registration;Image
resolution;Iterative closest point algorithm;Least squares
approximation;Mechatronics;Navigation;Robotics and
automation;Robustness;Autonomous Exploration;ICP;Octree;Range
Image;Registration},
doi={10.1109/ICMA.2007.4303792},
ISSN={2152-7431},
month={Aug},}
@INPROCEEDINGS{4296783,
author={A. Nuchter and K. Lingemann and J. Hertzberg},
booktitle={Sixth International Conference on 3-D Digital Imaging and
Modeling (3DIM 2007)},
title={Cached k-d tree search for ICP algorithms},
year={2007},
pages={419-426},
abstract={The ICP (iterative closest point) algorithm is the de facto
standard for geometric alignment of three-dimensional models when an
initial relative pose estimate is available. The basis of ICP is the
search for closest points. Since the development of ICP, k-d trees have
been used to accelerate the search. This paper presents a novel search
procedure, namely cached k-d trees, exploiting iterative behavior of the
ICP algorithm. It results in a significant speedup of about 50% as we
show in an evaluation using different data sets.},
keywords={computational geometry;iterative methods;solid modelling;tree
searching;cached k-d tree search;data sets;de facto standard;geometric
alignment;iterative closest point algorithm;Cache memory;Clouds;Computer
science;Iterative algorithms;Iterative closest point algorithm;Knowledge
based systems;Neodymium;Quaternions;Robotics and automation;Solid
modeling},
doi={10.1109/3DIM.2007.15},
ISSN={1550-6185},
month={Aug},}
@INPROCEEDINGS{4150421,
author={Z. Zhao and E. K. Teoh},
booktitle={2006 9th International Conference on Control, Automation,
Robotics and Vision},
title={Robust MR Image Segmentation Using 3D Partitioned Active Shape
Models},
year={2006},
pages={1-6},
abstract={A 3D partitioned active shape model (PASM) is proposed in this
paper to address the problems of 3D active shape models (ASM) brought by
small training sets, which is usually the case in 3D applications. When
numbers of training samples are limited, 3D ASMs tend to be restrictive,
because the plausible area/allowable region spanned by relatively few
eigenvectors cannot capture the full range of shape variability. 3D
PASMs overcome this limitation by using a partitioned representation of
ASM. Given a point distribution model, the mean mesh is partitioned into
a group of small tiles. The statistical priors of tiles are estimated by
applying principal component analysis to each tile, and the priors serve
as constraints for corresponding tiles during deformations. To avoid the
shape inconsistency introduced by the independent estimations between
tiles, samples and deformed model points are projected as curves in one
hyperspace, instead of point clouds in several hyperspaces. The deformed
model points are then fitted into the allowable region of the model
using a curve alignment scheme. The experiments on 3D human brain MRIs
show that the 3D PASMs segment objects more accurately and are more
robust to noise and low contrast in images than two other current active
shape models. Furthermore, a study for the PASM's sensitivity to
different initializations shows that PASMs perform stable when
initialization positions change},
keywords={biomedical MRI;feature extraction;image segmentation;medical
image processing;principal component analysis;stereo image processing;3D
human brain MRI;3D partitioned active shape models;curve
alignment;hyperspace;magnetic resonance image segmentation;mesh
partitioning;object segmentation;point cloud;point distribution
model;principal component analysis;shape inconsistency;shape
variability;tile deformation;Active noise reduction;Active shape
model;Clouds;Deformable models;Humans;Image segmentation;Magnetic
resonance imaging;Noise robustness;Principal component analysis;Tiles;3D
MR image segmentation;Active Shape Model;Hierarchical Active Shape
Model;Partitioned Active Shape Model;Small training samples},
doi={10.1109/ICARCV.2006.345079},
month={Dec},}
@INPROCEEDINGS{4234361,
author={R. Liu and D. Burschka and G. Hirzinger and B. Strackenbrock},
booktitle={2007 Urban Remote Sensing Joint Event},
title={Real Time Fully Automatic 3D-Modelling of HRSC Landscape Data},
year={2007},
pages={1-7},
abstract={This paper presents a fully automatic 3D-modelings-process of
the landscape data obtained by the High Resolution Stereo Camera (HRSC)
assembled on a airplane. The input of this 3D-modellingsprocess is the
huge 2.5D point-clouds resulted from the photogrammetric preprocessing
and the output is a group of simplified colored meshes. The
3D-modelingsprocess consists of six steps: tilling with overlap, mesh
generation, mesh simplifying, mesh cut, mesh merger and texture mapping.
The whole process is fully automatic and is performed in real time.
Validation of the method has been done on the reconstruction of various
famous regions of Bavaria with tourist features.},
keywords={aircraft;cameras;geophysical signal processing;image colour
analysis;image reconstruction;image resolution;image texture;mesh
generation;photogrammetry;solid modelling;stereo image
processing;airplane;automatic 3D-modelling process;high resolution
stereo camera landscape data;photogrammetric preprocessing;simplified
colored meshes;texture mapping;Airplanes;Cameras;Cities and
towns;Corporate acquisitions;Data preprocessing;Mesh generation;Remote
sensing;Robotic assembly;Tiles;Visualization},
doi={10.1109/URS.2007.371762},
ISSN={2334-0932},
month={April},}
@INPROCEEDINGS{4209141,
author={D. M. Bradley and R. Unnikrishnan and J. Bagnell},
booktitle={Proceedings 2007 IEEE International Conference on Robotics
and Automation},
title={Vegetation Detection for Driving in Complex Environments},
year={2007},
pages={503-508},
abstract={A key challenge for autonomous navigation in cluttered outdoor
environments is the reliable discrimination between obstacles that must
be avoided at all costs, and lesser obstacles which the robot can drive
over if necessary. Chlorophyll-rich vegetation in particular is often
not an obstacle to a capable off-road vehicle, and it has long been
recognized in the satellite imaging community that a simple comparison
of the red and near-infrared (NIR) reflectance of a material provides a
reliable technique for measuring chlorophyll content in natural scenes.
This paper evaluates the effectiveness of using this
chlorophyll-detection technique to improve autonomous navigation in
natural, off-road environments. We demonstrate through extensive
experiments that this feature has properties complementary to the color
and shape descriptors traditionally used for point cloud analysis, and
show significant improvement in classification performance for tasks
relevant to outdoor navigation. Results are shown from field testing
onboard a robot operating in off-road terrain.},
keywords={feature extraction;image classification;image colour
analysis;mobile robots;navigation;object detection;robot
vision;vegetation;autonomous navigation;chlorophyll
detection;chlorophyll-rich vegetation;cluttered outdoor
environment;color descriptors;obstacle discrimination;off-road
vehicle;shape descriptors;vegetation detection;Costs;Drives;Image
recognition;Materials reliability;Navigation;Reflectivity;Remotely
operated vehicles;Robots;Satellites;Vegetation mapping},
doi={10.1109/ROBOT.2007.363836},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{4209831,
author={Q. Shi and N. Xi and W. Sheng},
booktitle={Proceedings 2007 IEEE International Conference on Robotics
and Automation},
title={Recursive Measurement Process for Improving Accuracy of
Dimensional Inspection of Automotive Body Parts},
year={2007},
pages={4764-4769},
abstract={Accuracy is essential to surface quality control when a range
sensor is applied to measure the 3D shape of an automotive body part. A
sensor's viewing pose, including location and orientation, is related to
measurement accuracy. It is usually difficult to find an optimal
solution by manual control of sensor viewpoints. A CAD-guided robot view
planner developed previously can automatically generate viewpoints.
Measurement accuracy can be satisfied in a certain range. However, the
unpredictable image noises, especially in regions with low intensity
contrast, cannot be compensated by the CAD-guided robot view planner. In
another aspect, measurement accuracy is evaluated all over the part
surface. The local accuracy of a small patch may exceed the measurement
tolerance. In this paper, feedback design is applied to the CAD-guided
robot sensor planning system. The feedback controller can evaluate the
accuracy of obtained point clouds, identify problem regions, and
generate new viewpoints. This process is recursively executed until the
measurement accuracy reaches to a tolerant value. This feedback-based
inspection system had been implemented in previous work to fill holes of
a point cloud, which are caused by shadows and light reflections. In
this paper, the feedback controller is specifically designed to improve
the measurement accuracy. Experimental results show the success of
applying this feedback system for dimensional inspection of an
automotive body part.},
keywords={CAD;automotive components;control system
synthesis;feedback;industrial robots;inspection;quality control;robot
vision;3D shape;CAD-guided robot sensor planning;CAD-guided robot view
planner;automotive body parts;dimensional inspection;feedback controller
design;feedback-based inspection;point cloud;range sensor;recursive
measurement;sensor viewpoint;surface quality control;Adaptive
control;Automotive engineering;Clouds;Feedback;Inspection;Quality
control;Robot sensing systems;Robotics and automation;Shape
control;Shape measurement},
doi={10.1109/ROBOT.2007.364213},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{4099052,
author={K. Richmond and S. M. Rock},
booktitle={OCEANS 2006},
title={An Operational Real-Time Large-Scale Visual Mosaicking and
Navigation System},
year={2006},
pages={1-6},
abstract={One use of ROVs (and potentially AUVs) is the generation of
visual mosaics of areas of the ocean floor. A visual mosaic is formed by
joining together multiple images taken by the vehicle as it surveys the
sea floor to form a larger, composite view. In order to cover an area
completely while guaranteeing overlap between images, precise
measurement and control of the vehicle is required. External arrays such
as LBLs can provide the precise navigation information required, at
considerable expense in deployment and with limited operational range.
Self-contained systems using DVLs and IMUS, do not suffer from these
limitations but are subject to dead-reckoning drift. However, the
imaging system itself can provide a means to eliminate drift. As long as
the vision system can see a previously-visited area, the position
measurement error relative to that part of the environment is bounded,
and does not depend on vehicle path length or integration time.
Additionally, the position information can be used to display a
real-time mosaic to the user, which grows as the vehicle moves. Vision
is subject to outages which do not affect a dead-reckoning sensor, such
as dust clouds obscuring the view, and it is limited in its range from
bottom. However, fusing the measurements of these two self-contained,
complementary sensors - dead-reckoning and vision -provides a robust
sensor which can provide the precision to guarantee complete area
coverage and sufficient overlap in large-scale mosaics without the need
to deploy an external positioning array. This paper presents an online
seafloor mosaicking and navigation system which exploits the
complementarity of dead-reckoning with a DVL and direct
environment-relative sensing using vision. The mosaicking and navigation
system operates in real time, and the calculated position provides a
measurement which can be fed back to control the vehicle position
relative to the environment and to display a navigation-grade mosaic to
the user in real ti- - me. The results of field trials conducted in
Monterey Bay using the MBARI ROVs Ventana and Tiburon are presented},
keywords={image segmentation;navigation;oceanographic regions;remotely
operated vehicles;underwater vehicles;DVL;IMUS;LBL;MBARI ROV;Monterey
Bay;Tiburon;Ventana;dead-reckoning drift;dust clouds;imaging
system;navigation system;robust sensor;sea floor;vehicle position
measurement error;visual mosaicking;Area
measurement;Displays;Large-scale systems;Navigation;Position
measurement;Real time systems;Remotely operated vehicles;Sea floor;Sea
measurements;Sensor arrays},
doi={10.1109/OCEANS.2006.306897},
ISSN={0197-7385},
month={Sept},}
@INPROCEEDINGS{4059056,
author={J. N. Bakambu and S. Gemme and E. Dupuis},
booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={Rover Localization through 3D Terrain Registration in Natural
Environments},
year={2006},
pages={4121-4126},
abstract={The registration of 3D points clouds is an important and
challenging task in computer vision. In this paper we consider the
problem of localizing a rover through 3D terrain registration in a
natural environment. Two different local feature-based 3D terrain
registration approaches are investigated: spin-image matching and point
fingerprint matching. To overcome the huge memory storage problem of
local features-based registration algorithms and improve the accuracy of
the matching results, while reducing the computing time of the matching
process, we developed an enhanced matching algorithm. The rover global
localization scenario was conducted in the Mars Yard located at the
Canadian Space Agency. The experimental results using natural
environment data sensed by a high resolution and accurate 3D range
sensor (LIDAR), demonstrate the effectiveness our enhanced matching
algorithm},
keywords={aerospace robotics;image matching;image registration;mobile
robots;path planning;planetary rovers;robot vision;3D range sensor;3D
terrain registration;Canadian Space Agency;computer vision;local
features-based registration algorithms;point fingerprint matching;rover
localization;spin-image matching;Clouds;Computer vision;Fingerprint
recognition;Geometry;Image reconstruction;Intelligent robots;Laser
radar;Mars;Navigation;Space technology;3-D Localization;Autonomous
Navigation;Spin-image;Terrain Registration;fingerprint},
doi={10.1109/IROS.2006.281899},
ISSN={2153-0858},
month={Oct},}
@INPROCEEDINGS{4018858,
author={L. Zhang and F. Sun and Z. Sun},
booktitle={2006 IEEE Conference on Robotics, Automation and Mechatronics},
title={Cloud Model-based Controller Design for Flexible-Link Manipulators},
year={2006},
pages={1-5},
abstract={A cloud model-based controller which needs no mathematical
models of plant is presented in this paper for the trajectory tracking
control of a flexible-link manipulator with poorly known dynamics. Based
on the singular perturbation method and the time-scale decomposition,
the flexible-link manipulator model is decomposed into a slow subsystem
of an equivalent rigid-link manipulator and a fast subsystem of flexible
mode. A composite adaptive controller is proposed to implement the angle
position control of the slow subsystem and simultaneous suppressing the
tip vibration. In the proposed control strategy, the control experience
qualitatively expressed by linguistic is transformed into the control
rulers using the normal cloud models. Experiment studies on the test-bed
of a two-link flexible manipulator are carried out to show the
robustness, viability and effectiveness of the proposed control approach},
keywords={adaptive control;control system synthesis;flexible
manipulators;large-scale systems;manipulator dynamics;model reference
adaptive control systems;perturbation techniques;position
control;vibration control;angle position control;cloud model-based
controller design;composite adaptive controller;flexible link
manipulators;flexible mode;intelligent control;manipulator
dynamics;normal cloud models;rigid-link manipulator;singular
perturbation method;time-scale decomposition;tip vibration
suppression;trajectory tracking control;Adaptive
control;Clouds;Manipulator dynamics;Mathematical model;Perturbation
methods;Position control;Programmable
control;Testing;Trajectory;Vibration control;cloud model;flexible-link
manipulators;intelligent control},
doi={10.1109/RAMECH.2006.252742},
ISSN={2158-2181},
month={Dec},}
@INPROCEEDINGS{1716755,
author={D. Deodhare and M. Vidyasagar and M. N. Murty},
booktitle={The 2006 IEEE International Joint Conference on Neural
Network Proceedings},
title={Bimodal Projection-based Features for Pattern Classification},
year={2006},
pages={4719-4726},
abstract={Classification tasks involving high dimensional vectors are
affected by the curse of dimensionality requiring large amount of
training data. This is because a high-dimensional space with a modest
number of samples is mostly empty. To overcome this we employ the
principle of Projection Pursuit. The principle is motivated by the aim
to search for clusters in high-dimensional space. Data points are
projected onto an appropriate projection direction. Search for clusters
is in this single dimensional projection space. As a result inherent
sparsity of the high-dimensional space is avoided. Classical
discriminant analysis methods also seek clusters but require class
labels to be specified. One such technique, the Fisher's linear
discriminant (FLD) method, has been used to arrive at an unsupervised
algorithm that seeks bimodal projection directions.},
keywords={pattern classification;pattern clustering;search
problems;vectors;Fisher linear discriminant method;bimodal
projection;high-dimensional space;pattern classification;unsupervised
algorithm;Clouds;Clustering algorithms;Data mining;Feature
extraction;NIST;Pattern classification;Robotics and
automation;Scattering;Training data;Vectors},
doi={10.1109/IJCNN.2006.247126},
ISSN={2161-4393},
month={},}
@INPROCEEDINGS{1699468,
author={W. Sepp},
booktitle={18th International Conference on Pattern Recognition (ICPR'06)},
title={Efficient Tracking in 6-DoF based on the Image-Constancy
Assumption in 3-D},
year={2006},
volume={3},
pages={59-62},
abstract={In this contribution maximum likelihood (ML) based approaches
are presented which track an a-priori known surface and texture in
monocular video streams. In contrast to established tracking algorithms
based on homographies the surface is not modeled as planar or piecewise
planar but as a collection of 3D surface points and surface normals.
Thus, any free-form surface can be modeled. This paper introduces a
novel description of the image Jacobian in terms of a reference Jacobian
based on the image-constancy (IC) assumption in 3D. Tracking with this
computationally efficient description is compared to the standard ML
approach with respect to the region and speed of convergence},
keywords={image texture;maximum likelihood estimation;target
tracking;video signal processing;video streaming;3D surface
points;free-form surface modeling;image Jacobian;maximum
likelihood;monocular video stream;surface normals;surface
tracking;tracking algorithm;Clouds;Computer interfaces;Jacobian
matrices;Maximum likelihood estimation;Mechatronics;Physics
computing;Robots;Spline;Streaming media;Surface texture},
doi={10.1109/ICPR.2006.485},
ISSN={1051-4651},
month={},}
@INPROCEEDINGS{1656025,
author={A. Agarwal and Lim Meng Hiot and Nguyen Trung Nghia and Er Meng
Joo},
booktitle={2006 IEEE Aerospace Conference},
title={Parallel region coverage using multiple UAVs},
year={2006},
pages={8 pp.-},
abstract={Low flying, small endurance UAVs are well-suited for region
coverage over airbases or in urban zones since they are cheap, highly
maneuverable and expendable. In this paper we consider the problem of
minimizing the time needed to cover the region of interest, a contiguous
rectilinear polygonal workspace, Pscr, using eta UAVs. Our approach is
based on partitioning Pscr into eta interior-disjoint polygons, P _i and
making a one-to-one assignment of the polygons to the UAVs. The
partition comprises polygons that are (a) rectilinear and (b) contiguous
(c) whose area-ratios equal to the ratio of rates at which the UAVs can
do coverage (d) and whose boundaries include points at which the
respective UAV begins coverage. Our work appears to be the first that
directly generates partitions that satisfy all the aforementioned
conditions. We discuss the operational requirements that motivate this
particular partitioning problem. We prove that our algorithm runs in O(N
log N + N + eta^2 N) time; N is the complexity of Pscr},
keywords={aerospace robotics;computational complexity;computational
geometry;mobile robots;multi-robot systems;airbases;contiguous
rectilinear polygonal workspace partitioning;interior-disjoint
polygons;low flying UAV;multiple unmanned aerial vehicles;parallel
region coverage;small endurance UAV;time complexity;urban
zones;Aerospace
electronics;Biographies;Clouds;Erbium;Geometry;Intelligent
systems;Partitioning algorithms;Payloads;Reconnaissance;Unmanned aerial
vehicles},
doi={10.1109/AERO.2006.1656025},
ISSN={1095-323X},
month={},}
@INPROCEEDINGS{1641869,
author={P. Newman and D. Cole and K. Ho},
booktitle={Proceedings 2006 IEEE International Conference on Robotics
and Automation, 2006. ICRA 2006.},
title={Outdoor SLAM using visual appearance and laser ranging},
year={2006},
pages={1180-1187},
abstract={This paper describes a 3D SLAM system using information from
an actuated laser scanner and camera installed on a mobile robot. The
laser samples the local geometry of the environment and is used to
incrementally build a 3D point-cloud map of the workspace. Sequences of
images from the camera are used to detect loop closure events (without
reference to the internal estimates of vehicle location) using a novel
appearance-based retrieval system. The loop closure detection is robust
to repetitive visual structure and provides a probabilistic measure of
confidence. The images suggesting loop closure are then further
processed with their corresponding local laser scans to yield putative
Euclidean image-image transformations. We show how naive application of
this transformation to effect the loop closure can lead to catastrophic
linearization errors and go on to describe a way in which gross,
pre-loop closing errors can be successfully annulled. We demonstrate our
system working in a challenging, outdoor setting containing substantial
loops and beguiling, gently curving traversals. The results are overlaid
on an aerial image to provide a ground truth comparison with the
estimated map. The paper concludes with an extension into the
multi-robot domain in which 3D maps resulting from distinct SLAM
sessions (no common reference frame) are combined without recourse to
mutual observation},
keywords={image sequences;laser ranging;mobile robots;path
planning;robot vision;3D SLAM system;3D point-cloud map;Euclidean
image-image transformations;appearance-based retrieval system;image
sequences;laser ranging;loop closure detection;mobile robot;outdoor
SLAM;visual appearance;Cameras;Computational geometry;Event
detection;Geometrical optics;Image retrieval;Mobile robots;Robot vision
systems;Simultaneous localization and mapping;Vehicle detection;Vehicles},
doi={10.1109/ROBOT.2006.1641869},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{1642041,
author={P. Biber and W. Strasser},
booktitle={Proceedings 2006 IEEE International Conference on Robotics
and Automation, 2006. ICRA 2006.},
title={nScan-matching: simultaneous matching of multiple scans and
application to SLAM},
year={2006},
pages={2270-2276},
abstract={Scan matching is a popular way of recovering a mobile robot's
motion and constitutes the basis of many localization and mapping
approaches. Consequently, a variety of scan matching algorithms have
been proposed in the past. All these algorithms share one common
attribute: They match pairs of scans to obtain spatial relations between
two robot poses. In this paper we present a method for matching multiple
scans simultaneously. We discuss the need for such a method and describe
how the result of such a multi-scan matching can be incorporated into
relation-based SLAM in the Lu and Milios style},
keywords={mobile robots;robot vision;localization and mapping
approach;mobile robot motion;nScan-matching;scan matching
algorithms;Clouds;Computer vision;Convergence;Covariance
matrix;Interpolation;Iterative algorithms;Minimization methods;Mobile
robots;Simultaneous localization and mapping},
doi={10.1109/ROBOT.2006.1642041},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{1641929,
author={D. M. Cole and P. M. Newman},
booktitle={Proceedings 2006 IEEE International Conference on Robotics
and Automation, 2006. ICRA 2006.},
title={Using laser range data for 3D SLAM in outdoor environments},
year={2006},
pages={1556-1563},
abstract={Traditional simultaneous localization and mapping (SLAM)
algorithms have been used to great effect in flat, indoor environments
such as corridors and offices. We demonstrate that with a few
augmentations, existing 2D SLAM technology can be extended to perform
full 3D SLAM in less benign, outdoor, undulating environments. In
particular, we use data acquired with a 3D laser range finder. We use a
simple segmentation algorithm to separate the data stream into distinct
point clouds, each referenced to a vehicle position. The SLAM technique
we then adopt inherits much from 2D delayed state (or scan-matching)
SLAM in that the state vector is an ever growing stack of past vehicle
positions and inter-scan registrations are used to form measurements
between them. The registration algorithm used is a novel combination of
previous techniques carefully balancing the need for maximally wide
convergence basins, robustness and speed. In addition, we introduce a
novel post-registration classification technique to detect matches which
have converged to incorrect local minima},
keywords={laser ranging;mobile robots;robot vision;3D SLAM;3D laser
range finder;laser range data;mobile robotics;registration
algorithm;segmentation algorithm;simultaneous localization and mapping
algorithms;Clouds;Convergence;Data engineering;Delay;Indoor
environments;Laser theory;Mobile robots;Position
measurement;Simultaneous localization and mapping;Vehicles},
doi={10.1109/ROBOT.2006.1641929},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{1640374,
author={S. Fleck and F. Busch and P. Biber and W. Straber},
booktitle={The 3rd Canadian Conference on Computer and Robot Vision
(CRV'06)},
title={Graph Cut based Panoramic 3D Modeling and Ground Truth Comparison
with a Mobile Platform - The Wagele -},
year={2006},
pages={19-19},
abstract={Efficient and comfortable acquisition of large 3D scenes is an
important topic for many current and future applications in the field of
robotics, factory and office visualization, 3DTV and cultural heritage.
In this paper we present both an omnidirectional stereo vision approach
for 3D modeling based on graph cut techniques and also a new mobile 3D
model acquisition platform where it is employed. The platform comprises
a panoramic camera and a 2D laser range scanner for self localization by
scan matching. 3D models are acquired just by moving the platform around
and recording images in regular intervals. Additionally, we concurrently
build 3D models using two supplementary laser range scanners. This
enables the investigation of the stereo algorithm’s quality by comparing
it with the laser scanner based 3D model as ground truth. This offers a
more objective point of view on the achieved 3D model quality.},
keywords={3D model acquisition;3DTV;Graph Cut;Cameras;Clouds;Laser beam
cutting;Laser modes;Layout;Mobile robots;Pipelines;Production
facilities;Stereo vision;Visualization;3D model acquisition;3DTV;Graph
Cut},
doi={10.1109/CRV.2006.37},
month={June},}
@INPROCEEDINGS{1570093,
author={J. M. Saez and F. Escolano},
booktitle={Proceedings of the 2005 IEEE International Conference on
Robotics and Automation},
title={Entropy Minimization SLAM Using Stereo Vision},
year={2005},
pages={36-43},
abstract={In this paper we present an information-based approach to
solve the SLAM problem using stereo vision. This approach results for an
improvement, in terms of both efficiency and robustness, of our early
multi-view ICP randomized algorithm. Instead of minimizing an ICP-based
cost, we propose the minimization of the entropy of the 2D distribution
induced by the projection of the 3D point cloud. In addition we embed
both the egomotion/action estimation algorithm which precedes global
rectification and the new global rectification algorithm in an
autonomous exploration schema. We assume plane-parallel environments
and, for the sake of efficiency, we also assume a flat floor and a fixed
stereo camera mounted on the robot. We show successful experiments both
under tele-operating the robot and under autonomous navigation.},
keywords={3D Mapping;Entropy Minimization;Navigation;SLAM;Stereo
Vision;Cameras;Clouds;Costs;Entropy;Iterative closest point
algorithm;Navigation;Robot vision systems;Robustness;Simultaneous
localization and mapping;Stereo vision;3D Mapping;Entropy
Minimization;Navigation;SLAM;Stereo Vision},
doi={10.1109/ROBOT.2005.1570093},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{1570366,
author={S. Fleck and F. Busch and P. Biber and W. Strasser and H.
Andreasson},
booktitle={Proceedings of the 2005 IEEE International Conference on
Robotics and Automation},
title={Omnidirectional 3D Modeling on a Mobile Robot using Graph Cuts},
year={2005},
pages={1748-1754},
abstract={For a mobile robot it is a natural task to build a 3D model of
its environment. Such a model is not only useful for planning robot
actions but also to provide a remote human surveillant a realistic
visualization of the robot’s state with respect to the environment.
Acquiring 3D models of environments is also an important task on its own
with many possible applications like creating virtual interactive
walkthroughs or as basis for 3D-TV. In this paper we present our method
to acquire a 3D model using a mobile robot that is equipped with a laser
scanner and a panoramic camera. The method is based on calculating dense
depth maps for panoramic images using pairs of panoramic images taken
from different positions using stereo matching. Traditional 2D-SLAM
using laser-scan-matching is used to determine the needed camera poses.
To receive high-quality results we use a high-quality stereo matching
algorithm – the graph cut method. We describe the necessary
modifications to handle panoramic images and specialized post-processing
methods.},
keywords={Cameras;Charge-coupled image sensors;Clouds;Data
acquisition;Humans;Laser beam cutting;Laser modes;Mobile robots;Robot
vision systems;Visualization},
doi={10.1109/ROBOT.2005.1570366},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{1570801,
author={W. Stocher and G. Biegelbauer},
booktitle={Proceedings of the 2005 IEEE International Conference on
Robotics and Automation},
title={Automated Simultaneous Calibration of a Multi-View Laser Stripe
Profiler},
year={2005},
pages={4424-4429},
abstract={We present enhancements of existing calibration methods that
allow the automated calibration of a laser stripe profiler in an
industrial environment. The first and most important enhancement is a
new design of the calibration object. Basically it is a “wire-frame
model” of a cube, where the edges are simply standard aluminium
profiles. This allows establishing point correspondences in a simple and
robust way without the need of intensity information. Furthermore this
hollow kind of a calibration object allows simultaneous multi-view
calibration since enough points of six different planes can be seen from
many directions. Given a rough calibration (based on rough measurements
with ± 50% accuracy) the scanned points can be automatically allocated
to the respective planes of the calibration object. Secondly we propose
a new way of solving the linear model of a laser stripe profiler. By
firstly solving the camera-independent geometry these values can be
averaged over the multiple views, which further reduces the local
offsets of point clouds and therefore enhances the output of subsequent
operations like mesh generation on the combined calibrated point cloud.
The paper describes the design of the calibration object and the
calibration method in detail and presents experimental results on
simulated and real range data sets. The industrial applicability has
been demonstrated in a sensor cell with four cameras under real-world
conditions including moderate oscillations of the calibration object.},
keywords={laser stripe profiler;multi-view;projective
calibration;Automation;Calibration;Cameras;Clouds;Laser modes;Laser
noise;Laser theory;Lenses;Nonlinear distortion;Optical sensors;laser
stripe profiler;multi-view;projective calibration},
doi={10.1109/ROBOT.2005.1570801},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{1570411,
author={D. F. Wolf and G. S. Sukhatme and D. Fox and W. Burgard},
booktitle={Proceedings of the 2005 IEEE International Conference on
Robotics and Automation},
title={Autonomous Terrain Mapping and Classification Using Hidden Markov
Models},
year={2005},
pages={2026-2031},
abstract={This paper presents a new approach for terrain mapping and
classification using mobile robots with 2D laser range finders. Our
algorithm generates 3D terrain maps and classifies navigable and
non-navigable regions on those maps using Hidden Markov models. The maps
generated by our approach can be used for path planning, navigation,
local obstacle avoidance, detection of changes in the terrain, and
object recognition. We propose a map segmentation algorithm based on
Markov Random Fields, which removes small errors in the classification.
In order to validate our algorithms, we present experimental results
using two robotic platforms.},
keywords={Application software;Clouds;Computer science;Hidden Markov
models;Laboratories;Mobile robots;Navigation;Path planning;Robot sensing
systems;Terrain mapping},
doi={10.1109/ROBOT.2005.1570411},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{1545285,
author={J. Weingarten and R. Siegwart},
booktitle={2005 IEEE/RSJ International Conference on Intelligent Robots
and Systems},
title={EKF-based 3D SLAM for structured environment reconstruction},
year={2005},
pages={3834-3839},
abstract={This paper presents the extension and experimental validation
of the widely used EKF-based SLAM algorithm to 3D space. It uses planar
features extracted probabilistically from dense three-dimensional point
clouds generated by a rotating 2D laser scanner. These features are
represented in compliance with the symmetries and perturbation model
(SPmodel) in a stochastic map. As the robot moves, this map is updated
incrementally while its pose is tracked by using an extended Kalman
filter. After showing how three-dimensional data can be generated, the
probabilistic feature extraction method is described, capable of
robustly extracting (infinite) planes from structured environments. The
SLAM algorithm is then used to track a robot moving through an indoor
environment and its capabilities in terms of 3D reconstruction are
analyzed.},
keywords={Kalman filters;feature extraction;image reconstruction;mobile
robots;probability;stereo image processing;stochastic processes;target
tracking;EKF-based 3D SLAM;SPmodel;extended Kalman filter;perturbation
model;pose tracking;probabilistic feature extraction;robot
tracking;rotating 2D laser scanner;simultaneous localization and
mapping;stochastic map;structured environment
reconstruction;three-dimensional point clouds;Clouds;Data mining;Feature
extraction;Indoor environments;Laser modes;Orbital
robotics;Robots;Robustness;Simultaneous localization and
mapping;Stochastic processes;3D SLAM;Extended Kalman
Filter;Probabilistic Plane Extraction;SPmodel},
doi={10.1109/IROS.2005.1545285},
ISSN={2153-0858},
month={Aug},}
@INPROCEEDINGS{1507419,
author={A. Nuchter and K. Lingemann and J. Hertzberg and H. Surmann},
booktitle={ICAR '05. Proceedings., 12th International Conference on
Advanced Robotics, 2005.},
title={6D SLAM with approximate data association},
year={2005},
pages={242-249},
abstract={This paper provides a new solution to the simultaneous
localization and mapping (SLAM) problem with six degrees of freedom. A
fast variant of the iterative closest points (ICP) algorithm registers
3D scans taken by a mobile robot into a common coordinate system and
thus provides relocalization. Hereby, data association is reduced to the
problem of searching for closest points. Approximation algorithms for
this searching, namely, approximate kd-trees and box decomposition
trees, are presented and evaluated in this paper. A solution to 6D SLAM
that considers all free parameters in the robot pose is built based on
3D scan matching},
keywords={approximation theory;data analysis;iterative methods;mobile
robots;tree searching;3D scan matching;6D SLAM;approximate data
association;approximate kd-trees;approximation algorithm;box
decomposition trees;coordinate system;iterative closest points
algorithm;mapping problem;mobile robot;simultaneous
localization;Approximation algorithms;Cloud computing;Iterative
algorithms;Iterative closest point algorithm;Laser modes;Mobile
robots;Robot kinematics;Robot sensing systems;Robotics and
automation;Simultaneous localization and mapping},
doi={10.1109/ICAR.2005.1507419},
month={July},}
@INPROCEEDINGS{1417376,
author={M. Gomercic and D. Winter},
booktitle={Industrial Informatics, 2004. INDIN '04. 2004 2nd IEEE
International Conference on},
title={Robot-based 3D imaging in industrial inspection},
year={2004},
pages={421-424},
abstract={In this paper, 3D scanners are presented for industrial
quality assurance and 3D inspection. The paper also shows a typical
measuring procedure of a full-field measurement using a modern 3D
scanner. For the inspection of large and complex components, a robot is
used, together with a rotation table. The data of a digitized cylinder
head cover compared to the respective CAD data set were also shown},
keywords={image scanners;industrial robots;inspection;production
engineering computing;quality assurance;3D imaging;3D
scanners;industrial inspection;industrial quality assurance;robot;Area
measurement;Clouds;Inspection;Position measurement;Robot
kinematics;Robotics and automation;Rotation measurement;Service
robots;Shape control;Shape measurement},
doi={10.1109/INDIN.2004.1417376},
month={June},}
@INPROCEEDINGS{1389797,
author={D. Burschka and Ming Li and R. Taylor and G. D. Hager},
booktitle={2004 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS) (IEEE Cat. No.04CH37566)},
title={Scale-invariant registration of monocular stereo images to 3D
surface models},
year={2004},
volume={3},
pages={2581-2586 vol.3},
abstract={We present an approach for scale recovery from monocular
stereo images of an endoscopic camera with simultaneous registration to
dense 3D surface models. We assume the camera motion to be unknown or at
least uncertain. An example application is the registration of endoscope
images to pre-operative CT scans that allows instrument navigation
during surgical procedures. The application field is not restricted to
the medical field. It can be extended to registration of monocular video
images to laser-based surface reconstructions in, e.g., mobile
navigation area or to autonomous aircraft navigation from topological
surveys. A novel way for depth estimation from arbitrary camera motion
is presented. In this paper, we focus on the robust initialization of
the system and on the scale recovery for the reconstructed 3D point
clouds with accurate registration to the candidate surfaces extracted
from the CT data. We provide experimental validation of the algorithm
with data obtained from our experiments with a phantom skull.},
keywords={endoscopes;image reconstruction;medical robotics;robot
vision;stereo image processing;video signal processing;3D surface
model;CT scan;arbitrary camera motion;depth estimation;endoscopic
camera;laser-based surface reconstructions;monocular stereo
images;scale-invariant registration;Aircraft navigation;Biomedical
imaging;Cameras;Computed tomography;Endoscopes;Image
reconstruction;Laser surgery;Surface emitting lasers;Surface
reconstruction;Surgical instruments},
doi={10.1109/IROS.2004.1389797},
month={Sept},}
@INPROCEEDINGS{1389448,
author={N. Vandapel and M. Hebert},
booktitle={2004 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS) (IEEE Cat. No.04CH37566)},
title={Finding organized structures in 3-D ladar data},
year={2004},
volume={1},
pages={786-791 vol.1},
abstract={In this paper, we address the problem of finding organized
thin structures in three-dimensional (3-D) data. Linear and planar
structures segmentation received much attention but thin structures
organized in complex patterns remain a challenge for segmentation
algorithms. We are interested especially in the problems posed by
repetitive and symmetric structures acquired with a laser range finder.
The method relies on 3-D data projections along specific directions and
2-D histograms comparison. The sensitivity of the classification
algorithm to the parameter settings is evaluated and a segmentation
method proposed. We illustrate our approach with data from a concertina
wire in terrain with vegetation.},
keywords={image classification;image segmentation;laser ranging;mobile
robots;thin wall structures;vegetation mapping;3D ladar data;concertina
wire;laser range finder;linear structures segmentation;planar structures
segmentation;thin structures;Clouds;Landmine detection;Laser
radar;Layout;Millimeter wave radar;Periodic structures;Radar
detection;Shape;Vegetation mapping;Wire},
doi={10.1109/IROS.2004.1389448},
month={Sept},}
@INPROCEEDINGS{1307987,
author={J. M. Saez and F. Escolano},
booktitle={Robotics and Automation, 2004. Proceedings. ICRA '04. 2004
IEEE International Conference on},
title={A global 3D map-building approach using stereo vision},
year={2004},
volume={2},
pages={1197-1202 Vol.2},
abstract={We present a stereo-based approach for building 3D maps.
First, the best local alignment between successive point clouds is
computed by a fast ego-motion/action-estimation algorithm which relies
on an incremental matches filtering process followed by energy
minimization. Then, a quasi-random updating algorithm, a kind of
multi-view ICP, minimizes the global inconsistency of the map. Such an
inconsistency is defined in terms of the sum of local inconsistencies
and an additional entropy-based regularization term which is effective
in plane-parallel environments. For the sake of efficiency, we assume a
flat floor and a fixed stereo camera mounted on the robot. We have
successfully tested the approach by performing several indoor mapping
experiments.},
keywords={entropy;mobile robots;motion estimation;robot vision;stereo
image processing;3D map building method;action estimation algorithm;ego
motion algorithm;energy minimization;entropy-based
regularization;incremental matches filtering process;indoor mapping
experiments;plane-parallel environments;quasirandom updating
algorithm;robot;stereo camera;stereo vision;Cameras;Filtering
algorithms;Iterative closest point algorithm;Matched
filters;Minimization methods;Robot vision systems;Stereo vision;Terrain
mapping;Three-dimensional displays},
doi={10.1109/ROBOT.2004.1307987},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{1307255,
author={M. A. Garcia and A. Solanas},
booktitle={Robotics and Automation, 2004. Proceedings. ICRA '04. 2004
IEEE International Conference on},
title={3D simultaneous localization and modeling from stereo vision},
year={2004},
volume={1},
pages={847-853 Vol.1},
abstract={This work presents a new algorithm for determining the
trajectory of a mobile robot and, simultaneously, creating a detailed
volumetric 3D model of its workspace. The algorithm exclusively utilizes
information provided by a single stereo vision system, avoiding thus the
use both of more costly laser systems and error-prone odometry.
Six-degrees-of-freedom egomotion is directly estimated from images
acquired at relatively close positions along the robot's path. Thus, the
algorithm can deal with both planar and uneven terrain in a natural way,
without requiring extra processing stages or additional orientation
sensors. The 3D model is based on an octree that encapsulates clouds of
3D points obtained through stereo vision, which are integrated after
each egomotion stage. Every point has three spatial coordinates referred
to a single frame, as well as true-color components. The spatial
location of those points is continuously improved as new images are
acquired and integrated into the model.},
keywords={mobile robots;path planning;robot vision;stereo image
processing;3D simultaneous localization;egomotion stage;error-prone
odometry;mobile robot;octree;six-degrees-of-freedom egomotion;stereo
vision;Computer vision;Intelligent robots;Laser modes;Mathematical
model;Mobile robots;Robot kinematics;Robot sensing systems;Robot vision
systems;Simultaneous localization and mapping;Stereo vision},
doi={10.1109/ROBOT.2004.1307255},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{1285760,
author={Li Chengjun and Wei Ying and Shi Zeling},
booktitle={IEEE International Conference on Robotics, Intelligent
Systems and Signal Processing, 2003. Proceedings. 2003},
title={A small target detection algorithm based on multi-scale energy
cross},
year={2003},
volume={2},
pages={1191-1196 vol.2},
abstract={In accordance with the characteristics of the small target
against sea and sky background, a small target detection algorithm based
on multi-scale mutual energy cross is proposed in this paper. In order
to determinate potential region in which the targets lie, sea-level line
first is detected by using direction gradient operator. Then on the
basis of wavelet decomposition, we define the multi-scale mutual energy
cross function to eliminate the intense infrared clutter. The precise
position of the small target is finally found by region-growth
technique. Experimental results show that the algorithm proposed has
better performance with respect to detection probability of single frame
and less computation complexity. It is an effective small infrared
target detection algorithm against sea and sky background.},
keywords={clutter;discrete wavelet transforms;gradient methods;image
enhancement;infrared imaging;object detection;computation
complexity;direction gradient operator;infrared clutter;infrared target
detection algorithm;multiscale mutual energy cross function;region
growth technique;sea background;sea level line detection;single frame
detection probability;sky background;wavelet decomposition;Automatic
control;Automation;Clouds;Detection algorithms;Filtering;Functional
analysis;Infrared detectors;Object detection;Optical reflection;Sea
surface},
doi={10.1109/RISSP.2003.1285760},
month={Oct},}
@INPROCEEDINGS{1285780,
author={Ming Li and Hua Qin and Weidong Sun},
booktitle={IEEE International Conference on Robotics, Intelligent
Systems and Signal Processing, 2003. Proceedings. 2003},
title={Shape similarities measures based on two-level ARG for the
retrieval of remote sensing images},
year={2003},
volume={2},
pages={1300-1305 vol.2},
abstract={Content-based image retrieval is widely investigated in image
database area. However, work on content-based retrieval for remote
sensing images (RSI) is seldom reported in literature. In this paper, a
new strategy for content-based remote sensing images retrieval based on
shape similarities measures has been proposed, which includes the shape
description of each object and spatial relational representation between
objects based on the two-level attributed relational graph (ARG). We
have also showed that Zernike moments are very effective in describing
shape features of each region, and the two-level ARG is efficient for
spatial relational information of each disjoint region of an object in
RSI. As one of the applications, we have applied this method in our
network-oriented multiple satellites images management and online
distribution system, and the cloud objects of NOAA AVHRR images are used
for verifications. An actual case for the descriptions of a cloud object
with Zernike moments, the representations of relations with a two-level
ARG and the shape similarity measure results in the database is given.},
keywords={Zernike polynomials;content-based retrieval;graph theory;image
retrieval;remote sensing;visual databases;Zernike moments;attributed
relational graph;content based image retrieval;image database
area;network oriented multiple satellites image management system;online
distribution system;remote sensing images;shape similarity
measures;spatial relational information;spatial relational
representation;Clouds;Content based retrieval;Image databases;Image
retrieval;Information retrieval;Pixel;Remote sensing;Satellites;Shape
measurement;Vegetation mapping},
doi={10.1109/RISSP.2003.1285780},
month={Oct},}
@INPROCEEDINGS{1248828,
author={R. Unnikrishnan and M. Hebert},
booktitle={Proceedings 2003 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)},
title={Robust extraction of multiple structures from non-uniformly
sampled data},
year={2003},
volume={2},
pages={1322-1329 vol.2},
abstract={The extraction of multiple coherent structures from point
clouds is crucial to the problem of scene modeling. While many
statistical methods exist for robust estimation from noisy data, they
are inadequate for addressing issues of scale, semi-structured clutter,
and large point density variation together with the computational
restriction of autonomous navigation. This paper extends an approach of
nonparametric projection-pursuit based regression to compensate for the
non-uniform and directional nature of data sampled in outdoor
environments. The proposed algorithm is employed for extraction of
planar structures and clutter grouping. Results are shown for scene
abstraction of 3D range data in large urban scenes.},
keywords={estimation theory;feature extraction;nonparametric
statistics;regression analysis;autonomous navigation;clutter
grouping;multiple structures extraction;noisy data
estimation;nonparametric projection-pursuit based
regression;nonuniformly sampled data;point density variation;scene
abstraction;semistructured clutter;statistical method;Acoustic
noise;Clouds;Data mining;Laser modes;Layout;Noise robustness;Parameter
estimation;Robots;Sampling methods;Statistical analysis},
doi={10.1109/IROS.2003.1248828},
month={Oct},}
@INPROCEEDINGS{1248827,
author={C. Pantofaru and R. Unnikrishnan and M. Hebert},
booktitle={Proceedings 2003 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)},
title={Toward generating labeled maps from color and range data for
robot navigation},
year={2003},
volume={2},
pages={1314-1321 vol.2},
abstract={This paper addresses the problem of extracting information
from range and color data acquired by a mobile robot in urban
environments. Our approach extracts geometric structures from clouds of
3-D points and regions from the corresponding color images, labels them
based on prior models of the objects expected in the environment -
buildings in the current experiments - and combines the two sources of
information into a composite labeled map. Ultimately, our goal is to
generate maps that are segmented into objects of interest, each of which
is labeled by its type, e.g., building, vegetation, etc. Such a map
provides a higher-level representation of the environment than the
geometric maps normally used for mobile robot navigation. The techniques
presented here are a step toward the automatic construction of such
labeled maps.},
keywords={feature extraction;image classification;image colour
analysis;image segmentation;laser ranging;mobile robots;color
data;geometric structures;information extraction;labeled maps
generation;mobile robot;object segmentation;range data;robot
navigation;Buildings;Clouds;Color;Data mining;Image
segmentation;Information resources;Mobile robots;Navigation;Solid
modeling;Vegetation mapping},
doi={10.1109/IROS.2003.1248827},
month={Oct},}
@INPROCEEDINGS{1234961,
author={R. Brad and I. A. Letia},
booktitle={7th International Conference on Control, Automation, Robotics
and Vision, 2002. ICARCV 2002.},
title={Extracting cloud motion from satellite image sequences},
year={2002},
volume={3},
pages={1303-1307 vol.3},
abstract={This paper present a new technique for the estimation of cloud
motion, using a sequence of infrared satellite images. It can be
considered a challenging task due to the complexity of phenomena
implied, as non-linear events and a non-rigid motion. In this
circumstances most motion models are not suitable and new algorithms
have to be developed. We propose a novel method, combining an Automatic
Multilevel Thresholding for image segmentation, a Block Matching
Algorithm (BMA) and a best candidate block search along with a vector
median regularization.},
keywords={artificial satellites;clouds;feature extraction;image
matching;image segmentation;image sequences;motion
estimation;vectors;automatic multilevel thresholding;block matching
algorithm;cloud motion estimation;cloud motion extraction;image
segmentation;infrared satellite images;nonlinear events;nonrigid
motion;satellite image sequences;vector median
regularization;Clouds;Computer science;Deformable models;Image
analysis;Image segmentation;Image sequences;Infrared
imaging;Meteorology;Motion estimation;Satellites},
doi={10.1109/ICARCV.2002.1234961},
month={Dec},}
@INPROCEEDINGS{1036849,
author={E. Mahr and T. Mosher},
booktitle={Proceedings, IEEE Aerospace Conference},
title={Mission architectures for the exploration of the lunar poles},
year={2002},
volume={1},
pages={1-295-1-303 vol.1},
abstract={Mars has been a recent focus of solar system exploration in
the search for extraterrestrial water. However, Earth's nearest neighbor
the Moon may yield the nearest aquifer. Scientific results returned by
Clementine and Lunar Prospector showed a hydrogen anomaly to be present
at the lunar poles that may be water ice. This result, however, cannot
be confirmed without further investigation. The purpose of this paper is
to discuss a mission architecture to explore the lunar poles and answer
the lingering scientific questions. This is done through a comparison of
possible architectures using common criteria such as science merit and
implementation risk. A survey of recently proposed missions is conducted
to establish if the best route to exploring the poles is being pursued.},
keywords={ice;lunar surface;planetary rovers;planning;space
research;space vehicles;water;Clementine;Lunar
Prospector;Moon;aquifer;common mission criteria;extraterrestrial
water;hydrogen anomaly;implementation risk;lunar poles
exploration;mission architectures;science merit;solar system
exploration;water ice;Clouds;Humans;Hydrogen;Ice;Moon;Nearest neighbor
searches;Orbital robotics;Probes;Solar system;Space exploration},
doi={10.1109/AERO.2002.1036849},
month={},}
@INPROCEEDINGS{1041691,
author={M. A. Kovacina and D. Palmer and Guang Yang and R. Vaidyanathan},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and
Systems},
title={Multi-agent control algorithms for chemical cloud detection and
mapping using unmanned air vehicles},
year={2002},
volume={3},
pages={2782-2788 vol.3},
abstract={Traditional control approaches fall well short of the
necessary flexibility and efficiency needed to meet the commercial and
military demands placed upon UAV swarms. Effective coordination of these
swarms requires development of control strategies based on emergent
behavior. We have developed a rule-based, decentralized control
algorithm that relies on constrained randomized behavior and respects
UAV restrictions on sensors, computation, and flight envelope. To
demonstrate and evaluate the effectiveness of our approach, we have
created a simulation of an air vehicle swarm searching for and mapping a
chemical cloud within a patrolled region. We then consider several
different detection and mapping strategies based on emergent behavior.
We then establish an inverse linear relation between the size of the
swarm and the time to detect the cloud, regardless of the size of the
cloud. Further, we also show the size of the swarm has a linear relation
with the successful detection of the cloud.},
keywords={aerospace robotics;air pollution measurement;atmospheric
techniques;decentralised control;intelligent control;mobile
robots;multi-agent systems;UAV swarm coordination;chemical cloud
detection;chemical cloud mapping;constrained randomized
behavior;emergent behavior;flight envelope;inverse linear
relation;multiagent control algorithms;rule-based decentralized control
algorithm;unmanned air vehicles;Aircraft;Chemical
sensors;Clouds;Computational modeling;Distributed
control;Insects;Reconnaissance;Surveillance;Unmanned aerial
vehicles;Vehicle detection},
doi={10.1109/IRDS.2002.1041691},
month={},}
@INPROCEEDINGS{1039098,
author={D. L. Page and Y. Sun and A. F. Koschan and J. K. Paik and M. A.
Abidi},
booktitle={Proceedings. International Conference on Image Processing},
title={Simultaneous mesh simplification and noise smoothing of range
images},
year={2002},
volume={3},
pages={821-824 vol.3},
abstract={We propose a novel algorithm to smooth and simplify
simultaneously range images and also triangle meshes derived from those
images. These data sets often suffer from noise and over-sampling. To
overcome these issues, smoothing from image processing and
simplification from computer graphics attempt to minimize noise and
reduce complexity, respectively. Typically, these algorithms are
separate and distinct steps, but we combine them into one algorithm. We
employ surface normal voting to generate robust orientation estimates
and then extend the quadric error metric framework to smooth noise while
simplifying the surface. We demonstrate the capabilities of this
algorithm with both synthetic and real data. The proposed algorithm
provides significant noise smoothing improvement when compared to the
standard Garland and Heckbert (1998) quadric simplification algorithm.},
keywords={image processing;noise;smoothing methods;complexity
reduction;computer graphics;data sets;image processing;mesh
simplification;noise minimization;noise smoothing;over-sampling;quadric
error metric;quadric simplification algorithm;range images;real
data;robust orientation estimates;surface normal voting;synthetic
data;triangle meshes;Clouds;Computer graphics;Image sampling;Intelligent
robots;Intelligent systems;Laboratories;Smoothing
methods;Sun;Voting;Working environment noise},
doi={10.1109/ICIP.2002.1039098},
ISSN={1522-4880},
month={June},}
@ARTICLE{938384,
author={Jianbing Huang and Chia-Hsiang Menq},
journal={IEEE Transactions on Robotics and Automation},
title={Automatic data segmentation for geometric feature extraction from
unorganized 3-D coordinate points},
year={2001},
volume={17},
number={3},
pages={268-279},
abstract={A systematic approach is proposed to automatically extract
geometric surface features from a point cloud composed of a set of
unorganized three-dimensional coordinate points by data segmentation.
The point cloud is sampled from the boundary surface of a mechanical
component with arbitrary shape. The proposed approach is composed of
three steps. In the first step, a mesh surface domain is reconstructed
to establish an explicit topological relation among the discrete points.
The topological adjacency is further optimized to recover the second
order object geometry. In the second step, curvature-based border
detection is applied on the irregular mesh to extract both sharp borders
with tangent discontinuity and smooth borders with curvature
discontinuity. Finally, the mesh patches separated by the extracted
borders are grouped together in the third step. For objects with complex
shape, a multilevel segmentation scheme is proposed for better results.
The capability of the proposed approach is demonstrated using various
point clouds having distinct characteristics. Integrated with state of
art scanning devices, the developed segmentation scheme can support
reverse engineering of high precision mechanical components. It has
potential applications in a whole spectrum of engineering problems with
a major impact on rapid design and prototyping, shape analysis, and
virtual reality},
keywords={edge detection;feature extraction;geometry;image
segmentation;mechanical engineering;reverse
engineering;topology;automatic data segmentation;boundary
surface;curvature discontinuity;curvature-based border
detection;discrete points;explicit topological relation;geometric
feature extraction;geometric surface features;high precision mechanical
component;irregular mesh;mechanical component;mesh patches;mesh surface
domain;multilevel segmentation scheme;point cloud;rapid design;rapid
prototyping;reverse engineering;second order object geometry;shape
analysis;sharp borders;smooth borders;state of art scanning
devices;systematic approach;tangent discontinuity;topological
adjacency;unorganized 3D coordinate points;virtual
reality;Art;Clouds;Data mining;Design engineering;Feature
extraction;Geometry;Reverse engineering;Shape;Surface
reconstruction;Virtual prototyping},
doi={10.1109/70.938384},
ISSN={1042-296X},
month={Jun},}
@INPROCEEDINGS{1013616,
author={F. Prieto and P. Boulanger and R. Lepage and T. Redarce},
booktitle={Proceedings 2002 IEEE International Conference on Robotics
and Automation (Cat. No.02CH37292)},
title={Automated inspection system using range data},
year={2002},
volume={3},
pages={2557-2562 vol.3},
abstract={We propose an automated inspection system of manufactured
parts using a cloud of 3D measured points of a part provided by a range
sensor, and its CAD model. Inspection consists in verifying the accuracy
of a part related to a given set of tolerances. It is thus necessary
that the 3D measurements be accurate. In the 3D capture of a part,
several sources of error can alter the measured values. So, we have to
find and model the most influential parameters affecting the accuracy of
the range sensor in the digitalization process. This model is used to
produce a sensing plan to acquire accurately the geometry of a part. By
using the noise model, we introduce a dispersion value for each 3D point
acquired. This value of dispersion is shown as a weight factor in the
inspection results.},
keywords={CAD;computer vision;distance measurement;inspection;splines
(mathematics);tolerance analysis;3D capture;3D measurements;CAD
model;accuracy;automated inspection system;digitalization
process;dispersion value;machine vision;manufactured parts;noise
model;optimal sensor placement;range data;range sensor;tolerances;view
planning;Calibration;Cameras;Clouds;Computer aided
manufacturing;Coordinate measuring machines;Inspection;Laser
beams;Manufacturing industries;Solid modeling;Surface fitting},
doi={10.1109/ROBOT.2002.1013616},
month={May},}
@ARTICLE{54741,
author={V. H. L. Cheng},
journal={IEEE Transactions on Robotics and Automation},
title={Concept development of automatic guidance for rotorcraft obstacle
avoidance},
year={1990},
volume={6},
number={2},
pages={252-257},
abstract={The automatic guidance of rotorcraft for obstacle avoidance in
nap-of-the-earth flight is studied. The author considers a hierarchical
breakdown of the guidance components to identify the functional
requirements. These requirements and anticipated sensor capabilities
lead to a preliminary guidance concept, which has been evaluated via
computer simulations},
keywords={aircraft control;computerised navigation;helicopters;automatic
guidance;concept;obstacle avoidance;rotorcraft;Clouds;Defense
industry;Earth;Electric breakdown;Mobile
robots;NASA;Navigation;Production facilities;Vegetation mapping;Vehicles},
doi={10.1109/70.54741},
ISSN={1042-296X},
month={Apr},}
@INPROCEEDINGS{131865,
author={P. Dauchez and X. Delebarre and Y. Bouffard and E. Degoulange},
booktitle={Proceedings. 1991 IEEE International Conference on Robotics
and Automation},
title={Task modeling and force control for a two-arm robot},
year={1991},
pages={1702-1707 vol.2},
abstract={The authors present solutions for manipulating objects with
two robotic arms, when force control is necessary. The applications of
interest are the transport and assembly of rigid objects, the
deformation, transport, and assembly of flexible objects, and the
assembly in space of two objects, each being held by one arm. The
authors present a theoretical solution for modeling the tasks and
practical solutions implemented for controlling two six-axis arms},
keywords={aerospace control;assembling;control system analysis;force
control;materials handling;robots;aerospace control;flexible object
assembly;force control;rigid object assembly;six-axis arms;task
modelling;two-arm robot;Arm;Clouds;Force control;Force sensors;IEEE
members;Oceans;Orbital robotics;Robotic assembly;Robots;Velocity control},
doi={10.1109/ROBOT.1991.131865},
month={Apr},}
@INPROCEEDINGS{284493,
author={Yang Chen and G. Medioni},
booktitle={Proceedings of 1994 IEEE 2nd CAD-Based Vision Workshop},
title={Fitting a surface to 3-D points using an inflating balloon model},
year={1994},
pages={266-273},
abstract={We address the problem of fitting a surface to a cloud of 3-D
points, as obtained from a set of registered range images. Our approach
is based on a dynamic balloon model represented using a triangulated
mesh. The vertices in the mesh are linked to their neighboring vertices
through springs to simulate the surface tension and to keep the shell
smooth. Unlike other dynamic models proposed by previous researchers,
our balloon model is purely driven by an applied inflation force towards
the object surface from inside of the object, until all the triangles
have reached the surface. We present results on simple as well as
nonstar-shaped objects from real range images},
keywords={mesh generation;solid modelling;surface fitting;surface
tension;3D points;dynamic balloon model;dynamic models;inflating balloon
model;inflation force;nonstar-shaped objects;object surface;real range
images;registered range images;springs;surface fitting;surface tension
simulation;three dimensional points;triangulated
mesh;vertices;Clouds;Contracts;Intelligent robots;Intelligent
systems;Iron;Monitoring;Shape;Springs;Surface fitting;US Government},
doi={10.1109/CADVIS.1994.284493},
month={Feb},}
@INPROCEEDINGS{220024,
author={F. Pipitone and W. Adams},
booktitle={Proceedings 1992 IEEE International Conference on Robotics
and Automation},
title={Tripod operators for recognizing objects in range images: rapid
rejection of library objects},
year={1992},
pages={1596-1601 vol.2},
abstract={The tripod operator, a class of feature extraction operators
for range images which facilitate the recognition and localization of
objects is described. It consists of three points in 3-space fixed at
the vertices of an equilateral triangle and a procedure for making
several scalar measurements in the coordinate frame of the triangle. The
triangle is moved as a rigid body until the three vertices lie on the
surface of some range image or modeled object. The resulting
measurements are local shape features which are invariant under rigid
motions. These features contain all the shape information in the surface
points involved, and no other information. Tripod operators are
applicable to all 3-D shapes. A cloud of points in feature space was
generated for each object by random placement of the operator. Then new
feature measurements were made by operator placements in a range image
containing one of those objects. Using a simple nearest-neighbor
approach, the authors determined which objects were rejected and which
remained as recognition candidates. Experiments showed that tripod
operators had excellent discriminating power},
keywords={feature extraction;discriminating power;feature extraction
operators;local shape features;localization;nearest-neighbor
approach;object recognition;range images;scalar measurements;shape
information;tripod operator;Artificial intelligence;Clouds;Feature
extraction;Image recognition;Laboratories;Libraries;Machine
vision;Object recognition;Shape measurement;Solid modeling},
doi={10.1109/ROBOT.1992.220024},
month={May},}
@INPROCEEDINGS{24955,
author={D. Morin and C. Rizzi and W. Stohr and C. Tahon},
booktitle={[Proceedings] 1988 IEEE Workshop on Languages for
Automation@m_Symbiotic and Intelligent Robotics},
title={VITAMIN toolkit: a UIMS for CIM applications},
year={1988},
pages={80-89},
abstract={The analysis and design of a user interface management system
(UIMS) for computer-integrated manufacturing (CIM) applications is
presented. An architecture is proposed that considers three types of
users: the system-building user, the application administrator, and the
end user. This architecture is a modular toolkit comprised of a logical
model builder, a monitor builder, and a module to external database
builder, each of which is outlined. The focus is on the logical model
builder and its components, a presentation techniques module, a dialogue
control module, and an application interface module},
keywords={CAD/CAM;manufacturing data processing;software tools;user
interfaces;CIM;UIMS;application administrator;application interface
module;computer-integrated manufacturing;database builder;dialogue
control module;end user;logical model builder;modular toolkit;monitor
builder;presentation techniques module;software tools;system-building
user;user interface management system;Application software;Automatic
control;Computer architecture;Computer integrated manufacturing;Control
systems;Environmental economics;Hardware;Packaging;Production
systems;User interfaces},
doi={10.1109/LFA.1988.24955},
month={Aug},}
@INPROCEEDINGS{407406,
author={A. Baader and G. Hirzinger},
booktitle={Intelligent Robots and Systems '94. 'Advanced Robotic Systems
and the Real World', IROS '94. Proceedings of the IEEE/RSJ/GI
International Conference on},
title={A self-organizing algorithm for multisensory surface
reconstruction},
year={1994},
volume={1},
pages={81-88 vol.1},
abstract={In this paper a method is presented to reconstruct and model
an unknown three-dimensional surface which is described by an unordered
cloud of sampled surface points. For that purpose Kohonen's
self-organizing feature map is modified accordingly. This algorithm
models the 2-D subspace in the 3-D input space by defining the
appropriate parameter grid. In the second part of the paper the authors
discuss the extensions of Kohonen's algorithm, which were necessary to
handle multiple sensor input, addressing orientation discontinuities and
defining the reconstruction resolution according to surface properties.
The result is a method which can perform satisfactorily on sparse as
well as on dense input data. Because the surface is described in a
parameterized form viewpoint independence is inherent. In the last part
experiments with real and simulated data are presented, in which the
ROTEX telerobotic station served as background scenario},
keywords={image reconstruction;robot vision;self-organising feature
maps;sensor fusion;2-D subspace;3-D input space;Kohonen's
self-organizing feature map;ROTEX telerobotic station;multisensory
surface reconstruction;orientation discontinuities;reconstruction
resolution;sampled surface points;surface properties;unknown
three-dimensional surface;unordered cloud;viewpoint
independence;Aerodynamics;Computer vision;Electronic
mail;Layout;Robots;Sensor phenomena and characterization;Sensor
systems;Shape;Surface fitting;Surface reconstruction},
doi={10.1109/IROS.1994.407406},
month={Sep},}
@INPROCEEDINGS{284492,
author={Chia-Wei Liao and G. Medioni},
booktitle={Proceedings of 1994 IEEE 2nd CAD-Based Vision Workshop},
title={Surface approximation of a cloud of 3D points},
year={1994},
pages={274-281},
abstract={Presents an implementation of deformable models to approximate
a 3-D surface given by a cloud of 3-D points. It is an extension of the
authors previous work on “B-snakes” (1990), (1992) which approximates
curves and surfaces using B-splines. The user (or the system itself)
provides an initial simple surface, such as a closed cylinder, which is
subject to internal forces (describing implicit continuity properties
such as smoothness) and external forces which attract it toward the data
points. The problem is cast in terms of energy minimization. The authors
solve this non-convex optimization problem by using the well known
Powell algorithm which guarantees convergence and does not require
gradient information. Tire variables are the positions of the control
points. The number of control points processed by Powell at one time is
controlled. This methodology leads to a reasonable complexity,
robustness, and good numerical stability},
keywords={computer vision;curve fitting;splines (mathematics);3D
points;B-splines;complexity;deformable models;energy
minimization;non-convex optimization;numerical
stability;robustness;surface approximation;Algorithm design and
analysis;Clouds;Convergence;Deformable models;Intelligent
robots;Intelligent systems;Magnetic resonance imaging;Process
control;Robust stability;Spline},
doi={10.1109/CADVIS.1994.284492},
month={Feb},}
@INPROCEEDINGS{855837,
author={D. D. Morris and T. Kanade},
booktitle={Proceedings IEEE Conference on Computer Vision and Pattern
Recognition. CVPR 2000 (Cat. No.PR00662)},
title={Image-consistent surface triangulation},
year={2000},
volume={1},
pages={332-338 vol.1},
abstract={Given a set of 3D points that we know lie on the surface of an
object, we can define many possible surfaces that pass through all of
these points. Even when we consider only surface triangulations, there
are still an exponential number of valid triangulations that all fit the
data. Each triangulation will produce a different faceted surface
connecting the points. Our goal is to overcome this ambiguity and find
the particular surface that is closest to the true object surface. We do
not know the true surface but instead we assume that we have a set of
images of the object. We propose selecting a triangulation based on its
consistency with this set of images of the object. We present an
algorithm that starts with an initial rough triangulation and refines
the triangulation until it obtains a surface that best accounts for the
images of the object. Our method is thus able to overcome the surface
ambiguity problem and at the same time capture sharp corners and handle
concave regions and occlusions. We show results for a few real objects},
keywords={image representation;surface fitting;faceted
surface;image-consistent;surface triangulation;triangulation;Clouds;Face
detection;Graphics;Mesh generation;Robots;Rough surfaces;Shape;Surface
roughness;Surface texture},
doi={10.1109/CVPR.2000.855837},
ISSN={1063-6919},
month={},}
@ARTICLE{338529,
author={K. T. Sutherland and W. B. Thompson},
journal={IEEE Transactions on Robotics and Automation},
title={Localizing in unstructured environments: dealing with the errors},
year={1994},
volume={10},
number={6},
pages={740-754},
abstract={A robot navigating in an unstructured outdoor environment must
determine its own location in spite of problems due to environmental
conditions, sensor limitations and map inaccuracies, exact measurements
are seldom known, and the combination of approximate measures can lead
to large errors in self-localization. The conventional approach to this
problem has been to deal with the errors either during processing or
after they occur. The authors maintain that it is possible to limit the
errors before they occur. The authors analyze how measurement errors
affect errors in localization and propose that a simple algorithm can be
used to exploit the geometric properties of landmarks in the environment
in order to decrease errors in localization. The authors' goal is to
choose landmarks that will provide the best localization regardless of
measurement error, determine the best areas in which to identify new
landmarks to be used for further localization and choose paths that will
provide the least chance of “straying”. The authors show the result of
implementing this concept in experiments run in simulation with USGS 30
m DEM data for a robot statically locating, following a path and
identifying new landmarks},
keywords={computerised navigation;measurement errors;mobile robots;path
planning;robot vision;environmental conditions;geometric
properties;landmarks;map inaccuracies;measurement
errors;self-localization;sensor limitations;unstructured outdoor
environment;Clouds;Computer science;Delay;Error analysis;Global
Positioning System;Ice;Measurement errors;Mobile robots;Navigation;Robot
sensing systems},
doi={10.1109/70.338529},
ISSN={1042-296X},
month={Dec},}
@INPROCEEDINGS{601418,
author={M. Botta and C. Barogio and A. Giorclana and B. Graziano},
booktitle={Proceedings of the IEEE/RSJ International Conference on
Intelligent Robots and Systems},
title={Learning Behavioral Knowledge In Robotic Domains},
year={1992},
volume={3},
pages={1816-1822},
abstract={Not Available},
keywords={Clouds;Encoding;Information analysis;Learning
systems;Logic;Robots},
doi={10.1109/IROS.1992.601418},
ISSN={1},
month={Jul},}
@INPROCEEDINGS{262428,
author={P. Dauchez and X. Delebarre and R. Jourdan},
booktitle={EEE International Workshop on Intelligent Robots and Systems,
Towards a New Frontier of Applications},
title={Force control of a two-arm robot: implementation on a
multiprocessor architecture},
year={1990},
pages={487-492 vol.1},
abstract={Presents some practical solutions chosen to implement the
force control of a two-arm robot on a multiprocessor architecture. The
force control considered is the symmetric hybrid control proposed by M.
Uchiyama and P. Dauchez (1987) for the manipulation of a single rigid
object firmly held by two arms. The authors recall the definitions of
the controlled vectors in this case. They present their experimental
setup which includes two six-axis PUMA arms, equipped with two six-axis
force sensors, and a multiprocessor controller based on a VME bus. The
main part of the paper deals with the implementation of the hybrid
control on the equipment and emphasizes: (i) the modifications of the
control scheme; and (ii) the programming solutions used for reducing the
computation time. Some preliminary experimental results and directions
for future work are included},
keywords={computerised control;force control;microcomputer
applications;robots;PUMA arms;VME bus;computerised control;force
control;force sensors;multiprocessor architecture;multiprocessor
controller;symmetric hybrid control;two-arm robot;Arm;Clouds;Error
correction;Force control;Force sensors;Manipulators;Master-slave;Robot
programming;Robotic assembly;Stability},
doi={10.1109/IROS.1990.262428},
month={Jul},}
@ARTICLE{7583767,
author={K. Goldberg},
journal={IEEE Transactions on Automation Science and Engineering},
title={Editorial: #x201C;One Robot is Robotics, Ten Robots is Automation
#x201D;},
year={2016},
volume={13},
number={4},
pages={1418-1419},
abstract={Automation has come of age. As Raja Chatila aptly summarized
in the quote above, automation addresses the challenges that arise when
robots scale beyond proof-of-concept. Germany approved a $5B sale of
Kuka to Midea Corp in China to provide robots for assembly automation.
General Electric Corporation is now focusing on automation algorithms
and data analytics and predicts this will be a $225 billion market
within five years. These developments build on ongoing research in Big
Data, Cloud Computing, Deep Learning, Open-Source Software, and
Government/Industry initiatives, such as The “Internet of Things,”
“Smarter Planet,” “Industrial Internet,” “Industrie 4.0,” and “Made in
China 2025.” Automation is playing an increasingly central role in the
global economy and in our daily lives.},
doi={10.1109/TASE.2016.2606859},
ISSN={1545-5955},
month={Oct},}
@ARTICLE{6201209,
author={S. l. Kim and D. O. Wu and K. Schilling},
journal={IEEE Network},
title={Machine and robotic networking [Guest Editorial]},
year={2012},
volume={26},
number={3},
pages={4-5},
abstract={The advances in communication networking technologies have
been successful in academia, industry, and consumer markets. "Being
connected" becomes an essential part of our daily life. Regarding
wireless communications, we have reached a new turning point, where Long
Term Evolution (LTE) started commercial service in many countries and
smart phones are rapidly spreading out in the market. We believe there
will be four essential components that support future wireless systems
and applications: user experience, user interface, connectivity, and
clouds. The user experience goes far beyond our forecast. Users
continuously need not just human-to-human (H2H) information exchange,
but also machine-to-human (M2H), device-to-device (D2D), and
machine-to-machine (M2M) communications. The spatial and temporal
connectivity becomes a key engineering task, where one single system
cannot achieve both connectivity and high capacity. Instead,
heterogeneous wireless systems coexist to provide spectrally efficient
and economical services. With constraints on memory and computing power
of mobile terminals, the cloud servers are now core parts of networking
in academia and industry.},
keywords={Networked control systems;Robot kinematics;Robot sensing
systems;Special issues and sections;Wireless communication},
doi={10.1109/MNET.2012.6201209},
ISSN={0890-8044},
month={May},}
@INPROCEEDINGS{7583061,
booktitle={2016 World Automation Congress (WAC)},
title={[Title page]},
year={2016},
pages={1-1},
abstract={The following topics are dealt with: medical and human assist
robots; cloud computing; systems and control; autonomous systems;
manufacturing systems; medical imaging; and fuzzy logic.},
keywords={biomedical imaging;cloud computing;image
processing;manufacturing systems;medical computing;robots;autonomous
systems;cloud computing;control systems;fuzzy logic;human assist
robots;manufacturing systems;medical imaging;medical robotics},
doi={10.1109/WAC.2016.7583061},
month={July},}
@INPROCEEDINGS{7519367,
booktitle={2016 IEEE Region 10 Symposium (TENSYMP)},
title={Table of contents},
year={2016},
pages={1-3},
abstract={The following topics are dealt with: social opportunistic
networks; local area network; VANET; LTE TDD system; wireless sensor
networks; image processing; virtual reality; IoT-cloud infrastructure;
home automation; S-band vivaldi-based antennas; millimeter-wave 5G
mobile devices; iris recognition; three-level ZVZCS converter; surgical
robots; DVB-T2; indoor localization system; assembly language
translation; and AC power systems.},
keywords={5G mobile communication;Internet of Things;Long Term
Evolution;UHF antennas;cloud computing;digital video broadcasting;home
automation;image processing;indoor navigation;iris recognition;language
translation;local area networks;medical robotics;microwave
antennas;millimetre wave devices;power systems;social networking
(online);surgery;time division multiplexing;vehicular ad hoc
networks;virtual reality;wireless sensor networks;zero current
switching;zero voltage switching;AC power systems;DVB-T2;IoT-cloud
infrastructure;LTE TDD system;S-band Vivaldi-based
antennas;VANET;assembly language translation;home automation;image
processing;indoor localization system;iris recognition;local area
network;millimeter-wave 5G mobile devices;social opportunistic
networks;surgical robots;three-level ZVZCS converter;virtual
reality;wireless sensor networks},
doi={10.1109/TENCONSpring.2016.7519367},
month={May},}
@INPROCEEDINGS{7152194,
booktitle={2015 6th International Conference on Modeling, Simulation,
and Applied Optimization (ICMSAO)},
title={Table of contents},
year={2015},
pages={1-14},
abstract={The following topics are dealt with: wind farm; intrusion
detection systems; wireless sensor networks; XML information retrieval;
force control; hydraulic actuator; support vector machines; dynamic
vehicle routing problem; rehabilitation robot; Web-based programming
model; Kalman filters; and 3D radar.},
keywords={Internet;Kalman filters;XML;force control;hydraulic
actuators;information retrieval;medical robotics;programming;radar
applications;security of data;support vector machines;vehicle
routing;wind power plants;wireless sensor networks;3D radar;Kalman
filters;Web-based programming model;XML information retrieval;cloud
computing infrastructures;dynamic vehicle routing problem;force
control;hydraulic actuator;intrusion detection systems;rehabilitation
robot;support vector machines;wind farm;wireless sensor networks},
doi={10.1109/ICMSAO.2015.7152194},
month={May},}
@INPROCEEDINGS{6913248,
booktitle={2014 IIAI 3rd International Conference on Advanced Applied
Informatics},
title={Table of contents},
year={2014},
pages={v-xix},
abstract={The following topics are dealt with: data mining; Japanese
WordNet synonym misplacement detection; social network; recommender
system; sentiment analysis; workshop-based instruction; Japanese public
libraries; machine learning methods; collaborative Web presentation
support system; SMS4 ultracompact hardware implementation; wireless
sensor networks; personalized public transportation recommendation
system; adaptive user interface; NIS-Apriori algorithm; GetRNIA software
tool; rough set-based rule generation; tree-Ga bump hunting; neural
network model; weighted citation network analysis; sound proofing
ventilation unit; touch interaction; mutually dependent Markov decision
processes; ozone treatment; dynamic query optimization; big data;
learner activity recognition; IoT-security approach; nutrition-based
vegetable production; farm product cultivation; polynomial time mat
learning; C-deterministic regular formal graph system; article abstract
key expression extraction; English text comprehension; online social
games; knowledge creation; knowledge utilization; online stock trading;
customer behavior analysis; project-based collaborative learning;
in-field mobile game-based learning activities; e-portfolio system
design; self-regulated learning ontological model; mobile augmented
reality based scaffolding platform; context-aware mobile Japanese
conversation learning system; English writing error classification;
image processing; outside-class learning; exercise-centric teaching
materials; UML modeling; online historical document reading literacy;
MMORPG-based learning environment; computer courses; undergraduate
education; energy management system; higher education; decentralised
auction-based bandwidth allocation; wireless networked control systems;
resource scheduling algorithm; embedded cloud computing; Poisson
distribution; Japanese seismic activity; suspect vehicle detection; 3D
network traffic visualization; Web information retrieval; agent based
disaster evacua- ion assist system; electroencephalogram; random number
generator; multiagent simulations; multicore environment; CPU scheduler;
multithreaded processes; reserve-price biddings; real-time traffic
signal control; evolutionary computation; robot-assisted rehabilitation
system; hybrid automata; Batik motif classification; color-texture-based
feature extraction; backpropagation; multimedia storytelling; e-tourism
service; Web mining; search engine; simulation-based e-learning mobile
application software; library classification training system; WebQuest
learning strategy; context-aware ubiquitous English learning; support
vector machine; RFID tag ownership transfer protocol; cognitive
linguistics; collaborative software engineering learning; write-access
reduction method; NVM-DRAM hybrid memory; garbage collection; parallel
indexing scheme; lazy-updating snoop cache protocol; distributed storage
system; ITS application; software engineering education; ophthalmic
multimodal imaging system; injected bug classification; secure live
virtual machine migration; flash memory management; genetic programming;
heterogeneous databases; time series similarity search; concurrency
control program generation; incremental data migration; multidatabase
system; software release time decision making; analytic hierarchy
process; interactive genetic algorithm; biometric intelligence; talking
robots; archaeological ruin analysis; GIS; optical wireless
pedestrian-support systems; visual impairment; extreme programming;
Japanese e-commerce Web sites; Chinese sign language animation;
hearing-impaired people mammography inspection; geographical maps;
electroculogram; XML element retrieval technique; image recognition;
reinforcement learning; ECU formal verification; gasoline direct
injection engines; earthquake disaster simulation; smart devices for
autistic children; RoboCup rescue simulation; inductive logic
programming; master-slave asynchronous evolutionary hybrid algorithm;
VANET routing opt},
keywords={Big Data;DRAM chips;Internet of Things;Markov
processes;Poisson distribution;Unified Modeling
Language;XML;agriculture;analytic hierarchy
process;archaeology;augmented reality;automata
theory;backpropagation;bandwidth allocation;biometrics (access
control);cache storage;citation analysis;cloud computing;computational
complexity;computer animation;computer games;computer science
education;concurrency control;consumer behaviour;data mining;data
visualisation;distributed databases;educational
courses;electro-oculography;electroencephalography;electronic
commerce;emergency management;energy management systems;engines;feature
extraction;flash memories;formal verification;further education;genetic
algorithms;geographic information systems;groupware;handicapped
aids;human computer interaction;humanoid robots;image
classification;image colour analysis;image texture;inductive logic
programming;intelligent tutoring systems;investment;library
automation;linguistics;mammography;medical robotics;mobile
computing;multi-agent systems;multi-threading;multimedia
computing;multiprocessing systems;natural language processing;networked
control systems;neural nets;object detection;ozonation (materials
processing);patient rehabilitation;pedestrians;processor
scheduling;public transport;query processing;random number
generation;recommender systems;rescue robots;resource allocation;rough
set theory;search engines;security of data;seismology;social networking
(online);software prototyping;software tools;stock markets;storage
management;support vector machines;teaching;telecommunication network
routing;text analysis;time series;traffic control;traffic engineering
computing;travel industry;trees (mathematics);unsupervised learning;user
interfaces;vehicular ad hoc networks;ventilation;virtual
machines;wireless sensor networks;3D network traffic visualization;Batik
motif classification;C-deterministic regular formal graph system;CPU
scheduler;Chinese sign language animation;ECU formal
verification;English text comprehension;English writing error
classification;GIS;GetRNIA software tool;ITS application;IoT-security
approach;Japanese WordNet synonym misplacement detection;Japanese
e-commerce Web sites;Japanese public libraries;Japanese seismic
activity;MMORPG-based learning environment;NIS-Apriori
algorithm;NVM-DRAM hybrid memory;Poisson distribution;RFID tag ownership
transfer protocol;RoboCup rescue simulation;SMS4 ultracompact hardware
implementation;UML modeling;VANET routing optimization;Web image sharing
services;Web information retrieval;Web mining;WebQuest learning
strategy;XML element retrieval technique;adaptive user interface;agent
based disaster evacuation assist system;analytic hierarchy
process;archaeological ruin analysis;article abstract key expression
extraction;autistic children;backpropagation;big data;biometric
intelligence;cognitive linguistics;collaborative Web presentation
support system;collaborative software engineering
learning;color-texture-based feature extraction;computer
courses;concurrency control program generation;context-aware mobile
Japanese conversation learning system;context-aware ubiquitous English
learning;customer behavior analysis;data mining;decentralised
auction-based bandwidth allocation;distributed storage system;dynamic
query optimization;e-portfolio system design;e-tourism
service;earthquake disaster
simulation;electroencephalogram;electrooculogram;embedded cloud
computing;energy management system;evolutionary
computation;exercise-centric teaching materials;extreme programming;farm
product cultivation;flash memory management;garbage collection;gasoline
direct injection engines;genetic programming;geographical
maps;hearing-impaired people mammography inspection;heterogeneous
databases;higher education;hybrid automata;image processing;image
recognition;in-field mobile game-based learning activities;incremental
data migration;inductive logic programming;injected bug
classification;interactive genetic algorithm;knowledge
creation;knowledge utilization;lazy-updating snoop cache
protocol;learner activity recognition;library classification training
system;machine learning methods;master-slave asynchronous evolutionary
hybrid algorithm;mobile augmented reality based scaffolding
platform;multiagent simulations;multicore environment;multidatabase
system;multimedia storytelling;multithreaded processes;mutually
dependent Markov decision processes;neural network model;nutrition-based
vegetable production;online historical document reading literacy;online
social games;online stock trading;ophthalmic multimodal imaging
system;optical wireless pedestrian-support systems;outside-class
learning;ozone treatment;parallel indexing scheme;personalized public
transportation recommendation system;polynomial time mat
learning;project-based collaborative learning;random number
generator;real-time traffic signal control;recommender
system;reinforcement learning;reserve-price biddings;resource scheduling
algorithm;robot-assisted rehabilitation system;rough set-based rule
generation;search engine;secure live virtual machine
migration;self-regulated learning ontological model;sentiment
analysis;simulation-based e-learning mobile application software;social
network;software engineering education;software release time decision
making;sound proofing ventilation unit;support vector machine;suspect
vehicle detection;talking robots;time series similarity search;touch
interaction;tree-Ga bump hunting;undergraduate education;visual
impairment;weighted citation network analysis;wireless networked control
systems;wireless sensor networks;workshop-based instruction;write-access
reduction method},
doi={10.1109/IIAI-AAI.2014.4},
month={Aug},}
@ARTICLE{6353470,
author={G. Lawton},
journal={IEEE Intelligent Systems},
title={In the News},
year={2012},
volume={27},
number={5},
pages={5-9},
abstract={Researchers are working on a technique that enables a humanoid
robot to learn basic language skills by communicating directly with
people. Google and Stanford University scientists have developed a
large, cloud-based, deep-learning neural network suitable for complex
tasks such as object recognition and machine translation. Scientists
have developed an approach designed to let robots work with in concert
with humans in factories and other settings.},
keywords={Caroline Lyon;DeeChee;Google;ITALK;Julie Shah;MIT;RobotCub
Consortium;Stanford University;University of Hertfordshire;cloud
computing;corpus linguistics;deep learning;iCub;language;machine
learning;machine vision;networking;neural network;object
recognition;robot;robotics;robots;speech;training},
doi={10.1109/MIS.2012.99},
ISSN={1541-1672},
month={Sept},}
@ARTICLE{5286166,
journal={IEEE Intelligent Systems},
title={In the News},
year={2009},
volume={24},
number={5},
pages={5-8},
abstract={"AI and the Mobile Cloud," explores the growing use of cloud
computing to deliver AI-empowered applications to mobile devices. Over
time, mobile devices could become the principal means by which users
interact with cloud-based, autonomously operating agents. "A Fish Called
Filose" describes efforts of researchers at four European universities
to design a fishlike marine robot that can navigate shallow waters or
tricky currents. "Desktop Data Organization Grows Up" describes the
emerging semantic desktop, a personal version of the Semantic Web for
organizing an individual's thousands of files, e-mail messages, and
other digital data in an graph of related items.},
keywords={Artificial intelligence;Cloud computing;Educational
institutions;Electronic mail;Marine animals;Mobile
computing;Navigation;Organizing;Robots;Semantic Web;Filose;In the
News;Nepomuk;Semantic Web;cloud computing;mobile AI;mobile cloud;mobile
computing;robotics;semantic desktop},
doi={10.1109/MIS.2009.99},
ISSN={1541-1672},
month={Sept},}
